WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
start programstart program
start program
start program

wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: WARNING Unable to verify login in offline mode.
wandb: WARNING Unable to verify login in offline mode.
wandb: WARNING Unable to verify login in offline mode.
wandb: WARNING Unable to verify login in offline mode.
wandb: Tracking run with wandb version 0.19.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Tracking run with wandb version 0.19.8
wandb: Tracking run with wandb version 0.19.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Tracking run with wandb version 0.19.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
{
    "model": {
        "name": "dino2 + cross_attention",
        "dino-model": "dinov2_vits14",
        "layer1": 11,
        "facet1": "query",
        "facet2": "query",
        "num-heads": 1,
        "dropout-rate": 0.1,
        "target-shp-row": 504,
        "target-shp-col": 504,
        "num-blocks": 1,
        "freeze-dino": true,
        "unfreeze-dino-last-n-layer": 0
    },
    "optimizer": {
        "epochs": 100,
        "early-stop": 5,
        "warmup-epoch": 10,
        "learn-rate": 0.0001,
        "loss-weight": true,
        "lr-scheduler": "cosine",
        "grad-scaler": true
    },
    "dataset": {
        "batch-size": 2,
        "num-workers": 4,
        "dataset": "Combined",
        "hflip-prob": 0.5,
        "diff-augment": false
    },
    "evaluation": {
        "trainset": 0
    },
    "wandb": {
        "project": "",
        "name": "",
        "output-path": "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/output/2025-05-09.18-48-33",
        "save-checkpoint-freq": 10
    },
    "environment": {
        "dry": true,
        "seed": 123,
        "verbose": true,
        "device": "cuda"
    }
}
{
    "model": {
        "name": "dino2 + cross_attention",
        "dino-model": "dinov2_vits14",
        "layer1": 11,
        "facet1": "query",
        "facet2": "query",
        "num-heads": 1,
        "dropout-rate": 0.1,
        "target-shp-row": 504,
        "target-shp-col": 504,
        "num-blocks": 1,
        "freeze-dino": true,
        "unfreeze-dino-last-n-layer": 0
    },
    "optimizer": {
        "epochs": 100,
        "early-stop": 5,
        "warmup-epoch": 10,
        "learn-rate": 0.0001,
        "loss-weight": true,
        "lr-scheduler": "cosine",
        "grad-scaler": true
    },
    "dataset": {
        "batch-size": 2,
        "num-workers": 4,
        "dataset": "Combined",
        "hflip-prob": 0.5,
        "diff-augment": false
    },
    "evaluation": {
        "trainset": 0
    },
    "wandb": {
        "project": "",
        "name": "",
        "output-path": "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/output/2025-05-09.18-48-33",
        "save-checkpoint-freq": 10
    },
    "environment": {
        "dry": true,
        "seed": 123,
        "verbose": true,
        "device": "cuda"
    }
}{
    "model": {
        "name": "dino2 + cross_attention",
        "dino-model": "dinov2_vits14",
        "layer1": 11,
        "facet1": "query",
        "facet2": "query",
        "num-heads": 1,
        "dropout-rate": 0.1,
        "target-shp-row": 504,
        "target-shp-col": 504,
        "num-blocks": 1,
        "freeze-dino": true,
        "unfreeze-dino-last-n-layer": 0
    },
    "optimizer": {
        "epochs": 100,
        "early-stop": 5,
        "warmup-epoch": 10,
        "learn-rate": 0.0001,
        "loss-weight": true,
        "lr-scheduler": "cosine",
        "grad-scaler": true
    },
    "dataset": {
        "batch-size": 2,
        "num-workers": 4,
        "dataset": "Combined",
        "hflip-prob": 0.5,
        "diff-augment": false
    },
    "evaluation": {
        "trainset": 0
    },
    "wandb": {
        "project": "",
        "name": "",
        "output-path": "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/output/2025-05-09.18-48-33",
        "save-checkpoint-freq": 10
    },
    "environment": {
        "dry": true,
        "seed": 123,
        "verbose": true,
        "device": "cuda"
    }
}{
    "model": {
        "name": "dino2 + cross_attention",
        "dino-model": "dinov2_vits14",
        "layer1": 11,
        "facet1": "query",
        "facet2": "query",
        "num-heads": 1,
        "dropout-rate": 0.1,
        "target-shp-row": 504,
        "target-shp-col": 504,
        "num-blocks": 1,
        "freeze-dino": true,
        "unfreeze-dino-last-n-layer": 0
    },
    "optimizer": {
        "epochs": 100,
        "early-stop": 5,
        "warmup-epoch": 10,
        "learn-rate": 0.0001,
        "loss-weight": true,
        "lr-scheduler": "cosine",
        "grad-scaler": true
    },
    "dataset": {
        "batch-size": 2,
        "num-workers": 4,
        "dataset": "Combined",
        "hflip-prob": 0.5,
        "diff-augment": false
    },
    "evaluation": {
        "trainset": 0
    },
    "wandb": {
        "project": "",
        "name": "",
        "output-path": "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/output/2025-05-09.18-48-33",
        "save-checkpoint-freq": 10
    },
    "environment": {
        "dry": true,
        "seed": 123,
        "verbose": true,
        "device": "cuda"
    }
}


Using cache found in /home/ds5725/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /home/ds5725/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /home/ds5725/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /home/ds5725/.cache/torch/hub/facebookresearch_dinov2_main
A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
LORA added to the encoderLORA added to the encoder

LORA added to the encoder
LORA added to the encoder
final text_encoder_type: bert-base-uncasedfinal text_encoder_type: bert-base-uncased

final text_encoder_type: bert-base-uncased
final text_encoder_type: bert-base-uncased
Unfreeze fusion_layers
Unfreeze fusion_layers
Unfreeze fusion_layers
Unfreeze fusion_layers
get combined datasetget combined dataset

get combined dataset
get combined dataset
initialize cmu datasetinitialize cmu datasetinitialize cmu dataset
initialize cmu dataset


initialize our train datasetinitialize our train datasetinitialize our train datasetinitialize our train dataset



Using DistributedSampler for trainUsing DistributedSampler for train

Using DistributedSampler for train
Using DistributedSampler for train
initialize cmu dataset
initialize cmu datasetinitialize cmu dataset

initialize cmu dataset
initialize our train datasetinitialize our train dataset

initialize our train dataset
initialize our train dataset
Using DistributedSampler for valUsing DistributedSampler for valUsing DistributedSampler for valUsing DistributedSampler for val



initialize cmu datasetinitialize cmu dataset

initialize cmu dataset
initialize cmu dataset
initialize our test datasetinitialize our test datasetinitialize our test datasetinitialize our test dataset



Using DistributedSampler for test
Using DistributedSampler for testUsing DistributedSampler for testUsing DistributedSampler for test


epochs: 100
     ===== running 0 epoch =====
epochs: 100     2025-05-09.18-48-54

    epochs: 100      
===== running 0 epoch =====
    
     *** train one epoch ***     
 2025-05-09.18-48-54===== running 0 epoch =====

          
2025-05-09.18-48-54    
     *** train one epoch *** 

     *** train one epoch ***
epochs: 100
     ===== running 0 epoch =====
     2025-05-09.18-48-54
     
     *** train one epoch ***
after set grad
after set gradafter set grad

after prog
start loop
after progafter prog

start loopstart loop

after set grad
after prog
start loop
     input_1:  torch.Size([2, 3, 504, 504])
     input_2:  torch.Size([2, 3, 504, 504])
     targets:  torch.Size([2, 504, 504])
     caption:  ['none.', 'truck. traffic cones.']
     input_1:  torch.Size([2, 3, 504, 504])
     input_2:  torch.Size([2, 3, 504, 504])
     targets:  torch.Size([2, 504, 504])
     caption:  [' bare tree.', 'smoke or haze in the top left area.']
     input_1:  torch.Size([2, 3, 504, 504])
     input_2:  torch.Size([2, 3, 504, 504])
     targets:  torch.Size([2, 504, 504])
     caption:  ['white car. bicyclist.', 'none observed.']
     input_1:  torch.Size([2, 3, 504, 504])
     input_2:  torch.Size([2, 3, 504, 504])
     targets:  torch.Size([2, 504, 504])
     caption:  ['outdoor seating. tables. ', 'truck with takanashi logo.']
                   output:  output: output:   output:  torch.Size([2, 504, 504, 2])torch.Size([2, 504, 504, 2]) torch.Size([2, 504, 504, 2])  torch.Size([2, 504, 504, 2]) ->-> ->  -> torch.Size([2, 2, 504, 504])torch.Size([2, 2, 504, 504]) torch.Size([2, 2, 504, 504])

torch.Size([2, 2, 504, 504])

     Train Epoch: [0/2467]	Loss: 0.686943
     Train Epoch: [0/2467]	Loss: 0.689440
     Train Epoch: [0/2467]	Loss: 0.677381
     Train Epoch: [0/2467]	Loss: 0.692742
Traceback (most recent call last):
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 536, in <module>
    main(args)
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 334, in main
    loss, data_inds, batch_losses, train_time = train_one_epoch(
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 128, in train_one_epoch
    outputs = model(input_1, input_2, caption)   # (n, 504, 504, 2)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1026, in forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 0: 108 109 110 111 112 113 114 115 116 117 118 119
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
Traceback (most recent call last):
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 536, in <module>
    main(args)
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 334, in main
    loss, data_inds, batch_losses, train_time = train_one_epoch(
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 128, in train_one_epoch
    outputs = model(input_1, input_2, caption)   # (n, 504, 504, 2)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1026, in forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 1: 108 109 110 111 112 113 114 115 116 117 118 119
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
Traceback (most recent call last):
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 536, in <module>
    main(args)
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 334, in main
    loss, data_inds, batch_losses, train_time = train_one_epoch(
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 128, in train_one_epoch
    outputs = model(input_1, input_2, caption)   # (n, 504, 504, 2)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1026, in forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 2: 108 109 110 111 112 113 114 115 116 117 118 119
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/output/2025-05-09.18-48-33/wandb/offline-run-20250509_144834-i90v983f[0m
[1;34mwandb[0m: Find logs at: [1;35moutput/2025-05-09.18-48-33/wandb/offline-run-20250509_144834-i90v983f/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/output/2025-05-09.18-48-33/wandb/offline-run-20250509_144834-cbyhp7vg[0m
[1;34mwandb[0m: Find logs at: [1;35moutput/2025-05-09.18-48-33/wandb/offline-run-20250509_144834-cbyhp7vg/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/output/2025-05-09.18-48-33/wandb/offline-run-20250509_144834-ix57ewar[0m
[1;34mwandb[0m: Find logs at: [1;35moutput/2025-05-09.18-48-33/wandb/offline-run-20250509_144834-ix57ewar/logs[0m
Traceback (most recent call last):
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 536, in <module>
    main(args)
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 334, in main
    loss, data_inds, batch_losses, train_time = train_one_epoch(
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 128, in train_one_epoch
    outputs = model(input_1, input_2, caption)   # (n, 504, 504, 2)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1026, in forward
    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 3: 108 109 110 111 112 113 114 115 116 117 118 119
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/output/2025-05-09.18-48-33/wandb/offline-run-20250509_144834-9tqxkzpl[0m
[1;34mwandb[0m: Find logs at: [1;35moutput/2025-05-09.18-48-33/wandb/offline-run-20250509_144834-9tqxkzpl/logs[0m
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3319278) of binary: /ext3/miniconda3/envs/rscd/bin/python
Traceback (most recent call last):
  File "/ext3/miniconda3/envs/rscd/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
src/scripts/train_para.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-05-09_14:49:06
  host      : gr012.hpc.nyu.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3319279)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-05-09_14:49:06
  host      : gr012.hpc.nyu.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3319280)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-05-09_14:49:06
  host      : gr012.hpc.nyu.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3319281)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-09_14:49:06
  host      : gr012.hpc.nyu.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3319278)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
