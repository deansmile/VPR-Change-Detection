WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
start programstart program

start programstart program

wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: WARNING Unable to verify login in offline mode.
wandb: WARNING Unable to verify login in offline mode.
wandb: WARNING Unable to verify login in offline mode.
wandb: WARNING Unable to verify login in offline mode.
wandb: Tracking run with wandb version 0.19.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Tracking run with wandb version 0.19.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Tracking run with wandb version 0.19.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Tracking run with wandb version 0.19.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
{
    "model": {
        "name": "dino2 + cross_attention",
        "dino-model": "dinov2_vits14",
        "layer1": 11,
        "facet1": "query",
        "facet2": "query",
        "num-heads": 1,
        "dropout-rate": 0.1,
        "target-shp-row": 504,
        "target-shp-col": 504,
        "num-blocks": 1,
        "freeze-dino": true,
        "unfreeze-dino-last-n-layer": 0
    },
    "optimizer": {
        "epochs": 100,
        "early-stop": 5,
        "warmup-epoch": 10,
        "learn-rate": 0.0001,
        "loss-weight": true,
        "lr-scheduler": "cosine",
        "grad-scaler": true
    },
    "dataset": {
        "batch-size": 2,
        "num-workers": 4,
        "dataset": "Combined",
        "hflip-prob": 0.5,
        "diff-augment": false
    },
    "evaluation": {
        "trainset": 0
    },
    "wandb": {
        "project": "",
        "name": "",
        "output-path": "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/output/2025-05-11.00-02-49",
        "save-checkpoint-freq": 10
    },
    "environment": {
        "dry": true,
        "seed": 123,
        "verbose": true,
        "device": "cuda"
    }
}
{
    "model": {
        "name": "dino2 + cross_attention",
        "dino-model": "dinov2_vits14",
        "layer1": 11,
        "facet1": "query",
        "facet2": "query",
        "num-heads": 1,
        "dropout-rate": 0.1,
        "target-shp-row": 504,
        "target-shp-col": 504,
        "num-blocks": 1,
        "freeze-dino": true,
        "unfreeze-dino-last-n-layer": 0
    },
    "optimizer": {
        "epochs": 100,
        "early-stop": 5,
        "warmup-epoch": 10,
        "learn-rate": 0.0001,
        "loss-weight": true,
        "lr-scheduler": "cosine",
        "grad-scaler": true
    },
    "dataset": {
        "batch-size": 2,
        "num-workers": 4,
        "dataset": "Combined",
        "hflip-prob": 0.5,
        "diff-augment": false
    },
    "evaluation": {
        "trainset": 0
    },
    "wandb": {
        "project": "",
        "name": "",
        "output-path": "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/output/2025-05-11.00-02-49",
        "save-checkpoint-freq": 10
    },
    "environment": {
        "dry": true,
        "seed": 123,
        "verbose": true,
        "device": "cuda"
    }
}{
    "model": {
        "name": "dino2 + cross_attention",
        "dino-model": "dinov2_vits14",
        "layer1": 11,
        "facet1": "query",
        "facet2": "query",
        "num-heads": 1,
        "dropout-rate": 0.1,
        "target-shp-row": 504,
        "target-shp-col": 504,
        "num-blocks": 1,
        "freeze-dino": true,
        "unfreeze-dino-last-n-layer": 0
    },
    "optimizer": {
        "epochs": 100,
        "early-stop": 5,
        "warmup-epoch": 10,
        "learn-rate": 0.0001,
        "loss-weight": true,
        "lr-scheduler": "cosine",
        "grad-scaler": true
    },
    "dataset": {
        "batch-size": 2,
        "num-workers": 4,
        "dataset": "Combined",
        "hflip-prob": 0.5,
        "diff-augment": false
    },
    "evaluation": {
        "trainset": 0
    },
    "wandb": {
        "project": "",
        "name": "",
        "output-path": "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/output/2025-05-11.00-02-49",
        "save-checkpoint-freq": 10
    },
    "environment": {
        "dry": true,
        "seed": 123,
        "verbose": true,
        "device": "cuda"
    }
}

{
    "model": {
        "name": "dino2 + cross_attention",
        "dino-model": "dinov2_vits14",
        "layer1": 11,
        "facet1": "query",
        "facet2": "query",
        "num-heads": 1,
        "dropout-rate": 0.1,
        "target-shp-row": 504,
        "target-shp-col": 504,
        "num-blocks": 1,
        "freeze-dino": true,
        "unfreeze-dino-last-n-layer": 0
    },
    "optimizer": {
        "epochs": 100,
        "early-stop": 5,
        "warmup-epoch": 10,
        "learn-rate": 0.0001,
        "loss-weight": true,
        "lr-scheduler": "cosine",
        "grad-scaler": true
    },
    "dataset": {
        "batch-size": 2,
        "num-workers": 4,
        "dataset": "Combined",
        "hflip-prob": 0.5,
        "diff-augment": false
    },
    "evaluation": {
        "trainset": 0
    },
    "wandb": {
        "project": "",
        "name": "",
        "output-path": "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/output/2025-05-11.00-02-49",
        "save-checkpoint-freq": 10
    },
    "environment": {
        "dry": true,
        "seed": 123,
        "verbose": true,
        "device": "cuda"
    }
}
Using cache found in /home/ds5725/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /home/ds5725/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /home/ds5725/.cache/torch/hub/facebookresearch_dinov2_main
Using cache found in /home/ds5725/.cache/torch/hub/facebookresearch_dinov2_main
A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
A matching Triton is not available, some optimizations will not be enabled.
Error caught was: No module named 'triton'
LORA added to the encoderLORA added to the encoderLORA added to the encoderLORA added to the encoder



final text_encoder_type: bert-base-uncased
final text_encoder_type: bert-base-uncasedfinal text_encoder_type: bert-base-uncased

final text_encoder_type: bert-base-uncased
Unfreeze fusion_layers
Unfreeze fusion_layers
Unfreeze fusion_layers
Unfreeze fusion_layers
get combined dataset
get combined datasetget combined dataset

get combined dataset
initialize cmu datasetinitialize cmu datasetinitialize cmu dataset


initialize cmu dataset
initialize our train datasetinitialize our train datasetinitialize our train datasetinitialize our train dataset



Using DistributedSampler for trainUsing DistributedSampler for trainUsing DistributedSampler for train


Using DistributedSampler for train
initialize cmu datasetinitialize cmu dataset

initialize cmu dataset
initialize cmu dataset
initialize our train datasetinitialize our train dataset

initialize our train dataset
initialize our train dataset
Using DistributedSampler for val
Using DistributedSampler for valUsing DistributedSampler for valUsing DistributedSampler for val


initialize cmu dataset
initialize cmu datasetinitialize cmu dataset
initialize cmu dataset

initialize our test datasetinitialize our test datasetinitialize our test datasetinitialize our test dataset



Using DistributedSampler for testUsing DistributedSampler for testUsing DistributedSampler for test


Using DistributedSampler for test
epochs: 200
     ===== running 0 epoch =====
     epochs: 2002025-05-11.00-03-07epochs: 200
epochs: 200    
 


                   ===== running 0 epoch =====*** train one epoch ***===== running 0 epoch =====
 

    ===== running 0 epoch =====     
 2025-05-11.00-03-07    2025-05-11.00-03-07
 
    2025-05-11.00-03-07     
     
 
         
 *** train one epoch ***    *** train one epoch ***
 
*** train one epoch ***
after set grad
after set grad
after set gradafter set grad

after progafter prog
start loop

start loop
after prog
after progstart loop

start loop
     input_1:  torch.Size([2, 3, 504, 504])
     input_2:  torch.Size([2, 3, 504, 504])
     targets:  torch.Size([2, 504, 504])
     caption:  ['none.', 'truck. traffic cones.']
     input_1:  torch.Size([2, 3, 504, 504])
     input_2:  torch.Size([2, 3, 504, 504])
     targets:  torch.Size([2, 504, 504])
     caption:  [' bare tree.', 'smoke or haze in the top left area.']
     input_1:  torch.Size([2, 3, 504, 504])
     input_2:  torch.Size([2, 3, 504, 504])
     targets:  torch.Size([2, 504, 504])
     caption:  ['outdoor seating. tables. ', 'truck with takanashi logo.']
     input_1:  torch.Size([2, 3, 504, 504])
     input_2:  torch.Size([2, 3, 504, 504])
     targets:  torch.Size([2, 504, 504])
     caption:  ['white car. bicyclist.', 'none observed.']
                 output:    output:  torch.Size([2, 504, 504, 2])output:   output:  torch.Size([2, 504, 504, 2])-> torch.Size([2, 504, 504, 2])  torch.Size([2, 504, 504, 2]) ->torch.Size([2, 2, 504, 504]) -> 
-> torch.Size([2, 2, 504, 504]) torch.Size([2, 2, 504, 504])
torch.Size([2, 2, 504, 504])

[W reducer.cpp:325] Warning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2, 384, 1, 1], strides() = [384, 1, 384, 384]
bucket_view.sizes() = [2, 384, 1, 1], strides() = [384, 1, 1, 1] (function operator())
[W reducer.cpp:325] Warning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2, 384, 1, 1], strides() = [384, 1, 384, 384]
bucket_view.sizes() = [2, 384, 1, 1], strides() = [384, 1, 1, 1] (function operator())
[W reducer.cpp:325] Warning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2, 384, 1, 1], strides() = [384, 1, 384, 384]
bucket_view.sizes() = [2, 384, 1, 1], strides() = [384, 1, 1, 1] (function operator())
[W reducer.cpp:325] Warning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2, 384, 1, 1], strides() = [384, 1, 384, 384]
bucket_view.sizes() = [2, 384, 1, 1], strides() = [384, 1, 1, 1] (function operator())
     Train Epoch: [0/2467]	Loss: 0.692742
     Train Epoch: [0/2467]	Loss: 0.677381
     Train Epoch: [0/2467]	Loss: 0.686943
     Train Epoch: [0/2467]	Loss: 0.689440
         Train Epoch: [20/2467]	Loss: 0.692556
     Train Epoch: [20/2467]	Loss: 0.687332
 Train Epoch: [20/2467]	Loss: 0.695466
     Train Epoch: [20/2467]	Loss: 0.695406
     Train Epoch: [40/2467]	Loss: 0.700921
     Train Epoch: [40/2467]	Loss: 0.686920
     Train Epoch: [40/2467]	Loss: 0.688716
     Train Epoch: [40/2467]	Loss: 0.694468
     Train Epoch: [60/2467]	Loss: 0.691824
          Train Epoch: [60/2467]	Loss: 0.676319Train Epoch: [60/2467]	Loss: 0.691676

     Train Epoch: [60/2467]	Loss: 0.693200
     Train Epoch: [80/2467]	Loss: 0.705661
     Train Epoch: [80/2467]	Loss: 0.693557
     Train Epoch: [80/2467]	Loss: 0.693917
     Train Epoch: [80/2467]	Loss: 0.686206
          Train Epoch: [100/2467]	Loss: 0.693448Train Epoch: [100/2467]	Loss: 0.685403
    
 Train Epoch: [100/2467]	Loss: 0.694504
     Train Epoch: [100/2467]	Loss: 0.694145
     Train Epoch: [120/2467]	Loss: 0.675934
     Train Epoch: [120/2467]	Loss: 0.685393
          Train Epoch: [120/2467]	Loss: 0.694854Train Epoch: [120/2467]	Loss: 0.693272

     Train Epoch: [140/2467]	Loss: 0.694496
         Train Epoch: [140/2467]	Loss: 0.670425
 Train Epoch: [140/2467]	Loss: 0.687903
     Train Epoch: [140/2467]	Loss: 0.682799
         Train Epoch: [160/2467]	Loss: 0.690427
 Train Epoch: [160/2467]	Loss: 0.677871
     Train Epoch: [160/2467]	Loss: 0.671485
     Train Epoch: [160/2467]	Loss: 0.693474
              Train Epoch: [180/2467]	Loss: 0.687501 
Train Epoch: [180/2467]	Loss: 0.681548Train Epoch: [180/2467]	Loss: 0.708909

     Train Epoch: [180/2467]	Loss: 0.692278
     Train Epoch: [200/2467]	Loss: 0.693521
     Train Epoch: [200/2467]	Loss: 0.684496
         Train Epoch: [200/2467]	Loss: 0.691626
 Train Epoch: [200/2467]	Loss: 0.689218
     Train Epoch: [220/2467]	Loss: 0.693433
     Train Epoch: [220/2467]	Loss: 0.689370
         Train Epoch: [220/2467]	Loss: 0.695451
 Train Epoch: [220/2467]	Loss: 0.677207
     Train Epoch: [240/2467]	Loss: 0.698322
         Train Epoch: [240/2467]	Loss: 0.690499
 Train Epoch: [240/2467]	Loss: 0.692103
     Train Epoch: [240/2467]	Loss: 0.686779
     Train Epoch: [260/2467]	Loss: 0.681040
         Train Epoch: [260/2467]	Loss: 0.662096
 Train Epoch: [260/2467]	Loss: 0.692714
     Train Epoch: [260/2467]	Loss: 0.691751
     Train Epoch: [280/2467]	Loss: 0.696453
     Train Epoch: [280/2467]	Loss: 0.689903
     Train Epoch: [280/2467]	Loss: 0.693288
     Train Epoch: [280/2467]	Loss: 0.685167
     Train Epoch: [300/2467]	Loss: 0.683928
     Train Epoch: [300/2467]	Loss: 0.643258
     Train Epoch: [300/2467]	Loss: 0.693938
     Train Epoch: [300/2467]	Loss: 0.692856
                 Train Epoch: [320/2467]	Loss: 0.697043
  Train Epoch: [320/2467]	Loss: 0.693002 
Train Epoch: [320/2467]	Loss: 0.693179Train Epoch: [320/2467]	Loss: 0.693349

     Train Epoch: [340/2467]	Loss: 0.699477
     Train Epoch: [340/2467]	Loss: 0.690352
     Train Epoch: [340/2467]	Loss: 0.690844
     Train Epoch: [340/2467]	Loss: 0.692745
         Train Epoch: [360/2467]	Loss: 0.689741 
Train Epoch: [360/2467]	Loss: 0.688136
     Train Epoch: [360/2467]	Loss: 0.678203
     Train Epoch: [360/2467]	Loss: 0.696891
     Train Epoch: [380/2467]	Loss: 0.694312
     Train Epoch: [380/2467]	Loss: 0.669161
     Train Epoch: [380/2467]	Loss: 0.661973
     Train Epoch: [380/2467]	Loss: 0.685592
     Train Epoch: [400/2467]	Loss: 0.693991
         Train Epoch: [400/2467]	Loss: 0.687354 
Train Epoch: [400/2467]	Loss: 0.691562
     Train Epoch: [400/2467]	Loss: 0.691675
     Train Epoch: [420/2467]	Loss: 0.695108
          Train Epoch: [420/2467]	Loss: 0.691767    Train Epoch: [420/2467]	Loss: 0.694338

 Train Epoch: [420/2467]	Loss: 0.696455
         Train Epoch: [440/2467]	Loss: 0.692406
         Train Epoch: [440/2467]	Loss: 0.662910
 Train Epoch: [440/2467]	Loss: 0.694673
 Train Epoch: [440/2467]	Loss: 0.691836
     Train Epoch: [460/2467]	Loss: 0.687983
         Train Epoch: [460/2467]	Loss: 0.695678
 Train Epoch: [460/2467]	Loss: 0.674676
     Train Epoch: [460/2467]	Loss: 0.654935
     Train Epoch: [480/2467]	Loss: 0.687356
             Train Epoch: [480/2467]	Loss: 0.693172
  Train Epoch: [480/2467]	Loss: 0.691271Train Epoch: [480/2467]	Loss: 0.691150

         Train Epoch: [500/2467]	Loss: 0.682938
 Train Epoch: [500/2467]	Loss: 0.702203
     Train Epoch: [500/2467]	Loss: 0.692783
     Train Epoch: [500/2467]	Loss: 0.677965
         Train Epoch: [520/2467]	Loss: 0.691350
     Train Epoch: [520/2467]	Loss: 0.686523
 Train Epoch: [520/2467]	Loss: 0.692218
     Train Epoch: [520/2467]	Loss: 0.687874
         Train Epoch: [540/2467]	Loss: 0.692977
 Train Epoch: [540/2467]	Loss: 0.682412
     Train Epoch: [540/2467]	Loss: 0.663840
     Train Epoch: [540/2467]	Loss: 0.691688
         Train Epoch: [560/2467]	Loss: 0.689728
 Train Epoch: [560/2467]	Loss: 0.681798
     Train Epoch: [560/2467]	Loss: 0.700966
     Train Epoch: [560/2467]	Loss: 0.691106
         Train Epoch: [580/2467]	Loss: 0.694928
 Train Epoch: [580/2467]	Loss: 0.684713
     Train Epoch: [580/2467]	Loss: 0.695265
     Train Epoch: [580/2467]	Loss: 0.689101
     Train Epoch: [600/2467]	Loss: 0.664201
     Train Epoch: [600/2467]	Loss: 0.688899
     Train Epoch: [600/2467]	Loss: 0.685827    
 Train Epoch: [600/2467]	Loss: 0.676760
         Train Epoch: [620/2467]	Loss: 0.718336
 Train Epoch: [620/2467]	Loss: 0.653256
     Train Epoch: [620/2467]	Loss: 0.685701
     Train Epoch: [620/2467]	Loss: 0.691684
     Train Epoch: [640/2467]	Loss: 0.659913    
     Train Epoch: [640/2467]	Loss: 0.688496
 Train Epoch: [640/2467]	Loss: 0.689572
     Train Epoch: [640/2467]	Loss: 0.662508
     Train Epoch: [660/2467]	Loss: 0.694915
         Train Epoch: [660/2467]	Loss: 0.691830
 Train Epoch: [660/2467]	Loss: 0.693173
     Train Epoch: [660/2467]	Loss: 0.691409
         Train Epoch: [680/2467]	Loss: 0.691720
 Train Epoch: [680/2467]	Loss: 0.686278
     Train Epoch: [680/2467]	Loss: 0.688486
     Train Epoch: [680/2467]	Loss: 0.710893
         Train Epoch: [700/2467]	Loss: 0.685157
     Train Epoch: [700/2467]	Loss: 0.679441
     Train Epoch: [700/2467]	Loss: 0.683241 
Train Epoch: [700/2467]	Loss: 0.690530
     Train Epoch: [720/2467]	Loss: 0.677939
         Train Epoch: [720/2467]	Loss: 0.685259
 Train Epoch: [720/2467]	Loss: 0.687786
     Train Epoch: [720/2467]	Loss: 0.703688
     Train Epoch: [740/2467]	Loss: 0.676253
         Train Epoch: [740/2467]	Loss: 0.688376
 Train Epoch: [740/2467]	Loss: 0.689342
     Train Epoch: [740/2467]	Loss: 0.682523
     Train Epoch: [760/2467]	Loss: 0.693448
     Train Epoch: [760/2467]	Loss: 0.693004
     Train Epoch: [760/2467]	Loss: 0.693087
     Train Epoch: [760/2467]	Loss: 0.692978
     Train Epoch: [780/2467]	Loss: 0.694723
     Train Epoch: [780/2467]	Loss: 0.696001
     Train Epoch: [780/2467]	Loss: 0.691922
     Train Epoch: [780/2467]	Loss: 0.693863
         Train Epoch: [800/2467]	Loss: 0.694933
 Train Epoch: [800/2467]	Loss: 0.687643
     Train Epoch: [800/2467]	Loss: 0.675267
     Train Epoch: [800/2467]	Loss: 0.694701
          Train Epoch: [820/2467]	Loss: 0.684854Train Epoch: [820/2467]	Loss: 0.689621

         Train Epoch: [820/2467]	Loss: 0.693379
 Train Epoch: [820/2467]	Loss: 0.661625
         Train Epoch: [840/2467]	Loss: 0.685710 
Train Epoch: [840/2467]	Loss: 0.693147
     Train Epoch: [840/2467]	Loss: 0.689253
     Train Epoch: [840/2467]	Loss: 0.679605
     Train Epoch: [860/2467]	Loss: 0.693597
     Train Epoch: [860/2467]	Loss: 0.696169
     Train Epoch: [860/2467]	Loss: 0.695536
     Train Epoch: [860/2467]	Loss: 0.692920
          Train Epoch: [880/2467]	Loss: 0.691725
Train Epoch: [880/2467]	Loss: 0.680783
     Train Epoch: [880/2467]	Loss: 0.679246
     Train Epoch: [880/2467]	Loss: 0.658619
     Train Epoch: [900/2467]	Loss: 0.693805
     Train Epoch: [900/2467]	Loss: 0.688945
     Train Epoch: [900/2467]	Loss: 0.695894
     Train Epoch: [900/2467]	Loss: 0.692163
               Train Epoch: [920/2467]	Loss: 0.685437Train Epoch: [920/2467]	Loss: 0.696892Train Epoch: [920/2467]	Loss: 0.693310


     Train Epoch: [920/2467]	Loss: 0.691760
         Train Epoch: [940/2467]	Loss: 0.689014    
 Train Epoch: [940/2467]	Loss: 0.679463
 Train Epoch: [940/2467]	Loss: 0.695447
     Train Epoch: [940/2467]	Loss: 0.690866
     Train Epoch: [960/2467]	Loss: 0.693466
     Train Epoch: [960/2467]	Loss: 0.676544
     Train Epoch: [960/2467]	Loss: 0.692878
     Train Epoch: [960/2467]	Loss: 0.695713
         Train Epoch: [980/2467]	Loss: 0.693362
     Train Epoch: [980/2467]	Loss: 0.704755
 Train Epoch: [980/2467]	Loss: 0.684562
     Train Epoch: [980/2467]	Loss: 0.697530
             Train Epoch: [1000/2467]	Loss: 0.695614 
 Train Epoch: [1000/2467]	Loss: 0.693519Train Epoch: [1000/2467]	Loss: 0.691418

     Train Epoch: [1000/2467]	Loss: 0.682489
          Train Epoch: [1020/2467]	Loss: 0.693153Train Epoch: [1020/2467]	Loss: 0.691308

     Train Epoch: [1020/2467]	Loss: 0.732048
     Train Epoch: [1020/2467]	Loss: 0.690539
     Train Epoch: [1040/2467]	Loss: 0.693323
     Train Epoch: [1040/2467]	Loss: 0.673966
     Train Epoch: [1040/2467]	Loss: 0.694519
     Train Epoch: [1040/2467]	Loss: 0.688295
         Train Epoch: [1060/2467]	Loss: 0.695600
 Train Epoch: [1060/2467]	Loss: 0.694255
     Train Epoch: [1060/2467]	Loss: 0.690852
     Train Epoch: [1060/2467]	Loss: 0.676403
     Train Epoch: [1080/2467]	Loss: 0.680408    
 Train Epoch: [1080/2467]	Loss: 0.690296    
 Train Epoch: [1080/2467]	Loss: 0.693633
     Train Epoch: [1080/2467]	Loss: 0.693727
     Train Epoch: [1100/2467]	Loss: 0.699123    
         Train Epoch: [1100/2467]	Loss: 0.691001
 Train Epoch: [1100/2467]	Loss: 0.691240 
Train Epoch: [1100/2467]	Loss: 0.668993
     Train Epoch: [1120/2467]	Loss: 0.691496
         Train Epoch: [1120/2467]	Loss: 0.692338 
Train Epoch: [1120/2467]	Loss: 0.684149
     Train Epoch: [1120/2467]	Loss: 0.693491
     Train Epoch: [1140/2467]	Loss: 0.695125
     Train Epoch: [1140/2467]	Loss: 0.688637
     Train Epoch: [1140/2467]	Loss: 0.689118
     Train Epoch: [1140/2467]	Loss: 0.693460
     Train Epoch: [1160/2467]	Loss: 0.692421
     Train Epoch: [1160/2467]	Loss: 0.668778
     Train Epoch: [1160/2467]	Loss: 0.687406
     Train Epoch: [1160/2467]	Loss: 0.679368
     Train Epoch: [1180/2467]	Loss: 0.693174
         Train Epoch: [1180/2467]	Loss: 0.700640 
Train Epoch: [1180/2467]	Loss: 0.679434
     Train Epoch: [1180/2467]	Loss: 0.684363
          Train Epoch: [1200/2467]	Loss: 0.693197
Train Epoch: [1200/2467]	Loss: 0.687369
     Train Epoch: [1200/2467]	Loss: 0.687010
     Train Epoch: [1200/2467]	Loss: 0.691648
         Train Epoch: [1220/2467]	Loss: 0.688713
 Train Epoch: [1220/2467]	Loss: 0.695053
     Train Epoch: [1220/2467]	Loss: 0.691655
     Train Epoch: [1220/2467]	Loss: 0.693579
     Train Epoch: [1240/2467]	Loss: 0.684196
         Train Epoch: [1240/2467]	Loss: 0.692706
 Train Epoch: [1240/2467]	Loss: 0.692628
     Train Epoch: [1240/2467]	Loss: 0.693464
     Train Epoch: [1260/2467]	Loss: 0.676780
     Train Epoch: [1260/2467]	Loss: 0.681656    
 Train Epoch: [1260/2467]	Loss: 0.693303
     Train Epoch: [1260/2467]	Loss: 0.679026
     Train Epoch: [1280/2467]	Loss: 0.694315
          Train Epoch: [1280/2467]	Loss: 0.689851Train Epoch: [1280/2467]	Loss: 0.688849

     Train Epoch: [1280/2467]	Loss: 0.716810
             Train Epoch: [1300/2467]	Loss: 0.670498
 Train Epoch: [1300/2467]	Loss: 0.694427 
Train Epoch: [1300/2467]	Loss: 0.693135
     Train Epoch: [1300/2467]	Loss: 0.669961
     Train Epoch: [1320/2467]	Loss: 0.693693
     Train Epoch: [1320/2467]	Loss: 0.686229
     Train Epoch: [1320/2467]	Loss: 0.688091
     Train Epoch: [1320/2467]	Loss: 0.684507
     Train Epoch: [1340/2467]	Loss: 0.699433
         Train Epoch: [1340/2467]	Loss: 0.694265
 Train Epoch: [1340/2467]	Loss: 0.692211
     Train Epoch: [1340/2467]	Loss: 0.673057
          Train Epoch: [1360/2467]	Loss: 0.672997Train Epoch: [1360/2467]	Loss: 0.680008

     Train Epoch: [1360/2467]	Loss: 0.692362
     Train Epoch: [1360/2467]	Loss: 0.693046
     Train Epoch: [1380/2467]	Loss: 0.690777
     Train Epoch: [1380/2467]	Loss: 0.693115
         Train Epoch: [1380/2467]	Loss: 0.694587
 Train Epoch: [1380/2467]	Loss: 0.693125
     Train Epoch: [1400/2467]	Loss: 0.680341
         Train Epoch: [1400/2467]	Loss: 0.686698
 Train Epoch: [1400/2467]	Loss: 0.691678
     Train Epoch: [1400/2467]	Loss: 0.681838
         Train Epoch: [1420/2467]	Loss: 0.693099
     Train Epoch: [1420/2467]	Loss: 0.692208
 Train Epoch: [1420/2467]	Loss: 0.693760
     Train Epoch: [1420/2467]	Loss: 0.691305
     Train Epoch: [1440/2467]	Loss: 0.667055
     Train Epoch: [1440/2467]	Loss: 0.682117
     Train Epoch: [1440/2467]	Loss: 0.693112
     Train Epoch: [1440/2467]	Loss: 0.693101
     Train Epoch: [1460/2467]	Loss: 0.692110
     Train Epoch: [1460/2467]	Loss: 0.682190
     Train Epoch: [1460/2467]	Loss: 0.692486
     Train Epoch: [1460/2467]	Loss: 0.692382
          Train Epoch: [1480/2467]	Loss: 0.693019
Train Epoch: [1480/2467]	Loss: 0.696813
         Train Epoch: [1480/2467]	Loss: 0.687545
 Train Epoch: [1480/2467]	Loss: 0.697343
         Train Epoch: [1500/2467]	Loss: 0.700384
 Train Epoch: [1500/2467]	Loss: 0.697241
     Train Epoch: [1500/2467]	Loss: 0.693337
     Train Epoch: [1500/2467]	Loss: 0.686388
     Train Epoch: [1520/2467]	Loss: 0.690770
     Train Epoch: [1520/2467]	Loss: 0.688862
     Train Epoch: [1520/2467]	Loss: 0.681446
     Train Epoch: [1520/2467]	Loss: 0.708744
          Train Epoch: [1540/2467]	Loss: 0.687700Train Epoch: [1540/2467]	Loss: 0.693922

     Train Epoch: [1540/2467]	Loss: 0.690549
     Train Epoch: [1540/2467]	Loss: 0.692403
     Train Epoch: [1560/2467]	Loss: 0.692483
     Train Epoch: [1560/2467]	Loss: 0.658666
     Train Epoch: [1560/2467]	Loss: 0.693840
     Train Epoch: [1560/2467]	Loss: 0.681773
              Train Epoch: [1580/2467]	Loss: 0.693121Train Epoch: [1580/2467]	Loss: 0.691468

     Train Epoch: [1580/2467]	Loss: 0.697828
 Train Epoch: [1580/2467]	Loss: 0.696404
     Train Epoch: [1600/2467]	Loss: 0.688422
     Train Epoch: [1600/2467]	Loss: 0.689144
     Train Epoch: [1600/2467]	Loss: 0.697670
     Train Epoch: [1600/2467]	Loss: 0.680823
     Train Epoch: [1620/2467]	Loss: 0.691118
         Train Epoch: [1620/2467]	Loss: 0.695559
 Train Epoch: [1620/2467]	Loss: 0.691427
     Train Epoch: [1620/2467]	Loss: 0.695779
         Train Epoch: [1640/2467]	Loss: 0.693529 
Train Epoch: [1640/2467]	Loss: 0.688993    
     Train Epoch: [1640/2467]	Loss: 0.691450
 Train Epoch: [1640/2467]	Loss: 0.694014
         Train Epoch: [1660/2467]	Loss: 0.684391 
Train Epoch: [1660/2467]	Loss: 0.693870
     Train Epoch: [1660/2467]	Loss: 0.693172
     Train Epoch: [1660/2467]	Loss: 0.696808
     Train Epoch: [1680/2467]	Loss: 0.694253
     Train Epoch: [1680/2467]	Loss: 0.693025
     Train Epoch: [1680/2467]	Loss: 0.674616
     Train Epoch: [1680/2467]	Loss: 0.673536
     Train Epoch: [1700/2467]	Loss: 0.694837
     Train Epoch: [1700/2467]	Loss: 0.695484
     Train Epoch: [1700/2467]	Loss: 0.681677
     Train Epoch: [1700/2467]	Loss: 0.696152
     Train Epoch: [1720/2467]	Loss: 0.683994    
 Train Epoch: [1720/2467]	Loss: 0.686331
     Train Epoch: [1720/2467]	Loss: 0.680358
     Train Epoch: [1720/2467]	Loss: 0.692608
         Train Epoch: [1740/2467]	Loss: 0.691105 
Train Epoch: [1740/2467]	Loss: 0.689633
     Train Epoch: [1740/2467]	Loss: 0.693304
     Train Epoch: [1740/2467]	Loss: 0.690770
     Train Epoch: [1760/2467]	Loss: 0.693224
     Train Epoch: [1760/2467]	Loss: 0.692256
     Train Epoch: [1760/2467]	Loss: 0.686372
     Train Epoch: [1760/2467]	Loss: 0.691227
             Train Epoch: [1780/2467]	Loss: 0.692697 
 Train Epoch: [1780/2467]	Loss: 0.693440
Train Epoch: [1780/2467]	Loss: 0.691064
     Train Epoch: [1780/2467]	Loss: 0.693811
         Train Epoch: [1800/2467]	Loss: 0.693104
 Train Epoch: [1800/2467]	Loss: 0.698919
     Train Epoch: [1800/2467]	Loss: 0.703941
     Train Epoch: [1800/2467]	Loss: 0.691804
         Train Epoch: [1820/2467]	Loss: 0.687747
 Train Epoch: [1820/2467]	Loss: 0.693373
     Train Epoch: [1820/2467]	Loss: 0.687054
     Train Epoch: [1820/2467]	Loss: 0.677891
         Train Epoch: [1840/2467]	Loss: 0.693018
     Train Epoch: [1840/2467]	Loss: 0.687460
     Train Epoch: [1840/2467]	Loss: 0.686127
 Train Epoch: [1840/2467]	Loss: 0.685715
          Train Epoch: [1860/2467]	Loss: 0.692883Train Epoch: [1860/2467]	Loss: 0.678160

     Train Epoch: [1860/2467]	Loss: 0.690813
     Train Epoch: [1860/2467]	Loss: 0.689288
     Train Epoch: [1880/2467]	Loss: 0.692839
     Train Epoch: [1880/2467]	Loss: 0.694938
     Train Epoch: [1880/2467]	Loss: 0.666036
     Train Epoch: [1880/2467]	Loss: 0.701991
     Train Epoch: [1900/2467]	Loss: 0.691911    
 Train Epoch: [1900/2467]	Loss: 0.693349
     Train Epoch: [1900/2467]	Loss: 0.680048
     Train Epoch: [1900/2467]	Loss: 0.692399
              Train Epoch: [1920/2467]	Loss: 0.693270Train Epoch: [1920/2467]	Loss: 0.655024    

 Train Epoch: [1920/2467]	Loss: 0.697724
 Train Epoch: [1920/2467]	Loss: 0.676673
              Train Epoch: [1940/2467]	Loss: 0.693385Train Epoch: [1940/2467]	Loss: 0.686602

 Train Epoch: [1940/2467]	Loss: 0.691577
     Train Epoch: [1940/2467]	Loss: 0.693194
             Train Epoch: [1960/2467]	Loss: 0.693717
  Train Epoch: [1960/2467]	Loss: 0.691111Train Epoch: [1960/2467]	Loss: 0.692085

     Train Epoch: [1960/2467]	Loss: 0.692572
             Train Epoch: [1980/2467]	Loss: 0.700333
  Train Epoch: [1980/2467]	Loss: 0.688894Train Epoch: [1980/2467]	Loss: 0.692694

     Train Epoch: [1980/2467]	Loss: 0.662759
     Train Epoch: [2000/2467]	Loss: 0.692168
     Train Epoch: [2000/2467]	Loss: 0.698657
     Train Epoch: [2000/2467]	Loss: 0.689588
     Train Epoch: [2000/2467]	Loss: 0.692854
     Train Epoch: [2020/2467]	Loss: 0.698037
     Train Epoch: [2020/2467]	Loss: 0.690774
         Train Epoch: [2020/2467]	Loss: 0.690306
 Train Epoch: [2020/2467]	Loss: 0.695231
     Train Epoch: [2040/2467]	Loss: 0.696419
             Train Epoch: [2040/2467]	Loss: 0.685711
 Train Epoch: [2040/2467]	Loss: 0.700516
 Train Epoch: [2040/2467]	Loss: 0.691111
         Train Epoch: [2060/2467]	Loss: 0.684047
 Train Epoch: [2060/2467]	Loss: 0.686148
         Train Epoch: [2060/2467]	Loss: 0.680103 
Train Epoch: [2060/2467]	Loss: 0.696707
         Train Epoch: [2080/2467]	Loss: 0.657088
 Train Epoch: [2080/2467]	Loss: 0.691257
     Train Epoch: [2080/2467]	Loss: 0.693705
     Train Epoch: [2080/2467]	Loss: 0.690728
     Train Epoch: [2100/2467]	Loss: 0.694636
     Train Epoch: [2100/2467]	Loss: 0.692943
     Train Epoch: [2100/2467]	Loss: 0.687320
     Train Epoch: [2100/2467]	Loss: 0.696415
     Train Epoch: [2120/2467]	Loss: 0.678158
     Train Epoch: [2120/2467]	Loss: 0.687985
     Train Epoch: [2120/2467]	Loss: 0.690064
     Train Epoch: [2120/2467]	Loss: 0.682606
          Train Epoch: [2140/2467]	Loss: 0.692626Train Epoch: [2140/2467]	Loss: 0.693254

     Train Epoch: [2140/2467]	Loss: 0.707235
     Train Epoch: [2140/2467]	Loss: 0.690988
     Train Epoch: [2160/2467]	Loss: 0.696004
         Train Epoch: [2160/2467]	Loss: 0.695648
 Train Epoch: [2160/2467]	Loss: 0.693490
     Train Epoch: [2160/2467]	Loss: 0.686138
         Train Epoch: [2180/2467]	Loss: 0.688847
 Train Epoch: [2180/2467]	Loss: 0.683962
     Train Epoch: [2180/2467]	Loss: 0.693185
     Train Epoch: [2180/2467]	Loss: 0.687165
     Train Epoch: [2200/2467]	Loss: 0.684697
     Train Epoch: [2200/2467]	Loss: 0.691613
     Train Epoch: [2200/2467]	Loss: 0.689981
     Train Epoch: [2200/2467]	Loss: 0.694562
     Train Epoch: [2220/2467]	Loss: 0.695754
              Train Epoch: [2220/2467]	Loss: 0.680345
Train Epoch: [2220/2467]	Loss: 0.682491
 Train Epoch: [2220/2467]	Loss: 0.674263
     Train Epoch: [2240/2467]	Loss: 0.691379
     Train Epoch: [2240/2467]	Loss: 0.677687
     Train Epoch: [2240/2467]	Loss: 0.693624
     Train Epoch: [2240/2467]	Loss: 0.689010
         Train Epoch: [2260/2467]	Loss: 0.688791 
Train Epoch: [2260/2467]	Loss: 0.679587
     Train Epoch: [2260/2467]	Loss: 0.691325
     Train Epoch: [2260/2467]	Loss: 0.691973
     Train Epoch: [2280/2467]	Loss: 0.692200
     Train Epoch: [2280/2467]	Loss: 0.679401
     Train Epoch: [2280/2467]	Loss: 0.704279
     Train Epoch: [2280/2467]	Loss: 0.677573
     Train Epoch: [2300/2467]	Loss: 0.686339
     Train Epoch: [2300/2467]	Loss: 0.685060
     Train Epoch: [2300/2467]	Loss: 0.694834
     Train Epoch: [2300/2467]	Loss: 0.686283
     Train Epoch: [2320/2467]	Loss: 0.694907
         Train Epoch: [2320/2467]	Loss: 0.693391
 Train Epoch: [2320/2467]	Loss: 0.693993
     Train Epoch: [2320/2467]	Loss: 0.690264
         Train Epoch: [2340/2467]	Loss: 0.678004
 Train Epoch: [2340/2467]	Loss: 0.693642    
 Train Epoch: [2340/2467]	Loss: 0.693279
     Train Epoch: [2340/2467]	Loss: 0.693220
     Train Epoch: [2360/2467]	Loss: 0.675940
     Train Epoch: [2360/2467]	Loss: 0.690753
     Train Epoch: [2360/2467]	Loss: 0.695326
     Train Epoch: [2360/2467]	Loss: 0.691755
          Train Epoch: [2380/2467]	Loss: 0.700891Train Epoch: [2380/2467]	Loss: 0.678943

         Train Epoch: [2380/2467]	Loss: 0.695116
 Train Epoch: [2380/2467]	Loss: 0.693703
              Train Epoch: [2400/2467]	Loss: 0.681328 
Train Epoch: [2400/2467]	Loss: 0.692507Train Epoch: [2400/2467]	Loss: 0.691210

     Train Epoch: [2400/2467]	Loss: 0.678222
     Train Epoch: [2420/2467]	Loss: 0.677378    
         Train Epoch: [2420/2467]	Loss: 0.696875
 Train Epoch: [2420/2467]	Loss: 0.711472 
Train Epoch: [2420/2467]	Loss: 0.692758
         Train Epoch: [2440/2467]	Loss: 0.686293
 Train Epoch: [2440/2467]	Loss: 0.692771
     Train Epoch: [2440/2467]	Loss: 0.692939
     Train Epoch: [2440/2467]	Loss: 0.701057
         Train Epoch: [2460/2467]	Loss: 0.675911
 Train Epoch: [2460/2467]	Loss: 0.691133
     Train Epoch: [2460/2467]	Loss: 0.697018
     Train Epoch: [2460/2467]	Loss: 0.697684
     *** train one epoch ***
     
     *** train one epoch ***
     
     *** train one epoch ***
     
     *** train one epoch ***
     
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 1 epoch =====
     2025-05-11.00-25-06
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 1 epoch =====
     2025-05-11.00-25-06
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 1 epoch =====
     2025-05-11.00-25-06
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 1 epoch =====
     2025-05-11.00-25-07
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
         Train Epoch: [0/2467]	Loss: 0.693384
 Train Epoch: [0/2467]	Loss: 0.688589
          Train Epoch: [0/2467]	Loss: 0.692601Train Epoch: [0/2467]	Loss: 0.693442

         Train Epoch: [20/2467]	Loss: 0.744115
 Train Epoch: [20/2467]	Loss: 0.514112
     Train Epoch: [20/2467]	Loss: 0.782475
     Train Epoch: [20/2467]	Loss: 0.536789
         Train Epoch: [40/2467]	Loss: 0.431550    
 Train Epoch: [40/2467]	Loss: 0.913358
 Train Epoch: [40/2467]	Loss: 0.749484
     Train Epoch: [40/2467]	Loss: 0.561585
         Train Epoch: [60/2467]	Loss: 0.740025 
Train Epoch: [60/2467]	Loss: 0.523303
     Train Epoch: [60/2467]	Loss: 0.441625
     Train Epoch: [60/2467]	Loss: 0.490368
          Train Epoch: [80/2467]	Loss: 1.052676Train Epoch: [80/2467]	Loss: 0.594792    

     Train Epoch: [80/2467]	Loss: 0.612889
 Train Epoch: [80/2467]	Loss: 0.377639
         Train Epoch: [100/2467]	Loss: 0.712825
     Train Epoch: [100/2467]	Loss: 0.690928
 Train Epoch: [100/2467]	Loss: 0.917162
     Train Epoch: [100/2467]	Loss: 0.637029
     Train Epoch: [120/2467]	Loss: 0.407749
     Train Epoch: [120/2467]	Loss: 0.472339    
 Train Epoch: [120/2467]	Loss: 0.441913
     Train Epoch: [120/2467]	Loss: 0.715899
         Train Epoch: [140/2467]	Loss: 0.525456
         Train Epoch: [140/2467]	Loss: 0.530859 
Train Epoch: [140/2467]	Loss: 0.677939
 Train Epoch: [140/2467]	Loss: 0.486624
     Train Epoch: [160/2467]	Loss: 0.501171        
  Train Epoch: [160/2467]	Loss: 0.445196Train Epoch: [160/2467]	Loss: 0.602412

     Train Epoch: [160/2467]	Loss: 0.703411
     Train Epoch: [180/2467]	Loss: 0.422981    
      Train Epoch: [180/2467]	Loss: 0.711067Train Epoch: [180/2467]	Loss: 0.397366

     Train Epoch: [180/2467]	Loss: 0.806135
     Train Epoch: [200/2467]	Loss: 0.478042
              Train Epoch: [200/2467]	Loss: 0.545897Train Epoch: [200/2467]	Loss: 0.787252

 Train Epoch: [200/2467]	Loss: 0.635269
             Train Epoch: [220/2467]	Loss: 0.770900 
Train Epoch: [220/2467]	Loss: 0.559304
 Train Epoch: [220/2467]	Loss: 0.284156
     Train Epoch: [220/2467]	Loss: 0.590452
             Train Epoch: [240/2467]	Loss: 0.520738
  Train Epoch: [240/2467]	Loss: 0.830047Train Epoch: [240/2467]	Loss: 0.710308

     Train Epoch: [240/2467]	Loss: 0.362969
     Train Epoch: [260/2467]	Loss: 0.457144
             Train Epoch: [260/2467]	Loss: 0.482678
 Train Epoch: [260/2467]	Loss: 0.463124 
Train Epoch: [260/2467]	Loss: 0.559816
          Train Epoch: [280/2467]	Loss: 0.257884Train Epoch: [280/2467]	Loss: 0.708601

     Train Epoch: [280/2467]	Loss: 0.565896
     Train Epoch: [280/2467]	Loss: 0.854634
          Train Epoch: [300/2467]	Loss: 0.705172
Train Epoch: [300/2467]	Loss: 0.688919
     Train Epoch: [300/2467]	Loss: 0.530709
     Train Epoch: [300/2467]	Loss: 0.693198
         Train Epoch: [320/2467]	Loss: 0.712914 
Train Epoch: [320/2467]	Loss: 0.744206
     Train Epoch: [320/2467]	Loss: 0.691735
     Train Epoch: [320/2467]	Loss: 0.733710
          Train Epoch: [340/2467]	Loss: 0.421592Train Epoch: [340/2467]	Loss: 0.834025
    
 Train Epoch: [340/2467]	Loss: 0.497317    
 Train Epoch: [340/2467]	Loss: 0.632346
     Train Epoch: [360/2467]	Loss: 0.488212
          Train Epoch: [360/2467]	Loss: 0.594884    Train Epoch: [360/2467]	Loss: 0.763378

 Train Epoch: [360/2467]	Loss: 0.546927
         Train Epoch: [380/2467]	Loss: 0.775632
 Train Epoch: [380/2467]	Loss: 0.647925
     Train Epoch: [380/2467]	Loss: 0.909018
     Train Epoch: [380/2467]	Loss: 0.509428
          Train Epoch: [400/2467]	Loss: 0.440011Train Epoch: [400/2467]	Loss: 0.512125

     Train Epoch: [400/2467]	Loss: 0.804904
     Train Epoch: [400/2467]	Loss: 0.680036
          Train Epoch: [420/2467]	Loss: 0.513057Train Epoch: [420/2467]	Loss: 0.699396

     Train Epoch: [420/2467]	Loss: 0.682868    
 Train Epoch: [420/2467]	Loss: 0.749700
         Train Epoch: [440/2467]	Loss: 0.774517
 Train Epoch: [440/2467]	Loss: 0.439870
         Train Epoch: [440/2467]	Loss: 0.417965
 Train Epoch: [440/2467]	Loss: 0.401921
         Train Epoch: [460/2467]	Loss: 0.681836
 Train Epoch: [460/2467]	Loss: 0.389459
     Train Epoch: [460/2467]	Loss: 0.675834
     Train Epoch: [460/2467]	Loss: 0.428231
     Train Epoch: [480/2467]	Loss: 0.502275
         Train Epoch: [480/2467]	Loss: 0.518011
 Train Epoch: [480/2467]	Loss: 0.476189
     Train Epoch: [480/2467]	Loss: 0.644306
             Train Epoch: [500/2467]	Loss: 1.022192 
Train Epoch: [500/2467]	Loss: 0.309765
 Train Epoch: [500/2467]	Loss: 0.646340
     Train Epoch: [500/2467]	Loss: 0.440706
     Train Epoch: [520/2467]	Loss: 0.526749
         Train Epoch: [520/2467]	Loss: 0.649064
 Train Epoch: [520/2467]	Loss: 0.386198
     Train Epoch: [520/2467]	Loss: 0.750969
         Train Epoch: [540/2467]	Loss: 0.583944
 Train Epoch: [540/2467]	Loss: 0.463188
          Train Epoch: [540/2467]	Loss: 0.424682Train Epoch: [540/2467]	Loss: 0.465677

         Train Epoch: [560/2467]	Loss: 0.427981
     Train Epoch: [560/2467]	Loss: 0.413666
 Train Epoch: [560/2467]	Loss: 0.637708
     Train Epoch: [560/2467]	Loss: 0.402290
     Train Epoch: [580/2467]	Loss: 0.668897
          Train Epoch: [580/2467]	Loss: 0.454620Train Epoch: [580/2467]	Loss: 0.534974

     Train Epoch: [580/2467]	Loss: 0.718549
             Train Epoch: [600/2467]	Loss: 0.511325 
Train Epoch: [600/2467]	Loss: 0.684160 
Train Epoch: [600/2467]	Loss: 0.439462
     Train Epoch: [600/2467]	Loss: 0.400473
     Train Epoch: [620/2467]	Loss: 0.979060
         Train Epoch: [620/2467]	Loss: 0.683882    
  Train Epoch: [620/2467]	Loss: 0.524362Train Epoch: [620/2467]	Loss: 0.317617

     Train Epoch: [640/2467]	Loss: 0.376162
              Train Epoch: [640/2467]	Loss: 0.384425Train Epoch: [640/2467]	Loss: 0.665356

 Train Epoch: [640/2467]	Loss: 0.614334
          Train Epoch: [660/2467]	Loss: 0.550612Train Epoch: [660/2467]	Loss: 0.548371
    
 Train Epoch: [660/2467]	Loss: 0.475337
     Train Epoch: [660/2467]	Loss: 0.648673
         Train Epoch: [680/2467]	Loss: 0.522694 
Train Epoch: [680/2467]	Loss: 0.335638
     Train Epoch: [680/2467]	Loss: 1.466749
     Train Epoch: [680/2467]	Loss: 0.506498
     Train Epoch: [700/2467]	Loss: 0.402463
         Train Epoch: [700/2467]	Loss: 0.304916 
Train Epoch: [700/2467]	Loss: 0.451029
     Train Epoch: [700/2467]	Loss: 0.590572
         Train Epoch: [720/2467]	Loss: 0.708047 
Train Epoch: [720/2467]	Loss: 0.763165    
     Train Epoch: [720/2467]	Loss: 0.612876
 Train Epoch: [720/2467]	Loss: 0.363642
         Train Epoch: [740/2467]	Loss: 0.561742
 Train Epoch: [740/2467]	Loss: 0.532983
     Train Epoch: [740/2467]	Loss: 0.322036
     Train Epoch: [740/2467]	Loss: 0.375681
         Train Epoch: [760/2467]	Loss: 0.368388
 Train Epoch: [760/2467]	Loss: 0.749679
     Train Epoch: [760/2467]	Loss: 0.624452
     Train Epoch: [760/2467]	Loss: 0.647793
     Train Epoch: [780/2467]	Loss: 0.617413
         Train Epoch: [780/2467]	Loss: 0.674140    
  Train Epoch: [780/2467]	Loss: 0.690707
Train Epoch: [780/2467]	Loss: 0.622688
              Train Epoch: [800/2467]	Loss: 0.877923Train Epoch: [800/2467]	Loss: 0.496984

 Train Epoch: [800/2467]	Loss: 0.539386
     Train Epoch: [800/2467]	Loss: 0.290302
          Train Epoch: [820/2467]	Loss: 0.493802Train Epoch: [820/2467]	Loss: 0.429887

     Train Epoch: [820/2467]	Loss: 0.676106
     Train Epoch: [820/2467]	Loss: 0.524135
     Train Epoch: [840/2467]	Loss: 0.492689
     Train Epoch: [840/2467]	Loss: 0.473541
     Train Epoch: [840/2467]	Loss: 0.669272
     Train Epoch: [840/2467]	Loss: 0.474416
         Train Epoch: [860/2467]	Loss: 0.759546
     Train Epoch: [860/2467]	Loss: 0.685191
     Train Epoch: [860/2467]	Loss: 0.770512
 Train Epoch: [860/2467]	Loss: 0.645715
          Train Epoch: [880/2467]	Loss: 0.515103Train Epoch: [880/2467]	Loss: 0.294940

     Train Epoch: [880/2467]	Loss: 0.468960
     Train Epoch: [880/2467]	Loss: 0.392421
     Train Epoch: [900/2467]	Loss: 0.756213
         Train Epoch: [900/2467]	Loss: 0.741731
     Train Epoch: [900/2467]	Loss: 0.689313
 Train Epoch: [900/2467]	Loss: 0.706648
         Train Epoch: [920/2467]	Loss: 0.575572
     Train Epoch: [920/2467]	Loss: 0.673317
 Train Epoch: [920/2467]	Loss: 0.491336
     Train Epoch: [920/2467]	Loss: 0.587482
         Train Epoch: [940/2467]	Loss: 0.196997
 Train Epoch: [940/2467]	Loss: 0.350257
     Train Epoch: [940/2467]	Loss: 0.483113
     Train Epoch: [940/2467]	Loss: 0.623613
         Train Epoch: [960/2467]	Loss: 0.254607    
 Train Epoch: [960/2467]	Loss: 0.546180
 Train Epoch: [960/2467]	Loss: 0.336510
     Train Epoch: [960/2467]	Loss: 0.682101
         Train Epoch: [980/2467]	Loss: 0.690903
     Train Epoch: [980/2467]	Loss: 0.490599
     Train Epoch: [980/2467]	Loss: 0.631196
 Train Epoch: [980/2467]	Loss: 0.662795
         Train Epoch: [1000/2467]	Loss: 0.435403
     Train Epoch: [1000/2467]	Loss: 0.580926
 Train Epoch: [1000/2467]	Loss: 0.328732
     Train Epoch: [1000/2467]	Loss: 0.697067
     Train Epoch: [1020/2467]	Loss: 0.573379
             Train Epoch: [1020/2467]	Loss: 0.699764 
Train Epoch: [1020/2467]	Loss: 0.857349
 Train Epoch: [1020/2467]	Loss: 0.166295
         Train Epoch: [1040/2467]	Loss: 0.748791
 Train Epoch: [1040/2467]	Loss: 0.198991
         Train Epoch: [1040/2467]	Loss: 0.605971
 Train Epoch: [1040/2467]	Loss: 0.456513
     Train Epoch: [1060/2467]	Loss: 0.571642
         Train Epoch: [1060/2467]	Loss: 0.465503
 Train Epoch: [1060/2467]	Loss: 0.395664
     Train Epoch: [1060/2467]	Loss: 0.670110
              Train Epoch: [1080/2467]	Loss: 0.568577
Train Epoch: [1080/2467]	Loss: 0.594868 
Train Epoch: [1080/2467]	Loss: 0.206366
     Train Epoch: [1080/2467]	Loss: 0.670008
     Train Epoch: [1100/2467]	Loss: 0.656915
         Train Epoch: [1100/2467]	Loss: 0.550493 
Train Epoch: [1100/2467]	Loss: 0.680597
     Train Epoch: [1100/2467]	Loss: 0.517413
         Train Epoch: [1120/2467]	Loss: 0.390138
 Train Epoch: [1120/2467]	Loss: 0.513963
     Train Epoch: [1120/2467]	Loss: 0.685556
     Train Epoch: [1120/2467]	Loss: 0.431699
     Train Epoch: [1140/2467]	Loss: 0.704229
     Train Epoch: [1140/2467]	Loss: 0.361457
     Train Epoch: [1140/2467]	Loss: 0.679333
     Train Epoch: [1140/2467]	Loss: 0.298990
         Train Epoch: [1160/2467]	Loss: 0.481023
     Train Epoch: [1160/2467]	Loss: 0.816945
     Train Epoch: [1160/2467]	Loss: 0.224106
 Train Epoch: [1160/2467]	Loss: 0.287055
     Train Epoch: [1180/2467]	Loss: 0.279031
     Train Epoch: [1180/2467]	Loss: 0.451643    
 Train Epoch: [1180/2467]	Loss: 0.576091
     Train Epoch: [1180/2467]	Loss: 0.664183
         Train Epoch: [1200/2467]	Loss: 0.531335
     Train Epoch: [1200/2467]	Loss: 0.739783
     Train Epoch: [1200/2467]	Loss: 0.587371
 Train Epoch: [1200/2467]	Loss: 0.479875
         Train Epoch: [1220/2467]	Loss: 0.635843
     Train Epoch: [1220/2467]	Loss: 0.705828
 Train Epoch: [1220/2467]	Loss: 0.724176
     Train Epoch: [1220/2467]	Loss: 0.529337
          Train Epoch: [1240/2467]	Loss: 0.545670Train Epoch: [1240/2467]	Loss: 0.609076

     Train Epoch: [1240/2467]	Loss: 0.502280
     Train Epoch: [1240/2467]	Loss: 0.766340
         Train Epoch: [1260/2467]	Loss: 0.334703
 Train Epoch: [1260/2467]	Loss: 0.267889
         Train Epoch: [1260/2467]	Loss: 0.356509
 Train Epoch: [1260/2467]	Loss: 0.670700
          Train Epoch: [1280/2467]	Loss: 1.223441Train Epoch: [1280/2467]	Loss: 0.549841

     Train Epoch: [1280/2467]	Loss: 0.238661
     Train Epoch: [1280/2467]	Loss: 0.588398
     Train Epoch: [1300/2467]	Loss: 0.404961
         Train Epoch: [1300/2467]	Loss: 0.291959
 Train Epoch: [1300/2467]	Loss: 0.615566
     Train Epoch: [1300/2467]	Loss: 0.603801
         Train Epoch: [1320/2467]	Loss: 0.425159
 Train Epoch: [1320/2467]	Loss: 0.553795
          Train Epoch: [1320/2467]	Loss: 0.476941Train Epoch: [1320/2467]	Loss: 0.612275

         Train Epoch: [1340/2467]	Loss: 0.683556    
 Train Epoch: [1340/2467]	Loss: 0.218792    
 Train Epoch: [1340/2467]	Loss: 0.755858
 Train Epoch: [1340/2467]	Loss: 0.590453
     Train Epoch: [1360/2467]	Loss: 0.530802    
     Train Epoch: [1360/2467]	Loss: 0.579270
 Train Epoch: [1360/2467]	Loss: 0.346593
     Train Epoch: [1360/2467]	Loss: 0.267680
         Train Epoch: [1380/2467]	Loss: 0.602368 
Train Epoch: [1380/2467]	Loss: 0.741875
         Train Epoch: [1380/2467]	Loss: 0.493317 
Train Epoch: [1380/2467]	Loss: 0.656063
     Train Epoch: [1400/2467]	Loss: 0.469044
     Train Epoch: [1400/2467]	Loss: 0.445345    
 Train Epoch: [1400/2467]	Loss: 0.584512
     Train Epoch: [1400/2467]	Loss: 0.524711
             Train Epoch: [1420/2467]	Loss: 0.340151
 Train Epoch: [1420/2467]	Loss: 0.679573
 Train Epoch: [1420/2467]	Loss: 0.443540
     Train Epoch: [1420/2467]	Loss: 0.696916
         Train Epoch: [1440/2467]	Loss: 0.172358
 Train Epoch: [1440/2467]	Loss: 0.496037
     Train Epoch: [1440/2467]	Loss: 0.666599
     Train Epoch: [1440/2467]	Loss: 0.244434
          Train Epoch: [1460/2467]	Loss: 0.501335Train Epoch: [1460/2467]	Loss: 0.261613

     Train Epoch: [1460/2467]	Loss: 0.197455
     Train Epoch: [1460/2467]	Loss: 0.608411
         Train Epoch: [1480/2467]	Loss: 0.679759 
Train Epoch: [1480/2467]	Loss: 0.692088
         Train Epoch: [1480/2467]	Loss: 0.587567
 Train Epoch: [1480/2467]	Loss: 0.477802
         Train Epoch: [1500/2467]	Loss: 0.549177
 Train Epoch: [1500/2467]	Loss: 0.255454    
 Train Epoch: [1500/2467]	Loss: 0.307149
     Train Epoch: [1500/2467]	Loss: 0.661782
         Train Epoch: [1520/2467]	Loss: 0.240665
 Train Epoch: [1520/2467]	Loss: 0.584574
     Train Epoch: [1520/2467]	Loss: 0.393162
     Train Epoch: [1520/2467]	Loss: 0.642436
         Train Epoch: [1540/2467]	Loss: 0.481348    
 Train Epoch: [1540/2467]	Loss: 0.632462
 Train Epoch: [1540/2467]	Loss: 0.566394
     Train Epoch: [1540/2467]	Loss: 0.401566
                  Train Epoch: [1560/2467]	Loss: 0.293601 
Train Epoch: [1560/2467]	Loss: 0.599245 Train Epoch: [1560/2467]	Loss: 0.737452

Train Epoch: [1560/2467]	Loss: 0.791613
         Train Epoch: [1580/2467]	Loss: 0.623354
     Train Epoch: [1580/2467]	Loss: 0.682077
     Train Epoch: [1580/2467]	Loss: 0.599239
 Train Epoch: [1580/2467]	Loss: 0.788086
          Train Epoch: [1600/2467]	Loss: 0.617650    Train Epoch: [1600/2467]	Loss: 0.586677

     Train Epoch: [1600/2467]	Loss: 0.763885
 Train Epoch: [1600/2467]	Loss: 0.760124
     Train Epoch: [1620/2467]	Loss: 0.227404
         Train Epoch: [1620/2467]	Loss: 0.462668
     Train Epoch: [1620/2467]	Loss: 0.174474
 Train Epoch: [1620/2467]	Loss: 0.560417
         Train Epoch: [1640/2467]	Loss: 0.401652
     Train Epoch: [1640/2467]	Loss: 0.490394
 Train Epoch: [1640/2467]	Loss: 0.553803
     Train Epoch: [1640/2467]	Loss: 0.411160
          Train Epoch: [1660/2467]	Loss: 0.360519Train Epoch: [1660/2467]	Loss: 0.771956
    
 Train Epoch: [1660/2467]	Loss: 0.638576
     Train Epoch: [1660/2467]	Loss: 0.330225
         Train Epoch: [1680/2467]	Loss: 0.852076
 Train Epoch: [1680/2467]	Loss: 0.463828
         Train Epoch: [1680/2467]	Loss: 0.346037
 Train Epoch: [1680/2467]	Loss: 0.353580
         Train Epoch: [1700/2467]	Loss: 0.473140
 Train Epoch: [1700/2467]	Loss: 0.566730
          Train Epoch: [1700/2467]	Loss: 0.530776
Train Epoch: [1700/2467]	Loss: 0.541333
              Train Epoch: [1720/2467]	Loss: 0.598957Train Epoch: [1720/2467]	Loss: 0.431277

 Train Epoch: [1720/2467]	Loss: 0.475641
     Train Epoch: [1720/2467]	Loss: 0.576420
     Train Epoch: [1740/2467]	Loss: 0.451434    
     Train Epoch: [1740/2467]	Loss: 0.576987
 Train Epoch: [1740/2467]	Loss: 0.316037
     Train Epoch: [1740/2467]	Loss: 0.429218
     Train Epoch: [1760/2467]	Loss: 0.798911
     Train Epoch: [1760/2467]	Loss: 0.288006
          Train Epoch: [1760/2467]	Loss: 0.421333
Train Epoch: [1760/2467]	Loss: 0.443069
     Train Epoch: [1780/2467]	Loss: 0.575395
          Train Epoch: [1780/2467]	Loss: 0.328417Train Epoch: [1780/2467]	Loss: 0.572190

     Train Epoch: [1780/2467]	Loss: 0.431012
         Train Epoch: [1800/2467]	Loss: 0.705611
 Train Epoch: [1800/2467]	Loss: 0.421609    
     Train Epoch: [1800/2467]	Loss: 0.569772
 Train Epoch: [1800/2467]	Loss: 0.997215
         Train Epoch: [1820/2467]	Loss: 0.419103
     Train Epoch: [1820/2467]	Loss: 0.231166
     Train Epoch: [1820/2467]	Loss: 0.588089
 Train Epoch: [1820/2467]	Loss: 0.803439
     Train Epoch: [1840/2467]	Loss: 0.724171
     Train Epoch: [1840/2467]	Loss: 0.366141
     Train Epoch: [1840/2467]	Loss: 0.609163
     Train Epoch: [1840/2467]	Loss: 0.263772
         Train Epoch: [1860/2467]	Loss: 0.401554    
 Train Epoch: [1860/2467]	Loss: 0.324547
 Train Epoch: [1860/2467]	Loss: 0.673994
     Train Epoch: [1860/2467]	Loss: 0.653339
         Train Epoch: [1880/2467]	Loss: 0.224792
 Train Epoch: [1880/2467]	Loss: 0.278919
         Train Epoch: [1880/2467]	Loss: 0.606439
 Train Epoch: [1880/2467]	Loss: 0.633992
     Train Epoch: [1900/2467]	Loss: 0.623423
         Train Epoch: [1900/2467]	Loss: 0.750589
 Train Epoch: [1900/2467]	Loss: 0.378338
     Train Epoch: [1900/2467]	Loss: 0.553711
         Train Epoch: [1920/2467]	Loss: 0.391670
 Train Epoch: [1920/2467]	Loss: 0.176750    
     Train Epoch: [1920/2467]	Loss: 0.551394
 Train Epoch: [1920/2467]	Loss: 0.553023
     Train Epoch: [1940/2467]	Loss: 0.596325
             Train Epoch: [1940/2467]	Loss: 0.659961 
Train Epoch: [1940/2467]	Loss: 0.393133
 Train Epoch: [1940/2467]	Loss: 0.347599
         Train Epoch: [1960/2467]	Loss: 0.620505
 Train Epoch: [1960/2467]	Loss: 0.524422
     Train Epoch: [1960/2467]	Loss: 0.491763
     Train Epoch: [1960/2467]	Loss: 0.374061
     Train Epoch: [1980/2467]	Loss: 0.699056
         Train Epoch: [1980/2467]	Loss: 0.911175
 Train Epoch: [1980/2467]	Loss: 0.442660
     Train Epoch: [1980/2467]	Loss: 0.624002
         Train Epoch: [2000/2467]	Loss: 0.352938
     Train Epoch: [2000/2467]	Loss: 0.640503
 Train Epoch: [2000/2467]	Loss: 0.771413
     Train Epoch: [2000/2467]	Loss: 0.569867
              Train Epoch: [2020/2467]	Loss: 0.733496Train Epoch: [2020/2467]	Loss: 0.593877

 Train Epoch: [2020/2467]	Loss: 0.509493
     Train Epoch: [2020/2467]	Loss: 0.400945
         Train Epoch: [2040/2467]	Loss: 0.554132 
Train Epoch: [2040/2467]	Loss: 0.757260
     Train Epoch: [2040/2467]	Loss: 0.523458
     Train Epoch: [2040/2467]	Loss: 0.251655
          Train Epoch: [2060/2467]	Loss: 0.615213Train Epoch: [2060/2467]	Loss: 0.550580

     Train Epoch: [2060/2467]	Loss: 0.482540
     Train Epoch: [2060/2467]	Loss: 0.769167
         Train Epoch: [2080/2467]	Loss: 0.836417
 Train Epoch: [2080/2467]	Loss: 0.485649
     Train Epoch: [2080/2467]	Loss: 0.339699
     Train Epoch: [2080/2467]	Loss: 0.447230
         Train Epoch: [2100/2467]	Loss: 0.288407
         Train Epoch: [2100/2467]	Loss: 0.652353 
Train Epoch: [2100/2467]	Loss: 0.543954
 Train Epoch: [2100/2467]	Loss: 0.591697
         Train Epoch: [2120/2467]	Loss: 0.306836
 Train Epoch: [2120/2467]	Loss: 0.355690
     Train Epoch: [2120/2467]	Loss: 0.315502
     Train Epoch: [2120/2467]	Loss: 0.655235
              Train Epoch: [2140/2467]	Loss: 0.847476Train Epoch: [2140/2467]	Loss: 0.710748

 Train Epoch: [2140/2467]	Loss: 0.635971
     Train Epoch: [2140/2467]	Loss: 0.628341
         Train Epoch: [2160/2467]	Loss: 0.410336
     Train Epoch: [2160/2467]	Loss: 0.693015
     Train Epoch: [2160/2467]	Loss: 0.896374
 Train Epoch: [2160/2467]	Loss: 0.506439
     Train Epoch: [2180/2467]	Loss: 0.615143
         Train Epoch: [2180/2467]	Loss: 0.383079
 Train Epoch: [2180/2467]	Loss: 0.607917
     Train Epoch: [2180/2467]	Loss: 0.297902
     Train Epoch: [2200/2467]	Loss: 0.255228
     Train Epoch: [2200/2467]	Loss: 0.965007    
 Train Epoch: [2200/2467]	Loss: 0.478766
     Train Epoch: [2200/2467]	Loss: 0.713526
             Train Epoch: [2220/2467]	Loss: 0.549520
 Train Epoch: [2220/2467]	Loss: 0.340467
     Train Epoch: [2220/2467]	Loss: 0.629252
 Train Epoch: [2220/2467]	Loss: 0.213906
         Train Epoch: [2240/2467]	Loss: 0.403786
     Train Epoch: [2240/2467]	Loss: 0.166530
 Train Epoch: [2240/2467]	Loss: 0.334347
     Train Epoch: [2240/2467]	Loss: 0.267301
     Train Epoch: [2260/2467]	Loss: 0.848427
     Train Epoch: [2260/2467]	Loss: 0.415875
     Train Epoch: [2260/2467]	Loss: 0.294597
     Train Epoch: [2260/2467]	Loss: 0.451008
         Train Epoch: [2280/2467]	Loss: 0.258367
     Train Epoch: [2280/2467]	Loss: 0.701209    
  Train Epoch: [2280/2467]	Loss: 0.472204Train Epoch: [2280/2467]	Loss: 0.470764

     Train Epoch: [2300/2467]	Loss: 0.209595    
     Train Epoch: [2300/2467]	Loss: 0.259972
     Train Epoch: [2300/2467]	Loss: 0.794141
 Train Epoch: [2300/2467]	Loss: 0.467087
         Train Epoch: [2320/2467]	Loss: 0.784937    
 Train Epoch: [2320/2467]	Loss: 0.184017 
    Train Epoch: [2320/2467]	Loss: 0.591893
 Train Epoch: [2320/2467]	Loss: 0.447351
          Train Epoch: [2340/2467]	Loss: 0.331533
Train Epoch: [2340/2467]	Loss: 0.259684
         Train Epoch: [2340/2467]	Loss: 0.711841
 Train Epoch: [2340/2467]	Loss: 0.323368
     Train Epoch: [2360/2467]	Loss: 0.709010
     Train Epoch: [2360/2467]	Loss: 0.539631
     Train Epoch: [2360/2467]	Loss: 0.297451
     Train Epoch: [2360/2467]	Loss: 0.401901
     Train Epoch: [2380/2467]	Loss: 0.402278
     Train Epoch: [2380/2467]	Loss: 0.605659
     Train Epoch: [2380/2467]	Loss: 0.677609
     Train Epoch: [2380/2467]	Loss: 0.665442
     Train Epoch: [2400/2467]	Loss: 0.432972    
     Train Epoch: [2400/2467]	Loss: 0.361990
 Train Epoch: [2400/2467]	Loss: 0.309860
     Train Epoch: [2400/2467]	Loss: 0.434168
              Train Epoch: [2420/2467]	Loss: 0.510976Train Epoch: [2420/2467]	Loss: 0.699101

 Train Epoch: [2420/2467]	Loss: 0.417148
     Train Epoch: [2420/2467]	Loss: 0.646918
          Train Epoch: [2440/2467]	Loss: 0.194762
Train Epoch: [2440/2467]	Loss: 0.576842
         Train Epoch: [2440/2467]	Loss: 0.748965
 Train Epoch: [2440/2467]	Loss: 0.432460
         Train Epoch: [2460/2467]	Loss: 0.379638 
Train Epoch: [2460/2467]	Loss: 0.417762
     Train Epoch: [2460/2467]	Loss: 0.617173
     Train Epoch: [2460/2467]	Loss: 0.756880
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 2 epoch =====
     2025-05-11.00-46-54
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 2 epoch =====
     2025-05-11.00-46-54
     ===== running 2 epoch =====
     2025-05-11.00-46-54
     ===== running 2 epoch =====
     2025-05-11.00-46-54
after set grad
after prog
start loop
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.554761
     Train Epoch: [0/2467]	Loss: 0.472583
     Train Epoch: [0/2467]	Loss: 0.432518
     Train Epoch: [0/2467]	Loss: 0.478346
         Train Epoch: [20/2467]	Loss: 0.454608
 Train Epoch: [20/2467]	Loss: 0.417390
     Train Epoch: [20/2467]	Loss: 0.716457
     Train Epoch: [20/2467]	Loss: 0.686874
         Train Epoch: [40/2467]	Loss: 0.408432
     Train Epoch: [40/2467]	Loss: 0.669413
     Train Epoch: [40/2467]	Loss: 0.619249
 Train Epoch: [40/2467]	Loss: 0.453959
     Train Epoch: [60/2467]	Loss: 0.656011
             Train Epoch: [60/2467]	Loss: 0.267546
 Train Epoch: [60/2467]	Loss: 0.327636
 Train Epoch: [60/2467]	Loss: 0.346727
          Train Epoch: [80/2467]	Loss: 0.600942
Train Epoch: [80/2467]	Loss: 0.568265
         Train Epoch: [80/2467]	Loss: 0.418547
 Train Epoch: [80/2467]	Loss: 0.327292
          Train Epoch: [100/2467]	Loss: 0.690570Train Epoch: [100/2467]	Loss: 0.558965     
Train Epoch: [100/2467]	Loss: 0.504071

     Train Epoch: [100/2467]	Loss: 0.492572
     Train Epoch: [120/2467]	Loss: 0.298661
         Train Epoch: [120/2467]	Loss: 0.149480 
Train Epoch: [120/2467]	Loss: 0.677513
     Train Epoch: [120/2467]	Loss: 0.420825
          Train Epoch: [140/2467]	Loss: 0.511105Train Epoch: [140/2467]	Loss: 0.735964

     Train Epoch: [140/2467]	Loss: 0.258667
     Train Epoch: [140/2467]	Loss: 0.342852
         Train Epoch: [160/2467]	Loss: 0.540923
     Train Epoch: [160/2467]	Loss: 0.478153 
Train Epoch: [160/2467]	Loss: 1.014788
     Train Epoch: [160/2467]	Loss: 0.310406
     Train Epoch: [180/2467]	Loss: 0.426618
         Train Epoch: [180/2467]	Loss: 0.502276
      Train Epoch: [180/2467]	Loss: 0.559596
Train Epoch: [180/2467]	Loss: 0.406663
          Train Epoch: [200/2467]	Loss: 0.301729    Train Epoch: [200/2467]	Loss: 0.416664

     Train Epoch: [200/2467]	Loss: 0.638928
 Train Epoch: [200/2467]	Loss: 0.440817
     Train Epoch: [220/2467]	Loss: 0.305199
         Train Epoch: [220/2467]	Loss: 0.751532
 Train Epoch: [220/2467]	Loss: 0.547650
     Train Epoch: [220/2467]	Loss: 0.485100
              Train Epoch: [240/2467]	Loss: 0.417998Train Epoch: [240/2467]	Loss: 0.522285

 Train Epoch: [240/2467]	Loss: 0.645118    
 Train Epoch: [240/2467]	Loss: 0.222948
         Train Epoch: [260/2467]	Loss: 0.331558
     Train Epoch: [260/2467]	Loss: 0.371754    
  Train Epoch: [260/2467]	Loss: 0.771007Train Epoch: [260/2467]	Loss: 0.386490

             Train Epoch: [280/2467]	Loss: 0.702204 
Train Epoch: [280/2467]	Loss: 0.542224
 Train Epoch: [280/2467]	Loss: 0.650691
     Train Epoch: [280/2467]	Loss: 0.519881
         Train Epoch: [300/2467]	Loss: 0.548445
     Train Epoch: [300/2467]	Loss: 0.684137
     Train Epoch: [300/2467]	Loss: 0.824506
 Train Epoch: [300/2467]	Loss: 0.351656
         Train Epoch: [320/2467]	Loss: 0.498658 
Train Epoch: [320/2467]	Loss: 0.738656
     Train Epoch: [320/2467]	Loss: 0.708269
     Train Epoch: [320/2467]	Loss: 0.707314
     Train Epoch: [340/2467]	Loss: 0.439787    
 Train Epoch: [340/2467]	Loss: 0.271036
     Train Epoch: [340/2467]	Loss: 0.338684
     Train Epoch: [340/2467]	Loss: 0.438767
         Train Epoch: [360/2467]	Loss: 0.975325 
Train Epoch: [360/2467]	Loss: 0.674798
     Train Epoch: [360/2467]	Loss: 0.635689
     Train Epoch: [360/2467]	Loss: 0.327247
     Train Epoch: [380/2467]	Loss: 0.724224
         Train Epoch: [380/2467]	Loss: 0.465243
 Train Epoch: [380/2467]	Loss: 0.404838
     Train Epoch: [380/2467]	Loss: 0.524412
     Train Epoch: [400/2467]	Loss: 0.504812
         Train Epoch: [400/2467]	Loss: 0.460251
     Train Epoch: [400/2467]	Loss: 0.383823 
Train Epoch: [400/2467]	Loss: 0.762221
     Train Epoch: [420/2467]	Loss: 0.450423
         Train Epoch: [420/2467]	Loss: 0.544286
 Train Epoch: [420/2467]	Loss: 0.547702
     Train Epoch: [420/2467]	Loss: 0.296164
     Train Epoch: [440/2467]	Loss: 0.621093
     Train Epoch: [440/2467]	Loss: 0.312393
         Train Epoch: [440/2467]	Loss: 0.344339
 Train Epoch: [440/2467]	Loss: 0.337053
             Train Epoch: [460/2467]	Loss: 0.559387
      Train Epoch: [460/2467]	Loss: 0.283485
Train Epoch: [460/2467]	Loss: 0.448571
 Train Epoch: [460/2467]	Loss: 0.604650
     Train Epoch: [480/2467]	Loss: 0.381275
     Train Epoch: [480/2467]	Loss: 0.590265
     Train Epoch: [480/2467]	Loss: 0.592909
     Train Epoch: [480/2467]	Loss: 0.349292
         Train Epoch: [500/2467]	Loss: 0.850519
 Train Epoch: [500/2467]	Loss: 0.250665
     Train Epoch: [500/2467]	Loss: 0.295017
     Train Epoch: [500/2467]	Loss: 0.578884
         Train Epoch: [520/2467]	Loss: 0.641035 
    Train Epoch: [520/2467]	Loss: 0.458904
 Train Epoch: [520/2467]	Loss: 0.648061
     Train Epoch: [520/2467]	Loss: 0.291293
         Train Epoch: [540/2467]	Loss: 0.256283
 Train Epoch: [540/2467]	Loss: 0.286992
     Train Epoch: [540/2467]	Loss: 0.307484
     Train Epoch: [540/2467]	Loss: 0.365213
              Train Epoch: [560/2467]	Loss: 0.405615Train Epoch: [560/2467]	Loss: 0.339213

 Train Epoch: [560/2467]	Loss: 0.614830
     Train Epoch: [560/2467]	Loss: 0.523445
     Train Epoch: [580/2467]	Loss: 0.537072
          Train Epoch: [580/2467]	Loss: 0.455239Train Epoch: [580/2467]	Loss: 0.300464

     Train Epoch: [580/2467]	Loss: 0.486707
     Train Epoch: [600/2467]	Loss: 0.267534
         Train Epoch: [600/2467]	Loss: 0.427557
 Train Epoch: [600/2467]	Loss: 0.330924
     Train Epoch: [600/2467]	Loss: 0.469522
         Train Epoch: [620/2467]	Loss: 0.426215
 Train Epoch: [620/2467]	Loss: 0.210786
     Train Epoch: [620/2467]	Loss: 0.553479    
 Train Epoch: [620/2467]	Loss: 0.835807
              Train Epoch: [640/2467]	Loss: 0.411332Train Epoch: [640/2467]	Loss: 0.512135

     Train Epoch: [640/2467]	Loss: 0.658166
 Train Epoch: [640/2467]	Loss: 0.755859
         Train Epoch: [660/2467]	Loss: 0.611880     
Train Epoch: [660/2467]	Loss: 0.306131
     Train Epoch: [660/2467]	Loss: 0.374301
 Train Epoch: [660/2467]	Loss: 0.699177
     Train Epoch: [680/2467]	Loss: 0.427813
     Train Epoch: [680/2467]	Loss: 0.349737
     Train Epoch: [680/2467]	Loss: 0.680333
     Train Epoch: [680/2467]	Loss: 0.453354
     Train Epoch: [700/2467]	Loss: 0.364869
          Train Epoch: [700/2467]	Loss: 0.322320Train Epoch: [700/2467]	Loss: 0.402724

     Train Epoch: [700/2467]	Loss: 0.652287
     Train Epoch: [720/2467]	Loss: 0.440009
         Train Epoch: [720/2467]	Loss: 0.615497 
Train Epoch: [720/2467]	Loss: 0.292873
     Train Epoch: [720/2467]	Loss: 0.549958
     Train Epoch: [740/2467]	Loss: 0.494869
         Train Epoch: [740/2467]	Loss: 0.288805
 Train Epoch: [740/2467]	Loss: 0.194709
     Train Epoch: [740/2467]	Loss: 0.322788
     Train Epoch: [760/2467]	Loss: 0.687144
     Train Epoch: [760/2467]	Loss: 0.671537
     Train Epoch: [760/2467]	Loss: 0.211891
     Train Epoch: [760/2467]	Loss: 0.683418
         Train Epoch: [780/2467]	Loss: 0.655346
 Train Epoch: [780/2467]	Loss: 0.596995
     Train Epoch: [780/2467]	Loss: 0.510368
     Train Epoch: [780/2467]	Loss: 0.605780
         Train Epoch: [800/2467]	Loss: 0.443560
 Train Epoch: [800/2467]	Loss: 0.418533
     Train Epoch: [800/2467]	Loss: 0.786123
     Train Epoch: [800/2467]	Loss: 0.263857
     Train Epoch: [820/2467]	Loss: 0.473075    
     Train Epoch: [820/2467]	Loss: 0.614556
 Train Epoch: [820/2467]	Loss: 0.389644
     Train Epoch: [820/2467]	Loss: 0.313701
     Train Epoch: [840/2467]	Loss: 0.699198
     Train Epoch: [840/2467]	Loss: 0.600492
     Train Epoch: [840/2467]	Loss: 0.318934
     Train Epoch: [840/2467]	Loss: 0.445827
          Train Epoch: [860/2467]	Loss: 0.677054Train Epoch: [860/2467]	Loss: 0.491135

     Train Epoch: [860/2467]	Loss: 0.540047
     Train Epoch: [860/2467]	Loss: 0.452733
         Train Epoch: [880/2467]	Loss: 0.299063
     Train Epoch: [880/2467]	Loss: 0.483774
     Train Epoch: [880/2467]	Loss: 0.368568
 Train Epoch: [880/2467]	Loss: 0.490032
     Train Epoch: [900/2467]	Loss: 0.656555    
     Train Epoch: [900/2467]	Loss: 0.585651
 Train Epoch: [900/2467]	Loss: 0.445123
     Train Epoch: [900/2467]	Loss: 0.664043
     Train Epoch: [920/2467]	Loss: 0.579885
          Train Epoch: [920/2467]	Loss: 0.600725Train Epoch: [920/2467]	Loss: 0.522399

     Train Epoch: [920/2467]	Loss: 0.339050
             Train Epoch: [940/2467]	Loss: 0.409024
 Train Epoch: [940/2467]	Loss: 0.587145 
Train Epoch: [940/2467]	Loss: 0.397207
     Train Epoch: [940/2467]	Loss: 0.081578
             Train Epoch: [960/2467]	Loss: 0.110722
  Train Epoch: [960/2467]	Loss: 0.567905
Train Epoch: [960/2467]	Loss: 0.248361
     Train Epoch: [960/2467]	Loss: 0.528153
     Train Epoch: [980/2467]	Loss: 0.578437
         Train Epoch: [980/2467]	Loss: 0.410316
 Train Epoch: [980/2467]	Loss: 0.534443
     Train Epoch: [980/2467]	Loss: 0.464614
         Train Epoch: [1000/2467]	Loss: 0.320911
 Train Epoch: [1000/2467]	Loss: 0.619793
     Train Epoch: [1000/2467]	Loss: 0.512759
     Train Epoch: [1000/2467]	Loss: 0.234877
             Train Epoch: [1020/2467]	Loss: 0.635559 
Train Epoch: [1020/2467]	Loss: 0.894093 
Train Epoch: [1020/2467]	Loss: 0.122228
     Train Epoch: [1020/2467]	Loss: 0.586401
     Train Epoch: [1040/2467]	Loss: 0.654836
         Train Epoch: [1040/2467]	Loss: 0.174963
 Train Epoch: [1040/2467]	Loss: 0.534902
     Train Epoch: [1040/2467]	Loss: 0.376598
     Train Epoch: [1060/2467]	Loss: 0.602426
     Train Epoch: [1060/2467]	Loss: 0.316408
         Train Epoch: [1060/2467]	Loss: 0.647078
 Train Epoch: [1060/2467]	Loss: 0.521284
         Train Epoch: [1080/2467]	Loss: 0.246235
 Train Epoch: [1080/2467]	Loss: 0.529457
     Train Epoch: [1080/2467]	Loss: 0.548866
     Train Epoch: [1080/2467]	Loss: 0.671147
         Train Epoch: [1100/2467]	Loss: 0.581410
     Train Epoch: [1100/2467]	Loss: 0.514351
 Train Epoch: [1100/2467]	Loss: 0.505628
     Train Epoch: [1100/2467]	Loss: 0.472886
             Train Epoch: [1120/2467]	Loss: 0.300088
 Train Epoch: [1120/2467]	Loss: 0.884995
 Train Epoch: [1120/2467]	Loss: 0.420227
     Train Epoch: [1120/2467]	Loss: 0.323705
     Train Epoch: [1140/2467]	Loss: 0.251473
     Train Epoch: [1140/2467]	Loss: 0.618643
     Train Epoch: [1140/2467]	Loss: 0.605986
     Train Epoch: [1140/2467]	Loss: 0.296162
     Train Epoch: [1160/2467]	Loss: 0.351380
         Train Epoch: [1160/2467]	Loss: 0.231686
 Train Epoch: [1160/2467]	Loss: 0.609772
     Train Epoch: [1160/2467]	Loss: 0.285172
          Train Epoch: [1180/2467]	Loss: 0.343348Train Epoch: [1180/2467]	Loss: 0.650006

     Train Epoch: [1180/2467]	Loss: 0.510935
     Train Epoch: [1180/2467]	Loss: 0.200347
         Train Epoch: [1200/2467]	Loss: 0.509650
     Train Epoch: [1200/2467]	Loss: 0.571945
 Train Epoch: [1200/2467]	Loss: 0.615807
     Train Epoch: [1200/2467]	Loss: 0.485438
          Train Epoch: [1220/2467]	Loss: 0.549378
Train Epoch: [1220/2467]	Loss: 0.555295
     Train Epoch: [1220/2467]	Loss: 0.671182
     Train Epoch: [1220/2467]	Loss: 0.742960
     Train Epoch: [1240/2467]	Loss: 0.644545
         Train Epoch: [1240/2467]	Loss: 0.789307 
Train Epoch: [1240/2467]	Loss: 0.615123
     Train Epoch: [1240/2467]	Loss: 0.486213
     Train Epoch: [1260/2467]	Loss: 0.200789
     Train Epoch: [1260/2467]	Loss: 0.278586
     Train Epoch: [1260/2467]	Loss: 0.241764
     Train Epoch: [1260/2467]	Loss: 0.665464
          Train Epoch: [1280/2467]	Loss: 0.213739Train Epoch: [1280/2467]	Loss: 1.152085    

 Train Epoch: [1280/2467]	Loss: 0.511785
     Train Epoch: [1280/2467]	Loss: 0.625496
         Train Epoch: [1300/2467]	Loss: 0.393803 
    Train Epoch: [1300/2467]	Loss: 0.220704
 Train Epoch: [1300/2467]	Loss: 0.630466
     Train Epoch: [1300/2467]	Loss: 0.687417
         Train Epoch: [1320/2467]	Loss: 0.446717
 Train Epoch: [1320/2467]	Loss: 0.400227
     Train Epoch: [1320/2467]	Loss: 0.429629
     Train Epoch: [1320/2467]	Loss: 0.560270
         Train Epoch: [1340/2467]	Loss: 0.628197
     Train Epoch: [1340/2467]	Loss: 0.513037
 Train Epoch: [1340/2467]	Loss: 0.158585    
 Train Epoch: [1340/2467]	Loss: 0.514611
         Train Epoch: [1360/2467]	Loss: 0.468880 
Train Epoch: [1360/2467]	Loss: 0.227570    
     Train Epoch: [1360/2467]	Loss: 0.508757
 Train Epoch: [1360/2467]	Loss: 0.337699
         Train Epoch: [1380/2467]	Loss: 0.483745
 Train Epoch: [1380/2467]	Loss: 0.685217
     Train Epoch: [1380/2467]	Loss: 0.659646
     Train Epoch: [1380/2467]	Loss: 0.523535
     Train Epoch: [1400/2467]	Loss: 0.450963
     Train Epoch: [1400/2467]	Loss: 0.367111
     Train Epoch: [1400/2467]	Loss: 0.454270
     Train Epoch: [1400/2467]	Loss: 0.504918
         Train Epoch: [1420/2467]	Loss: 0.311540
     Train Epoch: [1420/2467]	Loss: 0.704906
 Train Epoch: [1420/2467]	Loss: 0.376622
     Train Epoch: [1420/2467]	Loss: 0.509773
         Train Epoch: [1440/2467]	Loss: 0.102172
 Train Epoch: [1440/2467]	Loss: 0.200956
     Train Epoch: [1440/2467]	Loss: 0.468130
     Train Epoch: [1440/2467]	Loss: 0.539640
     Train Epoch: [1460/2467]	Loss: 0.348852
     Train Epoch: [1460/2467]	Loss: 0.258147
     Train Epoch: [1460/2467]	Loss: 0.553411
     Train Epoch: [1460/2467]	Loss: 0.113741
              Train Epoch: [1480/2467]	Loss: 0.628810
Train Epoch: [1480/2467]	Loss: 0.545903
 Train Epoch: [1480/2467]	Loss: 0.283309
     Train Epoch: [1480/2467]	Loss: 0.344942
               Train Epoch: [1500/2467]	Loss: 0.260194Train Epoch: [1500/2467]	Loss: 0.159979Train Epoch: [1500/2467]	Loss: 0.368184


     Train Epoch: [1500/2467]	Loss: 0.524114
          Train Epoch: [1520/2467]	Loss: 0.183536Train Epoch: [1520/2467]	Loss: 0.550195

         Train Epoch: [1520/2467]	Loss: 0.386772 
Train Epoch: [1520/2467]	Loss: 0.500057
             Train Epoch: [1540/2467]	Loss: 0.406894 
Train Epoch: [1540/2467]	Loss: 0.389387
 Train Epoch: [1540/2467]	Loss: 0.431656
     Train Epoch: [1540/2467]	Loss: 0.588248
             Train Epoch: [1560/2467]	Loss: 0.531450
  Train Epoch: [1560/2467]	Loss: 0.264609Train Epoch: [1560/2467]	Loss: 0.637960

     Train Epoch: [1560/2467]	Loss: 0.758470
         Train Epoch: [1580/2467]	Loss: 0.636721 
Train Epoch: [1580/2467]	Loss: 0.557872    
     Train Epoch: [1580/2467]	Loss: 0.675219
 Train Epoch: [1580/2467]	Loss: 0.566090
         Train Epoch: [1600/2467]	Loss: 0.525315 
Train Epoch: [1600/2467]	Loss: 0.599552
     Train Epoch: [1600/2467]	Loss: 0.524671
     Train Epoch: [1600/2467]	Loss: 0.688778
     Train Epoch: [1620/2467]	Loss: 0.174305
     Train Epoch: [1620/2467]	Loss: 0.427608
     Train Epoch: [1620/2467]	Loss: 0.212810
     Train Epoch: [1620/2467]	Loss: 0.401511
     Train Epoch: [1640/2467]	Loss: 0.471577    
 Train Epoch: [1640/2467]	Loss: 0.435233    
 Train Epoch: [1640/2467]	Loss: 0.355645
     Train Epoch: [1640/2467]	Loss: 0.364934
          Train Epoch: [1660/2467]	Loss: 0.277399Train Epoch: [1660/2467]	Loss: 0.567719
    
 Train Epoch: [1660/2467]	Loss: 0.561117
     Train Epoch: [1660/2467]	Loss: 0.259448
     Train Epoch: [1680/2467]	Loss: 0.872177
             Train Epoch: [1680/2467]	Loss: 0.251342
  Train Epoch: [1680/2467]	Loss: 0.490961Train Epoch: [1680/2467]	Loss: 0.325898

             Train Epoch: [1700/2467]	Loss: 0.407934    
 Train Epoch: [1700/2467]	Loss: 0.480505 
 Train Epoch: [1700/2467]	Loss: 0.484778Train Epoch: [1700/2467]	Loss: 0.458679

             Train Epoch: [1720/2467]	Loss: 0.465360
 Train Epoch: [1720/2467]	Loss: 0.432171
 Train Epoch: [1720/2467]	Loss: 0.420727
     Train Epoch: [1720/2467]	Loss: 0.584477
         Train Epoch: [1740/2467]	Loss: 0.380518
 Train Epoch: [1740/2467]	Loss: 0.562471
          Train Epoch: [1740/2467]	Loss: 0.308499Train Epoch: [1740/2467]	Loss: 0.280706

     Train Epoch: [1760/2467]	Loss: 0.657166
     Train Epoch: [1760/2467]	Loss: 0.452473
     Train Epoch: [1760/2467]	Loss: 0.417933
     Train Epoch: [1760/2467]	Loss: 0.214373
     Train Epoch: [1780/2467]	Loss: 0.500034
         Train Epoch: [1780/2467]	Loss: 0.434116
 Train Epoch: [1780/2467]	Loss: 0.250669
     Train Epoch: [1780/2467]	Loss: 0.418212
     Train Epoch: [1800/2467]	Loss: 0.345004
         Train Epoch: [1800/2467]	Loss: 0.871556
     Train Epoch: [1800/2467]	Loss: 0.621371
 Train Epoch: [1800/2467]	Loss: 0.449389
     Train Epoch: [1820/2467]	Loss: 0.342587
     Train Epoch: [1820/2467]	Loss: 0.590590
     Train Epoch: [1820/2467]	Loss: 0.697908
     Train Epoch: [1820/2467]	Loss: 0.168255
             Train Epoch: [1840/2467]	Loss: 0.321204
  Train Epoch: [1840/2467]	Loss: 0.661203Train Epoch: [1840/2467]	Loss: 0.399804    

 Train Epoch: [1840/2467]	Loss: 0.215212
     Train Epoch: [1860/2467]	Loss: 0.389884
         Train Epoch: [1860/2467]	Loss: 0.554146
 Train Epoch: [1860/2467]	Loss: 0.380531
     Train Epoch: [1860/2467]	Loss: 0.281793
         Train Epoch: [1880/2467]	Loss: 0.218423
          Train Epoch: [1880/2467]	Loss: 0.139278Train Epoch: [1880/2467]	Loss: 0.396961

 Train Epoch: [1880/2467]	Loss: 0.394568
         Train Epoch: [1900/2467]	Loss: 0.420365
     Train Epoch: [1900/2467]	Loss: 0.665231
     Train Epoch: [1900/2467]	Loss: 0.549459
 Train Epoch: [1900/2467]	Loss: 0.364027
     Train Epoch: [1920/2467]	Loss: 0.353728    
     Train Epoch: [1920/2467]	Loss: 0.475507
 Train Epoch: [1920/2467]	Loss: 0.086691
     Train Epoch: [1920/2467]	Loss: 0.519202
     Train Epoch: [1940/2467]	Loss: 0.479354
         Train Epoch: [1940/2467]	Loss: 0.606493
 Train Epoch: [1940/2467]	Loss: 0.256834
     Train Epoch: [1940/2467]	Loss: 0.276224
         Train Epoch: [1960/2467]	Loss: 0.528707
 Train Epoch: [1960/2467]	Loss: 0.390037
         Train Epoch: [1960/2467]	Loss: 0.501830
 Train Epoch: [1960/2467]	Loss: 0.503658
     Train Epoch: [1980/2467]	Loss: 0.559238
     Train Epoch: [1980/2467]	Loss: 0.751518    
 Train Epoch: [1980/2467]	Loss: 0.392202
     Train Epoch: [1980/2467]	Loss: 0.514193
          Train Epoch: [2000/2467]	Loss: 0.422148Train Epoch: [2000/2467]	Loss: 0.351055

         Train Epoch: [2000/2467]	Loss: 0.635563
 Train Epoch: [2000/2467]	Loss: 0.431515
         Train Epoch: [2020/2467]	Loss: 0.759339
     Train Epoch: [2020/2467]	Loss: 0.468003
 Train Epoch: [2020/2467]	Loss: 0.567128
     Train Epoch: [2020/2467]	Loss: 0.264455
          Train Epoch: [2040/2467]	Loss: 0.698468Train Epoch: [2040/2467]	Loss: 0.480621

     Train Epoch: [2040/2467]	Loss: 0.435753
     Train Epoch: [2040/2467]	Loss: 0.193224
     Train Epoch: [2060/2467]	Loss: 0.576343
     Train Epoch: [2060/2467]	Loss: 0.432355
     Train Epoch: [2060/2467]	Loss: 0.869593
     Train Epoch: [2060/2467]	Loss: 0.458981
              Train Epoch: [2080/2467]	Loss: 0.709293Train Epoch: [2080/2467]	Loss: 0.283422

     Train Epoch: [2080/2467]	Loss: 0.453590
 Train Epoch: [2080/2467]	Loss: 0.457479
          Train Epoch: [2100/2467]	Loss: 0.744273
Train Epoch: [2100/2467]	Loss: 0.264623
     Train Epoch: [2100/2467]	Loss: 0.365869
     Train Epoch: [2100/2467]	Loss: 0.589435
         Train Epoch: [2120/2467]	Loss: 0.273270    
 Train Epoch: [2120/2467]	Loss: 0.641752
 Train Epoch: [2120/2467]	Loss: 0.340494
     Train Epoch: [2120/2467]	Loss: 0.260890
     Train Epoch: [2140/2467]	Loss: 0.587925
         Train Epoch: [2140/2467]	Loss: 0.651168
 Train Epoch: [2140/2467]	Loss: 0.591858
     Train Epoch: [2140/2467]	Loss: 0.702276
     Train Epoch: [2160/2467]	Loss: 0.356443
     Train Epoch: [2160/2467]	Loss: 0.349664
     Train Epoch: [2160/2467]	Loss: 0.763468
     Train Epoch: [2160/2467]	Loss: 0.428778
          Train Epoch: [2180/2467]	Loss: 0.371288Train Epoch: [2180/2467]	Loss: 0.525660

     Train Epoch: [2180/2467]	Loss: 0.560089
     Train Epoch: [2180/2467]	Loss: 0.254048
              Train Epoch: [2200/2467]	Loss: 0.768744Train Epoch: [2200/2467]	Loss: 0.243875

 Train Epoch: [2200/2467]	Loss: 0.838598
     Train Epoch: [2200/2467]	Loss: 0.364874
     Train Epoch: [2220/2467]	Loss: 0.545990    
     Train Epoch: [2220/2467]	Loss: 0.531869
     Train Epoch: [2220/2467]	Loss: 0.227849
 Train Epoch: [2220/2467]	Loss: 0.176720
          Train Epoch: [2240/2467]	Loss: 0.421710Train Epoch: [2240/2467]	Loss: 0.154780
    
 Train Epoch: [2240/2467]	Loss: 0.294730
     Train Epoch: [2240/2467]	Loss: 0.235738
     Train Epoch: [2260/2467]	Loss: 0.810128
     Train Epoch: [2260/2467]	Loss: 0.472839
     Train Epoch: [2260/2467]	Loss: 0.328246
     Train Epoch: [2260/2467]	Loss: 0.372630
     Train Epoch: [2280/2467]	Loss: 0.368494
         Train Epoch: [2280/2467]	Loss: 0.639838
 Train Epoch: [2280/2467]	Loss: 0.368859
     Train Epoch: [2280/2467]	Loss: 0.292053
     Train Epoch: [2300/2467]	Loss: 0.179461
         Train Epoch: [2300/2467]	Loss: 0.224585
 Train Epoch: [2300/2467]	Loss: 0.766747    
 Train Epoch: [2300/2467]	Loss: 0.314015
     Train Epoch: [2320/2467]	Loss: 0.602300
          Train Epoch: [2320/2467]	Loss: 0.410442Train Epoch: [2320/2467]	Loss: 0.178595

     Train Epoch: [2320/2467]	Loss: 0.353490
          Train Epoch: [2340/2467]	Loss: 0.235344Train Epoch: [2340/2467]	Loss: 0.162770
    
     Train Epoch: [2340/2467]	Loss: 0.258891
 Train Epoch: [2340/2467]	Loss: 0.734480
     Train Epoch: [2360/2467]	Loss: 0.177522
     Train Epoch: [2360/2467]	Loss: 0.516241
     Train Epoch: [2360/2467]	Loss: 0.349479
     Train Epoch: [2360/2467]	Loss: 0.568252
     Train Epoch: [2380/2467]	Loss: 0.373008
     Train Epoch: [2380/2467]	Loss: 0.683462
         Train Epoch: [2380/2467]	Loss: 0.563867
 Train Epoch: [2380/2467]	Loss: 0.483769
     Train Epoch: [2400/2467]	Loss: 0.460110
     Train Epoch: [2400/2467]	Loss: 0.393619
     Train Epoch: [2400/2467]	Loss: 0.299105
     Train Epoch: [2400/2467]	Loss: 0.356920
         Train Epoch: [2420/2467]	Loss: 0.486438
 Train Epoch: [2420/2467]	Loss: 0.469598
     Train Epoch: [2420/2467]	Loss: 0.550270
     Train Epoch: [2420/2467]	Loss: 0.384692
     Train Epoch: [2440/2467]	Loss: 0.180658    
         Train Epoch: [2440/2467]	Loss: 0.616846
  Train Epoch: [2440/2467]	Loss: 0.280765Train Epoch: [2440/2467]	Loss: 0.350260

         Train Epoch: [2460/2467]	Loss: 0.305528
 Train Epoch: [2460/2467]	Loss: 0.327487
     Train Epoch: [2460/2467]	Loss: 0.786579
     Train Epoch: [2460/2467]	Loss: 0.436444
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
TOKENIZERS_PARALLELISMTo disable this warning, you can either:
=(true | false)
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 3 epoch =====
     2025-05-11.01-08-42
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 3 epoch =====
     2025-05-11.01-08-43
     ===== running 3 epoch =====
     2025-05-11.01-08-43
     ===== running 3 epoch =====
     2025-05-11.01-08-43
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
after set grad
after prog
start loop
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
         Train Epoch: [0/2467]	Loss: 0.346002
 Train Epoch: [0/2467]	Loss: 0.651336
     Train Epoch: [0/2467]	Loss: 0.432291
     Train Epoch: [0/2467]	Loss: 0.436706
     Train Epoch: [20/2467]	Loss: 0.321350
             Train Epoch: [20/2467]	Loss: 0.569805
 Train Epoch: [20/2467]	Loss: 0.670559
 Train Epoch: [20/2467]	Loss: 0.271304
     Train Epoch: [40/2467]	Loss: 0.457869
     Train Epoch: [40/2467]	Loss: 0.701403
     Train Epoch: [40/2467]	Loss: 0.594760
     Train Epoch: [40/2467]	Loss: 0.335614
          Train Epoch: [60/2467]	Loss: 0.626801
Train Epoch: [60/2467]	Loss: 0.357711    
     Train Epoch: [60/2467]	Loss: 0.197242
 Train Epoch: [60/2467]	Loss: 0.293823
         Train Epoch: [80/2467]	Loss: 0.488606 
Train Epoch: [80/2467]	Loss: 0.412811
     Train Epoch: [80/2467]	Loss: 0.411702
     Train Epoch: [80/2467]	Loss: 0.349808
             Train Epoch: [100/2467]	Loss: 0.711086
  Train Epoch: [100/2467]	Loss: 0.513053Train Epoch: [100/2467]	Loss: 0.525851

     Train Epoch: [100/2467]	Loss: 0.433127
             Train Epoch: [120/2467]	Loss: 0.406493
 Train Epoch: [120/2467]	Loss: 0.107444 
Train Epoch: [120/2467]	Loss: 0.581436
     Train Epoch: [120/2467]	Loss: 0.421661
         Train Epoch: [140/2467]	Loss: 0.419987
 Train Epoch: [140/2467]	Loss: 0.676203
     Train Epoch: [140/2467]	Loss: 0.293941
     Train Epoch: [140/2467]	Loss: 0.229682
             Train Epoch: [160/2467]	Loss: 0.450172    
 Train Epoch: [160/2467]	Loss: 0.372000
 Train Epoch: [160/2467]	Loss: 0.372407 
Train Epoch: [160/2467]	Loss: 0.758935
     Train Epoch: [180/2467]	Loss: 0.282424
     Train Epoch: [180/2467]	Loss: 0.409145
         Train Epoch: [180/2467]	Loss: 0.435991 
Train Epoch: [180/2467]	Loss: 0.422829
              Train Epoch: [200/2467]	Loss: 0.251151
Train Epoch: [200/2467]	Loss: 0.332178
 Train Epoch: [200/2467]	Loss: 0.586643
     Train Epoch: [200/2467]	Loss: 0.557697
     Train Epoch: [220/2467]	Loss: 0.552857
          Train Epoch: [220/2467]	Loss: 0.476536
Train Epoch: [220/2467]	Loss: 0.524769
     Train Epoch: [220/2467]	Loss: 0.385879
         Train Epoch: [240/2467]	Loss: 0.502014 
Train Epoch: [240/2467]	Loss: 0.202282
     Train Epoch: [240/2467]	Loss: 0.138526
     Train Epoch: [240/2467]	Loss: 0.752110
         Train Epoch: [260/2467]	Loss: 0.264601
     Train Epoch: [260/2467]	Loss: 0.323287
 Train Epoch: [260/2467]	Loss: 0.332083
     Train Epoch: [260/2467]	Loss: 0.599787
             Train Epoch: [280/2467]	Loss: 0.404613
 Train Epoch: [280/2467]	Loss: 0.419149
     Train Epoch: [280/2467]	Loss: 0.442053
 Train Epoch: [280/2467]	Loss: 0.552705
     Train Epoch: [300/2467]	Loss: 0.638994    
 Train Epoch: [300/2467]	Loss: 1.197783
     Train Epoch: [300/2467]	Loss: 0.302228
     Train Epoch: [300/2467]	Loss: 0.411007
              Train Epoch: [320/2467]	Loss: 0.359405Train Epoch: [320/2467]	Loss: 0.488655

 Train Epoch: [320/2467]	Loss: 0.645961    
 Train Epoch: [320/2467]	Loss: 0.661904
     Train Epoch: [340/2467]	Loss: 0.286297
         Train Epoch: [340/2467]	Loss: 0.241973
 Train Epoch: [340/2467]	Loss: 0.311483    
 Train Epoch: [340/2467]	Loss: 0.303396
     Train Epoch: [360/2467]	Loss: 0.709090
         Train Epoch: [360/2467]	Loss: 0.505247
 Train Epoch: [360/2467]	Loss: 0.414914
     Train Epoch: [360/2467]	Loss: 0.239699
     Train Epoch: [380/2467]	Loss: 0.584194
             Train Epoch: [380/2467]	Loss: 0.465046
 Train Epoch: [380/2467]	Loss: 0.372805 
Train Epoch: [380/2467]	Loss: 0.514907
     Train Epoch: [400/2467]	Loss: 0.488887
     Train Epoch: [400/2467]	Loss: 0.524127
          Train Epoch: [400/2467]	Loss: 0.365439Train Epoch: [400/2467]	Loss: 0.699066

     Train Epoch: [420/2467]	Loss: 0.504349
         Train Epoch: [420/2467]	Loss: 0.221862
 Train Epoch: [420/2467]	Loss: 0.422379
     Train Epoch: [420/2467]	Loss: 0.363383
          Train Epoch: [440/2467]	Loss: 0.595818
Train Epoch: [440/2467]	Loss: 0.279793
         Train Epoch: [440/2467]	Loss: 0.248603
 Train Epoch: [440/2467]	Loss: 0.303067
         Train Epoch: [460/2467]	Loss: 0.468276
 Train Epoch: [460/2467]	Loss: 0.256589
     Train Epoch: [460/2467]	Loss: 0.461286
     Train Epoch: [460/2467]	Loss: 0.319278
     Train Epoch: [480/2467]	Loss: 0.574472    
 Train Epoch: [480/2467]	Loss: 0.352662
     Train Epoch: [480/2467]	Loss: 0.523112
     Train Epoch: [480/2467]	Loss: 0.300791
                    Train Epoch: [500/2467]	Loss: 0.212344
Train Epoch: [500/2467]	Loss: 0.225197
Train Epoch: [500/2467]	Loss: 0.538146
Train Epoch: [500/2467]	Loss: 0.573537
         Train Epoch: [520/2467]	Loss: 0.521470
 Train Epoch: [520/2467]	Loss: 0.672261
     Train Epoch: [520/2467]	Loss: 0.429526
     Train Epoch: [520/2467]	Loss: 0.208606
         Train Epoch: [540/2467]	Loss: 0.334832
 Train Epoch: [540/2467]	Loss: 0.187672
     Train Epoch: [540/2467]	Loss: 0.346375
     Train Epoch: [540/2467]	Loss: 0.241511
     Train Epoch: [560/2467]	Loss: 0.357819
     Train Epoch: [560/2467]	Loss: 0.302718
     Train Epoch: [560/2467]	Loss: 0.485043
     Train Epoch: [560/2467]	Loss: 0.462067
               Train Epoch: [580/2467]	Loss: 0.265734Train Epoch: [580/2467]	Loss: 0.406522
Train Epoch: [580/2467]	Loss: 0.242463

     Train Epoch: [580/2467]	Loss: 0.577202
              Train Epoch: [600/2467]	Loss: 0.246244
Train Epoch: [600/2467]	Loss: 0.345975    
 Train Epoch: [600/2467]	Loss: 0.269940 
Train Epoch: [600/2467]	Loss: 0.344577
     Train Epoch: [620/2467]	Loss: 0.408192
         Train Epoch: [620/2467]	Loss: 0.149205
 Train Epoch: [620/2467]	Loss: 0.817412
     Train Epoch: [620/2467]	Loss: 0.566816
             Train Epoch: [640/2467]	Loss: 0.542387 
Train Epoch: [640/2467]	Loss: 0.554411 
Train Epoch: [640/2467]	Loss: 0.514639
     Train Epoch: [640/2467]	Loss: 0.387592
     Train Epoch: [660/2467]	Loss: 0.147474
         Train Epoch: [660/2467]	Loss: 0.511342
     Train Epoch: [660/2467]	Loss: 0.501909
 Train Epoch: [660/2467]	Loss: 0.133036
     Train Epoch: [680/2467]	Loss: 0.381956
     Train Epoch: [680/2467]	Loss: 0.538206
         Train Epoch: [680/2467]	Loss: 0.240615 
Train Epoch: [680/2467]	Loss: 0.272623
     Train Epoch: [700/2467]	Loss: 0.311659
     Train Epoch: [700/2467]	Loss: 0.260481
     Train Epoch: [700/2467]	Loss: 0.446949
     Train Epoch: [700/2467]	Loss: 0.429972
          Train Epoch: [720/2467]	Loss: 0.364662Train Epoch: [720/2467]	Loss: 0.516218
    
 Train Epoch: [720/2467]	Loss: 0.276366
     Train Epoch: [720/2467]	Loss: 0.336735
          Train Epoch: [740/2467]	Loss: 0.185038
Train Epoch: [740/2467]	Loss: 0.442294
         Train Epoch: [740/2467]	Loss: 0.294604
 Train Epoch: [740/2467]	Loss: 0.249762
     Train Epoch: [760/2467]	Loss: 0.638318
             Train Epoch: [760/2467]	Loss: 0.540579
 Train Epoch: [760/2467]	Loss: 0.672871
 Train Epoch: [760/2467]	Loss: 0.284131
             Train Epoch: [780/2467]	Loss: 0.562828
 Train Epoch: [780/2467]	Loss: 0.584765
 Train Epoch: [780/2467]	Loss: 0.590044
     Train Epoch: [780/2467]	Loss: 0.502678
             Train Epoch: [800/2467]	Loss: 0.641210
      Train Epoch: [800/2467]	Loss: 0.223358
Train Epoch: [800/2467]	Loss: 0.416408
 Train Epoch: [800/2467]	Loss: 0.469891
     Train Epoch: [820/2467]	Loss: 0.452244
         Train Epoch: [820/2467]	Loss: 0.243871
 Train Epoch: [820/2467]	Loss: 0.581727
     Train Epoch: [820/2467]	Loss: 0.204937
         Train Epoch: [840/2467]	Loss: 0.610368
     Train Epoch: [840/2467]	Loss: 0.472906
 Train Epoch: [840/2467]	Loss: 0.490420    
 Train Epoch: [840/2467]	Loss: 0.292717
         Train Epoch: [860/2467]	Loss: 0.347039 
Train Epoch: [860/2467]	Loss: 0.680333
         Train Epoch: [860/2467]	Loss: 0.281516
 Train Epoch: [860/2467]	Loss: 0.359717
          Train Epoch: [880/2467]	Loss: 0.396389Train Epoch: [880/2467]	Loss: 0.314240

         Train Epoch: [880/2467]	Loss: 0.366373 
Train Epoch: [880/2467]	Loss: 0.399831
         Train Epoch: [900/2467]	Loss: 0.516971
     Train Epoch: [900/2467]	Loss: 0.593659
     Train Epoch: [900/2467]	Loss: 0.368803
 Train Epoch: [900/2467]	Loss: 0.595754
     Train Epoch: [920/2467]	Loss: 0.592630
     Train Epoch: [920/2467]	Loss: 0.381752
     Train Epoch: [920/2467]	Loss: 0.577784
     Train Epoch: [920/2467]	Loss: 0.284305
     Train Epoch: [940/2467]	Loss: 0.093755
          Train Epoch: [940/2467]	Loss: 0.450511Train Epoch: [940/2467]	Loss: 0.371922

     Train Epoch: [940/2467]	Loss: 0.604799
         Train Epoch: [960/2467]	Loss: 0.124950
     Train Epoch: [960/2467]	Loss: 0.500688
     Train Epoch: [960/2467]	Loss: 0.210432
 Train Epoch: [960/2467]	Loss: 0.312059
     Train Epoch: [980/2467]	Loss: 0.443867
         Train Epoch: [980/2467]	Loss: 0.261236
     Train Epoch: [980/2467]	Loss: 0.527551
 Train Epoch: [980/2467]	Loss: 0.503152
          Train Epoch: [1000/2467]	Loss: 0.425843Train Epoch: [1000/2467]	Loss: 0.596007

     Train Epoch: [1000/2467]	Loss: 0.161398
     Train Epoch: [1000/2467]	Loss: 0.281763
         Train Epoch: [1020/2467]	Loss: 0.706973     
Train Epoch: [1020/2467]	Loss: 0.448111
 Train Epoch: [1020/2467]	Loss: 0.616717
     Train Epoch: [1020/2467]	Loss: 0.124614
     Train Epoch: [1040/2467]	Loss: 0.556445
         Train Epoch: [1040/2467]	Loss: 0.144766
 Train Epoch: [1040/2467]	Loss: 0.372562
     Train Epoch: [1040/2467]	Loss: 0.387965
         Train Epoch: [1060/2467]	Loss: 0.552764
     Train Epoch: [1060/2467]	Loss: 0.578891
     Train Epoch: [1060/2467]	Loss: 0.283687
 Train Epoch: [1060/2467]	Loss: 0.663427
     Train Epoch: [1080/2467]	Loss: 0.172941    
     Train Epoch: [1080/2467]	Loss: 0.475036 
Train Epoch: [1080/2467]	Loss: 0.426378
     Train Epoch: [1080/2467]	Loss: 0.554941
     Train Epoch: [1100/2467]	Loss: 0.601839
          Train Epoch: [1100/2467]	Loss: 0.390743
Train Epoch: [1100/2467]	Loss: 0.448111
     Train Epoch: [1100/2467]	Loss: 0.499992
             Train Epoch: [1120/2467]	Loss: 0.251451
  Train Epoch: [1120/2467]	Loss: 0.631592Train Epoch: [1120/2467]	Loss: 0.396449

     Train Epoch: [1120/2467]	Loss: 0.281840
     Train Epoch: [1140/2467]	Loss: 0.190969
             Train Epoch: [1140/2467]	Loss: 0.551612  
Train Epoch: [1140/2467]	Loss: 0.295458Train Epoch: [1140/2467]	Loss: 0.481818

         Train Epoch: [1160/2467]	Loss: 0.352105
     Train Epoch: [1160/2467]	Loss: 0.151378
 Train Epoch: [1160/2467]	Loss: 0.308109
     Train Epoch: [1160/2467]	Loss: 0.398635
     Train Epoch: [1180/2467]	Loss: 0.177528
     Train Epoch: [1180/2467]	Loss: 0.496557
         Train Epoch: [1180/2467]	Loss: 0.246475
 Train Epoch: [1180/2467]	Loss: 0.554460
     Train Epoch: [1200/2467]	Loss: 0.429151
         Train Epoch: [1200/2467]	Loss: 0.486032
 Train Epoch: [1200/2467]	Loss: 0.508188
     Train Epoch: [1200/2467]	Loss: 0.468618
         Train Epoch: [1220/2467]	Loss: 0.531021 
Train Epoch: [1220/2467]	Loss: 0.433462
     Train Epoch: [1220/2467]	Loss: 0.803898
     Train Epoch: [1220/2467]	Loss: 0.449452
     Train Epoch: [1240/2467]	Loss: 0.565068
              Train Epoch: [1240/2467]	Loss: 0.757375Train Epoch: [1240/2467]	Loss: 0.474255

 Train Epoch: [1240/2467]	Loss: 0.502880
         Train Epoch: [1260/2467]	Loss: 0.212655 
Train Epoch: [1260/2467]	Loss: 0.216605
     Train Epoch: [1260/2467]	Loss: 0.197142
     Train Epoch: [1260/2467]	Loss: 0.595543
             Train Epoch: [1280/2467]	Loss: 1.105401
 Train Epoch: [1280/2467]	Loss: 0.190378
     Train Epoch: [1280/2467]	Loss: 0.519176
 Train Epoch: [1280/2467]	Loss: 0.581891
              Train Epoch: [1300/2467]	Loss: 0.261477Train Epoch: [1300/2467]	Loss: 0.195152

 Train Epoch: [1300/2467]	Loss: 0.554165
     Train Epoch: [1300/2467]	Loss: 0.754253
     Train Epoch: [1320/2467]	Loss: 0.445546
     Train Epoch: [1320/2467]	Loss: 0.414864
     Train Epoch: [1320/2467]	Loss: 0.551670
     Train Epoch: [1320/2467]	Loss: 0.388847
     Train Epoch: [1340/2467]	Loss: 0.564779    
 Train Epoch: [1340/2467]	Loss: 0.152893
     Train Epoch: [1340/2467]	Loss: 0.371963
     Train Epoch: [1340/2467]	Loss: 0.374569
         Train Epoch: [1360/2467]	Loss: 0.413642
     Train Epoch: [1360/2467]	Loss: 0.171131
 Train Epoch: [1360/2467]	Loss: 0.507416
     Train Epoch: [1360/2467]	Loss: 0.316982
     Train Epoch: [1380/2467]	Loss: 0.746470
             Train Epoch: [1380/2467]	Loss: 0.436098
  Train Epoch: [1380/2467]	Loss: 0.750261Train Epoch: [1380/2467]	Loss: 0.522794

         Train Epoch: [1400/2467]	Loss: 0.373903
 Train Epoch: [1400/2467]	Loss: 0.357979    
      Train Epoch: [1400/2467]	Loss: 0.394401Train Epoch: [1400/2467]	Loss: 0.394549

          Train Epoch: [1420/2467]	Loss: 0.714057
Train Epoch: [1420/2467]	Loss: 0.320772
     Train Epoch: [1420/2467]	Loss: 0.310822
     Train Epoch: [1420/2467]	Loss: 0.478067
              Train Epoch: [1440/2467]	Loss: 0.309208Train Epoch: [1440/2467]	Loss: 0.170854

 Train Epoch: [1440/2467]	Loss: 0.082257
     Train Epoch: [1440/2467]	Loss: 0.621852
         Train Epoch: [1460/2467]	Loss: 0.349978
 Train Epoch: [1460/2467]	Loss: 0.233658    
     Train Epoch: [1460/2467]	Loss: 0.104609
 Train Epoch: [1460/2467]	Loss: 0.488805
              Train Epoch: [1480/2467]	Loss: 0.423251    Train Epoch: [1480/2467]	Loss: 0.507121

 Train Epoch: [1480/2467]	Loss: 0.248504 
Train Epoch: [1480/2467]	Loss: 0.523909
              Train Epoch: [1500/2467]	Loss: 0.229772Train Epoch: [1500/2467]	Loss: 0.280518

 Train Epoch: [1500/2467]	Loss: 0.168240
     Train Epoch: [1500/2467]	Loss: 0.472882
              Train Epoch: [1520/2467]	Loss: 0.186267Train Epoch: [1520/2467]	Loss: 0.512271

     Train Epoch: [1520/2467]	Loss: 0.479246
 Train Epoch: [1520/2467]	Loss: 0.362240
     Train Epoch: [1540/2467]	Loss: 0.342033
     Train Epoch: [1540/2467]	Loss: 0.721841
     Train Epoch: [1540/2467]	Loss: 0.413406
     Train Epoch: [1540/2467]	Loss: 0.338781
             Train Epoch: [1560/2467]	Loss: 0.312259
 Train Epoch: [1560/2467]	Loss: 0.680007
 Train Epoch: [1560/2467]	Loss: 0.515047
     Train Epoch: [1560/2467]	Loss: 0.672196
          Train Epoch: [1580/2467]	Loss: 0.477623Train Epoch: [1580/2467]	Loss: 0.410086

         Train Epoch: [1580/2467]	Loss: 0.622728
 Train Epoch: [1580/2467]	Loss: 0.496917
     Train Epoch: [1600/2467]	Loss: 0.503527    
     Train Epoch: [1600/2467]	Loss: 0.592298
 Train Epoch: [1600/2467]	Loss: 0.398736
     Train Epoch: [1600/2467]	Loss: 0.493177
     Train Epoch: [1620/2467]	Loss: 0.152606
     Train Epoch: [1620/2467]	Loss: 0.261988
     Train Epoch: [1620/2467]	Loss: 0.324683
     Train Epoch: [1620/2467]	Loss: 0.346752
     Train Epoch: [1640/2467]	Loss: 0.392225    
     Train Epoch: [1640/2467]	Loss: 0.356046
 Train Epoch: [1640/2467]	Loss: 0.333385
     Train Epoch: [1640/2467]	Loss: 0.436159
     Train Epoch: [1660/2467]	Loss: 0.505375
     Train Epoch: [1660/2467]	Loss: 0.411221
     Train Epoch: [1660/2467]	Loss: 0.210560
     Train Epoch: [1660/2467]	Loss: 0.198163
         Train Epoch: [1680/2467]	Loss: 0.194275
     Train Epoch: [1680/2467]	Loss: 0.463994
 Train Epoch: [1680/2467]	Loss: 0.880346
     Train Epoch: [1680/2467]	Loss: 0.291454
     Train Epoch: [1700/2467]	Loss: 0.311669
          Train Epoch: [1700/2467]	Loss: 0.380918Train Epoch: [1700/2467]	Loss: 0.455238

     Train Epoch: [1700/2467]	Loss: 0.480906
     Train Epoch: [1720/2467]	Loss: 0.461209
     Train Epoch: [1720/2467]	Loss: 0.503522
     Train Epoch: [1720/2467]	Loss: 0.386994
     Train Epoch: [1720/2467]	Loss: 0.411154
     Train Epoch: [1740/2467]	Loss: 0.192656        
      Train Epoch: [1740/2467]	Loss: 0.531978Train Epoch: [1740/2467]	Loss: 0.189682

 Train Epoch: [1740/2467]	Loss: 0.282101
     Train Epoch: [1760/2467]	Loss: 0.388999
     Train Epoch: [1760/2467]	Loss: 0.221475    
     Train Epoch: [1760/2467]	Loss: 0.320393
 Train Epoch: [1760/2467]	Loss: 0.284164
         Train Epoch: [1780/2467]	Loss: 0.432014 
Train Epoch: [1780/2467]	Loss: 0.205968
     Train Epoch: [1780/2467]	Loss: 0.403179
     Train Epoch: [1780/2467]	Loss: 0.392302
     Train Epoch: [1800/2467]	Loss: 0.260104
     Train Epoch: [1800/2467]	Loss: 0.524659
     Train Epoch: [1800/2467]	Loss: 0.459428
     Train Epoch: [1800/2467]	Loss: 0.924199
     Train Epoch: [1820/2467]	Loss: 0.308401    
 Train Epoch: [1820/2467]	Loss: 0.582421
         Train Epoch: [1820/2467]	Loss: 0.619169
 Train Epoch: [1820/2467]	Loss: 0.169618
     Train Epoch: [1840/2467]	Loss: 0.348730    
     Train Epoch: [1840/2467]	Loss: 0.172104
 Train Epoch: [1840/2467]	Loss: 0.601670
     Train Epoch: [1840/2467]	Loss: 0.281263
         Train Epoch: [1860/2467]	Loss: 0.364635
 Train Epoch: [1860/2467]	Loss: 0.449464
     Train Epoch: [1860/2467]	Loss: 0.266079
     Train Epoch: [1860/2467]	Loss: 0.225247
     Train Epoch: [1880/2467]	Loss: 0.226004
         Train Epoch: [1880/2467]	Loss: 0.345795
 Train Epoch: [1880/2467]	Loss: 0.251199
     Train Epoch: [1880/2467]	Loss: 0.168787
         Train Epoch: [1900/2467]	Loss: 0.321606
 Train Epoch: [1900/2467]	Loss: 0.657966
         Train Epoch: [1900/2467]	Loss: 0.360193
 Train Epoch: [1900/2467]	Loss: 0.425044
          Train Epoch: [1920/2467]	Loss: 0.071263Train Epoch: [1920/2467]	Loss: 0.320947

          Train Epoch: [1920/2467]	Loss: 0.406701Train Epoch: [1920/2467]	Loss: 0.441623

     Train Epoch: [1940/2467]	Loss: 0.342167
             Train Epoch: [1940/2467]	Loss: 0.222984
 Train Epoch: [1940/2467]	Loss: 0.534179
 Train Epoch: [1940/2467]	Loss: 0.246175
     Train Epoch: [1960/2467]	Loss: 0.437190
          Train Epoch: [1960/2467]	Loss: 0.275357
Train Epoch: [1960/2467]	Loss: 0.403538
     Train Epoch: [1960/2467]	Loss: 0.364474
         Train Epoch: [1980/2467]	Loss: 0.422513
     Train Epoch: [1980/2467]	Loss: 0.649217
 Train Epoch: [1980/2467]	Loss: 0.286682
     Train Epoch: [1980/2467]	Loss: 0.525715
         Train Epoch: [2000/2467]	Loss: 0.380964    
 Train Epoch: [2000/2467]	Loss: 0.246891 
Train Epoch: [2000/2467]	Loss: 0.323559
     Train Epoch: [2000/2467]	Loss: 0.619206
          Train Epoch: [2020/2467]	Loss: 0.413265Train Epoch: [2020/2467]	Loss: 0.711035

     Train Epoch: [2020/2467]	Loss: 0.607353
     Train Epoch: [2020/2467]	Loss: 0.346844
     Train Epoch: [2040/2467]	Loss: 0.663789
     Train Epoch: [2040/2467]	Loss: 0.405045
     Train Epoch: [2040/2467]	Loss: 0.440166    
 Train Epoch: [2040/2467]	Loss: 0.167143
     Train Epoch: [2060/2467]	Loss: 0.388864
         Train Epoch: [2060/2467]	Loss: 0.360092
     Train Epoch: [2060/2467]	Loss: 0.424698
 Train Epoch: [2060/2467]	Loss: 0.750699
         Train Epoch: [2080/2467]	Loss: 0.464302    
 Train Epoch: [2080/2467]	Loss: 0.428999
 Train Epoch: [2080/2467]	Loss: 0.293547
     Train Epoch: [2080/2467]	Loss: 0.213002
         Train Epoch: [2100/2467]	Loss: 0.650559 
Train Epoch: [2100/2467]	Loss: 0.269072    
     Train Epoch: [2100/2467]	Loss: 0.345570
 Train Epoch: [2100/2467]	Loss: 0.508388
               Train Epoch: [2120/2467]	Loss: 0.564615Train Epoch: [2120/2467]	Loss: 0.370601Train Epoch: [2120/2467]	Loss: 0.262714


     Train Epoch: [2120/2467]	Loss: 0.217246
         Train Epoch: [2140/2467]	Loss: 0.492990
 Train Epoch: [2140/2467]	Loss: 0.639872
         Train Epoch: [2140/2467]	Loss: 0.641195
 Train Epoch: [2140/2467]	Loss: 0.555520
               Train Epoch: [2160/2467]	Loss: 0.633365Train Epoch: [2160/2467]	Loss: 0.266733Train Epoch: [2160/2467]	Loss: 0.254717


     Train Epoch: [2160/2467]	Loss: 0.333468
     Train Epoch: [2180/2467]	Loss: 0.432603    
 Train Epoch: [2180/2467]	Loss: 0.511302
     Train Epoch: [2180/2467]	Loss: 0.240451
     Train Epoch: [2180/2467]	Loss: 0.353859
     Train Epoch: [2200/2467]	Loss: 0.214490
     Train Epoch: [2200/2467]	Loss: 0.973457
     Train Epoch: [2200/2467]	Loss: 0.616879
     Train Epoch: [2200/2467]	Loss: 0.327170
             Train Epoch: [2220/2467]	Loss: 0.159589 
 Train Epoch: [2220/2467]	Loss: 0.460135Train Epoch: [2220/2467]	Loss: 0.312070

     Train Epoch: [2220/2467]	Loss: 0.165286
         Train Epoch: [2240/2467]	Loss: 0.391539
     Train Epoch: [2240/2467]	Loss: 0.139447
 Train Epoch: [2240/2467]	Loss: 0.324942
     Train Epoch: [2240/2467]	Loss: 0.204324
     Train Epoch: [2260/2467]	Loss: 0.832041    
 Train Epoch: [2260/2467]	Loss: 0.471932
     Train Epoch: [2260/2467]	Loss: 0.197195
     Train Epoch: [2260/2467]	Loss: 0.361437
     Train Epoch: [2280/2467]	Loss: 0.401107    
 Train Epoch: [2280/2467]	Loss: 0.333702
          Train Epoch: [2280/2467]	Loss: 0.330097Train Epoch: [2280/2467]	Loss: 0.349986

     Train Epoch: [2300/2467]	Loss: 0.146991
     Train Epoch: [2300/2467]	Loss: 0.185268
         Train Epoch: [2300/2467]	Loss: 0.241544
 Train Epoch: [2300/2467]	Loss: 0.614972
         Train Epoch: [2320/2467]	Loss: 0.420297
 Train Epoch: [2320/2467]	Loss: 0.196833
     Train Epoch: [2320/2467]	Loss: 0.311162
     Train Epoch: [2320/2467]	Loss: 0.361199
     Train Epoch: [2340/2467]	Loss: 0.138141    
 Train Epoch: [2340/2467]	Loss: 0.265845
         Train Epoch: [2340/2467]	Loss: 0.186534
 Train Epoch: [2340/2467]	Loss: 0.691288
             Train Epoch: [2360/2467]	Loss: 0.157880
 Train Epoch: [2360/2467]	Loss: 0.496300
 Train Epoch: [2360/2467]	Loss: 0.460522
     Train Epoch: [2360/2467]	Loss: 0.341253
     Train Epoch: [2380/2467]	Loss: 0.414262
         Train Epoch: [2380/2467]	Loss: 0.646654
 Train Epoch: [2380/2467]	Loss: 0.265086
     Train Epoch: [2380/2467]	Loss: 0.493416
             Train Epoch: [2400/2467]	Loss: 0.457096
  Train Epoch: [2400/2467]	Loss: 0.358660
Train Epoch: [2400/2467]	Loss: 0.251888
     Train Epoch: [2400/2467]	Loss: 0.355039
         Train Epoch: [2420/2467]	Loss: 0.387131    
 Train Epoch: [2420/2467]	Loss: 0.488942 
    Train Epoch: [2420/2467]	Loss: 0.371460
 Train Epoch: [2420/2467]	Loss: 0.447308
         Train Epoch: [2440/2467]	Loss: 0.273506
 Train Epoch: [2440/2467]	Loss: 0.553629
     Train Epoch: [2440/2467]	Loss: 0.174640
     Train Epoch: [2440/2467]	Loss: 0.242714
             Train Epoch: [2460/2467]	Loss: 0.273879
  Train Epoch: [2460/2467]	Loss: 0.253602
Train Epoch: [2460/2467]	Loss: 0.281579
     Train Epoch: [2460/2467]	Loss: 0.764458
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 4 epoch =====
     2025-05-11.01-30-32
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 4 epoch =====
     2025-05-11.01-30-32
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 4 epoch =====
     2025-05-11.01-30-32
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 4 epoch =====
     2025-05-11.01-30-32
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
         Train Epoch: [0/2467]	Loss: 0.729245 
Train Epoch: [0/2467]	Loss: 0.465517
          Train Epoch: [0/2467]	Loss: 0.377832
Train Epoch: [0/2467]	Loss: 0.285528
          Train Epoch: [20/2467]	Loss: 0.223172Train Epoch: [20/2467]	Loss: 0.504644

     Train Epoch: [20/2467]	Loss: 0.582041
     Train Epoch: [20/2467]	Loss: 0.249364
         Train Epoch: [40/2467]	Loss: 0.532111
 Train Epoch: [40/2467]	Loss: 0.445555
         Train Epoch: [40/2467]	Loss: 0.714565
 Train Epoch: [40/2467]	Loss: 0.284226
         Train Epoch: [60/2467]	Loss: 0.531173
     Train Epoch: [60/2467]	Loss: 0.326400
 Train Epoch: [60/2467]	Loss: 0.159938
     Train Epoch: [60/2467]	Loss: 0.306348
             Train Epoch: [80/2467]	Loss: 0.535344
  Train Epoch: [80/2467]	Loss: 0.311757Train Epoch: [80/2467]	Loss: 0.400670

     Train Epoch: [80/2467]	Loss: 0.368986
     Train Epoch: [100/2467]	Loss: 0.347066
         Train Epoch: [100/2467]	Loss: 0.480120
 Train Epoch: [100/2467]	Loss: 0.532887
     Train Epoch: [100/2467]	Loss: 0.464505
     Train Epoch: [120/2467]	Loss: 0.244531
         Train Epoch: [120/2467]	Loss: 0.589245 
Train Epoch: [120/2467]	Loss: 0.092400
     Train Epoch: [120/2467]	Loss: 0.378630
         Train Epoch: [140/2467]	Loss: 0.410373
 Train Epoch: [140/2467]	Loss: 0.203741    
     Train Epoch: [140/2467]	Loss: 0.434427
 Train Epoch: [140/2467]	Loss: 0.189459
         Train Epoch: [160/2467]	Loss: 0.365277 
Train Epoch: [160/2467]	Loss: 0.464758    
 Train Epoch: [160/2467]	Loss: 0.634817
     Train Epoch: [160/2467]	Loss: 0.222768
     Train Epoch: [180/2467]	Loss: 0.367473
         Train Epoch: [180/2467]	Loss: 0.380348
 Train Epoch: [180/2467]	Loss: 0.329581
     Train Epoch: [180/2467]	Loss: 0.243532
     Train Epoch: [200/2467]	Loss: 0.216986
         Train Epoch: [200/2467]	Loss: 0.349325
 Train Epoch: [200/2467]	Loss: 0.513313    
 Train Epoch: [200/2467]	Loss: 0.458243
     Train Epoch: [220/2467]	Loss: 0.339331
     Train Epoch: [220/2467]	Loss: 0.415005
     Train Epoch: [220/2467]	Loss: 0.335625
     Train Epoch: [220/2467]	Loss: 0.454917
         Train Epoch: [240/2467]	Loss: 0.442024
 Train Epoch: [240/2467]	Loss: 0.116442
     Train Epoch: [240/2467]	Loss: 0.551614
     Train Epoch: [240/2467]	Loss: 0.104860
     Train Epoch: [260/2467]	Loss: 0.165156
     Train Epoch: [260/2467]	Loss: 0.303778
     Train Epoch: [260/2467]	Loss: 0.390984
     Train Epoch: [260/2467]	Loss: 0.230492
     Train Epoch: [280/2467]	Loss: 0.354914    
     Train Epoch: [280/2467]	Loss: 0.415961
 Train Epoch: [280/2467]	Loss: 0.221024
     Train Epoch: [280/2467]	Loss: 0.502859
         Train Epoch: [300/2467]	Loss: 0.604005
 Train Epoch: [300/2467]	Loss: 0.310164    
 Train Epoch: [300/2467]	Loss: 0.777053    
 Train Epoch: [300/2467]	Loss: 0.226351
          Train Epoch: [320/2467]	Loss: 0.303390Train Epoch: [320/2467]	Loss: 0.416951

     Train Epoch: [320/2467]	Loss: 0.634365
     Train Epoch: [320/2467]	Loss: 0.539155
     Train Epoch: [340/2467]	Loss: 0.194969
     Train Epoch: [340/2467]	Loss: 0.278226
     Train Epoch: [340/2467]	Loss: 0.183768
     Train Epoch: [340/2467]	Loss: 0.263349
         Train Epoch: [360/2467]	Loss: 0.402971
 Train Epoch: [360/2467]	Loss: 0.273600
     Train Epoch: [360/2467]	Loss: 0.302527
     Train Epoch: [360/2467]	Loss: 0.220514
         Train Epoch: [380/2467]	Loss: 0.647597
 Train Epoch: [380/2467]	Loss: 0.373154
         Train Epoch: [380/2467]	Loss: 0.419400
 Train Epoch: [380/2467]	Loss: 0.640896
         Train Epoch: [400/2467]	Loss: 0.546375
 Train Epoch: [400/2467]	Loss: 0.243578    
 Train Epoch: [400/2467]	Loss: 0.720564
     Train Epoch: [400/2467]	Loss: 0.339142
     Train Epoch: [420/2467]	Loss: 0.425085
     Train Epoch: [420/2467]	Loss: 0.181853
         Train Epoch: [420/2467]	Loss: 0.454139
 Train Epoch: [420/2467]	Loss: 0.362837
          Train Epoch: [440/2467]	Loss: 0.570917Train Epoch: [440/2467]	Loss: 0.244544

     Train Epoch: [440/2467]	Loss: 0.173773
     Train Epoch: [440/2467]	Loss: 0.234057
         Train Epoch: [460/2467]	Loss: 0.220752    
 Train Epoch: [460/2467]	Loss: 0.513168
 Train Epoch: [460/2467]	Loss: 0.447981
     Train Epoch: [460/2467]	Loss: 0.253979
         Train Epoch: [480/2467]	Loss: 0.507091
 Train Epoch: [480/2467]	Loss: 0.409484
     Train Epoch: [480/2467]	Loss: 0.442736
     Train Epoch: [480/2467]	Loss: 0.287932
         Train Epoch: [500/2467]	Loss: 0.192445
 Train Epoch: [500/2467]	Loss: 0.428844
     Train Epoch: [500/2467]	Loss: 0.189062
     Train Epoch: [500/2467]	Loss: 0.424686
     Train Epoch: [520/2467]	Loss: 0.390266
         Train Epoch: [520/2467]	Loss: 0.418429
     Train Epoch: [520/2467]	Loss: 0.641312
 Train Epoch: [520/2467]	Loss: 0.182948
         Train Epoch: [540/2467]	Loss: 0.311503
 Train Epoch: [540/2467]	Loss: 0.142959
         Train Epoch: [540/2467]	Loss: 0.199412
 Train Epoch: [540/2467]	Loss: 0.322003
         Train Epoch: [560/2467]	Loss: 0.325491 
Train Epoch: [560/2467]	Loss: 0.277936    
     Train Epoch: [560/2467]	Loss: 0.414967
 Train Epoch: [560/2467]	Loss: 0.424829
     Train Epoch: [580/2467]	Loss: 0.508215
         Train Epoch: [580/2467]	Loss: 0.190984 
Train Epoch: [580/2467]	Loss: 0.252251
     Train Epoch: [580/2467]	Loss: 0.322900
             Train Epoch: [600/2467]	Loss: 0.229072
  Train Epoch: [600/2467]	Loss: 0.228057Train Epoch: [600/2467]	Loss: 0.340717

     Train Epoch: [600/2467]	Loss: 0.250958
         Train Epoch: [620/2467]	Loss: 0.683266
 Train Epoch: [620/2467]	Loss: 0.112308
         Train Epoch: [620/2467]	Loss: 0.516509
 Train Epoch: [620/2467]	Loss: 0.347885
              Train Epoch: [640/2467]	Loss: 0.363209Train Epoch: [640/2467]	Loss: 0.396508

 Train Epoch: [640/2467]	Loss: 0.499087
     Train Epoch: [640/2467]	Loss: 0.286665
          Train Epoch: [660/2467]	Loss: 0.102109    
Train Epoch: [660/2467]	Loss: 0.480303
     Train Epoch: [660/2467]	Loss: 0.094246
 Train Epoch: [660/2467]	Loss: 0.419625
         Train Epoch: [680/2467]	Loss: 0.290917
 Train Epoch: [680/2467]	Loss: 0.222274
          Train Epoch: [680/2467]	Loss: 0.252824Train Epoch: [680/2467]	Loss: 0.478864

     Train Epoch: [700/2467]	Loss: 0.221462
              Train Epoch: [700/2467]	Loss: 0.196268 
Train Epoch: [700/2467]	Loss: 0.413204Train Epoch: [700/2467]	Loss: 0.428741

          Train Epoch: [720/2467]	Loss: 0.245171Train Epoch: [720/2467]	Loss: 0.293411

         Train Epoch: [720/2467]	Loss: 0.291850
 Train Epoch: [720/2467]	Loss: 0.251287
     Train Epoch: [740/2467]	Loss: 0.404592    
     Train Epoch: [740/2467]	Loss: 0.251699
     Train Epoch: [740/2467]	Loss: 0.249937
 Train Epoch: [740/2467]	Loss: 0.163967
     Train Epoch: [760/2467]	Loss: 0.566732
     Train Epoch: [760/2467]	Loss: 0.448290
     Train Epoch: [760/2467]	Loss: 0.610044
     Train Epoch: [760/2467]	Loss: 0.252389
     Train Epoch: [780/2467]	Loss: 0.518982
     Train Epoch: [780/2467]	Loss: 0.438278
     Train Epoch: [780/2467]	Loss: 0.461233
     Train Epoch: [780/2467]	Loss: 0.522673
     Train Epoch: [800/2467]	Loss: 0.434493    
     Train Epoch: [800/2467]	Loss: 0.371659
 Train Epoch: [800/2467]	Loss: 0.704416
     Train Epoch: [800/2467]	Loss: 0.188817
         Train Epoch: [820/2467]	Loss: 0.145654
     Train Epoch: [820/2467]	Loss: 0.422574
     Train Epoch: [820/2467]	Loss: 0.567802
 Train Epoch: [820/2467]	Loss: 0.162889
          Train Epoch: [840/2467]	Loss: 0.629779Train Epoch: [840/2467]	Loss: 0.357118

         Train Epoch: [840/2467]	Loss: 0.490437
 Train Epoch: [840/2467]	Loss: 0.229497
         Train Epoch: [860/2467]	Loss: 0.606074
         Train Epoch: [860/2467]	Loss: 0.311898
 Train Epoch: [860/2467]	Loss: 0.286852
 Train Epoch: [860/2467]	Loss: 0.230330
              Train Epoch: [880/2467]	Loss: 0.311603Train Epoch: [880/2467]	Loss: 0.412198

 Train Epoch: [880/2467]	Loss: 0.353158
     Train Epoch: [880/2467]	Loss: 0.307142
     Train Epoch: [900/2467]	Loss: 0.331065    
 Train Epoch: [900/2467]	Loss: 0.409992
     Train Epoch: [900/2467]	Loss: 0.336896
     Train Epoch: [900/2467]	Loss: 0.255536
         Train Epoch: [920/2467]	Loss: 0.548963    
  Train Epoch: [920/2467]	Loss: 0.585080Train Epoch: [920/2467]	Loss: 0.363635

     Train Epoch: [920/2467]	Loss: 0.234153
     Train Epoch: [940/2467]	Loss: 0.072159
          Train Epoch: [940/2467]	Loss: 0.331374Train Epoch: [940/2467]	Loss: 0.418065

     Train Epoch: [940/2467]	Loss: 0.327932
     Train Epoch: [960/2467]	Loss: 0.430747
          Train Epoch: [960/2467]	Loss: 0.133792Train Epoch: [960/2467]	Loss: 0.237642

     Train Epoch: [960/2467]	Loss: 0.113086
     Train Epoch: [980/2467]	Loss: 0.373378
             Train Epoch: [980/2467]	Loss: 0.273562  
Train Epoch: [980/2467]	Loss: 0.420828Train Epoch: [980/2467]	Loss: 0.458745

     Train Epoch: [1000/2467]	Loss: 0.241155
         Train Epoch: [1000/2467]	Loss: 0.596011
 Train Epoch: [1000/2467]	Loss: 0.129304
     Train Epoch: [1000/2467]	Loss: 0.395631
     Train Epoch: [1020/2467]	Loss: 0.318967
         Train Epoch: [1020/2467]	Loss: 0.508054 
Train Epoch: [1020/2467]	Loss: 0.532027
     Train Epoch: [1020/2467]	Loss: 0.114809
     Train Epoch: [1040/2467]	Loss: 0.474554
     Train Epoch: [1040/2467]	Loss: 0.330125
         Train Epoch: [1040/2467]	Loss: 0.130423
 Train Epoch: [1040/2467]	Loss: 0.386847
             Train Epoch: [1060/2467]	Loss: 0.522744
 Train Epoch: [1060/2467]	Loss: 0.547269 
Train Epoch: [1060/2467]	Loss: 0.303631
     Train Epoch: [1060/2467]	Loss: 0.560618
         Train Epoch: [1080/2467]	Loss: 0.478261
     Train Epoch: [1080/2467]	Loss: 0.159479
     Train Epoch: [1080/2467]	Loss: 0.479928 
Train Epoch: [1080/2467]	Loss: 0.533563
     Train Epoch: [1100/2467]	Loss: 0.553604
     Train Epoch: [1100/2467]	Loss: 0.480742
         Train Epoch: [1100/2467]	Loss: 0.396317 
Train Epoch: [1100/2467]	Loss: 0.325063
         Train Epoch: [1120/2467]	Loss: 0.281522
     Train Epoch: [1120/2467]	Loss: 0.587907
 Train Epoch: [1120/2467]	Loss: 0.286789
     Train Epoch: [1120/2467]	Loss: 0.237924
         Train Epoch: [1140/2467]	Loss: 0.174462
 Train Epoch: [1140/2467]	Loss: 0.367767    
 Train Epoch: [1140/2467]	Loss: 0.230848
     Train Epoch: [1140/2467]	Loss: 0.650546
     Train Epoch: [1160/2467]	Loss: 0.294941
         Train Epoch: [1160/2467]	Loss: 0.158532
 Train Epoch: [1160/2467]	Loss: 0.279403
     Train Epoch: [1160/2467]	Loss: 0.320632
     Train Epoch: [1180/2467]	Loss: 0.159862
         Train Epoch: [1180/2467]	Loss: 0.505317
 Train Epoch: [1180/2467]	Loss: 0.275539
     Train Epoch: [1180/2467]	Loss: 0.454200
         Train Epoch: [1200/2467]	Loss: 0.313142
     Train Epoch: [1200/2467]	Loss: 0.423002
 Train Epoch: [1200/2467]	Loss: 0.303228
     Train Epoch: [1200/2467]	Loss: 0.434404
     Train Epoch: [1220/2467]	Loss: 0.468632
     Train Epoch: [1220/2467]	Loss: 0.276319
     Train Epoch: [1220/2467]	Loss: 0.384488
     Train Epoch: [1220/2467]	Loss: 0.695763
     Train Epoch: [1240/2467]	Loss: 0.492573
         Train Epoch: [1240/2467]	Loss: 1.106123 
Train Epoch: [1240/2467]	Loss: 0.441360
     Train Epoch: [1240/2467]	Loss: 0.435889
         Train Epoch: [1260/2467]	Loss: 0.239697    
 Train Epoch: [1260/2467]	Loss: 0.151924
 Train Epoch: [1260/2467]	Loss: 0.158937
     Train Epoch: [1260/2467]	Loss: 0.590270
         Train Epoch: [1280/2467]	Loss: 0.203711
 Train Epoch: [1280/2467]	Loss: 0.949696
          Train Epoch: [1280/2467]	Loss: 0.567960
Train Epoch: [1280/2467]	Loss: 0.500401
     Train Epoch: [1300/2467]	Loss: 0.168473    
     Train Epoch: [1300/2467]	Loss: 0.238017
 Train Epoch: [1300/2467]	Loss: 0.727483
     Train Epoch: [1300/2467]	Loss: 0.651280
         Train Epoch: [1320/2467]	Loss: 0.264138    
  Train Epoch: [1320/2467]	Loss: 0.407965Train Epoch: [1320/2467]	Loss: 0.343800

     Train Epoch: [1320/2467]	Loss: 0.547660
          Train Epoch: [1340/2467]	Loss: 0.482347Train Epoch: [1340/2467]	Loss: 0.334555

     Train Epoch: [1340/2467]	Loss: 0.161397
     Train Epoch: [1340/2467]	Loss: 0.317476
         Train Epoch: [1360/2467]	Loss: 0.173204 
        Train Epoch: [1360/2467]	Loss: 0.358661 
Train Epoch: [1360/2467]	Loss: 0.635407
 Train Epoch: [1360/2467]	Loss: 0.205068
         Train Epoch: [1380/2467]	Loss: 0.376242
 Train Epoch: [1380/2467]	Loss: 0.877240
          Train Epoch: [1380/2467]	Loss: 0.386335Train Epoch: [1380/2467]	Loss: 0.974274

         Train Epoch: [1400/2467]	Loss: 0.282312
 Train Epoch: [1400/2467]	Loss: 0.344562
     Train Epoch: [1400/2467]	Loss: 0.415368
     Train Epoch: [1400/2467]	Loss: 0.258845
     Train Epoch: [1420/2467]	Loss: 0.301268
         Train Epoch: [1420/2467]	Loss: 0.729592
 Train Epoch: [1420/2467]	Loss: 0.307067
     Train Epoch: [1420/2467]	Loss: 0.329714
     Train Epoch: [1440/2467]	Loss: 0.053734    
         Train Epoch: [1440/2467]	Loss: 0.359581
 Train Epoch: [1440/2467]	Loss: 0.121455
 Train Epoch: [1440/2467]	Loss: 0.240845
     Train Epoch: [1460/2467]	Loss: 0.392958
     Train Epoch: [1460/2467]	Loss: 0.203948
         Train Epoch: [1460/2467]	Loss: 0.106830
 Train Epoch: [1460/2467]	Loss: 0.375523
             Train Epoch: [1480/2467]	Loss: 0.498857
 Train Epoch: [1480/2467]	Loss: 0.439935
 Train Epoch: [1480/2467]	Loss: 0.251266    
 Train Epoch: [1480/2467]	Loss: 0.128075
         Train Epoch: [1500/2467]	Loss: 0.502010
 Train Epoch: [1500/2467]	Loss: 0.156304
     Train Epoch: [1500/2467]	Loss: 0.085890
     Train Epoch: [1500/2467]	Loss: 0.281381
         Train Epoch: [1520/2467]	Loss: 0.648194
 Train Epoch: [1520/2467]	Loss: 0.355621
         Train Epoch: [1520/2467]	Loss: 0.190452
 Train Epoch: [1520/2467]	Loss: 0.404726
     Train Epoch: [1540/2467]	Loss: 0.230062
         Train Epoch: [1540/2467]	Loss: 0.416763
 Train Epoch: [1540/2467]	Loss: 1.060219
     Train Epoch: [1540/2467]	Loss: 0.298126
     Train Epoch: [1560/2467]	Loss: 0.322614
         Train Epoch: [1560/2467]	Loss: 0.768132
 Train Epoch: [1560/2467]	Loss: 0.466149
     Train Epoch: [1560/2467]	Loss: 0.532382
         Train Epoch: [1580/2467]	Loss: 0.525988
 Train Epoch: [1580/2467]	Loss: 0.467013
     Train Epoch: [1580/2467]	Loss: 0.433285
     Train Epoch: [1580/2467]	Loss: 0.366172
     Train Epoch: [1600/2467]	Loss: 0.372188
     Train Epoch: [1600/2467]	Loss: 0.275768
     Train Epoch: [1600/2467]	Loss: 0.529014
     Train Epoch: [1600/2467]	Loss: 0.189879
     Train Epoch: [1620/2467]	Loss: 0.149986
     Train Epoch: [1620/2467]	Loss: 0.385341
         Train Epoch: [1620/2467]	Loss: 0.327172
 Train Epoch: [1620/2467]	Loss: 0.335702
     Train Epoch: [1640/2467]	Loss: 0.400263
         Train Epoch: [1640/2467]	Loss: 0.346897 
Train Epoch: [1640/2467]	Loss: 0.272694
     Train Epoch: [1640/2467]	Loss: 0.360551
          Train Epoch: [1660/2467]	Loss: 0.352830Train Epoch: [1660/2467]	Loss: 0.197033

     Train Epoch: [1660/2467]	Loss: 0.158100
     Train Epoch: [1660/2467]	Loss: 0.494060
         Train Epoch: [1680/2467]	Loss: 0.655104
 Train Epoch: [1680/2467]	Loss: 0.483879
         Train Epoch: [1680/2467]	Loss: 0.296649
 Train Epoch: [1680/2467]	Loss: 0.183512
     Train Epoch: [1700/2467]	Loss: 0.322517    
     Train Epoch: [1700/2467]	Loss: 0.294309
     Train Epoch: [1700/2467]	Loss: 0.402872
 Train Epoch: [1700/2467]	Loss: 0.396168
     Train Epoch: [1720/2467]	Loss: 0.361532
     Train Epoch: [1720/2467]	Loss: 0.596898
     Train Epoch: [1720/2467]	Loss: 0.343840
     Train Epoch: [1720/2467]	Loss: 0.419132
     Train Epoch: [1740/2467]	Loss: 0.307728
         Train Epoch: [1740/2467]	Loss: 0.187972
 Train Epoch: [1740/2467]	Loss: 0.472999
     Train Epoch: [1740/2467]	Loss: 0.256396
         Train Epoch: [1760/2467]	Loss: 0.268685
     Train Epoch: [1760/2467]	Loss: 0.293337
 Train Epoch: [1760/2467]	Loss: 0.275539
     Train Epoch: [1760/2467]	Loss: 0.222005
              Train Epoch: [1780/2467]	Loss: 0.368933Train Epoch: [1780/2467]	Loss: 0.426752 

Train Epoch: [1780/2467]	Loss: 0.167110
     Train Epoch: [1780/2467]	Loss: 0.371478
         Train Epoch: [1800/2467]	Loss: 0.262460
 Train Epoch: [1800/2467]	Loss: 0.994273
     Train Epoch: [1800/2467]	Loss: 0.417316
     Train Epoch: [1800/2467]	Loss: 0.487192
     Train Epoch: [1820/2467]	Loss: 0.269137
     Train Epoch: [1820/2467]	Loss: 0.146711
     Train Epoch: [1820/2467]	Loss: 0.528423
     Train Epoch: [1820/2467]	Loss: 0.529735
         Train Epoch: [1840/2467]	Loss: 0.613131
     Train Epoch: [1840/2467]	Loss: 0.260755
 Train Epoch: [1840/2467]	Loss: 0.315647    
 Train Epoch: [1840/2467]	Loss: 0.168677
         Train Epoch: [1860/2467]	Loss: 0.300884
 Train Epoch: [1860/2467]	Loss: 0.230917    
 Train Epoch: [1860/2467]	Loss: 0.414051
     Train Epoch: [1860/2467]	Loss: 0.228579
         Train Epoch: [1880/2467]	Loss: 0.151320
 Train Epoch: [1880/2467]	Loss: 0.196457
     Train Epoch: [1880/2467]	Loss: 0.217711
     Train Epoch: [1880/2467]	Loss: 0.275981
     Train Epoch: [1900/2467]	Loss: 0.528322
         Train Epoch: [1900/2467]	Loss: 0.374529 
Train Epoch: [1900/2467]	Loss: 0.343731
     Train Epoch: [1900/2467]	Loss: 0.292512
         Train Epoch: [1920/2467]	Loss: 0.401008
 Train Epoch: [1920/2467]	Loss: 0.052497
     Train Epoch: [1920/2467]	Loss: 0.416894
     Train Epoch: [1920/2467]	Loss: 0.317099
     Train Epoch: [1940/2467]	Loss: 0.385577
     Train Epoch: [1940/2467]	Loss: 0.522157    
 Train Epoch: [1940/2467]	Loss: 0.253870
     Train Epoch: [1940/2467]	Loss: 0.198455
         Train Epoch: [1960/2467]	Loss: 0.369456     
Train Epoch: [1960/2467]	Loss: 0.351060
 Train Epoch: [1960/2467]	Loss: 0.333554
     Train Epoch: [1960/2467]	Loss: 0.322843
         Train Epoch: [1980/2467]	Loss: 0.428691
     Train Epoch: [1980/2467]	Loss: 0.449724    
 Train Epoch: [1980/2467]	Loss: 0.372806 
Train Epoch: [1980/2467]	Loss: 0.335387
     Train Epoch: [2000/2467]	Loss: 0.235710
     Train Epoch: [2000/2467]	Loss: 0.508174    
 Train Epoch: [2000/2467]	Loss: 0.200102
     Train Epoch: [2000/2467]	Loss: 0.275438
     Train Epoch: [2020/2467]	Loss: 0.688167
         Train Epoch: [2020/2467]	Loss: 0.426910 
Train Epoch: [2020/2467]	Loss: 0.521711
     Train Epoch: [2020/2467]	Loss: 0.219606
         Train Epoch: [2040/2467]	Loss: 0.554866
     Train Epoch: [2040/2467]	Loss: 0.319144
     Train Epoch: [2040/2467]	Loss: 0.148623 
Train Epoch: [2040/2467]	Loss: 0.366681
         Train Epoch: [2060/2467]	Loss: 0.351935
 Train Epoch: [2060/2467]	Loss: 0.367571    
 Train Epoch: [2060/2467]	Loss: 0.349772
     Train Epoch: [2060/2467]	Loss: 0.645472
         Train Epoch: [2080/2467]	Loss: 0.180605
 Train Epoch: [2080/2467]	Loss: 0.329415
          Train Epoch: [2080/2467]	Loss: 0.417384
Train Epoch: [2080/2467]	Loss: 0.172296
     Train Epoch: [2100/2467]	Loss: 0.713461
     Train Epoch: [2100/2467]	Loss: 0.288592
     Train Epoch: [2100/2467]	Loss: 0.201723
     Train Epoch: [2100/2467]	Loss: 0.479896
         Train Epoch: [2120/2467]	Loss: 0.500952
     Train Epoch: [2120/2467]	Loss: 0.220926 
Train Epoch: [2120/2467]	Loss: 0.354217
     Train Epoch: [2120/2467]	Loss: 0.201140
         Train Epoch: [2140/2467]	Loss: 0.449953
     Train Epoch: [2140/2467]	Loss: 0.503927
 Train Epoch: [2140/2467]	Loss: 0.688128
     Train Epoch: [2140/2467]	Loss: 0.632921
          Train Epoch: [2160/2467]	Loss: 0.291818Train Epoch: [2160/2467]	Loss: 0.210397

         Train Epoch: [2160/2467]	Loss: 0.257135
 Train Epoch: [2160/2467]	Loss: 0.608291
          Train Epoch: [2180/2467]	Loss: 0.379281Train Epoch: [2180/2467]	Loss: 0.262018

     Train Epoch: [2180/2467]	Loss: 0.439452
     Train Epoch: [2180/2467]	Loss: 0.220647
         Train Epoch: [2200/2467]	Loss: 0.211338
     Train Epoch: [2200/2467]	Loss: 0.458444
     Train Epoch: [2200/2467]	Loss: 0.809307
 Train Epoch: [2200/2467]	Loss: 0.297881
         Train Epoch: [2220/2467]	Loss: 0.341653
 Train Epoch: [2220/2467]	Loss: 0.255983
     Train Epoch: [2220/2467]	Loss: 0.158047
     Train Epoch: [2220/2467]	Loss: 0.141085
     Train Epoch: [2240/2467]	Loss: 0.226193
     Train Epoch: [2240/2467]	Loss: 0.295970
     Train Epoch: [2240/2467]	Loss: 0.163298
     Train Epoch: [2240/2467]	Loss: 0.124142
     Train Epoch: [2260/2467]	Loss: 0.681960    
 Train Epoch: [2260/2467]	Loss: 0.418594
         Train Epoch: [2260/2467]	Loss: 0.213988
 Train Epoch: [2260/2467]	Loss: 0.301502
     Train Epoch: [2280/2467]	Loss: 0.264734
         Train Epoch: [2280/2467]	Loss: 0.278325
 Train Epoch: [2280/2467]	Loss: 0.251844
     Train Epoch: [2280/2467]	Loss: 0.288942
     Train Epoch: [2300/2467]	Loss: 0.131107
         Train Epoch: [2300/2467]	Loss: 0.641087
 Train Epoch: [2300/2467]	Loss: 0.172350
     Train Epoch: [2300/2467]	Loss: 0.211606
         Train Epoch: [2320/2467]	Loss: 0.132842    
  Train Epoch: [2320/2467]	Loss: 0.392780Train Epoch: [2320/2467]	Loss: 0.272705

     Train Epoch: [2320/2467]	Loss: 0.435416
         Train Epoch: [2340/2467]	Loss: 0.189778    
      Train Epoch: [2340/2467]	Loss: 0.143437Train Epoch: [2340/2467]	Loss: 0.693198

 Train Epoch: [2340/2467]	Loss: 0.253638
     Train Epoch: [2360/2467]	Loss: 0.152093
     Train Epoch: [2360/2467]	Loss: 0.513909
     Train Epoch: [2360/2467]	Loss: 0.293511
     Train Epoch: [2360/2467]	Loss: 0.370964
     Train Epoch: [2380/2467]	Loss: 0.372822
     Train Epoch: [2380/2467]	Loss: 0.658567
     Train Epoch: [2380/2467]	Loss: 0.514477
     Train Epoch: [2380/2467]	Loss: 0.340044
              Train Epoch: [2400/2467]	Loss: 0.233140Train Epoch: [2400/2467]	Loss: 0.433149

 Train Epoch: [2400/2467]	Loss: 0.399611
     Train Epoch: [2400/2467]	Loss: 0.326694
         Train Epoch: [2420/2467]	Loss: 0.444041
     Train Epoch: [2420/2467]	Loss: 0.329415
 Train Epoch: [2420/2467]	Loss: 0.341895
     Train Epoch: [2420/2467]	Loss: 0.448816
         Train Epoch: [2440/2467]	Loss: 0.183876
     Train Epoch: [2440/2467]	Loss: 0.172245
 Train Epoch: [2440/2467]	Loss: 0.509112
     Train Epoch: [2440/2467]	Loss: 0.207883
         Train Epoch: [2460/2467]	Loss: 0.255444    
 Train Epoch: [2460/2467]	Loss: 0.181538
     Train Epoch: [2460/2467]	Loss: 0.243259
 Train Epoch: [2460/2467]	Loss: 0.664979
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 5 epoch =====
     2025-05-11.01-52-19
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 5 epoch =====
     2025-05-11.01-52-19
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 5 epoch =====
     2025-05-11.01-52-20
     ===== running 5 epoch =====
     2025-05-11.01-52-20
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
TOKENIZERS_PARALLELISMTo disable this warning, you can either:
=(true | false)
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
             Train Epoch: [0/2467]	Loss: 0.278209
  Train Epoch: [0/2467]	Loss: 0.590954Train Epoch: [0/2467]	Loss: 0.398430

     Train Epoch: [0/2467]	Loss: 0.452222
             Train Epoch: [20/2467]	Loss: 0.226943
 Train Epoch: [20/2467]	Loss: 0.540990
 Train Epoch: [20/2467]	Loss: 0.210284
     Train Epoch: [20/2467]	Loss: 0.443547
     Train Epoch: [40/2467]	Loss: 0.379160
          Train Epoch: [40/2467]	Loss: 0.440542Train Epoch: [40/2467]	Loss: 0.700118

     Train Epoch: [40/2467]	Loss: 0.257146
         Train Epoch: [60/2467]	Loss: 0.322880 
Train Epoch: [60/2467]	Loss: 0.437376
         Train Epoch: [60/2467]	Loss: 0.129101
 Train Epoch: [60/2467]	Loss: 0.285458
         Train Epoch: [80/2467]	Loss: 0.475169 
Train Epoch: [80/2467]	Loss: 0.281519
     Train Epoch: [80/2467]	Loss: 0.324900
     Train Epoch: [80/2467]	Loss: 0.259190
     Train Epoch: [100/2467]	Loss: 0.309714
         Train Epoch: [100/2467]	Loss: 0.436695
 Train Epoch: [100/2467]	Loss: 0.406988
     Train Epoch: [100/2467]	Loss: 0.395124
     Train Epoch: [120/2467]	Loss: 0.512314
     Train Epoch: [120/2467]	Loss: 0.386106
     Train Epoch: [120/2467]	Loss: 0.254070
     Train Epoch: [120/2467]	Loss: 0.089345
         Train Epoch: [140/2467]	Loss: 0.452450
     Train Epoch: [140/2467]	Loss: 0.197163
     Train Epoch: [140/2467]	Loss: 0.347561
 Train Epoch: [140/2467]	Loss: 0.181923
         Train Epoch: [160/2467]	Loss: 0.338203 
Train Epoch: [160/2467]	Loss: 0.397931    
     Train Epoch: [160/2467]	Loss: 0.692538
 Train Epoch: [160/2467]	Loss: 0.239919
          Train Epoch: [180/2467]	Loss: 0.363982Train Epoch: [180/2467]	Loss: 0.474864    

 Train Epoch: [180/2467]	Loss: 0.283057
     Train Epoch: [180/2467]	Loss: 0.234049
         Train Epoch: [200/2467]	Loss: 0.222668     
Train Epoch: [200/2467]	Loss: 0.292901
 Train Epoch: [200/2467]	Loss: 0.268407    
 Train Epoch: [200/2467]	Loss: 0.348977
     Train Epoch: [220/2467]	Loss: 0.327400
         Train Epoch: [220/2467]	Loss: 0.451108
 Train Epoch: [220/2467]	Loss: 0.306017
     Train Epoch: [220/2467]	Loss: 0.485869
     Train Epoch: [240/2467]	Loss: 0.072048
     Train Epoch: [240/2467]	Loss: 0.376640
     Train Epoch: [240/2467]	Loss: 0.510852
     Train Epoch: [240/2467]	Loss: 0.179981
         Train Epoch: [260/2467]	Loss: 0.109852    
 Train Epoch: [260/2467]	Loss: 0.270055 
Train Epoch: [260/2467]	Loss: 0.156340
     Train Epoch: [260/2467]	Loss: 0.327316
          Train Epoch: [280/2467]	Loss: 0.589083Train Epoch: [280/2467]	Loss: 0.450656

     Train Epoch: [280/2467]	Loss: 0.416538
     Train Epoch: [280/2467]	Loss: 0.383059
     Train Epoch: [300/2467]	Loss: 0.281882
          Train Epoch: [300/2467]	Loss: 0.164877
Train Epoch: [300/2467]	Loss: 0.261338
     Train Epoch: [300/2467]	Loss: 0.577740
         Train Epoch: [320/2467]	Loss: 0.285894
     Train Epoch: [320/2467]	Loss: 0.616879
 Train Epoch: [320/2467]	Loss: 0.527475
     Train Epoch: [320/2467]	Loss: 0.492585
     Train Epoch: [340/2467]	Loss: 0.117199
     Train Epoch: [340/2467]	Loss: 0.138870
         Train Epoch: [340/2467]	Loss: 0.216135
 Train Epoch: [340/2467]	Loss: 0.281322
         Train Epoch: [360/2467]	Loss: 0.312784 
Train Epoch: [360/2467]	Loss: 0.379551
          Train Epoch: [360/2467]	Loss: 0.275679Train Epoch: [360/2467]	Loss: 0.218812

             Train Epoch: [380/2467]	Loss: 0.693201
  Train Epoch: [380/2467]	Loss: 0.397724    
Train Epoch: [380/2467]	Loss: 0.369462
 Train Epoch: [380/2467]	Loss: 0.509059
         Train Epoch: [400/2467]	Loss: 0.213764
 Train Epoch: [400/2467]	Loss: 0.439209    
 Train Epoch: [400/2467]	Loss: 0.619536
     Train Epoch: [400/2467]	Loss: 0.265389
         Train Epoch: [420/2467]	Loss: 0.398545
     Train Epoch: [420/2467]	Loss: 0.343083 
Train Epoch: [420/2467]	Loss: 0.169918
     Train Epoch: [420/2467]	Loss: 0.372619
         Train Epoch: [440/2467]	Loss: 0.546268
 Train Epoch: [440/2467]	Loss: 0.188702
          Train Epoch: [440/2467]	Loss: 0.211330Train Epoch: [440/2467]	Loss: 0.148603

         Train Epoch: [460/2467]	Loss: 0.476994
     Train Epoch: [460/2467]	Loss: 0.213818
 Train Epoch: [460/2467]	Loss: 0.417177
     Train Epoch: [460/2467]	Loss: 0.224816
         Train Epoch: [480/2467]	Loss: 0.469490
 Train Epoch: [480/2467]	Loss: 0.362200
     Train Epoch: [480/2467]	Loss: 0.283265
     Train Epoch: [480/2467]	Loss: 0.433693
         Train Epoch: [500/2467]	Loss: 0.190667
 Train Epoch: [500/2467]	Loss: 0.513494
     Train Epoch: [500/2467]	Loss: 0.183080
     Train Epoch: [500/2467]	Loss: 0.399655
          Train Epoch: [520/2467]	Loss: 0.540297Train Epoch: [520/2467]	Loss: 0.374438

     Train Epoch: [520/2467]	Loss: 0.156675
     Train Epoch: [520/2467]	Loss: 0.379395
     Train Epoch: [540/2467]	Loss: 0.281499
               Train Epoch: [540/2467]	Loss: 0.207554Train Epoch: [540/2467]	Loss: 0.106744
Train Epoch: [540/2467]	Loss: 0.320440

     Train Epoch: [560/2467]	Loss: 0.324838    
     Train Epoch: [560/2467]	Loss: 0.368527
 Train Epoch: [560/2467]	Loss: 0.331265
     Train Epoch: [560/2467]	Loss: 0.390685
         Train Epoch: [580/2467]	Loss: 0.234906
         Train Epoch: [580/2467]	Loss: 0.496067
  Train Epoch: [580/2467]	Loss: 0.277622Train Epoch: [580/2467]	Loss: 0.151448

     Train Epoch: [600/2467]	Loss: 0.262345
              Train Epoch: [600/2467]	Loss: 0.183375 
Train Epoch: [600/2467]	Loss: 0.379676Train Epoch: [600/2467]	Loss: 0.285966

         Train Epoch: [620/2467]	Loss: 0.308085
 Train Epoch: [620/2467]	Loss: 0.535707
     Train Epoch: [620/2467]	Loss: 0.466620
     Train Epoch: [620/2467]	Loss: 0.125922
         Train Epoch: [640/2467]	Loss: 0.385814
     Train Epoch: [640/2467]	Loss: 0.287404
 Train Epoch: [640/2467]	Loss: 0.397877
     Train Epoch: [640/2467]	Loss: 0.465359
          Train Epoch: [660/2467]	Loss: 0.445709Train Epoch: [660/2467]	Loss: 0.063805

         Train Epoch: [660/2467]	Loss: 0.099196
 Train Epoch: [660/2467]	Loss: 0.381065
          Train Epoch: [680/2467]	Loss: 0.247695Train Epoch: [680/2467]	Loss: 0.231159

     Train Epoch: [680/2467]	Loss: 0.462149
     Train Epoch: [680/2467]	Loss: 0.170088
     Train Epoch: [700/2467]	Loss: 0.182951    
     Train Epoch: [700/2467]	Loss: 0.370422
 Train Epoch: [700/2467]	Loss: 0.483501
     Train Epoch: [700/2467]	Loss: 0.206351
          Train Epoch: [720/2467]	Loss: 0.262682Train Epoch: [720/2467]	Loss: 0.210933

     Train Epoch: [720/2467]	Loss: 0.323977
     Train Epoch: [720/2467]	Loss: 0.273186
          Train Epoch: [740/2467]	Loss: 0.113895Train Epoch: [740/2467]	Loss: 0.380241
    
     Train Epoch: [740/2467]	Loss: 0.182607
 Train Epoch: [740/2467]	Loss: 0.217565
     Train Epoch: [760/2467]	Loss: 0.544166
     Train Epoch: [760/2467]	Loss: 0.417292
     Train Epoch: [760/2467]	Loss: 0.258726
     Train Epoch: [760/2467]	Loss: 0.500014
          Train Epoch: [780/2467]	Loss: 0.454693Train Epoch: [780/2467]	Loss: 0.500668
    
 Train Epoch: [780/2467]	Loss: 0.401828
     Train Epoch: [780/2467]	Loss: 0.408307
     Train Epoch: [800/2467]	Loss: 0.384125
     Train Epoch: [800/2467]	Loss: 0.342312
     Train Epoch: [800/2467]	Loss: 0.623904
     Train Epoch: [800/2467]	Loss: 0.170530
         Train Epoch: [820/2467]	Loss: 0.130805
         Train Epoch: [820/2467]	Loss: 0.390634
  Train Epoch: [820/2467]	Loss: 0.541868Train Epoch: [820/2467]	Loss: 0.114698

     Train Epoch: [840/2467]	Loss: 0.292850
     Train Epoch: [840/2467]	Loss: 0.511790
     Train Epoch: [840/2467]	Loss: 0.221685
     Train Epoch: [840/2467]	Loss: 0.306301
         Train Epoch: [860/2467]	Loss: 0.163703
 Train Epoch: [860/2467]	Loss: 0.518608
     Train Epoch: [860/2467]	Loss: 0.183540
     Train Epoch: [860/2467]	Loss: 0.258774
         Train Epoch: [880/2467]	Loss: 0.378122 
Train Epoch: [880/2467]	Loss: 0.271093
         Train Epoch: [880/2467]	Loss: 0.295393
 Train Epoch: [880/2467]	Loss: 0.339027
     Train Epoch: [900/2467]	Loss: 0.285787
     Train Epoch: [900/2467]	Loss: 0.323905    
     Train Epoch: [900/2467]	Loss: 0.300816
 Train Epoch: [900/2467]	Loss: 0.265235
     Train Epoch: [920/2467]	Loss: 0.473499
          Train Epoch: [920/2467]	Loss: 0.564255
Train Epoch: [920/2467]	Loss: 0.318112
     Train Epoch: [920/2467]	Loss: 0.194986
         Train Epoch: [940/2467]	Loss: 0.246248
 Train Epoch: [940/2467]	Loss: 0.286655    
 Train Epoch: [940/2467]	Loss: 0.065478
     Train Epoch: [940/2467]	Loss: 0.438247
         Train Epoch: [960/2467]	Loss: 0.094935
 Train Epoch: [960/2467]	Loss: 0.418244
     Train Epoch: [960/2467]	Loss: 0.183774
     Train Epoch: [960/2467]	Loss: 0.167105
         Train Epoch: [980/2467]	Loss: 0.404538
     Train Epoch: [980/2467]	Loss: 0.388938
     Train Epoch: [980/2467]	Loss: 0.196504
 Train Epoch: [980/2467]	Loss: 0.311549
         Train Epoch: [1000/2467]	Loss: 0.444042
 Train Epoch: [1000/2467]	Loss: 0.511136
     Train Epoch: [1000/2467]	Loss: 0.087505
     Train Epoch: [1000/2467]	Loss: 0.242197
     Train Epoch: [1020/2467]	Loss: 0.418779
     Train Epoch: [1020/2467]	Loss: 0.509778
     Train Epoch: [1020/2467]	Loss: 0.108843
     Train Epoch: [1020/2467]	Loss: 0.253882
     Train Epoch: [1040/2467]	Loss: 0.439656
         Train Epoch: [1040/2467]	Loss: 0.290592
 Train Epoch: [1040/2467]	Loss: 0.130965
     Train Epoch: [1040/2467]	Loss: 0.332635
     Train Epoch: [1060/2467]	Loss: 0.455769    
 Train Epoch: [1060/2467]	Loss: 0.555654
     Train Epoch: [1060/2467]	Loss: 0.420237
     Train Epoch: [1060/2467]	Loss: 0.224272
     Train Epoch: [1080/2467]	Loss: 0.333018
         Train Epoch: [1080/2467]	Loss: 0.183870
 Train Epoch: [1080/2467]	Loss: 0.408741
     Train Epoch: [1080/2467]	Loss: 0.424638
         Train Epoch: [1100/2467]	Loss: 0.476705
 Train Epoch: [1100/2467]	Loss: 0.403136
     Train Epoch: [1100/2467]	Loss: 0.328033
     Train Epoch: [1100/2467]	Loss: 0.489468
     Train Epoch: [1120/2467]	Loss: 0.207891    
     Train Epoch: [1120/2467]	Loss: 0.235940
 Train Epoch: [1120/2467]	Loss: 0.489615
     Train Epoch: [1120/2467]	Loss: 0.268160
     Train Epoch: [1140/2467]	Loss: 0.114531
         Train Epoch: [1140/2467]	Loss: 0.351359
 Train Epoch: [1140/2467]	Loss: 0.548307
     Train Epoch: [1140/2467]	Loss: 0.231089
             Train Epoch: [1160/2467]	Loss: 0.268281
  Train Epoch: [1160/2467]	Loss: 0.321687Train Epoch: [1160/2467]	Loss: 0.114675

     Train Epoch: [1160/2467]	Loss: 0.135427
     Train Epoch: [1180/2467]	Loss: 0.140629
     Train Epoch: [1180/2467]	Loss: 0.559383
     Train Epoch: [1180/2467]	Loss: 0.268541
     Train Epoch: [1180/2467]	Loss: 0.448409
              Train Epoch: [1200/2467]	Loss: 0.418437Train Epoch: [1200/2467]	Loss: 0.379314

     Train Epoch: [1200/2467]	Loss: 0.269701
 Train Epoch: [1200/2467]	Loss: 0.387716
         Train Epoch: [1220/2467]	Loss: 0.370115
 Train Epoch: [1220/2467]	Loss: 0.410296
         Train Epoch: [1220/2467]	Loss: 0.258813 
Train Epoch: [1220/2467]	Loss: 0.697530
          Train Epoch: [1240/2467]	Loss: 0.402255Train Epoch: [1240/2467]	Loss: 0.722939

     Train Epoch: [1240/2467]	Loss: 0.490497
     Train Epoch: [1240/2467]	Loss: 0.460511
     Train Epoch: [1260/2467]	Loss: 0.216185
     Train Epoch: [1260/2467]	Loss: 0.112840
     Train Epoch: [1260/2467]	Loss: 0.215045
     Train Epoch: [1260/2467]	Loss: 0.541504
     Train Epoch: [1280/2467]	Loss: 0.155173         Train Epoch: [1280/2467]	Loss: 0.870392 Train Epoch: [1280/2467]	Loss: 0.471657


     Train Epoch: [1280/2467]	Loss: 0.518105
     Train Epoch: [1300/2467]	Loss: 0.178573
         Train Epoch: [1300/2467]	Loss: 0.545223
 Train Epoch: [1300/2467]	Loss: 0.474304
     Train Epoch: [1300/2467]	Loss: 0.158416
     Train Epoch: [1320/2467]	Loss: 0.189772
     Train Epoch: [1320/2467]	Loss: 0.378504
     Train Epoch: [1320/2467]	Loss: 0.283478
     Train Epoch: [1320/2467]	Loss: 0.504302
             Train Epoch: [1340/2467]	Loss: 0.330174
  Train Epoch: [1340/2467]	Loss: 0.144624Train Epoch: [1340/2467]	Loss: 0.531172

     Train Epoch: [1340/2467]	Loss: 0.289794
              Train Epoch: [1360/2467]	Loss: 0.150008Train Epoch: [1360/2467]	Loss: 0.528257 

Train Epoch: [1360/2467]	Loss: 0.667933
     Train Epoch: [1360/2467]	Loss: 0.168234
     Train Epoch: [1380/2467]	Loss: 0.382346    
 Train Epoch: [1380/2467]	Loss: 0.864726
     Train Epoch: [1380/2467]	Loss: 0.336916
     Train Epoch: [1380/2467]	Loss: 0.661284
     Train Epoch: [1400/2467]	Loss: 0.215980
         Train Epoch: [1400/2467]	Loss: 0.342577
 Train Epoch: [1400/2467]	Loss: 0.247356
     Train Epoch: [1400/2467]	Loss: 0.458521
              Train Epoch: [1420/2467]	Loss: 0.616604
 Train Epoch: [1420/2467]	Loss: 0.254712Train Epoch: [1420/2467]	Loss: 0.262794

     Train Epoch: [1420/2467]	Loss: 0.332439
     Train Epoch: [1440/2467]	Loss: 0.049974    
     Train Epoch: [1440/2467]	Loss: 0.391176
 Train Epoch: [1440/2467]	Loss: 0.105724
     Train Epoch: [1440/2467]	Loss: 0.209967
         Train Epoch: [1460/2467]	Loss: 0.112811
 Train Epoch: [1460/2467]	Loss: 0.201467
     Train Epoch: [1460/2467]	Loss: 0.356140
     Train Epoch: [1460/2467]	Loss: 0.353874
          Train Epoch: [1480/2467]	Loss: 0.395941Train Epoch: [1480/2467]	Loss: 0.346265

     Train Epoch: [1480/2467]	Loss: 0.183784
     Train Epoch: [1480/2467]	Loss: 0.083019
         Train Epoch: [1500/2467]	Loss: 0.400699
 Train Epoch: [1500/2467]	Loss: 0.149108
          Train Epoch: [1500/2467]	Loss: 0.091561Train Epoch: [1500/2467]	Loss: 0.269094

     Train Epoch: [1520/2467]	Loss: 0.516666
         Train Epoch: [1520/2467]	Loss: 0.177087
 Train Epoch: [1520/2467]	Loss: 0.352325
     Train Epoch: [1520/2467]	Loss: 0.371117
                 Train Epoch: [1540/2467]	Loss: 0.208943 
 Train Epoch: [1540/2467]	Loss: 0.388976
 Train Epoch: [1540/2467]	Loss: 0.625877
Train Epoch: [1540/2467]	Loss: 0.301115
     Train Epoch: [1560/2467]	Loss: 0.469897
         Train Epoch: [1560/2467]	Loss: 0.511996
 Train Epoch: [1560/2467]	Loss: 0.652251
     Train Epoch: [1560/2467]	Loss: 0.306985
          Train Epoch: [1580/2467]	Loss: 0.352736
Train Epoch: [1580/2467]	Loss: 0.393429
         Train Epoch: [1580/2467]	Loss: 0.432754
 Train Epoch: [1580/2467]	Loss: 0.383641
     Train Epoch: [1600/2467]	Loss: 0.309539
         Train Epoch: [1600/2467]	Loss: 0.173198
 Train Epoch: [1600/2467]	Loss: 0.349720
     Train Epoch: [1600/2467]	Loss: 0.459202
     Train Epoch: [1620/2467]	Loss: 0.136213
     Train Epoch: [1620/2467]	Loss: 0.322916
     Train Epoch: [1620/2467]	Loss: 0.402803
     Train Epoch: [1620/2467]	Loss: 0.301019
         Train Epoch: [1640/2467]	Loss: 0.357889
 Train Epoch: [1640/2467]	Loss: 0.348504
     Train Epoch: [1640/2467]	Loss: 0.290887
     Train Epoch: [1640/2467]	Loss: 0.246807
     Train Epoch: [1660/2467]	Loss: 0.201516
              Train Epoch: [1660/2467]	Loss: 0.497192Train Epoch: [1660/2467]	Loss: 0.297199

 Train Epoch: [1660/2467]	Loss: 0.150113
     Train Epoch: [1680/2467]	Loss: 0.723355
         Train Epoch: [1680/2467]	Loss: 0.161289
 Train Epoch: [1680/2467]	Loss: 0.250994
     Train Epoch: [1680/2467]	Loss: 0.438477
                  Train Epoch: [1700/2467]	Loss: 0.325691Train Epoch: [1700/2467]	Loss: 0.281877

  Train Epoch: [1700/2467]	Loss: 0.374271
Train Epoch: [1700/2467]	Loss: 0.341436
         Train Epoch: [1720/2467]	Loss: 0.313419
     Train Epoch: [1720/2467]	Loss: 0.343700
 Train Epoch: [1720/2467]	Loss: 0.462570
     Train Epoch: [1720/2467]	Loss: 0.400623
     Train Epoch: [1740/2467]	Loss: 0.246438        
  Train Epoch: [1740/2467]	Loss: 0.208821Train Epoch: [1740/2467]	Loss: 0.465479

     Train Epoch: [1740/2467]	Loss: 0.230115
     Train Epoch: [1760/2467]	Loss: 0.253343
     Train Epoch: [1760/2467]	Loss: 0.242091    
 Train Epoch: [1760/2467]	Loss: 0.233116
     Train Epoch: [1760/2467]	Loss: 0.167719
         Train Epoch: [1780/2467]	Loss: 0.372881
     Train Epoch: [1780/2467]	Loss: 0.140440
 Train Epoch: [1780/2467]	Loss: 0.438206
     Train Epoch: [1780/2467]	Loss: 0.331071
         Train Epoch: [1800/2467]	Loss: 0.433026
     Train Epoch: [1800/2467]	Loss: 0.278789
     Train Epoch: [1800/2467]	Loss: 0.681120 
Train Epoch: [1800/2467]	Loss: 0.323886
     Train Epoch: [1820/2467]	Loss: 0.248849
     Train Epoch: [1820/2467]	Loss: 0.529316
     Train Epoch: [1820/2467]	Loss: 0.136150
     Train Epoch: [1820/2467]	Loss: 0.423900
          Train Epoch: [1840/2467]	Loss: 0.602190Train Epoch: [1840/2467]	Loss: 0.284468

         Train Epoch: [1840/2467]	Loss: 0.165147
 Train Epoch: [1840/2467]	Loss: 0.255634
             Train Epoch: [1860/2467]	Loss: 0.236048 
Train Epoch: [1860/2467]	Loss: 0.301247
     Train Epoch: [1860/2467]	Loss: 0.377569
 Train Epoch: [1860/2467]	Loss: 0.199070
         Train Epoch: [1880/2467]	Loss: 0.113081
 Train Epoch: [1880/2467]	Loss: 0.220914
     Train Epoch: [1880/2467]	Loss: 0.183402
     Train Epoch: [1880/2467]	Loss: 0.300679
         Train Epoch: [1900/2467]	Loss: 0.248919
     Train Epoch: [1900/2467]	Loss: 0.485221
     Train Epoch: [1900/2467]	Loss: 0.334174
 Train Epoch: [1900/2467]	Loss: 0.339105
             Train Epoch: [1920/2467]	Loss: 0.293890
 Train Epoch: [1920/2467]	Loss: 0.037748
 Train Epoch: [1920/2467]	Loss: 0.411313    
 Train Epoch: [1920/2467]	Loss: 0.342357
     Train Epoch: [1940/2467]	Loss: 0.524005
         Train Epoch: [1940/2467]	Loss: 0.185899 
Train Epoch: [1940/2467]	Loss: 0.219504
     Train Epoch: [1940/2467]	Loss: 0.401710
     Train Epoch: [1960/2467]	Loss: 0.345246
         Train Epoch: [1960/2467]	Loss: 0.236921
 Train Epoch: [1960/2467]	Loss: 0.261424
     Train Epoch: [1960/2467]	Loss: 0.319868
         Train Epoch: [1980/2467]	Loss: 0.437148 
    Train Epoch: [1980/2467]	Loss: 0.372920    
  Train Epoch: [1980/2467]	Loss: 0.472632Train Epoch: [1980/2467]	Loss: 0.266723

         Train Epoch: [2000/2467]	Loss: 0.263914
 Train Epoch: [2000/2467]	Loss: 0.199740
     Train Epoch: [2000/2467]	Loss: 0.199137
     Train Epoch: [2000/2467]	Loss: 0.455238
              Train Epoch: [2020/2467]	Loss: 0.410611Train Epoch: [2020/2467]	Loss: 0.638505

 Train Epoch: [2020/2467]	Loss: 0.470910
     Train Epoch: [2020/2467]	Loss: 0.210048
     Train Epoch: [2040/2467]	Loss: 0.515507
         Train Epoch: [2040/2467]	Loss: 0.367266
 Train Epoch: [2040/2467]	Loss: 0.132647
     Train Epoch: [2040/2467]	Loss: 0.352587
     Train Epoch: [2060/2467]	Loss: 0.313107    
      Train Epoch: [2060/2467]	Loss: 0.367298Train Epoch: [2060/2467]	Loss: 0.283376

     Train Epoch: [2060/2467]	Loss: 0.487729
               Train Epoch: [2080/2467]	Loss: 0.254173Train Epoch: [2080/2467]	Loss: 0.285764
Train Epoch: [2080/2467]	Loss: 0.391681

     Train Epoch: [2080/2467]	Loss: 0.207956
     Train Epoch: [2100/2467]	Loss: 0.196145    
 Train Epoch: [2100/2467]	Loss: 0.721897
     Train Epoch: [2100/2467]	Loss: 0.316798
     Train Epoch: [2100/2467]	Loss: 0.416950
                 Train Epoch: [2120/2467]	Loss: 0.205345
  Train Epoch: [2120/2467]	Loss: 0.474044 Train Epoch: [2120/2467]	Loss: 0.329483

Train Epoch: [2120/2467]	Loss: 0.195099
     Train Epoch: [2140/2467]	Loss: 0.360661
     Train Epoch: [2140/2467]	Loss: 0.646490
          Train Epoch: [2140/2467]	Loss: 0.398133Train Epoch: [2140/2467]	Loss: 0.562473

     Train Epoch: [2160/2467]	Loss: 0.265595
         Train Epoch: [2160/2467]	Loss: 0.495976
     Train Epoch: [2160/2467]	Loss: 0.282767
 Train Epoch: [2160/2467]	Loss: 0.138278
         Train Epoch: [2180/2467]	Loss: 0.350319
     Train Epoch: [2180/2467]	Loss: 0.270025
 Train Epoch: [2180/2467]	Loss: 0.379966
     Train Epoch: [2180/2467]	Loss: 0.215353
     Train Epoch: [2200/2467]	Loss: 0.440691
             Train Epoch: [2200/2467]	Loss: 0.200429
 Train Epoch: [2200/2467]	Loss: 0.259815 
Train Epoch: [2200/2467]	Loss: 0.780381
             Train Epoch: [2220/2467]	Loss: 0.262264 
Train Epoch: [2220/2467]	Loss: 0.298232
     Train Epoch: [2220/2467]	Loss: 0.113496
 Train Epoch: [2220/2467]	Loss: 0.152378
         Train Epoch: [2240/2467]	Loss: 0.146293
 Train Epoch: [2240/2467]	Loss: 0.301239
     Train Epoch: [2240/2467]	Loss: 0.180612
     Train Epoch: [2240/2467]	Loss: 0.119109
     Train Epoch: [2260/2467]	Loss: 0.583278
     Train Epoch: [2260/2467]	Loss: 0.410756
     Train Epoch: [2260/2467]	Loss: 0.153727
     Train Epoch: [2260/2467]	Loss: 0.281250
     Train Epoch: [2280/2467]	Loss: 0.244483
          Train Epoch: [2280/2467]	Loss: 0.307796Train Epoch: [2280/2467]	Loss: 0.271893

     Train Epoch: [2280/2467]	Loss: 0.183980
     Train Epoch: [2300/2467]	Loss: 0.119331
          Train Epoch: [2300/2467]	Loss: 0.187841Train Epoch: [2300/2467]	Loss: 0.556292

     Train Epoch: [2300/2467]	Loss: 0.172671
             Train Epoch: [2320/2467]	Loss: 0.130921
 Train Epoch: [2320/2467]	Loss: 0.257535
 Train Epoch: [2320/2467]	Loss: 0.317010
     Train Epoch: [2320/2467]	Loss: 0.455662
          Train Epoch: [2340/2467]	Loss: 0.138411Train Epoch: [2340/2467]	Loss: 0.190789

     Train Epoch: [2340/2467]	Loss: 0.650722
     Train Epoch: [2340/2467]	Loss: 0.242926
         Train Epoch: [2360/2467]	Loss: 0.128092
 Train Epoch: [2360/2467]	Loss: 0.307486
          Train Epoch: [2360/2467]	Loss: 0.498288Train Epoch: [2360/2467]	Loss: 0.248419

         Train Epoch: [2380/2467]	Loss: 0.391984
           Train Epoch: [2380/2467]	Loss: 0.786812Train Epoch: [2380/2467]	Loss: 0.390437
Train Epoch: [2380/2467]	Loss: 0.602719

     Train Epoch: [2400/2467]	Loss: 0.435195
     Train Epoch: [2400/2467]	Loss: 0.210903
     Train Epoch: [2400/2467]	Loss: 0.342775
     Train Epoch: [2400/2467]	Loss: 0.337019
             Train Epoch: [2420/2467]	Loss: 0.231344
      Train Epoch: [2420/2467]	Loss: 0.431543Train Epoch: [2420/2467]	Loss: 0.354648

 Train Epoch: [2420/2467]	Loss: 0.385456
     Train Epoch: [2440/2467]	Loss: 0.170004
     Train Epoch: [2440/2467]	Loss: 0.534971
     Train Epoch: [2440/2467]	Loss: 0.138131
     Train Epoch: [2440/2467]	Loss: 0.188422
     Train Epoch: [2460/2467]	Loss: 0.230751
         Train Epoch: [2460/2467]	Loss: 0.187986
 Train Epoch: [2460/2467]	Loss: 0.526282
     Train Epoch: [2460/2467]	Loss: 0.263784
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 6 epoch =====
     2025-05-11.02-14-10
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 6 epoch =====
     2025-05-11.02-14-10
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 6 epoch =====
     2025-05-11.02-14-11
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 6 epoch =====
     2025-05-11.02-14-11
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.474585
         Train Epoch: [0/2467]	Loss: 0.458766
 Train Epoch: [0/2467]	Loss: 0.259343
     Train Epoch: [0/2467]	Loss: 0.426114
                 Train Epoch: [20/2467]	Loss: 0.473825
 Train Epoch: [20/2467]	Loss: 0.556421 
 Train Epoch: [20/2467]	Loss: 0.201530Train Epoch: [20/2467]	Loss: 0.230277

         Train Epoch: [40/2467]	Loss: 0.390945
 Train Epoch: [40/2467]	Loss: 0.700872
     Train Epoch: [40/2467]	Loss: 0.414249
     Train Epoch: [40/2467]	Loss: 0.254392
         Train Epoch: [60/2467]	Loss: 0.430829
     Train Epoch: [60/2467]	Loss: 0.305344
 Train Epoch: [60/2467]	Loss: 0.115305
     Train Epoch: [60/2467]	Loss: 0.287459
          Train Epoch: [80/2467]	Loss: 0.203999    Train Epoch: [80/2467]	Loss: 0.513348

     Train Epoch: [80/2467]	Loss: 0.236447
 Train Epoch: [80/2467]	Loss: 0.256050
         Train Epoch: [100/2467]	Loss: 0.275179
 Train Epoch: [100/2467]	Loss: 0.352970
     Train Epoch: [100/2467]	Loss: 0.415125
     Train Epoch: [100/2467]	Loss: 0.454099
     Train Epoch: [120/2467]	Loss: 0.179573
         Train Epoch: [120/2467]	Loss: 0.080803 
Train Epoch: [120/2467]	Loss: 0.505269
     Train Epoch: [120/2467]	Loss: 0.356622
         Train Epoch: [140/2467]	Loss: 0.423734
     Train Epoch: [140/2467]	Loss: 0.368922
 Train Epoch: [140/2467]	Loss: 0.135217
     Train Epoch: [140/2467]	Loss: 0.152366
              Train Epoch: [160/2467]	Loss: 0.598880Train Epoch: [160/2467]	Loss: 0.479179

 Train Epoch: [160/2467]	Loss: 0.209496
     Train Epoch: [160/2467]	Loss: 0.370116
                   Train Epoch: [180/2467]	Loss: 0.182857Train Epoch: [180/2467]	Loss: 0.287155
Train Epoch: [180/2467]	Loss: 0.289641

 Train Epoch: [180/2467]	Loss: 0.219784
         Train Epoch: [200/2467]	Loss: 0.459384    
 Train Epoch: [200/2467]	Loss: 0.254583
     Train Epoch: [200/2467]	Loss: 0.247431
 Train Epoch: [200/2467]	Loss: 0.529277
     Train Epoch: [220/2467]	Loss: 0.330280    
 Train Epoch: [220/2467]	Loss: 0.351858
     Train Epoch: [220/2467]	Loss: 0.428168
     Train Epoch: [220/2467]	Loss: 0.288124
          Train Epoch: [240/2467]	Loss: 0.318434
Train Epoch: [240/2467]	Loss: 0.065778
     Train Epoch: [240/2467]	Loss: 0.150067
     Train Epoch: [240/2467]	Loss: 0.785599
         Train Epoch: [260/2467]	Loss: 0.091546     
    Train Epoch: [260/2467]	Loss: 0.121462
 Train Epoch: [260/2467]	Loss: 0.298586 
Train Epoch: [260/2467]	Loss: 0.287585
         Train Epoch: [280/2467]	Loss: 0.237053
 Train Epoch: [280/2467]	Loss: 0.443140
     Train Epoch: [280/2467]	Loss: 0.366015
     Train Epoch: [280/2467]	Loss: 0.392206
     Train Epoch: [300/2467]	Loss: 0.537600    
     Train Epoch: [300/2467]	Loss: 0.180595
     Train Epoch: [300/2467]	Loss: 0.239960
 Train Epoch: [300/2467]	Loss: 0.229316
         Train Epoch: [320/2467]	Loss: 0.258451
 Train Epoch: [320/2467]	Loss: 0.329089    
     Train Epoch: [320/2467]	Loss: 0.486373 
Train Epoch: [320/2467]	Loss: 0.580796
         Train Epoch: [340/2467]	Loss: 0.101376
     Train Epoch: [340/2467]	Loss: 0.128743
 Train Epoch: [340/2467]	Loss: 0.261931
     Train Epoch: [340/2467]	Loss: 0.272901
          Train Epoch: [360/2467]	Loss: 0.219519
Train Epoch: [360/2467]	Loss: 0.336165
     Train Epoch: [360/2467]	Loss: 0.248161
     Train Epoch: [360/2467]	Loss: 0.260605
     Train Epoch: [380/2467]	Loss: 0.346826
         Train Epoch: [380/2467]	Loss: 0.681724 
Train Epoch: [380/2467]	Loss: 0.319530
     Train Epoch: [380/2467]	Loss: 0.734574
     Train Epoch: [400/2467]	Loss: 0.518179
     Train Epoch: [400/2467]	Loss: 0.222938
     Train Epoch: [400/2467]	Loss: 0.285290
     Train Epoch: [400/2467]	Loss: 0.358335
     Train Epoch: [420/2467]	Loss: 0.394108
         Train Epoch: [420/2467]	Loss: 0.373847
 Train Epoch: [420/2467]	Loss: 0.322146
     Train Epoch: [420/2467]	Loss: 0.153757
         Train Epoch: [440/2467]	Loss: 0.472521
 Train Epoch: [440/2467]	Loss: 0.209000    
     Train Epoch: [440/2467]	Loss: 0.183691
 Train Epoch: [440/2467]	Loss: 0.131618
         Train Epoch: [460/2467]	Loss: 0.425627
 Train Epoch: [460/2467]	Loss: 0.256302    
 Train Epoch: [460/2467]	Loss: 0.289469
     Train Epoch: [460/2467]	Loss: 0.208613
     Train Epoch: [480/2467]	Loss: 0.296187
         Train Epoch: [480/2467]	Loss: 0.403945
     Train Epoch: [480/2467]	Loss: 0.437150
 Train Epoch: [480/2467]	Loss: 0.253268
         Train Epoch: [500/2467]	Loss: 0.182620
 Train Epoch: [500/2467]	Loss: 0.536434    
 Train Epoch: [500/2467]	Loss: 0.154683
     Train Epoch: [500/2467]	Loss: 0.383571
          Train Epoch: [520/2467]	Loss: 0.353712Train Epoch: [520/2467]	Loss: 0.532842

         Train Epoch: [520/2467]	Loss: 0.147741 
Train Epoch: [520/2467]	Loss: 0.435556
     Train Epoch: [540/2467]	Loss: 0.288018
             Train Epoch: [540/2467]	Loss: 0.111733 
Train Epoch: [540/2467]	Loss: 0.154469
 Train Epoch: [540/2467]	Loss: 0.310850
         Train Epoch: [560/2467]	Loss: 0.310086
     Train Epoch: [560/2467]	Loss: 0.417756
     Train Epoch: [560/2467]	Loss: 0.308744
 Train Epoch: [560/2467]	Loss: 0.325711
         Train Epoch: [580/2467]	Loss: 0.422193
 Train Epoch: [580/2467]	Loss: 0.213973    
     Train Epoch: [580/2467]	Loss: 0.147557
 Train Epoch: [580/2467]	Loss: 0.284093
         Train Epoch: [600/2467]	Loss: 0.236470
 Train Epoch: [600/2467]	Loss: 0.216194
         Train Epoch: [600/2467]	Loss: 0.185806
 Train Epoch: [600/2467]	Loss: 0.339556
         Train Epoch: [620/2467]	Loss: 0.292108
     Train Epoch: [620/2467]	Loss: 0.452877
 Train Epoch: [620/2467]	Loss: 0.118760    
 Train Epoch: [620/2467]	Loss: 0.481914
     Train Epoch: [640/2467]	Loss: 0.315862
     Train Epoch: [640/2467]	Loss: 0.336172
     Train Epoch: [640/2467]	Loss: 0.390233
     Train Epoch: [640/2467]	Loss: 0.342072
          Train Epoch: [660/2467]	Loss: 0.510478Train Epoch: [660/2467]	Loss: 0.068275

     Train Epoch: [660/2467]	Loss: 0.374686
     Train Epoch: [660/2467]	Loss: 0.098189
         Train Epoch: [680/2467]	Loss: 0.214982
         Train Epoch: [680/2467]	Loss: 0.221356 
 Train Epoch: [680/2467]	Loss: 0.304419Train Epoch: [680/2467]	Loss: 0.127468

     Train Epoch: [700/2467]	Loss: 0.163756
         Train Epoch: [700/2467]	Loss: 0.391162
 Train Epoch: [700/2467]	Loss: 0.429752
     Train Epoch: [700/2467]	Loss: 0.140213
         Train Epoch: [720/2467]	Loss: 0.278181
     Train Epoch: [720/2467]	Loss: 0.148439
 Train Epoch: [720/2467]	Loss: 0.364996
     Train Epoch: [720/2467]	Loss: 0.296252
         Train Epoch: [740/2467]	Loss: 0.109095 
Train Epoch: [740/2467]	Loss: 0.362382
     Train Epoch: [740/2467]	Loss: 0.195089
     Train Epoch: [740/2467]	Loss: 0.176589
             Train Epoch: [760/2467]	Loss: 0.482224 
Train Epoch: [760/2467]	Loss: 0.553067
 Train Epoch: [760/2467]	Loss: 0.363608
     Train Epoch: [760/2467]	Loss: 0.190547
             Train Epoch: [780/2467]	Loss: 0.414619
 Train Epoch: [780/2467]	Loss: 0.443170 
    Train Epoch: [780/2467]	Loss: 0.466065
 Train Epoch: [780/2467]	Loss: 0.322014
     Train Epoch: [800/2467]	Loss: 0.382415
     Train Epoch: [800/2467]	Loss: 0.326628
     Train Epoch: [800/2467]	Loss: 0.160358
     Train Epoch: [800/2467]	Loss: 0.543452
     Train Epoch: [820/2467]	Loss: 0.346469
     Train Epoch: [820/2467]	Loss: 0.514995
          Train Epoch: [820/2467]	Loss: 0.110506Train Epoch: [820/2467]	Loss: 0.070768

     Train Epoch: [840/2467]	Loss: 0.258572
     Train Epoch: [840/2467]	Loss: 0.191779
     Train Epoch: [840/2467]	Loss: 0.320245
     Train Epoch: [840/2467]	Loss: 0.392790
         Train Epoch: [860/2467]	Loss: 0.475428 
    Train Epoch: [860/2467]	Loss: 0.166767
     Train Epoch: [860/2467]	Loss: 0.250652
 Train Epoch: [860/2467]	Loss: 0.155061
     Train Epoch: [880/2467]	Loss: 0.253623
     Train Epoch: [880/2467]	Loss: 0.375352
     Train Epoch: [880/2467]	Loss: 0.320191
     Train Epoch: [880/2467]	Loss: 0.326096
         Train Epoch: [900/2467]	Loss: 0.275273
 Train Epoch: [900/2467]	Loss: 0.277926
         Train Epoch: [900/2467]	Loss: 0.229281
 Train Epoch: [900/2467]	Loss: 0.250610
         Train Epoch: [920/2467]	Loss: 0.555811
 Train Epoch: [920/2467]	Loss: 0.307735
          Train Epoch: [920/2467]	Loss: 0.230526Train Epoch: [920/2467]	Loss: 0.498950

     Train Epoch: [940/2467]	Loss: 0.070378
             Train Epoch: [940/2467]	Loss: 1.088171 
Train Epoch: [940/2467]	Loss: 0.426666
 Train Epoch: [940/2467]	Loss: 0.265660
     Train Epoch: [960/2467]	Loss: 0.084772    
 Train Epoch: [960/2467]	Loss: 0.421541
     Train Epoch: [960/2467]	Loss: 0.128630
     Train Epoch: [960/2467]	Loss: 0.176003
     Train Epoch: [980/2467]	Loss: 0.321929
         Train Epoch: [980/2467]	Loss: 0.185686 
Train Epoch: [980/2467]	Loss: 0.347652
     Train Epoch: [980/2467]	Loss: 0.429296
     Train Epoch: [1000/2467]	Loss: 0.211627
     Train Epoch: [1000/2467]	Loss: 0.515944
     Train Epoch: [1000/2467]	Loss: 0.067678
     Train Epoch: [1000/2467]	Loss: 0.443050
             Train Epoch: [1020/2467]	Loss: 0.330045 
Train Epoch: [1020/2467]	Loss: 0.489686 
Train Epoch: [1020/2467]	Loss: 0.102179
     Train Epoch: [1020/2467]	Loss: 0.220977
         Train Epoch: [1040/2467]	Loss: 0.134199
 Train Epoch: [1040/2467]	Loss: 0.314918
          Train Epoch: [1040/2467]	Loss: 0.397131Train Epoch: [1040/2467]	Loss: 0.304959

             Train Epoch: [1060/2467]	Loss: 0.592112
     Train Epoch: [1060/2467]	Loss: 0.423600 
Train Epoch: [1060/2467]	Loss: 0.440929
 Train Epoch: [1060/2467]	Loss: 0.316498
         Train Epoch: [1080/2467]	Loss: 0.169753 
Train Epoch: [1080/2467]	Loss: 0.249270
     Train Epoch: [1080/2467]	Loss: 0.418427
     Train Epoch: [1080/2467]	Loss: 0.405305
     Train Epoch: [1100/2467]	Loss: 0.476518
     Train Epoch: [1100/2467]	Loss: 0.465540
     Train Epoch: [1100/2467]	Loss: 0.336804
     Train Epoch: [1100/2467]	Loss: 0.328818
             Train Epoch: [1120/2467]	Loss: 0.244523
 Train Epoch: [1120/2467]	Loss: 0.485418 
Train Epoch: [1120/2467]	Loss: 0.202795
     Train Epoch: [1120/2467]	Loss: 0.244302
     Train Epoch: [1140/2467]	Loss: 0.106894
     Train Epoch: [1140/2467]	Loss: 0.355600
     Train Epoch: [1140/2467]	Loss: 0.519555
     Train Epoch: [1140/2467]	Loss: 0.204489
             Train Epoch: [1160/2467]	Loss: 0.259281
 Train Epoch: [1160/2467]	Loss: 0.128804
 Train Epoch: [1160/2467]	Loss: 0.222740
     Train Epoch: [1160/2467]	Loss: 0.145293
     Train Epoch: [1180/2467]	Loss: 0.132719
         Train Epoch: [1180/2467]	Loss: 0.603727
 Train Epoch: [1180/2467]	Loss: 0.213261
     Train Epoch: [1180/2467]	Loss: 0.525071
         Train Epoch: [1200/2467]	Loss: 0.429118
 Train Epoch: [1200/2467]	Loss: 0.201492
     Train Epoch: [1200/2467]	Loss: 0.400449
     Train Epoch: [1200/2467]	Loss: 0.259211
         Train Epoch: [1220/2467]	Loss: 0.536710
         Train Epoch: [1220/2467]	Loss: 0.325389
  Train Epoch: [1220/2467]	Loss: 0.168312
Train Epoch: [1220/2467]	Loss: 0.658511
     Train Epoch: [1240/2467]	Loss: 0.429832
             Train Epoch: [1240/2467]	Loss: 0.687324
  Train Epoch: [1240/2467]	Loss: 0.442421Train Epoch: [1240/2467]	Loss: 0.308435

          Train Epoch: [1260/2467]	Loss: 0.249265Train Epoch: [1260/2467]	Loss: 0.186186

         Train Epoch: [1260/2467]	Loss: 0.534298
 Train Epoch: [1260/2467]	Loss: 0.094651
             Train Epoch: [1280/2467]	Loss: 0.488461
  Train Epoch: [1280/2467]	Loss: 0.591864
Train Epoch: [1280/2467]	Loss: 0.099785
     Train Epoch: [1280/2467]	Loss: 1.025335
         Train Epoch: [1300/2467]	Loss: 0.152182
     Train Epoch: [1300/2467]	Loss: 0.452166
 Train Epoch: [1300/2467]	Loss: 0.601576
     Train Epoch: [1300/2467]	Loss: 0.134166
             Train Epoch: [1320/2467]	Loss: 0.242441
      Train Epoch: [1320/2467]	Loss: 0.351168Train Epoch: [1320/2467]	Loss: 0.158447

 Train Epoch: [1320/2467]	Loss: 0.444937
     Train Epoch: [1340/2467]	Loss: 0.502994    
 Train Epoch: [1340/2467]	Loss: 0.126351
     Train Epoch: [1340/2467]	Loss: 0.272565
     Train Epoch: [1340/2467]	Loss: 0.301790
          Train Epoch: [1360/2467]	Loss: 0.172836Train Epoch: [1360/2467]	Loss: 0.414288

     Train Epoch: [1360/2467]	Loss: 0.565977
     Train Epoch: [1360/2467]	Loss: 0.406682
         Train Epoch: [1380/2467]	Loss: 0.417756
     Train Epoch: [1380/2467]	Loss: 0.609405
 Train Epoch: [1380/2467]	Loss: 1.255683
     Train Epoch: [1380/2467]	Loss: 0.439750
     Train Epoch: [1400/2467]	Loss: 0.299909
          Train Epoch: [1400/2467]	Loss: 0.321902Train Epoch: [1400/2467]	Loss: 0.392903

     Train Epoch: [1400/2467]	Loss: 0.379613
         Train Epoch: [1420/2467]	Loss: 0.256123
 Train Epoch: [1420/2467]	Loss: 0.552524
     Train Epoch: [1420/2467]	Loss: 0.285034
     Train Epoch: [1420/2467]	Loss: 0.357557
             Train Epoch: [1440/2467]	Loss: 0.061051
  Train Epoch: [1440/2467]	Loss: 0.163158Train Epoch: [1440/2467]	Loss: 0.367260

     Train Epoch: [1440/2467]	Loss: 0.089134
         Train Epoch: [1460/2467]	Loss: 0.303296    
  Train Epoch: [1460/2467]	Loss: 0.216106Train Epoch: [1460/2467]	Loss: 0.087266

     Train Epoch: [1460/2467]	Loss: 0.332105
          Train Epoch: [1480/2467]	Loss: 0.361378Train Epoch: [1480/2467]	Loss: 0.395514

         Train Epoch: [1480/2467]	Loss: 0.069068
 Train Epoch: [1480/2467]	Loss: 0.149747
         Train Epoch: [1500/2467]	Loss: 0.318894 
Train Epoch: [1500/2467]	Loss: 0.145119
     Train Epoch: [1500/2467]	Loss: 0.051545    
 Train Epoch: [1500/2467]	Loss: 0.273894
         Train Epoch: [1520/2467]	Loss: 0.494837
     Train Epoch: [1520/2467]	Loss: 0.173504
 Train Epoch: [1520/2467]	Loss: 0.350659
     Train Epoch: [1520/2467]	Loss: 0.354973
         Train Epoch: [1540/2467]	Loss: 0.180891 
Train Epoch: [1540/2467]	Loss: 0.279224
     Train Epoch: [1540/2467]	Loss: 0.805030
     Train Epoch: [1540/2467]	Loss: 0.368627
              Train Epoch: [1560/2467]	Loss: 0.477155 Train Epoch: [1560/2467]	Loss: 0.305359

Train Epoch: [1560/2467]	Loss: 0.525216
     Train Epoch: [1560/2467]	Loss: 0.592797
     Train Epoch: [1580/2467]	Loss: 0.423629
     Train Epoch: [1580/2467]	Loss: 0.384153
     Train Epoch: [1580/2467]	Loss: 0.413773
     Train Epoch: [1580/2467]	Loss: 0.255018
         Train Epoch: [1600/2467]	Loss: 0.316622
     Train Epoch: [1600/2467]	Loss: 0.452552
 Train Epoch: [1600/2467]	Loss: 0.345682
     Train Epoch: [1600/2467]	Loss: 0.202857
     Train Epoch: [1620/2467]	Loss: 0.131113    
 Train Epoch: [1620/2467]	Loss: 0.309764
     Train Epoch: [1620/2467]	Loss: 0.118973
     Train Epoch: [1620/2467]	Loss: 0.282251
     Train Epoch: [1640/2467]	Loss: 0.319446    
     Train Epoch: [1640/2467]	Loss: 0.272654
 Train Epoch: [1640/2467]	Loss: 0.211221
     Train Epoch: [1640/2467]	Loss: 0.328530
         Train Epoch: [1660/2467]	Loss: 0.286226
 Train Epoch: [1660/2467]	Loss: 0.498305
          Train Epoch: [1660/2467]	Loss: 0.191408Train Epoch: [1660/2467]	Loss: 0.188228

               Train Epoch: [1680/2467]	Loss: 0.489370
Train Epoch: [1680/2467]	Loss: 0.247870
Train Epoch: [1680/2467]	Loss: 0.126571
     Train Epoch: [1680/2467]	Loss: 0.532355
              Train Epoch: [1700/2467]	Loss: 0.308259Train Epoch: [1700/2467]	Loss: 0.303091

 Train Epoch: [1700/2467]	Loss: 0.362488
     Train Epoch: [1700/2467]	Loss: 0.214235
         Train Epoch: [1720/2467]	Loss: 0.295152
     Train Epoch: [1720/2467]	Loss: 0.421967
 Train Epoch: [1720/2467]	Loss: 0.377664
     Train Epoch: [1720/2467]	Loss: 0.294249
     Train Epoch: [1740/2467]	Loss: 0.477109
             Train Epoch: [1740/2467]	Loss: 0.252567
 Train Epoch: [1740/2467]	Loss: 0.175265 
Train Epoch: [1740/2467]	Loss: 0.232972
             Train Epoch: [1760/2467]	Loss: 0.152869    
  Train Epoch: [1760/2467]	Loss: 0.163381Train Epoch: [1760/2467]	Loss: 0.254999

 Train Epoch: [1760/2467]	Loss: 0.203559
     Train Epoch: [1780/2467]	Loss: 0.340451
         Train Epoch: [1780/2467]	Loss: 0.398281    
  Train Epoch: [1780/2467]	Loss: 0.340358Train Epoch: [1780/2467]	Loss: 0.151328

                    Train Epoch: [1800/2467]	Loss: 0.480531Train Epoch: [1800/2467]	Loss: 0.641975Train Epoch: [1800/2467]	Loss: 0.265179
Train Epoch: [1800/2467]	Loss: 0.303679


                  Train Epoch: [1820/2467]	Loss: 0.503745Train Epoch: [1820/2467]	Loss: 0.238327  

Train Epoch: [1820/2467]	Loss: 0.138241Train Epoch: [1820/2467]	Loss: 0.238084

             Train Epoch: [1840/2467]	Loss: 0.256019
      Train Epoch: [1840/2467]	Loss: 0.571921Train Epoch: [1840/2467]	Loss: 0.300079

 Train Epoch: [1840/2467]	Loss: 0.142334
              Train Epoch: [1860/2467]	Loss: 0.230488     
Train Epoch: [1860/2467]	Loss: 0.322742Train Epoch: [1860/2467]	Loss: 0.264520

 Train Epoch: [1860/2467]	Loss: 0.204599
     Train Epoch: [1880/2467]	Loss: 0.125206
     Train Epoch: [1880/2467]	Loss: 0.212359
     Train Epoch: [1880/2467]	Loss: 0.145232
     Train Epoch: [1880/2467]	Loss: 0.128979
     Train Epoch: [1900/2467]	Loss: 0.461112
         Train Epoch: [1900/2467]	Loss: 0.281436
 Train Epoch: [1900/2467]	Loss: 0.326509
     Train Epoch: [1900/2467]	Loss: 0.218628
         Train Epoch: [1920/2467]	Loss: 0.329048
 Train Epoch: [1920/2467]	Loss: 0.391103
     Train Epoch: [1920/2467]	Loss: 0.325630
     Train Epoch: [1920/2467]	Loss: 0.042499
                  Train Epoch: [1940/2467]	Loss: 0.206740Train Epoch: [1940/2467]	Loss: 0.427492  

Train Epoch: [1940/2467]	Loss: 0.216440Train Epoch: [1940/2467]	Loss: 0.537751

             Train Epoch: [1960/2467]	Loss: 0.325745
  Train Epoch: [1960/2467]	Loss: 0.231119
Train Epoch: [1960/2467]	Loss: 0.328794
     Train Epoch: [1960/2467]	Loss: 0.351791
     Train Epoch: [1980/2467]	Loss: 0.342964
     Train Epoch: [1980/2467]	Loss: 0.464849
          Train Epoch: [1980/2467]	Loss: 0.358790Train Epoch: [1980/2467]	Loss: 0.325666

             Train Epoch: [2000/2467]	Loss: 0.161767 
Train Epoch: [2000/2467]	Loss: 0.252072
 Train Epoch: [2000/2467]	Loss: 0.156314
     Train Epoch: [2000/2467]	Loss: 0.439306
         Train Epoch: [2020/2467]	Loss: 0.522510
     Train Epoch: [2020/2467]	Loss: 0.610649
 Train Epoch: [2020/2467]	Loss: 0.384879
     Train Epoch: [2020/2467]	Loss: 0.199696
     Train Epoch: [2040/2467]	Loss: 0.274068
     Train Epoch: [2040/2467]	Loss: 0.128423
         Train Epoch: [2040/2467]	Loss: 0.480801
 Train Epoch: [2040/2467]	Loss: 0.413579
              Train Epoch: [2060/2467]	Loss: 0.271598Train Epoch: [2060/2467]	Loss: 0.288021

 Train Epoch: [2060/2467]	Loss: 0.468185
     Train Epoch: [2060/2467]	Loss: 0.410694
         Train Epoch: [2080/2467]	Loss: 0.314165
 Train Epoch: [2080/2467]	Loss: 0.371786
     Train Epoch: [2080/2467]	Loss: 0.163535
     Train Epoch: [2080/2467]	Loss: 0.120324
         Train Epoch: [2100/2467]	Loss: 0.176455    
 Train Epoch: [2100/2467]	Loss: 0.673352
 Train Epoch: [2100/2467]	Loss: 0.222548
     Train Epoch: [2100/2467]	Loss: 0.281448
     Train Epoch: [2120/2467]	Loss: 0.215243
     Train Epoch: [2120/2467]	Loss: 0.189698
         Train Epoch: [2120/2467]	Loss: 0.266804
 Train Epoch: [2120/2467]	Loss: 0.419397
         Train Epoch: [2140/2467]	Loss: 0.305671
     Train Epoch: [2140/2467]	Loss: 0.380234
 Train Epoch: [2140/2467]	Loss: 0.694757    
 Train Epoch: [2140/2467]	Loss: 0.487775
         Train Epoch: [2160/2467]	Loss: 0.232951    
 Train Epoch: [2160/2467]	Loss: 0.096956     
Train Epoch: [2160/2467]	Loss: 0.485037
 Train Epoch: [2160/2467]	Loss: 0.264075
                   Train Epoch: [2180/2467]	Loss: 0.351570Train Epoch: [2180/2467]	Loss: 0.221455Train Epoch: [2180/2467]	Loss: 0.383346


 Train Epoch: [2180/2467]	Loss: 0.204069
          Train Epoch: [2200/2467]	Loss: 0.220246Train Epoch: [2200/2467]	Loss: 0.325810

          Train Epoch: [2200/2467]	Loss: 0.644800Train Epoch: [2200/2467]	Loss: 0.283785

              Train Epoch: [2220/2467]	Loss: 0.259661Train Epoch: [2220/2467]	Loss: 0.125057

     Train Epoch: [2220/2467]	Loss: 0.207685
 Train Epoch: [2220/2467]	Loss: 0.153600
     Train Epoch: [2240/2467]	Loss: 0.278267    
     Train Epoch: [2240/2467]	Loss: 0.178890
 Train Epoch: [2240/2467]	Loss: 0.087814
     Train Epoch: [2240/2467]	Loss: 0.110632
     Train Epoch: [2260/2467]	Loss: 0.849556
     Train Epoch: [2260/2467]	Loss: 0.381901
     Train Epoch: [2260/2467]	Loss: 0.263634
     Train Epoch: [2260/2467]	Loss: 0.174135
     Train Epoch: [2280/2467]	Loss: 0.311030
     Train Epoch: [2280/2467]	Loss: 0.288833
     Train Epoch: [2280/2467]	Loss: 0.246223
     Train Epoch: [2280/2467]	Loss: 0.153079
         Train Epoch: [2300/2467]	Loss: 0.112990
 Train Epoch: [2300/2467]	Loss: 0.496585
     Train Epoch: [2300/2467]	Loss: 0.187569
     Train Epoch: [2300/2467]	Loss: 0.146846
                 Train Epoch: [2320/2467]	Loss: 0.416909  
Train Epoch: [2320/2467]	Loss: 0.233680Train Epoch: [2320/2467]	Loss: 0.095949

 Train Epoch: [2320/2467]	Loss: 0.278352
     Train Epoch: [2340/2467]	Loss: 0.139047
          Train Epoch: [2340/2467]	Loss: 0.228411Train Epoch: [2340/2467]	Loss: 0.631399

     Train Epoch: [2340/2467]	Loss: 0.170377
         Train Epoch: [2360/2467]	Loss: 0.108158
 Train Epoch: [2360/2467]	Loss: 0.451797
     Train Epoch: [2360/2467]	Loss: 0.212459
     Train Epoch: [2360/2467]	Loss: 0.280513
     Train Epoch: [2380/2467]	Loss: 0.343416
     Train Epoch: [2380/2467]	Loss: 0.655963
     Train Epoch: [2380/2467]	Loss: 0.608610
     Train Epoch: [2380/2467]	Loss: 0.434728
         Train Epoch: [2400/2467]	Loss: 0.333935
 Train Epoch: [2400/2467]	Loss: 0.220264
         Train Epoch: [2400/2467]	Loss: 0.376732
 Train Epoch: [2400/2467]	Loss: 0.382537
         Train Epoch: [2420/2467]	Loss: 0.381426
 Train Epoch: [2420/2467]	Loss: 0.279722    
 Train Epoch: [2420/2467]	Loss: 0.377265
     Train Epoch: [2420/2467]	Loss: 0.190360
              Train Epoch: [2440/2467]	Loss: 0.179192Train Epoch: [2440/2467]	Loss: 0.183841

 Train Epoch: [2440/2467]	Loss: 0.524668
     Train Epoch: [2440/2467]	Loss: 0.132781
     Train Epoch: [2460/2467]	Loss: 0.182356
         Train Epoch: [2460/2467]	Loss: 0.201989
 Train Epoch: [2460/2467]	Loss: 0.457287
     Train Epoch: [2460/2467]	Loss: 0.216942
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 7 epoch =====
     2025-05-11.02-35-59
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 7 epoch =====
     2025-05-11.02-35-59
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 7 epoch =====
     2025-05-11.02-36-00
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 7 epoch =====
     2025-05-11.02-36-00
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.508642
     Train Epoch: [0/2467]	Loss: 0.392815
          Train Epoch: [0/2467]	Loss: 0.435749Train Epoch: [0/2467]	Loss: 0.243115

         Train Epoch: [20/2467]	Loss: 0.146230
 Train Epoch: [20/2467]	Loss: 0.481390
     Train Epoch: [20/2467]	Loss: 0.385868
     Train Epoch: [20/2467]	Loss: 0.190706
                   Train Epoch: [40/2467]	Loss: 0.392674 Train Epoch: [40/2467]	Loss: 0.347275
Train Epoch: [40/2467]	Loss: 0.244628Train Epoch: [40/2467]	Loss: 0.633295


                   Train Epoch: [60/2467]	Loss: 0.295269Train Epoch: [60/2467]	Loss: 0.111420Train Epoch: [60/2467]	Loss: 0.294129


 Train Epoch: [60/2467]	Loss: 0.409971
             Train Epoch: [80/2467]	Loss: 0.227662 
Train Epoch: [80/2467]	Loss: 0.410740    
 Train Epoch: [80/2467]	Loss: 0.157861
 Train Epoch: [80/2467]	Loss: 0.248032
             Train Epoch: [100/2467]	Loss: 0.426583  
Train Epoch: [100/2467]	Loss: 0.295832Train Epoch: [100/2467]	Loss: 0.390662

     Train Epoch: [100/2467]	Loss: 0.250539
              Train Epoch: [120/2467]	Loss: 0.085256
 Train Epoch: [120/2467]	Loss: 0.410964
Train Epoch: [120/2467]	Loss: 0.456617
     Train Epoch: [120/2467]	Loss: 0.293308
             Train Epoch: [140/2467]	Loss: 0.122096
 Train Epoch: [140/2467]	Loss: 0.354887
 Train Epoch: [140/2467]	Loss: 0.295042
     Train Epoch: [140/2467]	Loss: 0.127419
         Train Epoch: [160/2467]	Loss: 0.300680     Train Epoch: [160/2467]	Loss: 0.372698

 Train Epoch: [160/2467]	Loss: 0.465656
     Train Epoch: [160/2467]	Loss: 0.187663
     Train Epoch: [180/2467]	Loss: 0.251137
     Train Epoch: [180/2467]	Loss: 0.304161    
 Train Epoch: [180/2467]	Loss: 0.237148
     Train Epoch: [180/2467]	Loss: 0.191657
         Train Epoch: [200/2467]	Loss: 0.178536
 Train Epoch: [200/2467]	Loss: 0.239108
         Train Epoch: [200/2467]	Loss: 0.242744
 Train Epoch: [200/2467]	Loss: 0.260937
              Train Epoch: [220/2467]	Loss: 0.164901Train Epoch: [220/2467]	Loss: 0.291596

 Train Epoch: [220/2467]	Loss: 0.417387
     Train Epoch: [220/2467]	Loss: 0.272167
         Train Epoch: [240/2467]	Loss: 0.046642 
    Train Epoch: [240/2467]	Loss: 0.288705
 Train Epoch: [240/2467]	Loss: 0.509248
     Train Epoch: [240/2467]	Loss: 0.117642
     Train Epoch: [260/2467]	Loss: 0.098720
     Train Epoch: [260/2467]	Loss: 0.187611
     Train Epoch: [260/2467]	Loss: 0.297680
     Train Epoch: [260/2467]	Loss: 0.095366
          Train Epoch: [280/2467]	Loss: 0.173673Train Epoch: [280/2467]	Loss: 0.299948

     Train Epoch: [280/2467]	Loss: 0.312001
     Train Epoch: [280/2467]	Loss: 0.345040
         Train Epoch: [300/2467]	Loss: 0.216546
 Train Epoch: [300/2467]	Loss: 0.101924
     Train Epoch: [300/2467]	Loss: 0.159938
     Train Epoch: [300/2467]	Loss: 0.531930
     Train Epoch: [320/2467]	Loss: 0.280694
     Train Epoch: [320/2467]	Loss: 0.385653
     Train Epoch: [320/2467]	Loss: 0.557652
     Train Epoch: [320/2467]	Loss: 0.320607
         Train Epoch: [340/2467]	Loss: 0.098654
 Train Epoch: [340/2467]	Loss: 0.211782
     Train Epoch: [340/2467]	Loss: 0.087594
     Train Epoch: [340/2467]	Loss: 0.270823
         Train Epoch: [360/2467]	Loss: 0.296754
 Train Epoch: [360/2467]	Loss: 0.259788
     Train Epoch: [360/2467]	Loss: 0.213681
     Train Epoch: [360/2467]	Loss: 0.231913
     Train Epoch: [380/2467]	Loss: 0.632991
         Train Epoch: [380/2467]	Loss: 0.217650
     Train Epoch: [380/2467]	Loss: 0.291401
 Train Epoch: [380/2467]	Loss: 1.044973
          Train Epoch: [400/2467]	Loss: 0.373102Train Epoch: [400/2467]	Loss: 0.132833

     Train Epoch: [400/2467]	Loss: 0.605294    
 Train Epoch: [400/2467]	Loss: 0.236537
     Train Epoch: [420/2467]	Loss: 0.323355
     Train Epoch: [420/2467]	Loss: 0.348299    
     Train Epoch: [420/2467]	Loss: 0.294662
 Train Epoch: [420/2467]	Loss: 0.154006
          Train Epoch: [440/2467]	Loss: 0.467663Train Epoch: [440/2467]	Loss: 0.173671

     Train Epoch: [440/2467]	Loss: 0.123902
     Train Epoch: [440/2467]	Loss: 0.219097
         Train Epoch: [460/2467]	Loss: 0.396044
 Train Epoch: [460/2467]	Loss: 0.229979
     Train Epoch: [460/2467]	Loss: 0.272053
     Train Epoch: [460/2467]	Loss: 0.216677
     Train Epoch: [480/2467]	Loss: 0.402621
     Train Epoch: [480/2467]	Loss: 0.234050
     Train Epoch: [480/2467]	Loss: 0.405998
     Train Epoch: [480/2467]	Loss: 0.271451
         Train Epoch: [500/2467]	Loss: 0.201069
 Train Epoch: [500/2467]	Loss: 0.147095
     Train Epoch: [500/2467]	Loss: 0.364573
     Train Epoch: [500/2467]	Loss: 0.594257
             Train Epoch: [520/2467]	Loss: 0.265479 
Train Epoch: [520/2467]	Loss: 0.314219
 Train Epoch: [520/2467]	Loss: 0.502378
     Train Epoch: [520/2467]	Loss: 0.147952
     Train Epoch: [540/2467]	Loss: 0.285575
          Train Epoch: [540/2467]	Loss: 0.319493Train Epoch: [540/2467]	Loss: 0.090181    

 Train Epoch: [540/2467]	Loss: 0.156927
     Train Epoch: [560/2467]	Loss: 0.330261
     Train Epoch: [560/2467]	Loss: 0.385930
     Train Epoch: [560/2467]	Loss: 0.375946
     Train Epoch: [560/2467]	Loss: 0.269096
          Train Epoch: [580/2467]	Loss: 0.204356    Train Epoch: [580/2467]	Loss: 0.137761

 Train Epoch: [580/2467]	Loss: 0.235499
     Train Epoch: [580/2467]	Loss: 0.397739
         Train Epoch: [600/2467]	Loss: 0.282812
 Train Epoch: [600/2467]	Loss: 0.354888    
     Train Epoch: [600/2467]	Loss: 0.250876
 Train Epoch: [600/2467]	Loss: 0.185723
          Train Epoch: [620/2467]	Loss: 0.426666Train Epoch: [620/2467]	Loss: 0.245957

     Train Epoch: [620/2467]	Loss: 0.087259    
 Train Epoch: [620/2467]	Loss: 0.417938
     Train Epoch: [640/2467]	Loss: 0.183262
     Train Epoch: [640/2467]	Loss: 0.420326
     Train Epoch: [640/2467]	Loss: 0.304143
     Train Epoch: [640/2467]	Loss: 0.288493
          Train Epoch: [660/2467]	Loss: 0.077824    Train Epoch: [660/2467]	Loss: 0.486183

 Train Epoch: [660/2467]	Loss: 0.126277
     Train Epoch: [660/2467]	Loss: 0.366367
         Train Epoch: [680/2467]	Loss: 0.292060
     Train Epoch: [680/2467]	Loss: 0.219462
 Train Epoch: [680/2467]	Loss: 0.134859
     Train Epoch: [680/2467]	Loss: 0.484943
         Train Epoch: [700/2467]	Loss: 0.124638
 Train Epoch: [700/2467]	Loss: 0.190100    
 Train Epoch: [700/2467]	Loss: 0.377176
     Train Epoch: [700/2467]	Loss: 0.482825
          Train Epoch: [720/2467]	Loss: 0.242550
Train Epoch: [720/2467]	Loss: 0.145908
         Train Epoch: [720/2467]	Loss: 0.267140
 Train Epoch: [720/2467]	Loss: 0.234880
         Train Epoch: [740/2467]	Loss: 0.102233 
Train Epoch: [740/2467]	Loss: 0.361701    
 Train Epoch: [740/2467]	Loss: 0.169058
     Train Epoch: [740/2467]	Loss: 0.167577
     Train Epoch: [760/2467]	Loss: 0.353425
         Train Epoch: [760/2467]	Loss: 0.562774
 Train Epoch: [760/2467]	Loss: 0.404249
     Train Epoch: [760/2467]	Loss: 0.155617
                 Train Epoch: [780/2467]	Loss: 0.332978 
Train Epoch: [780/2467]	Loss: 0.464300
  Train Epoch: [780/2467]	Loss: 0.390050Train Epoch: [780/2467]	Loss: 0.452299

             Train Epoch: [800/2467]	Loss: 0.467547
      Train Epoch: [800/2467]	Loss: 0.302516
 Train Epoch: [800/2467]	Loss: 0.367091Train Epoch: [800/2467]	Loss: 0.172505

     Train Epoch: [820/2467]	Loss: 0.082021
     Train Epoch: [820/2467]	Loss: 0.514091
     Train Epoch: [820/2467]	Loss: 0.335046
     Train Epoch: [820/2467]	Loss: 0.062719
          Train Epoch: [840/2467]	Loss: 0.383652Train Epoch: [840/2467]	Loss: 0.273497

         Train Epoch: [840/2467]	Loss: 0.156460 
Train Epoch: [840/2467]	Loss: 0.385925
         Train Epoch: [860/2467]	Loss: 0.432946
         Train Epoch: [860/2467]	Loss: 0.184584 
Train Epoch: [860/2467]	Loss: 0.220325 
Train Epoch: [860/2467]	Loss: 0.135275
     Train Epoch: [880/2467]	Loss: 0.265261
          Train Epoch: [880/2467]	Loss: 0.414316Train Epoch: [880/2467]	Loss: 0.513660

     Train Epoch: [880/2467]	Loss: 0.215021
             Train Epoch: [900/2467]	Loss: 0.275221 
Train Epoch: [900/2467]	Loss: 0.257809
 Train Epoch: [900/2467]	Loss: 0.186062
     Train Epoch: [900/2467]	Loss: 0.264065
         Train Epoch: [920/2467]	Loss: 0.539327 
Train Epoch: [920/2467]	Loss: 0.475574
         Train Epoch: [920/2467]	Loss: 0.293293
 Train Epoch: [920/2467]	Loss: 0.232071
     Train Epoch: [940/2467]	Loss: 0.075847
         Train Epoch: [940/2467]	Loss: 0.439457 
    Train Epoch: [940/2467]	Loss: 0.128507
 Train Epoch: [940/2467]	Loss: 0.233749
         Train Epoch: [960/2467]	Loss: 0.132045 
Train Epoch: [960/2467]	Loss: 0.453479    
     Train Epoch: [960/2467]	Loss: 0.114200
 Train Epoch: [960/2467]	Loss: 0.124894
     Train Epoch: [980/2467]	Loss: 0.316145
         Train Epoch: [980/2467]	Loss: 0.279212
     Train Epoch: [980/2467]	Loss: 0.409531
 Train Epoch: [980/2467]	Loss: 0.330514
     Train Epoch: [1000/2467]	Loss: 0.432491
     Train Epoch: [1000/2467]	Loss: 0.443790
     Train Epoch: [1000/2467]	Loss: 0.072155
     Train Epoch: [1000/2467]	Loss: 0.203107
         Train Epoch: [1020/2467]	Loss: 0.221578
 Train Epoch: [1020/2467]	Loss: 0.241461
          Train Epoch: [1020/2467]	Loss: 0.457725Train Epoch: [1020/2467]	Loss: 0.082866

         Train Epoch: [1040/2467]	Loss: 0.384183
 Train Epoch: [1040/2467]	Loss: 0.319969
     Train Epoch: [1040/2467]	Loss: 0.267560
     Train Epoch: [1040/2467]	Loss: 0.139615
          Train Epoch: [1060/2467]	Loss: 0.444359
Train Epoch: [1060/2467]	Loss: 0.581856
     Train Epoch: [1060/2467]	Loss: 0.225226
     Train Epoch: [1060/2467]	Loss: 0.387445
         Train Epoch: [1080/2467]	Loss: 0.261678
     Train Epoch: [1080/2467]	Loss: 0.172221
     Train Epoch: [1080/2467]	Loss: 0.383278
 Train Epoch: [1080/2467]	Loss: 0.408002
     Train Epoch: [1100/2467]	Loss: 0.451723
             Train Epoch: [1100/2467]	Loss: 0.319747 
Train Epoch: [1100/2467]	Loss: 0.466188
 Train Epoch: [1100/2467]	Loss: 0.357813
              Train Epoch: [1120/2467]	Loss: 0.175913Train Epoch: [1120/2467]	Loss: 0.212166

 Train Epoch: [1120/2467]	Loss: 0.500915
     Train Epoch: [1120/2467]	Loss: 0.269480
         Train Epoch: [1140/2467]	Loss: 0.095583
     Train Epoch: [1140/2467]	Loss: 0.321853
 Train Epoch: [1140/2467]	Loss: 0.228050
     Train Epoch: [1140/2467]	Loss: 0.464891
     Train Epoch: [1160/2467]	Loss: 0.268880
         Train Epoch: [1160/2467]	Loss: 0.108356 
Train Epoch: [1160/2467]	Loss: 0.284258
     Train Epoch: [1160/2467]	Loss: 0.271224
     Train Epoch: [1180/2467]	Loss: 0.133524
         Train Epoch: [1180/2467]	Loss: 0.214166
 Train Epoch: [1180/2467]	Loss: 0.401047
     Train Epoch: [1180/2467]	Loss: 0.345248
     Train Epoch: [1200/2467]	Loss: 0.385843
         Train Epoch: [1200/2467]	Loss: 0.261664
 Train Epoch: [1200/2467]	Loss: 0.360988
     Train Epoch: [1200/2467]	Loss: 0.187956
             Train Epoch: [1220/2467]	Loss: 0.303164
 Train Epoch: [1220/2467]	Loss: 0.195564 
Train Epoch: [1220/2467]	Loss: 0.495803
     Train Epoch: [1220/2467]	Loss: 0.594820
     Train Epoch: [1240/2467]	Loss: 0.363646
          Train Epoch: [1240/2467]	Loss: 0.306150Train Epoch: [1240/2467]	Loss: 0.766836

     Train Epoch: [1240/2467]	Loss: 0.346062
         Train Epoch: [1260/2467]	Loss: 0.288849
 Train Epoch: [1260/2467]	Loss: 0.233757
     Train Epoch: [1260/2467]	Loss: 0.107728
     Train Epoch: [1260/2467]	Loss: 0.520377
          Train Epoch: [1280/2467]	Loss: 0.410421Train Epoch: [1280/2467]	Loss: 0.136736

     Train Epoch: [1280/2467]	Loss: 0.504244
     Train Epoch: [1280/2467]	Loss: 1.117287
             Train Epoch: [1300/2467]	Loss: 0.160530
 Train Epoch: [1300/2467]	Loss: 0.386501
      Train Epoch: [1300/2467]	Loss: 0.472274
Train Epoch: [1300/2467]	Loss: 0.134199
         Train Epoch: [1320/2467]	Loss: 0.244250
 Train Epoch: [1320/2467]	Loss: 0.123925
     Train Epoch: [1320/2467]	Loss: 0.398579
     Train Epoch: [1320/2467]	Loss: 0.315653
          Train Epoch: [1340/2467]	Loss: 0.299974    
Train Epoch: [1340/2467]	Loss: 0.479785
     Train Epoch: [1340/2467]	Loss: 0.130764
 Train Epoch: [1340/2467]	Loss: 0.276225
     Train Epoch: [1360/2467]	Loss: 0.297496
         Train Epoch: [1360/2467]	Loss: 0.139657
 Train Epoch: [1360/2467]	Loss: 0.593451
     Train Epoch: [1360/2467]	Loss: 0.156313
     Train Epoch: [1380/2467]	Loss: 1.319530    
     Train Epoch: [1380/2467]	Loss: 0.450713 
Train Epoch: [1380/2467]	Loss: 0.382882    
 Train Epoch: [1380/2467]	Loss: 0.516119
              Train Epoch: [1400/2467]	Loss: 0.340986 Train Epoch: [1400/2467]	Loss: 0.297905

Train Epoch: [1400/2467]	Loss: 0.258974
     Train Epoch: [1400/2467]	Loss: 0.178855
         Train Epoch: [1420/2467]	Loss: 0.227214
 Train Epoch: [1420/2467]	Loss: 0.496922
         Train Epoch: [1420/2467]	Loss: 0.217410
 Train Epoch: [1420/2467]	Loss: 0.277978
              Train Epoch: [1440/2467]	Loss: 0.414096Train Epoch: [1440/2467]	Loss: 0.046694

 Train Epoch: [1440/2467]	Loss: 0.216428
     Train Epoch: [1440/2467]	Loss: 0.081363
             Train Epoch: [1460/2467]	Loss: 0.405390
 Train Epoch: [1460/2467]	Loss: 0.193532 
Train Epoch: [1460/2467]	Loss: 0.075610
     Train Epoch: [1460/2467]	Loss: 0.314378
         Train Epoch: [1480/2467]	Loss: 0.293091
 Train Epoch: [1480/2467]	Loss: 0.089359
     Train Epoch: [1480/2467]	Loss: 0.349569
     Train Epoch: [1480/2467]	Loss: 0.069208
         Train Epoch: [1500/2467]	Loss: 0.366886
 Train Epoch: [1500/2467]	Loss: 0.152112
     Train Epoch: [1500/2467]	Loss: 0.257227
     Train Epoch: [1500/2467]	Loss: 0.039285
     Train Epoch: [1520/2467]	Loss: 0.162931
     Train Epoch: [1520/2467]	Loss: 0.426892
     Train Epoch: [1520/2467]	Loss: 0.398112
     Train Epoch: [1520/2467]	Loss: 0.352510
             Train Epoch: [1540/2467]	Loss: 0.336099
  Train Epoch: [1540/2467]	Loss: 0.157408Train Epoch: [1540/2467]	Loss: 0.336683
    
 Train Epoch: [1540/2467]	Loss: 0.797184
             Train Epoch: [1560/2467]	Loss: 0.491929
  Train Epoch: [1560/2467]	Loss: 0.527937Train Epoch: [1560/2467]	Loss: 0.448411

     Train Epoch: [1560/2467]	Loss: 0.597851
         Train Epoch: [1580/2467]	Loss: 0.292602
         Train Epoch: [1580/2467]	Loss: 0.363750 
Train Epoch: [1580/2467]	Loss: 0.409132
 Train Epoch: [1580/2467]	Loss: 0.371339
               Train Epoch: [1600/2467]	Loss: 0.130880Train Epoch: [1600/2467]	Loss: 0.391890Train Epoch: [1600/2467]	Loss: 0.305350


     Train Epoch: [1600/2467]	Loss: 0.283220
         Train Epoch: [1620/2467]	Loss: 0.140448
 Train Epoch: [1620/2467]	Loss: 0.274500
     Train Epoch: [1620/2467]	Loss: 0.308164
     Train Epoch: [1620/2467]	Loss: 0.089918
         Train Epoch: [1640/2467]	Loss: 0.264838
     Train Epoch: [1640/2467]	Loss: 0.318709
 Train Epoch: [1640/2467]	Loss: 0.285501
     Train Epoch: [1640/2467]	Loss: 0.333350
         Train Epoch: [1660/2467]	Loss: 0.186915
     Train Epoch: [1660/2467]	Loss: 0.325114
     Train Epoch: [1660/2467]	Loss: 0.449048
 Train Epoch: [1660/2467]	Loss: 0.158672
         Train Epoch: [1680/2467]	Loss: 0.123086
         Train Epoch: [1680/2467]	Loss: 0.406984
 Train Epoch: [1680/2467]	Loss: 0.367278
 Train Epoch: [1680/2467]	Loss: 0.283239
              Train Epoch: [1700/2467]	Loss: 0.339628Train Epoch: [1700/2467]	Loss: 0.304711

     Train Epoch: [1700/2467]	Loss: 0.240662
 Train Epoch: [1700/2467]	Loss: 0.302200
     Train Epoch: [1720/2467]	Loss: 0.249586
         Train Epoch: [1720/2467]	Loss: 0.508150
     Train Epoch: [1720/2467]	Loss: 0.440022
 Train Epoch: [1720/2467]	Loss: 0.302050
             Train Epoch: [1740/2467]	Loss: 0.319265
 Train Epoch: [1740/2467]	Loss: 0.319523
     Train Epoch: [1740/2467]	Loss: 0.435953
 Train Epoch: [1740/2467]	Loss: 0.246909
     Train Epoch: [1760/2467]	Loss: 0.254097
         Train Epoch: [1760/2467]	Loss: 0.157435
 Train Epoch: [1760/2467]	Loss: 0.129916
     Train Epoch: [1760/2467]	Loss: 0.177565
     Train Epoch: [1780/2467]	Loss: 0.354740
         Train Epoch: [1780/2467]	Loss: 0.145752
 Train Epoch: [1780/2467]	Loss: 0.339096
     Train Epoch: [1780/2467]	Loss: 0.375649
         Train Epoch: [1800/2467]	Loss: 0.265987
     Train Epoch: [1800/2467]	Loss: 0.718321
 Train Epoch: [1800/2467]	Loss: 0.336768
     Train Epoch: [1800/2467]	Loss: 0.435791
              Train Epoch: [1820/2467]	Loss: 0.141972Train Epoch: [1820/2467]	Loss: 0.493546

     Train Epoch: [1820/2467]	Loss: 0.208405
 Train Epoch: [1820/2467]	Loss: 0.265589
          Train Epoch: [1840/2467]	Loss: 0.445656
Train Epoch: [1840/2467]	Loss: 0.192724
     Train Epoch: [1840/2467]	Loss: 0.341855
     Train Epoch: [1840/2467]	Loss: 0.157993
             Train Epoch: [1860/2467]	Loss: 0.229249
 Train Epoch: [1860/2467]	Loss: 0.266574
     Train Epoch: [1860/2467]	Loss: 0.291998
 Train Epoch: [1860/2467]	Loss: 0.149343
              Train Epoch: [1880/2467]	Loss: 0.080316Train Epoch: [1880/2467]	Loss: 0.206352

 Train Epoch: [1880/2467]	Loss: 0.185627
     Train Epoch: [1880/2467]	Loss: 0.164858
     Train Epoch: [1900/2467]	Loss: 0.448436
         Train Epoch: [1900/2467]	Loss: 0.251778
     Train Epoch: [1900/2467]	Loss: 0.358449
 Train Epoch: [1900/2467]	Loss: 0.214257
         Train Epoch: [1920/2467]	Loss: 0.279749
     Train Epoch: [1920/2467]	Loss: 0.036408
     Train Epoch: [1920/2467]	Loss: 0.359052
 Train Epoch: [1920/2467]	Loss: 0.285990
          Train Epoch: [1940/2467]	Loss: 0.458562Train Epoch: [1940/2467]	Loss: 0.549616

     Train Epoch: [1940/2467]	Loss: 0.180605
     Train Epoch: [1940/2467]	Loss: 0.149134
         Train Epoch: [1960/2467]	Loss: 0.255362
         Train Epoch: [1960/2467]	Loss: 0.367525
 Train Epoch: [1960/2467]	Loss: 0.388330 
Train Epoch: [1960/2467]	Loss: 0.293444
     Train Epoch: [1980/2467]	Loss: 0.342236
         Train Epoch: [1980/2467]	Loss: 0.404814
 Train Epoch: [1980/2467]	Loss: 0.396353
     Train Epoch: [1980/2467]	Loss: 0.371080
         Train Epoch: [2000/2467]	Loss: 0.146159    
 Train Epoch: [2000/2467]	Loss: 0.255005
 Train Epoch: [2000/2467]	Loss: 0.105084    
 Train Epoch: [2000/2467]	Loss: 0.541728
          Train Epoch: [2020/2467]	Loss: 0.679341Train Epoch: [2020/2467]	Loss: 0.394891
    
     Train Epoch: [2020/2467]	Loss: 0.380920
 Train Epoch: [2020/2467]	Loss: 0.182326
          Train Epoch: [2040/2467]	Loss: 0.479489Train Epoch: [2040/2467]	Loss: 0.286925

          Train Epoch: [2040/2467]	Loss: 0.342729
Train Epoch: [2040/2467]	Loss: 0.133287
         Train Epoch: [2060/2467]	Loss: 0.278325
     Train Epoch: [2060/2467]	Loss: 0.392318
 Train Epoch: [2060/2467]	Loss: 0.361865
     Train Epoch: [2060/2467]	Loss: 0.394774
                 Train Epoch: [2080/2467]	Loss: 0.260475
 Train Epoch: [2080/2467]	Loss: 0.177375
 Train Epoch: [2080/2467]	Loss: 0.158252
 Train Epoch: [2080/2467]	Loss: 0.340744
         Train Epoch: [2100/2467]	Loss: 0.704953    
  Train Epoch: [2100/2467]	Loss: 0.171844Train Epoch: [2100/2467]	Loss: 0.200242

     Train Epoch: [2100/2467]	Loss: 0.236820
     Train Epoch: [2120/2467]	Loss: 0.181907
         Train Epoch: [2120/2467]	Loss: 0.403677 
Train Epoch: [2120/2467]	Loss: 0.175834    
 Train Epoch: [2120/2467]	Loss: 0.255680
         Train Epoch: [2140/2467]	Loss: 0.279031
     Train Epoch: [2140/2467]	Loss: 0.442736
 Train Epoch: [2140/2467]	Loss: 0.624437
     Train Epoch: [2140/2467]	Loss: 0.521965
         Train Epoch: [2160/2467]	Loss: 0.091148    
 Train Epoch: [2160/2467]	Loss: 0.199639
 Train Epoch: [2160/2467]	Loss: 0.491398    
 Train Epoch: [2160/2467]	Loss: 0.295317
         Train Epoch: [2180/2467]	Loss: 0.380980
     Train Epoch: [2180/2467]	Loss: 0.197982
     Train Epoch: [2180/2467]	Loss: 0.387059
 Train Epoch: [2180/2467]	Loss: 0.218836
         Train Epoch: [2200/2467]	Loss: 0.183148
     Train Epoch: [2200/2467]	Loss: 0.236319
     Train Epoch: [2200/2467]	Loss: 0.669113
 Train Epoch: [2200/2467]	Loss: 0.213672
     Train Epoch: [2220/2467]	Loss: 0.301515
         Train Epoch: [2220/2467]	Loss: 0.231361
 Train Epoch: [2220/2467]	Loss: 0.157061
     Train Epoch: [2220/2467]	Loss: 0.147411
          Train Epoch: [2240/2467]	Loss: 0.094516Train Epoch: [2240/2467]	Loss: 0.110015    

 Train Epoch: [2240/2467]	Loss: 0.255138    
 Train Epoch: [2240/2467]	Loss: 0.161074
         Train Epoch: [2260/2467]	Loss: 0.356545
     Train Epoch: [2260/2467]	Loss: 0.496352
 Train Epoch: [2260/2467]	Loss: 0.202402
     Train Epoch: [2260/2467]	Loss: 0.253348
              Train Epoch: [2280/2467]	Loss: 0.135418Train Epoch: [2280/2467]	Loss: 0.189302

 Train Epoch: [2280/2467]	Loss: 0.272700
     Train Epoch: [2280/2467]	Loss: 0.256807
     Train Epoch: [2300/2467]	Loss: 0.095763
         Train Epoch: [2300/2467]	Loss: 0.490317
 Train Epoch: [2300/2467]	Loss: 0.150232
     Train Epoch: [2300/2467]	Loss: 0.198197
         Train Epoch: [2320/2467]	Loss: 0.397310
         Train Epoch: [2320/2467]	Loss: 0.084348 
Train Epoch: [2320/2467]	Loss: 0.207820
 Train Epoch: [2320/2467]	Loss: 0.326348
         Train Epoch: [2340/2467]	Loss: 0.143491
 Train Epoch: [2340/2467]	Loss: 0.189586    
     Train Epoch: [2340/2467]	Loss: 0.228949
 Train Epoch: [2340/2467]	Loss: 0.633559
     Train Epoch: [2360/2467]	Loss: 0.090713
     Train Epoch: [2360/2467]	Loss: 0.409928
          Train Epoch: [2360/2467]	Loss: 0.236680Train Epoch: [2360/2467]	Loss: 0.248547

         Train Epoch: [2380/2467]	Loss: 0.692534
 Train Epoch: [2380/2467]	Loss: 0.363247
     Train Epoch: [2380/2467]	Loss: 0.559584
     Train Epoch: [2380/2467]	Loss: 0.366742
         Train Epoch: [2400/2467]	Loss: 0.383759
 Train Epoch: [2400/2467]	Loss: 0.186299
         Train Epoch: [2400/2467]	Loss: 0.237406 
Train Epoch: [2400/2467]	Loss: 0.258509
             Train Epoch: [2420/2467]	Loss: 0.389494 
Train Epoch: [2420/2467]	Loss: 0.160700 
Train Epoch: [2420/2467]	Loss: 0.337617
     Train Epoch: [2420/2467]	Loss: 0.352427
         Train Epoch: [2440/2467]	Loss: 0.177252    
 Train Epoch: [2440/2467]	Loss: 0.098148
 Train Epoch: [2440/2467]	Loss: 0.497252
     Train Epoch: [2440/2467]	Loss: 0.204846
          Train Epoch: [2460/2467]	Loss: 0.179015Train Epoch: [2460/2467]	Loss: 0.217523

         Train Epoch: [2460/2467]	Loss: 0.397633
 Train Epoch: [2460/2467]	Loss: 0.189037
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 8 epoch =====
     2025-05-11.02-57-49
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 8 epoch =====
     2025-05-11.02-57-49
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 8 epoch =====
     2025-05-11.02-57-49
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 8 epoch =====
     2025-05-11.02-57-50
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.397424
         Train Epoch: [0/2467]	Loss: 0.241691
 Train Epoch: [0/2467]	Loss: 0.417350
     Train Epoch: [0/2467]	Loss: 0.542023
          Train Epoch: [20/2467]	Loss: 0.497966
Train Epoch: [20/2467]	Loss: 0.153448    
 Train Epoch: [20/2467]	Loss: 0.191720
     Train Epoch: [20/2467]	Loss: 0.399325
     Train Epoch: [40/2467]	Loss: 0.285983
     Train Epoch: [40/2467]	Loss: 0.600733
     Train Epoch: [40/2467]	Loss: 0.243495
     Train Epoch: [40/2467]	Loss: 0.324919
         Train Epoch: [60/2467]	Loss: 0.290646
     Train Epoch: [60/2467]	Loss: 0.093364
 Train Epoch: [60/2467]	Loss: 0.260406    
 Train Epoch: [60/2467]	Loss: 0.396788
         Train Epoch: [80/2467]	Loss: 0.467800
 Train Epoch: [80/2467]	Loss: 0.175383
         Train Epoch: [80/2467]	Loss: 0.223421
 Train Epoch: [80/2467]	Loss: 0.195923
     Train Epoch: [100/2467]	Loss: 0.292538
          Train Epoch: [100/2467]	Loss: 0.443100Train Epoch: [100/2467]	Loss: 0.259045

     Train Epoch: [100/2467]	Loss: 0.343736
     Train Epoch: [120/2467]	Loss: 0.188435
     Train Epoch: [120/2467]	Loss: 0.451114
     Train Epoch: [120/2467]	Loss: 0.352389
     Train Epoch: [120/2467]	Loss: 0.082151
         Train Epoch: [140/2467]	Loss: 0.278966
 Train Epoch: [140/2467]	Loss: 0.103330
     Train Epoch: [140/2467]	Loss: 0.405048
     Train Epoch: [140/2467]	Loss: 0.133805
         Train Epoch: [160/2467]	Loss: 0.402116 
Train Epoch: [160/2467]	Loss: 0.325172
         Train Epoch: [160/2467]	Loss: 0.522886
 Train Epoch: [160/2467]	Loss: 0.168813
             Train Epoch: [180/2467]	Loss: 0.190129 
    Train Epoch: [180/2467]	Loss: 0.277542
 Train Epoch: [180/2467]	Loss: 0.283807
 Train Epoch: [180/2467]	Loss: 0.234941
         Train Epoch: [200/2467]	Loss: 0.201900
 Train Epoch: [200/2467]	Loss: 0.235916
     Train Epoch: [200/2467]	Loss: 0.213691
     Train Epoch: [200/2467]	Loss: 0.253512
             Train Epoch: [220/2467]	Loss: 0.193949
 Train Epoch: [220/2467]	Loss: 0.286624 
Train Epoch: [220/2467]	Loss: 0.342161
     Train Epoch: [220/2467]	Loss: 0.253686
             Train Epoch: [240/2467]	Loss: 0.313104 
Train Epoch: [240/2467]	Loss: 0.043617
 Train Epoch: [240/2467]	Loss: 0.532628
     Train Epoch: [240/2467]	Loss: 0.106291
         Train Epoch: [260/2467]	Loss: 0.094104
 Train Epoch: [260/2467]	Loss: 0.111607    
     Train Epoch: [260/2467]	Loss: 0.183730
 Train Epoch: [260/2467]	Loss: 0.325174
     Train Epoch: [280/2467]	Loss: 0.338602    
 Train Epoch: [280/2467]	Loss: 0.325623
     Train Epoch: [280/2467]	Loss: 0.099384
     Train Epoch: [280/2467]	Loss: 0.334510
          Train Epoch: [300/2467]	Loss: 0.511536Train Epoch: [300/2467]	Loss: 0.207553

     Train Epoch: [300/2467]	Loss: 0.098621
     Train Epoch: [300/2467]	Loss: 0.151229
     Train Epoch: [320/2467]	Loss: 0.502936
     Train Epoch: [320/2467]	Loss: 0.532134
     Train Epoch: [320/2467]	Loss: 0.248533
     Train Epoch: [320/2467]	Loss: 0.302226
     Train Epoch: [340/2467]	Loss: 0.038456
         Train Epoch: [340/2467]	Loss: 0.095010
 Train Epoch: [340/2467]	Loss: 0.207966
     Train Epoch: [340/2467]	Loss: 0.253571
         Train Epoch: [360/2467]	Loss: 0.166782
     Train Epoch: [360/2467]	Loss: 0.344313
 Train Epoch: [360/2467]	Loss: 0.317485
     Train Epoch: [360/2467]	Loss: 0.213889
     Train Epoch: [380/2467]	Loss: 0.591245
     Train Epoch: [380/2467]	Loss: 0.189770
     Train Epoch: [380/2467]	Loss: 1.616686
     Train Epoch: [380/2467]	Loss: 0.291821
         Train Epoch: [400/2467]	Loss: 0.380416 
Train Epoch: [400/2467]	Loss: 0.156519
     Train Epoch: [400/2467]	Loss: 0.242722
     Train Epoch: [400/2467]	Loss: 0.669616
     Train Epoch: [420/2467]	Loss: 0.331351
     Train Epoch: [420/2467]	Loss: 0.155089
          Train Epoch: [420/2467]	Loss: 0.198450Train Epoch: [420/2467]	Loss: 0.364176

     Train Epoch: [440/2467]	Loss: 0.216491
         Train Epoch: [440/2467]	Loss: 0.166385 
Train Epoch: [440/2467]	Loss: 0.407979
     Train Epoch: [440/2467]	Loss: 0.147553
         Train Epoch: [460/2467]	Loss: 0.248227
 Train Epoch: [460/2467]	Loss: 0.189148    
 Train Epoch: [460/2467]	Loss: 0.233851
     Train Epoch: [460/2467]	Loss: 0.360609
     Train Epoch: [480/2467]	Loss: 0.418314    
 Train Epoch: [480/2467]	Loss: 0.203881
     Train Epoch: [480/2467]	Loss: 0.275061
     Train Epoch: [480/2467]	Loss: 0.400255
         Train Epoch: [500/2467]	Loss: 0.312026
 Train Epoch: [500/2467]	Loss: 0.163267
     Train Epoch: [500/2467]	Loss: 0.468528
     Train Epoch: [500/2467]	Loss: 0.146530
     Train Epoch: [520/2467]	Loss: 0.322308
     Train Epoch: [520/2467]	Loss: 0.272231
     Train Epoch: [520/2467]	Loss: 0.477203
     Train Epoch: [520/2467]	Loss: 0.117762
     Train Epoch: [540/2467]	Loss: 0.311867
     Train Epoch: [540/2467]	Loss: 0.097618
     Train Epoch: [540/2467]	Loss: 0.243295
     Train Epoch: [540/2467]	Loss: 0.153109
         Train Epoch: [560/2467]	Loss: 0.289284
 Train Epoch: [560/2467]	Loss: 0.392162
          Train Epoch: [560/2467]	Loss: 0.368049Train Epoch: [560/2467]	Loss: 0.234512

     Train Epoch: [580/2467]	Loss: 0.200869
         Train Epoch: [580/2467]	Loss: 0.120259
 Train Epoch: [580/2467]	Loss: 0.210504
     Train Epoch: [580/2467]	Loss: 0.432064
     Train Epoch: [600/2467]	Loss: 0.246014
          Train Epoch: [600/2467]	Loss: 0.168829Train Epoch: [600/2467]	Loss: 0.259344

     Train Epoch: [600/2467]	Loss: 0.282146
         Train Epoch: [620/2467]	Loss: 0.277521
     Train Epoch: [620/2467]	Loss: 0.374364
 Train Epoch: [620/2467]	Loss: 0.076193
     Train Epoch: [620/2467]	Loss: 0.352289
         Train Epoch: [640/2467]	Loss: 0.252209 
Train Epoch: [640/2467]	Loss: 0.277301    
 Train Epoch: [640/2467]	Loss: 0.277888
     Train Epoch: [640/2467]	Loss: 0.358254
         Train Epoch: [660/2467]	Loss: 0.510369
     Train Epoch: [660/2467]	Loss: 0.063807    
  Train Epoch: [660/2467]	Loss: 0.077947
Train Epoch: [660/2467]	Loss: 0.392244
          Train Epoch: [680/2467]	Loss: 0.212668Train Epoch: [680/2467]	Loss: 0.305427

         Train Epoch: [680/2467]	Loss: 0.099471
 Train Epoch: [680/2467]	Loss: 0.288151
         Train Epoch: [700/2467]	Loss: 0.110848
 Train Epoch: [700/2467]	Loss: 0.144964
     Train Epoch: [700/2467]	Loss: 0.374251
     Train Epoch: [700/2467]	Loss: 0.453794
         Train Epoch: [720/2467]	Loss: 0.290377
 Train Epoch: [720/2467]	Loss: 0.244906
     Train Epoch: [720/2467]	Loss: 0.383774
     Train Epoch: [720/2467]	Loss: 0.223054
          Train Epoch: [740/2467]	Loss: 0.359604Train Epoch: [740/2467]	Loss: 0.091364

     Train Epoch: [740/2467]	Loss: 0.187818    
 Train Epoch: [740/2467]	Loss: 0.162854
         Train Epoch: [760/2467]	Loss: 0.180357
     Train Epoch: [760/2467]	Loss: 0.321858
 Train Epoch: [760/2467]	Loss: 0.387942
     Train Epoch: [760/2467]	Loss: 0.556988
          Train Epoch: [780/2467]	Loss: 0.392519Train Epoch: [780/2467]	Loss: 0.371429
    
 Train Epoch: [780/2467]	Loss: 0.401378    
 Train Epoch: [780/2467]	Loss: 0.389992
         Train Epoch: [800/2467]	Loss: 0.378361
 Train Epoch: [800/2467]	Loss: 0.304671
     Train Epoch: [800/2467]	Loss: 0.166163
     Train Epoch: [800/2467]	Loss: 0.375250
         Train Epoch: [820/2467]	Loss: 0.082942
 Train Epoch: [820/2467]	Loss: 0.320043
         Train Epoch: [820/2467]	Loss: 0.075124
 Train Epoch: [820/2467]	Loss: 0.494486
          Train Epoch: [840/2467]	Loss: 0.401274Train Epoch: [840/2467]	Loss: 0.275569

     Train Epoch: [840/2467]	Loss: 0.276716
     Train Epoch: [840/2467]	Loss: 0.166674
          Train Epoch: [860/2467]	Loss: 0.182877Train Epoch: [860/2467]	Loss: 0.467060

     Train Epoch: [860/2467]	Loss: 0.122494
     Train Epoch: [860/2467]	Loss: 0.232952
             Train Epoch: [880/2467]	Loss: 0.407091
  Train Epoch: [880/2467]	Loss: 0.220183Train Epoch: [880/2467]	Loss: 0.206244

     Train Epoch: [880/2467]	Loss: 0.277271
         Train Epoch: [900/2467]	Loss: 0.267345    
 Train Epoch: [900/2467]	Loss: 0.176075
 Train Epoch: [900/2467]	Loss: 0.256614
     Train Epoch: [900/2467]	Loss: 0.240023
     Train Epoch: [920/2467]	Loss: 0.676197
     Train Epoch: [920/2467]	Loss: 0.106162
     Train Epoch: [920/2467]	Loss: 0.305729
     Train Epoch: [920/2467]	Loss: 0.446086
     Train Epoch: [940/2467]	Loss: 0.067171
          Train Epoch: [940/2467]	Loss: 0.215925
Train Epoch: [940/2467]	Loss: 1.217954
     Train Epoch: [940/2467]	Loss: 0.307906
         Train Epoch: [960/2467]	Loss: 0.148925
     Train Epoch: [960/2467]	Loss: 0.432329
     Train Epoch: [960/2467]	Loss: 0.130619
 Train Epoch: [960/2467]	Loss: 0.166185
              Train Epoch: [980/2467]	Loss: 0.300690 Train Epoch: [980/2467]	Loss: 0.142363    

Train Epoch: [980/2467]	Loss: 0.365968
 Train Epoch: [980/2467]	Loss: 0.293465
             Train Epoch: [1000/2467]	Loss: 0.434780 
    Train Epoch: [1000/2467]	Loss: 0.157744
 Train Epoch: [1000/2467]	Loss: 0.417470
 Train Epoch: [1000/2467]	Loss: 0.067734
         Train Epoch: [1020/2467]	Loss: 0.220925
 Train Epoch: [1020/2467]	Loss: 0.449738
     Train Epoch: [1020/2467]	Loss: 0.252313
     Train Epoch: [1020/2467]	Loss: 0.090421
     Train Epoch: [1040/2467]	Loss: 0.353427
     Train Epoch: [1040/2467]	Loss: 0.122508
          Train Epoch: [1040/2467]	Loss: 0.227595
Train Epoch: [1040/2467]	Loss: 0.300963
         Train Epoch: [1060/2467]	Loss: 0.380373
 Train Epoch: [1060/2467]	Loss: 0.575918
     Train Epoch: [1060/2467]	Loss: 0.314552
     Train Epoch: [1060/2467]	Loss: 0.187173
     Train Epoch: [1080/2467]	Loss: 0.241848
              Train Epoch: [1080/2467]	Loss: 0.405878Train Epoch: [1080/2467]	Loss: 0.159225

 Train Epoch: [1080/2467]	Loss: 0.406331
         Train Epoch: [1100/2467]	Loss: 0.314623    
 Train Epoch: [1100/2467]	Loss: 0.515236
 Train Epoch: [1100/2467]	Loss: 0.458706
     Train Epoch: [1100/2467]	Loss: 0.344576
     Train Epoch: [1120/2467]	Loss: 0.161651
     Train Epoch: [1120/2467]	Loss: 0.308836
     Train Epoch: [1120/2467]	Loss: 0.177320
     Train Epoch: [1120/2467]	Loss: 0.445684
         Train Epoch: [1140/2467]	Loss: 0.098766
 Train Epoch: [1140/2467]	Loss: 0.326706
     Train Epoch: [1140/2467]	Loss: 0.183031
     Train Epoch: [1140/2467]	Loss: 0.477735
         Train Epoch: [1160/2467]	Loss: 0.253717    
 Train Epoch: [1160/2467]	Loss: 0.239725
 Train Epoch: [1160/2467]	Loss: 0.116954
     Train Epoch: [1160/2467]	Loss: 0.237966
     Train Epoch: [1180/2467]	Loss: 0.139568
         Train Epoch: [1180/2467]	Loss: 0.406188 
Train Epoch: [1180/2467]	Loss: 0.228193
     Train Epoch: [1180/2467]	Loss: 0.343370
         Train Epoch: [1200/2467]	Loss: 0.388628 
Train Epoch: [1200/2467]	Loss: 0.207677
     Train Epoch: [1200/2467]	Loss: 0.324198
     Train Epoch: [1200/2467]	Loss: 0.273505
         Train Epoch: [1220/2467]	Loss: 0.298326     
Train Epoch: [1220/2467]	Loss: 0.475369
 Train Epoch: [1220/2467]	Loss: 0.154670
     Train Epoch: [1220/2467]	Loss: 0.506038
     Train Epoch: [1240/2467]	Loss: 0.356862
         Train Epoch: [1240/2467]	Loss: 0.340892
 Train Epoch: [1240/2467]	Loss: 0.488265    
 Train Epoch: [1240/2467]	Loss: 0.443657
         Train Epoch: [1260/2467]	Loss: 0.145974
     Train Epoch: [1260/2467]	Loss: 0.198401
 Train Epoch: [1260/2467]	Loss: 0.095535
     Train Epoch: [1260/2467]	Loss: 0.487894
         Train Epoch: [1280/2467]	Loss: 0.115365 
Train Epoch: [1280/2467]	Loss: 0.972363
         Train Epoch: [1280/2467]	Loss: 0.517061 
Train Epoch: [1280/2467]	Loss: 0.419843
     Train Epoch: [1300/2467]	Loss: 0.207661
     Train Epoch: [1300/2467]	Loss: 0.415344
     Train Epoch: [1300/2467]	Loss: 0.497752
     Train Epoch: [1300/2467]	Loss: 0.105146
         Train Epoch: [1320/2467]	Loss: 0.127805 
Train Epoch: [1320/2467]	Loss: 0.203567
     Train Epoch: [1320/2467]	Loss: 0.374863
     Train Epoch: [1320/2467]	Loss: 0.335062
          Train Epoch: [1340/2467]	Loss: 0.285780Train Epoch: [1340/2467]	Loss: 0.423618

         Train Epoch: [1340/2467]	Loss: 0.128269
 Train Epoch: [1340/2467]	Loss: 0.231796
         Train Epoch: [1360/2467]	Loss: 0.155787
     Train Epoch: [1360/2467]	Loss: 0.427693
 Train Epoch: [1360/2467]	Loss: 0.127536
     Train Epoch: [1360/2467]	Loss: 0.233369
     Train Epoch: [1380/2467]	Loss: 0.811121
         Train Epoch: [1380/2467]	Loss: 0.275400
 Train Epoch: [1380/2467]	Loss: 0.479816
     Train Epoch: [1380/2467]	Loss: 0.357451
     Train Epoch: [1400/2467]	Loss: 0.339228
     Train Epoch: [1400/2467]	Loss: 0.206978
          Train Epoch: [1400/2467]	Loss: 0.289318Train Epoch: [1400/2467]	Loss: 0.369355

         Train Epoch: [1420/2467]	Loss: 0.246840
 Train Epoch: [1420/2467]	Loss: 0.479914
     Train Epoch: [1420/2467]	Loss: 0.232998
     Train Epoch: [1420/2467]	Loss: 0.210332
     Train Epoch: [1440/2467]	Loss: 0.025358
              Train Epoch: [1440/2467]	Loss: 0.274213
Train Epoch: [1440/2467]	Loss: 0.113843 
Train Epoch: [1440/2467]	Loss: 0.079684
     Train Epoch: [1460/2467]	Loss: 0.277778
     Train Epoch: [1460/2467]	Loss: 0.192142    
     Train Epoch: [1460/2467]	Loss: 0.068605
 Train Epoch: [1460/2467]	Loss: 0.288134
     Train Epoch: [1480/2467]	Loss: 0.276456
     Train Epoch: [1480/2467]	Loss: 0.361252
     Train Epoch: [1480/2467]	Loss: 0.066315
     Train Epoch: [1480/2467]	Loss: 0.025612
     Train Epoch: [1500/2467]	Loss: 0.284529
          Train Epoch: [1500/2467]	Loss: 0.042791Train Epoch: [1500/2467]	Loss: 0.226682

     Train Epoch: [1500/2467]	Loss: 0.142187
     Train Epoch: [1520/2467]	Loss: 0.470083
     Train Epoch: [1520/2467]	Loss: 0.365147
         Train Epoch: [1520/2467]	Loss: 0.398679 
Train Epoch: [1520/2467]	Loss: 0.170647
     Train Epoch: [1540/2467]	Loss: 0.375844
             Train Epoch: [1540/2467]	Loss: 0.349361
 Train Epoch: [1540/2467]	Loss: 0.638108
 Train Epoch: [1540/2467]	Loss: 0.188345
         Train Epoch: [1560/2467]	Loss: 0.480895
 Train Epoch: [1560/2467]	Loss: 0.455472
     Train Epoch: [1560/2467]	Loss: 0.634168
     Train Epoch: [1560/2467]	Loss: 0.207644
             Train Epoch: [1580/2467]	Loss: 0.279921
  Train Epoch: [1580/2467]	Loss: 0.393388Train Epoch: [1580/2467]	Loss: 0.382455

     Train Epoch: [1580/2467]	Loss: 0.366125
     Train Epoch: [1600/2467]	Loss: 0.139801
         Train Epoch: [1600/2467]	Loss: 0.292659 
Train Epoch: [1600/2467]	Loss: 0.395538
     Train Epoch: [1600/2467]	Loss: 0.244054
     Train Epoch: [1620/2467]	Loss: 0.124062
     Train Epoch: [1620/2467]	Loss: 0.252903
     Train Epoch: [1620/2467]	Loss: 0.066138
     Train Epoch: [1620/2467]	Loss: 0.261077
     Train Epoch: [1640/2467]	Loss: 0.285022    
     Train Epoch: [1640/2467]	Loss: 0.259306
     Train Epoch: [1640/2467]	Loss: 0.221679
 Train Epoch: [1640/2467]	Loss: 0.300441
          Train Epoch: [1660/2467]	Loss: 0.331209Train Epoch: [1660/2467]	Loss: 0.250099

     Train Epoch: [1660/2467]	Loss: 0.139447
     Train Epoch: [1660/2467]	Loss: 0.389426
         Train Epoch: [1680/2467]	Loss: 0.645235
 Train Epoch: [1680/2467]	Loss: 0.291988
         Train Epoch: [1680/2467]	Loss: 0.282128
 Train Epoch: [1680/2467]	Loss: 0.151666
         Train Epoch: [1700/2467]	Loss: 0.288520
     Train Epoch: [1700/2467]	Loss: 0.311749
 Train Epoch: [1700/2467]	Loss: 0.207974
     Train Epoch: [1700/2467]	Loss: 0.257164
             Train Epoch: [1720/2467]	Loss: 0.333955
  Train Epoch: [1720/2467]	Loss: 0.260207Train Epoch: [1720/2467]	Loss: 0.185232

     Train Epoch: [1720/2467]	Loss: 0.327264
          Train Epoch: [1740/2467]	Loss: 0.156473    
Train Epoch: [1740/2467]	Loss: 0.165131
 Train Epoch: [1740/2467]	Loss: 0.546264
     Train Epoch: [1740/2467]	Loss: 0.253486
     Train Epoch: [1760/2467]	Loss: 0.257619
         Train Epoch: [1760/2467]	Loss: 0.187196 
Train Epoch: [1760/2467]	Loss: 0.122023
     Train Epoch: [1760/2467]	Loss: 0.339856
         Train Epoch: [1780/2467]	Loss: 0.136131
 Train Epoch: [1780/2467]	Loss: 0.373801
     Train Epoch: [1780/2467]	Loss: 0.381912
     Train Epoch: [1780/2467]	Loss: 0.307461
          Train Epoch: [1800/2467]	Loss: 0.342251Train Epoch: [1800/2467]	Loss: 0.431143

     Train Epoch: [1800/2467]	Loss: 0.540319
     Train Epoch: [1800/2467]	Loss: 0.299698
               Train Epoch: [1820/2467]	Loss: 0.527385Train Epoch: [1820/2467]	Loss: 0.202975Train Epoch: [1820/2467]	Loss: 0.137644


     Train Epoch: [1820/2467]	Loss: 0.278828
          Train Epoch: [1840/2467]	Loss: 0.426460
Train Epoch: [1840/2467]	Loss: 0.331309
     Train Epoch: [1840/2467]	Loss: 0.095956
     Train Epoch: [1840/2467]	Loss: 0.190069
     Train Epoch: [1860/2467]	Loss: 0.360514
     Train Epoch: [1860/2467]	Loss: 0.283126
     Train Epoch: [1860/2467]	Loss: 0.140281
     Train Epoch: [1860/2467]	Loss: 0.242776
         Train Epoch: [1880/2467]	Loss: 0.191390
 Train Epoch: [1880/2467]	Loss: 0.087907
     Train Epoch: [1880/2467]	Loss: 0.144137
     Train Epoch: [1880/2467]	Loss: 0.183182
     Train Epoch: [1900/2467]	Loss: 0.423202
     Train Epoch: [1900/2467]	Loss: 0.365155
     Train Epoch: [1900/2467]	Loss: 0.237851
     Train Epoch: [1900/2467]	Loss: 0.208303
     Train Epoch: [1920/2467]	Loss: 0.210298
         Train Epoch: [1920/2467]	Loss: 0.027169
     Train Epoch: [1920/2467]	Loss: 0.331323
 Train Epoch: [1920/2467]	Loss: 0.314982
         Train Epoch: [1940/2467]	Loss: 0.526214 
    Train Epoch: [1940/2467]	Loss: 0.144256
 Train Epoch: [1940/2467]	Loss: 0.157233
     Train Epoch: [1940/2467]	Loss: 0.322185
         Train Epoch: [1960/2467]	Loss: 0.249194
 Train Epoch: [1960/2467]	Loss: 0.303751    
     Train Epoch: [1960/2467]	Loss: 0.174525
 Train Epoch: [1960/2467]	Loss: 0.210983
     Train Epoch: [1980/2467]	Loss: 0.453659
         Train Epoch: [1980/2467]	Loss: 0.444106
 Train Epoch: [1980/2467]	Loss: 0.366580
     Train Epoch: [1980/2467]	Loss: 0.231230
          Train Epoch: [2000/2467]	Loss: 0.126202Train Epoch: [2000/2467]	Loss: 0.235297
    
     Train Epoch: [2000/2467]	Loss: 0.199220
 Train Epoch: [2000/2467]	Loss: 0.411913
     Train Epoch: [2020/2467]	Loss: 0.521901    
 Train Epoch: [2020/2467]	Loss: 0.322107
     Train Epoch: [2020/2467]	Loss: 0.388269
     Train Epoch: [2020/2467]	Loss: 0.144504
          Train Epoch: [2040/2467]	Loss: 0.460075
Train Epoch: [2040/2467]	Loss: 0.238732
         Train Epoch: [2040/2467]	Loss: 0.311635
 Train Epoch: [2040/2467]	Loss: 0.147485
         Train Epoch: [2060/2467]	Loss: 0.266754 
Train Epoch: [2060/2467]	Loss: 0.359382
         Train Epoch: [2060/2467]	Loss: 0.282539
 Train Epoch: [2060/2467]	Loss: 0.457045
     Train Epoch: [2080/2467]	Loss: 0.211233
     Train Epoch: [2080/2467]	Loss: 0.326418
     Train Epoch: [2080/2467]	Loss: 0.198342
     Train Epoch: [2080/2467]	Loss: 0.183821
         Train Epoch: [2100/2467]	Loss: 0.719621
     Train Epoch: [2100/2467]	Loss: 0.155106
 Train Epoch: [2100/2467]	Loss: 0.189675
     Train Epoch: [2100/2467]	Loss: 0.211975
     Train Epoch: [2120/2467]	Loss: 0.138740
          Train Epoch: [2120/2467]	Loss: 0.390814Train Epoch: [2120/2467]	Loss: 0.166352

     Train Epoch: [2120/2467]	Loss: 0.225263
     Train Epoch: [2140/2467]	Loss: 0.260464
         Train Epoch: [2140/2467]	Loss: 0.634256
 Train Epoch: [2140/2467]	Loss: 0.395167
     Train Epoch: [2140/2467]	Loss: 0.491198
     Train Epoch: [2160/2467]	Loss: 0.201480
     Train Epoch: [2160/2467]	Loss: 0.489496    
     Train Epoch: [2160/2467]	Loss: 0.079992
 Train Epoch: [2160/2467]	Loss: 0.249428
         Train Epoch: [2180/2467]	Loss: 0.327699 
Train Epoch: [2180/2467]	Loss: 0.167804
         Train Epoch: [2180/2467]	Loss: 0.347049
 Train Epoch: [2180/2467]	Loss: 0.218206
         Train Epoch: [2200/2467]	Loss: 0.190782
 Train Epoch: [2200/2467]	Loss: 0.270935
     Train Epoch: [2200/2467]	Loss: 0.579673
     Train Epoch: [2200/2467]	Loss: 0.191663
         Train Epoch: [2220/2467]	Loss: 0.230081
     Train Epoch: [2220/2467]	Loss: 0.139127
 Train Epoch: [2220/2467]	Loss: 0.208931
     Train Epoch: [2220/2467]	Loss: 0.150323
     Train Epoch: [2240/2467]	Loss: 0.068599
         Train Epoch: [2240/2467]	Loss: 0.231571
 Train Epoch: [2240/2467]	Loss: 0.082527    
 Train Epoch: [2240/2467]	Loss: 0.100249
     Train Epoch: [2260/2467]	Loss: 0.677354
     Train Epoch: [2260/2467]	Loss: 0.433840
     Train Epoch: [2260/2467]	Loss: 0.102061
     Train Epoch: [2260/2467]	Loss: 0.242951
     Train Epoch: [2280/2467]	Loss: 0.259527
         Train Epoch: [2280/2467]	Loss: 0.207487
 Train Epoch: [2280/2467]	Loss: 0.164351
     Train Epoch: [2280/2467]	Loss: 0.145130
     Train Epoch: [2300/2467]	Loss: 0.106873
     Train Epoch: [2300/2467]	Loss: 0.154256    
 Train Epoch: [2300/2467]	Loss: 0.522477
     Train Epoch: [2300/2467]	Loss: 0.157776
     Train Epoch: [2320/2467]	Loss: 0.414355
             Train Epoch: [2320/2467]	Loss: 0.302526
  Train Epoch: [2320/2467]	Loss: 0.076396Train Epoch: [2320/2467]	Loss: 0.187149

         Train Epoch: [2340/2467]	Loss: 0.132911 
    Train Epoch: [2340/2467]	Loss: 0.179977
 Train Epoch: [2340/2467]	Loss: 0.237985
     Train Epoch: [2340/2467]	Loss: 0.640749
         Train Epoch: [2360/2467]	Loss: 0.099888    
  Train Epoch: [2360/2467]	Loss: 0.214710Train Epoch: [2360/2467]	Loss: 0.433779

     Train Epoch: [2360/2467]	Loss: 0.222452
     Train Epoch: [2380/2467]	Loss: 0.369401
         Train Epoch: [2380/2467]	Loss: 0.564584
     Train Epoch: [2380/2467]	Loss: 0.302272
 Train Epoch: [2380/2467]	Loss: 0.445963
     Train Epoch: [2400/2467]	Loss: 0.431593    
 Train Epoch: [2400/2467]	Loss: 0.303313
         Train Epoch: [2400/2467]	Loss: 0.184938
 Train Epoch: [2400/2467]	Loss: 0.327027
          Train Epoch: [2420/2467]	Loss: 0.380311
Train Epoch: [2420/2467]	Loss: 0.143551
     Train Epoch: [2420/2467]	Loss: 0.350621
     Train Epoch: [2420/2467]	Loss: 0.290917
     Train Epoch: [2440/2467]	Loss: 0.194923
         Train Epoch: [2440/2467]	Loss: 0.123957 
Train Epoch: [2440/2467]	Loss: 0.457039
     Train Epoch: [2440/2467]	Loss: 0.364920
         Train Epoch: [2460/2467]	Loss: 0.164271
     Train Epoch: [2460/2467]	Loss: 0.197817
 Train Epoch: [2460/2467]	Loss: 0.140246
     Train Epoch: [2460/2467]	Loss: 0.457247
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 9 epoch =====
     2025-05-11.03-19-38
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 9 epoch =====
     2025-05-11.03-19-38
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 9 epoch =====
     2025-05-11.03-19-38
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 9 epoch =====
     2025-05-11.03-19-38
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.490167
          Train Epoch: [0/2467]	Loss: 0.442896Train Epoch: [0/2467]	Loss: 0.427019

     Train Epoch: [0/2467]	Loss: 0.248689
     Train Epoch: [20/2467]	Loss: 0.136614    
 Train Epoch: [20/2467]	Loss: 0.414770
     Train Epoch: [20/2467]	Loss: 0.182123
     Train Epoch: [20/2467]	Loss: 0.390417
         Train Epoch: [40/2467]	Loss: 0.372669
 Train Epoch: [40/2467]	Loss: 0.543714
         Train Epoch: [40/2467]	Loss: 0.382019
 Train Epoch: [40/2467]	Loss: 0.234646
         Train Epoch: [60/2467]	Loss: 0.325936
     Train Epoch: [60/2467]	Loss: 0.413385
 Train Epoch: [60/2467]	Loss: 0.076529
     Train Epoch: [60/2467]	Loss: 0.284278
         Train Epoch: [80/2467]	Loss: 0.414481
     Train Epoch: [80/2467]	Loss: 0.181998
     Train Epoch: [80/2467]	Loss: 0.214222
 Train Epoch: [80/2467]	Loss: 0.161239
         Train Epoch: [100/2467]	Loss: 0.262529
 Train Epoch: [100/2467]	Loss: 0.312464
     Train Epoch: [100/2467]	Loss: 0.396528
     Train Epoch: [100/2467]	Loss: 0.351346
     Train Epoch: [120/2467]	Loss: 0.108936
         Train Epoch: [120/2467]	Loss: 0.461366 
Train Epoch: [120/2467]	Loss: 0.390090
     Train Epoch: [120/2467]	Loss: 0.088223
         Train Epoch: [140/2467]	Loss: 0.125263
     Train Epoch: [140/2467]	Loss: 0.329683
 Train Epoch: [140/2467]	Loss: 0.310476
     Train Epoch: [140/2467]	Loss: 0.125576
         Train Epoch: [160/2467]	Loss: 0.187176    
     Train Epoch: [160/2467]	Loss: 0.217164  Train Epoch: [160/2467]	Loss: 0.407272

Train Epoch: [160/2467]	Loss: 0.470722
         Train Epoch: [180/2467]	Loss: 0.175840
     Train Epoch: [180/2467]	Loss: 0.317015    
 Train Epoch: [180/2467]	Loss: 0.264055
 Train Epoch: [180/2467]	Loss: 0.216788
             Train Epoch: [200/2467]	Loss: 0.175742
 Train Epoch: [200/2467]	Loss: 0.303181
 Train Epoch: [200/2467]	Loss: 0.281923
     Train Epoch: [200/2467]	Loss: 0.287398
     Train Epoch: [220/2467]	Loss: 0.128597
     Train Epoch: [220/2467]	Loss: 0.257737
     Train Epoch: [220/2467]	Loss: 0.384701
     Train Epoch: [220/2467]	Loss: 0.267685
          Train Epoch: [240/2467]	Loss: 0.259819
Train Epoch: [240/2467]	Loss: 0.031916
          Train Epoch: [240/2467]	Loss: 0.043919Train Epoch: [240/2467]	Loss: 0.420733

     Train Epoch: [260/2467]	Loss: 0.091415
         Train Epoch: [260/2467]	Loss: 0.239713
     Train Epoch: [260/2467]	Loss: 0.107498
 Train Epoch: [260/2467]	Loss: 0.090664
         Train Epoch: [280/2467]	Loss: 0.303122
     Train Epoch: [280/2467]	Loss: 0.900088
 Train Epoch: [280/2467]	Loss: 0.317172
     Train Epoch: [280/2467]	Loss: 0.310012
     Train Epoch: [300/2467]	Loss: 0.486646
         Train Epoch: [300/2467]	Loss: 0.108539
 Train Epoch: [300/2467]	Loss: 0.193347
     Train Epoch: [300/2467]	Loss: 0.180200
         Train Epoch: [320/2467]	Loss: 0.405870
 Train Epoch: [320/2467]	Loss: 0.485625
     Train Epoch: [320/2467]	Loss: 0.544788
     Train Epoch: [320/2467]	Loss: 0.294118
     Train Epoch: [340/2467]	Loss: 0.045573
     Train Epoch: [340/2467]	Loss: 0.103046
     Train Epoch: [340/2467]	Loss: 0.262376
     Train Epoch: [340/2467]	Loss: 0.174396
         Train Epoch: [360/2467]	Loss: 0.177222
     Train Epoch: [360/2467]	Loss: 0.272817
 Train Epoch: [360/2467]	Loss: 0.263032
     Train Epoch: [360/2467]	Loss: 0.242604
         Train Epoch: [380/2467]	Loss: 0.682603
     Train Epoch: [380/2467]	Loss: 0.297350
     Train Epoch: [380/2467]	Loss: 0.297073
 Train Epoch: [380/2467]	Loss: 0.862815
         Train Epoch: [400/2467]	Loss: 0.120725
 Train Epoch: [400/2467]	Loss: 0.336258
         Train Epoch: [400/2467]	Loss: 0.534453
 Train Epoch: [400/2467]	Loss: 0.233942
             Train Epoch: [420/2467]	Loss: 0.135145
  Train Epoch: [420/2467]	Loss: 0.211606Train Epoch: [420/2467]	Loss: 0.343691

     Train Epoch: [420/2467]	Loss: 0.287412
         Train Epoch: [440/2467]	Loss: 0.399592    
 Train Epoch: [440/2467]	Loss: 0.188056
 Train Epoch: [440/2467]	Loss: 0.158385
     Train Epoch: [440/2467]	Loss: 0.120940
         Train Epoch: [460/2467]	Loss: 0.271655
 Train Epoch: [460/2467]	Loss: 0.370334
         Train Epoch: [460/2467]	Loss: 0.244512
 Train Epoch: [460/2467]	Loss: 0.256813
          Train Epoch: [480/2467]	Loss: 0.347686Train Epoch: [480/2467]	Loss: 0.210037

         Train Epoch: [480/2467]	Loss: 0.400543
 Train Epoch: [480/2467]	Loss: 0.348602
         Train Epoch: [500/2467]	Loss: 0.467958
     Train Epoch: [500/2467]	Loss: 0.191762
     Train Epoch: [500/2467]	Loss: 0.303462
 Train Epoch: [500/2467]	Loss: 0.134880
             Train Epoch: [520/2467]	Loss: 0.347621
  Train Epoch: [520/2467]	Loss: 0.357279Train Epoch: [520/2467]	Loss: 0.486021

     Train Epoch: [520/2467]	Loss: 0.107209
             Train Epoch: [540/2467]	Loss: 0.320626 
Train Epoch: [540/2467]	Loss: 0.096667
 Train Epoch: [540/2467]	Loss: 0.186058
     Train Epoch: [540/2467]	Loss: 0.215499
     Train Epoch: [560/2467]	Loss: 0.277450
         Train Epoch: [560/2467]	Loss: 0.352985
 Train Epoch: [560/2467]	Loss: 0.227813
     Train Epoch: [560/2467]	Loss: 0.252601
         Train Epoch: [580/2467]	Loss: 0.106587
 Train Epoch: [580/2467]	Loss: 0.201168
     Train Epoch: [580/2467]	Loss: 0.208774
     Train Epoch: [580/2467]	Loss: 0.416504
     Train Epoch: [600/2467]	Loss: 0.235437
     Train Epoch: [600/2467]	Loss: 0.236841    
 Train Epoch: [600/2467]	Loss: 0.267485
     Train Epoch: [600/2467]	Loss: 0.149387
         Train Epoch: [620/2467]	Loss: 0.335658
     Train Epoch: [620/2467]	Loss: 0.252093 
Train Epoch: [620/2467]	Loss: 0.066129
     Train Epoch: [620/2467]	Loss: 0.371350
     Train Epoch: [640/2467]	Loss: 0.160455
             Train Epoch: [640/2467]	Loss: 0.379755
 Train Epoch: [640/2467]	Loss: 0.241140 
Train Epoch: [640/2467]	Loss: 0.083572
     Train Epoch: [660/2467]	Loss: 0.448471    
 Train Epoch: [660/2467]	Loss: 0.212641    
 Train Epoch: [660/2467]	Loss: 0.359613
     Train Epoch: [660/2467]	Loss: 0.056061
     Train Epoch: [680/2467]	Loss: 0.171780
         Train Epoch: [680/2467]	Loss: 0.096587 
Train Epoch: [680/2467]	Loss: 0.260245
     Train Epoch: [680/2467]	Loss: 0.310388
         Train Epoch: [700/2467]	Loss: 0.103980
     Train Epoch: [700/2467]	Loss: 0.148130    
 Train Epoch: [700/2467]	Loss: 0.547649
 Train Epoch: [700/2467]	Loss: 0.372802
     Train Epoch: [720/2467]	Loss: 0.222782
         Train Epoch: [720/2467]	Loss: 0.270137
     Train Epoch: [720/2467]	Loss: 0.244510
 Train Epoch: [720/2467]	Loss: 0.221183
             Train Epoch: [740/2467]	Loss: 0.369866 
Train Epoch: [740/2467]	Loss: 0.142797    
 Train Epoch: [740/2467]	Loss: 0.106917
 Train Epoch: [740/2467]	Loss: 0.110809
         Train Epoch: [760/2467]	Loss: 0.544195
          Train Epoch: [760/2467]	Loss: 0.324834 
Train Epoch: [760/2467]	Loss: 0.402866
Train Epoch: [760/2467]	Loss: 0.120477
         Train Epoch: [780/2467]	Loss: 0.382553
 Train Epoch: [780/2467]	Loss: 0.446097
     Train Epoch: [780/2467]	Loss: 0.320535
     Train Epoch: [780/2467]	Loss: 0.423212
     Train Epoch: [800/2467]	Loss: 0.340438    
 Train Epoch: [800/2467]	Loss: 0.246791
         Train Epoch: [800/2467]	Loss: 0.302407
 Train Epoch: [800/2467]	Loss: 0.131371
         Train Epoch: [820/2467]	Loss: 0.076959
 Train Epoch: [820/2467]	Loss: 0.316085
     Train Epoch: [820/2467]	Loss: 0.464848
     Train Epoch: [820/2467]	Loss: 0.074957
     Train Epoch: [840/2467]	Loss: 0.246320
              Train Epoch: [840/2467]	Loss: 0.418294Train Epoch: [840/2467]	Loss: 0.154032

 Train Epoch: [840/2467]	Loss: 0.195944
          Train Epoch: [860/2467]	Loss: 0.409284
Train Epoch: [860/2467]	Loss: 0.208470
     Train Epoch: [860/2467]	Loss: 0.138997
     Train Epoch: [860/2467]	Loss: 0.115309
         Train Epoch: [880/2467]	Loss: 0.186017
         Train Epoch: [880/2467]	Loss: 0.370732
 Train Epoch: [880/2467]	Loss: 0.259358
 Train Epoch: [880/2467]	Loss: 0.220330
         Train Epoch: [900/2467]	Loss: 0.249740
 Train Epoch: [900/2467]	Loss: 0.255113
     Train Epoch: [900/2467]	Loss: 0.196731
     Train Epoch: [900/2467]	Loss: 0.231525
     Train Epoch: [920/2467]	Loss: 0.568211
     Train Epoch: [920/2467]	Loss: 0.331744
     Train Epoch: [920/2467]	Loss: 0.134545
     Train Epoch: [920/2467]	Loss: 0.461903
     Train Epoch: [940/2467]	Loss: 0.067895
          Train Epoch: [940/2467]	Loss: 0.166035Train Epoch: [940/2467]	Loss: 0.322064

     Train Epoch: [940/2467]	Loss: 0.232895
             Train Epoch: [960/2467]	Loss: 0.486887
  Train Epoch: [960/2467]	Loss: 0.076041Train Epoch: [960/2467]	Loss: 0.109164

     Train Epoch: [960/2467]	Loss: 0.154542
          Train Epoch: [980/2467]	Loss: 0.123427
Train Epoch: [980/2467]	Loss: 0.294605
          Train Epoch: [980/2467]	Loss: 0.283035Train Epoch: [980/2467]	Loss: 0.351851

         Train Epoch: [1000/2467]	Loss: 0.102895
     Train Epoch: [1000/2467]	Loss: 0.431956
 Train Epoch: [1000/2467]	Loss: 0.058016
     Train Epoch: [1000/2467]	Loss: 0.347671
              Train Epoch: [1020/2467]	Loss: 0.296345Train Epoch: [1020/2467]	Loss: 0.216311

 Train Epoch: [1020/2467]	Loss: 0.512123
     Train Epoch: [1020/2467]	Loss: 0.083418
               Train Epoch: [1040/2467]	Loss: 0.112149Train Epoch: [1040/2467]	Loss: 0.245042Train Epoch: [1040/2467]	Loss: 0.353695


     Train Epoch: [1040/2467]	Loss: 0.230490
     Train Epoch: [1060/2467]	Loss: 0.331279
         Train Epoch: [1060/2467]	Loss: 0.520355 
Train Epoch: [1060/2467]	Loss: 0.387867
     Train Epoch: [1060/2467]	Loss: 0.178731
             Train Epoch: [1080/2467]	Loss: 0.246166
  Train Epoch: [1080/2467]	Loss: 0.154870Train Epoch: [1080/2467]	Loss: 0.375296

     Train Epoch: [1080/2467]	Loss: 0.402085
         Train Epoch: [1100/2467]	Loss: 0.415587
 Train Epoch: [1100/2467]	Loss: 0.351652
         Train Epoch: [1100/2467]	Loss: 0.488163
 Train Epoch: [1100/2467]	Loss: 0.322104
         Train Epoch: [1120/2467]	Loss: 0.171521
 Train Epoch: [1120/2467]	Loss: 0.195927
     Train Epoch: [1120/2467]	Loss: 0.230846
     Train Epoch: [1120/2467]	Loss: 0.365422
             Train Epoch: [1140/2467]	Loss: 0.329712 
Train Epoch: [1140/2467]	Loss: 0.179217
 Train Epoch: [1140/2467]	Loss: 0.436026
     Train Epoch: [1140/2467]	Loss: 0.083019
             Train Epoch: [1160/2467]	Loss: 0.247758
 Train Epoch: [1160/2467]	Loss: 0.106143
 Train Epoch: [1160/2467]	Loss: 0.234440
     Train Epoch: [1160/2467]	Loss: 0.127169
             Train Epoch: [1180/2467]	Loss: 0.413002
  Train Epoch: [1180/2467]	Loss: 0.212784Train Epoch: [1180/2467]	Loss: 0.116762    

 Train Epoch: [1180/2467]	Loss: 0.337648
     Train Epoch: [1200/2467]	Loss: 0.380239
         Train Epoch: [1200/2467]	Loss: 0.227308
 Train Epoch: [1200/2467]	Loss: 0.176471
     Train Epoch: [1200/2467]	Loss: 0.311219
         Train Epoch: [1220/2467]	Loss: 0.283869    
 Train Epoch: [1220/2467]	Loss: 0.432216
 Train Epoch: [1220/2467]	Loss: 0.137633
     Train Epoch: [1220/2467]	Loss: 0.493157
         Train Epoch: [1240/2467]	Loss: 0.280429    
 Train Epoch: [1240/2467]	Loss: 0.408596
 Train Epoch: [1240/2467]	Loss: 0.308850
     Train Epoch: [1240/2467]	Loss: 0.357636
     Train Epoch: [1260/2467]	Loss: 0.176713    
     Train Epoch: [1260/2467]	Loss: 0.110742
 Train Epoch: [1260/2467]	Loss: 0.506350
     Train Epoch: [1260/2467]	Loss: 0.088220
         Train Epoch: [1280/2467]	Loss: 0.132274    
  Train Epoch: [1280/2467]	Loss: 0.453431
Train Epoch: [1280/2467]	Loss: 1.066863
     Train Epoch: [1280/2467]	Loss: 0.484933
     Train Epoch: [1300/2467]	Loss: 0.194966
         Train Epoch: [1300/2467]	Loss: 0.382407
 Train Epoch: [1300/2467]	Loss: 0.458456
     Train Epoch: [1300/2467]	Loss: 0.134879
     Train Epoch: [1320/2467]	Loss: 0.308769
     Train Epoch: [1320/2467]	Loss: 0.212148
     Train Epoch: [1320/2467]	Loss: 0.420881
     Train Epoch: [1320/2467]	Loss: 0.106139
             Train Epoch: [1340/2467]	Loss: 0.125105    
   Train Epoch: [1340/2467]	Loss: 0.405759Train Epoch: [1340/2467]	Loss: 0.231435
Train Epoch: [1340/2467]	Loss: 0.293863

         Train Epoch: [1360/2467]	Loss: 0.318678
     Train Epoch: [1360/2467]	Loss: 0.135830
 Train Epoch: [1360/2467]	Loss: 0.888690
     Train Epoch: [1360/2467]	Loss: 0.154462
         Train Epoch: [1380/2467]	Loss: 0.753919
         Train Epoch: [1380/2467]	Loss: 0.376802
  Train Epoch: [1380/2467]	Loss: 0.381071Train Epoch: [1380/2467]	Loss: 0.272214

         Train Epoch: [1400/2467]	Loss: 0.207109
 Train Epoch: [1400/2467]	Loss: 0.187093    
 Train Epoch: [1400/2467]	Loss: 0.302381
     Train Epoch: [1400/2467]	Loss: 0.244308
         Train Epoch: [1420/2467]	Loss: 0.176342
 Train Epoch: [1420/2467]	Loss: 0.176532
     Train Epoch: [1420/2467]	Loss: 0.220335
     Train Epoch: [1420/2467]	Loss: 0.488982
              Train Epoch: [1440/2467]	Loss: 0.042346Train Epoch: [1440/2467]	Loss: 0.293597

 Train Epoch: [1440/2467]	Loss: 0.160200
     Train Epoch: [1440/2467]	Loss: 0.057571
             Train Epoch: [1460/2467]	Loss: 0.252906
  Train Epoch: [1460/2467]	Loss: 0.067002
Train Epoch: [1460/2467]	Loss: 0.139782
     Train Epoch: [1460/2467]	Loss: 0.288392
              Train Epoch: [1480/2467]	Loss: 0.370869
Train Epoch: [1480/2467]	Loss: 0.311198
 Train Epoch: [1480/2467]	Loss: 0.076216
     Train Epoch: [1480/2467]	Loss: 0.068884
     Train Epoch: [1500/2467]	Loss: 0.137271
          Train Epoch: [1500/2467]	Loss: 0.230241    
Train Epoch: [1500/2467]	Loss: 0.297979
 Train Epoch: [1500/2467]	Loss: 0.036243
     Train Epoch: [1520/2467]	Loss: 0.344314
             Train Epoch: [1520/2467]	Loss: 0.177452
 Train Epoch: [1520/2467]	Loss: 0.342158 
Train Epoch: [1520/2467]	Loss: 0.327501
             Train Epoch: [1540/2467]	Loss: 0.158309
  Train Epoch: [1540/2467]	Loss: 0.307148Train Epoch: [1540/2467]	Loss: 0.259376

     Train Epoch: [1540/2467]	Loss: 0.622553
     Train Epoch: [1560/2467]	Loss: 0.477376
          Train Epoch: [1560/2467]	Loss: 0.471902Train Epoch: [1560/2467]	Loss: 0.671702

     Train Epoch: [1560/2467]	Loss: 0.529916
          Train Epoch: [1580/2467]	Loss: 0.326461Train Epoch: [1580/2467]	Loss: 0.365334

     Train Epoch: [1580/2467]	Loss: 0.446037
     Train Epoch: [1580/2467]	Loss: 0.243658
     Train Epoch: [1600/2467]	Loss: 0.319869
         Train Epoch: [1600/2467]	Loss: 0.352570
 Train Epoch: [1600/2467]	Loss: 0.165077
     Train Epoch: [1600/2467]	Loss: 0.112715
     Train Epoch: [1620/2467]	Loss: 0.127043
     Train Epoch: [1620/2467]	Loss: 0.289608
     Train Epoch: [1620/2467]	Loss: 0.264442
     Train Epoch: [1620/2467]	Loss: 0.064950
     Train Epoch: [1640/2467]	Loss: 0.276390
     Train Epoch: [1640/2467]	Loss: 0.266517
     Train Epoch: [1640/2467]	Loss: 0.287782
     Train Epoch: [1640/2467]	Loss: 0.234239
          Train Epoch: [1660/2467]	Loss: 0.352340Train Epoch: [1660/2467]	Loss: 0.228165
    
     Train Epoch: [1660/2467]	Loss: 0.184152
 Train Epoch: [1660/2467]	Loss: 0.274242
         Train Epoch: [1680/2467]	Loss: 0.281719
 Train Epoch: [1680/2467]	Loss: 0.395375
         Train Epoch: [1680/2467]	Loss: 0.099026
 Train Epoch: [1680/2467]	Loss: 0.246870
     Train Epoch: [1700/2467]	Loss: 0.278496
         Train Epoch: [1700/2467]	Loss: 0.299691 
Train Epoch: [1700/2467]	Loss: 0.164149
     Train Epoch: [1700/2467]	Loss: 0.269658
              Train Epoch: [1720/2467]	Loss: 0.186041Train Epoch: [1720/2467]	Loss: 0.332074

 Train Epoch: [1720/2467]	Loss: 0.269097
     Train Epoch: [1720/2467]	Loss: 0.298968
     Train Epoch: [1740/2467]	Loss: 0.155606
         Train Epoch: [1740/2467]	Loss: 0.509116
 Train Epoch: [1740/2467]	Loss: 0.233585
     Train Epoch: [1740/2467]	Loss: 0.155944
         Train Epoch: [1760/2467]	Loss: 0.140834
 Train Epoch: [1760/2467]	Loss: 0.110007    
 Train Epoch: [1760/2467]	Loss: 0.148373
     Train Epoch: [1760/2467]	Loss: 0.208793
         Train Epoch: [1780/2467]	Loss: 0.312053
 Train Epoch: [1780/2467]	Loss: 0.129531
          Train Epoch: [1780/2467]	Loss: 0.313549
Train Epoch: [1780/2467]	Loss: 0.394008
         Train Epoch: [1800/2467]	Loss: 0.399628 
Train Epoch: [1800/2467]	Loss: 0.277571
          Train Epoch: [1800/2467]	Loss: 0.432606Train Epoch: [1800/2467]	Loss: 0.283909

         Train Epoch: [1820/2467]	Loss: 0.189353    
  Train Epoch: [1820/2467]	Loss: 0.447035Train Epoch: [1820/2467]	Loss: 0.138461

     Train Epoch: [1820/2467]	Loss: 0.325028
         Train Epoch: [1840/2467]	Loss: 0.411348
     Train Epoch: [1840/2467]	Loss: 0.222673
 Train Epoch: [1840/2467]	Loss: 0.298245
     Train Epoch: [1840/2467]	Loss: 0.088139
     Train Epoch: [1860/2467]	Loss: 0.260954    
      Train Epoch: [1860/2467]	Loss: 0.250980
Train Epoch: [1860/2467]	Loss: 0.284411
     Train Epoch: [1860/2467]	Loss: 0.155986
             Train Epoch: [1880/2467]	Loss: 0.088242
      Train Epoch: [1880/2467]	Loss: 0.071327
Train Epoch: [1880/2467]	Loss: 0.181996
 Train Epoch: [1880/2467]	Loss: 0.157905
          Train Epoch: [1900/2467]	Loss: 0.190066Train Epoch: [1900/2467]	Loss: 0.457568

     Train Epoch: [1900/2467]	Loss: 0.239739
     Train Epoch: [1900/2467]	Loss: 0.329484
         Train Epoch: [1920/2467]	Loss: 0.251531    
 Train Epoch: [1920/2467]	Loss: 0.322661 
Train Epoch: [1920/2467]	Loss: 0.023325
     Train Epoch: [1920/2467]	Loss: 0.257357
         Train Epoch: [1940/2467]	Loss: 0.408578
 Train Epoch: [1940/2467]	Loss: 0.550046
         Train Epoch: [1940/2467]	Loss: 0.139081
 Train Epoch: [1940/2467]	Loss: 0.169170
     Train Epoch: [1960/2467]	Loss: 0.235140
     Train Epoch: [1960/2467]	Loss: 0.263336
     Train Epoch: [1960/2467]	Loss: 0.293801
     Train Epoch: [1960/2467]	Loss: 0.277139
          Train Epoch: [1980/2467]	Loss: 0.368216
Train Epoch: [1980/2467]	Loss: 0.402175    
 Train Epoch: [1980/2467]	Loss: 0.256767
     Train Epoch: [1980/2467]	Loss: 0.417431
     Train Epoch: [2000/2467]	Loss: 0.089559
     Train Epoch: [2000/2467]	Loss: 0.371955
     Train Epoch: [2000/2467]	Loss: 0.103237
     Train Epoch: [2000/2467]	Loss: 0.208534
              Train Epoch: [2020/2467]	Loss: 0.279898 Train Epoch: [2020/2467]	Loss: 0.433626
Train Epoch: [2020/2467]	Loss: 0.491745
    
 Train Epoch: [2020/2467]	Loss: 0.142990
          Train Epoch: [2040/2467]	Loss: 0.438589Train Epoch: [2040/2467]	Loss: 0.262029

     Train Epoch: [2040/2467]	Loss: 0.351183
     Train Epoch: [2040/2467]	Loss: 0.112941
         Train Epoch: [2060/2467]	Loss: 0.261102
 Train Epoch: [2060/2467]	Loss: 0.324987
     Train Epoch: [2060/2467]	Loss: 0.244301
     Train Epoch: [2060/2467]	Loss: 0.338943
         Train Epoch: [2080/2467]	Loss: 0.107439
 Train Epoch: [2080/2467]	Loss: 0.152488
         Train Epoch: [2080/2467]	Loss: 0.228062
 Train Epoch: [2080/2467]	Loss: 0.303684
         Train Epoch: [2100/2467]	Loss: 0.147694 
    Train Epoch: [2100/2467]	Loss: 0.698996
     Train Epoch: [2100/2467]	Loss: 0.177738
 Train Epoch: [2100/2467]	Loss: 0.183790
          Train Epoch: [2120/2467]	Loss: 0.363452Train Epoch: [2120/2467]	Loss: 0.166549

     Train Epoch: [2120/2467]	Loss: 0.280503
     Train Epoch: [2120/2467]	Loss: 0.155273
          Train Epoch: [2140/2467]	Loss: 0.651407Train Epoch: [2140/2467]	Loss: 0.248012

     Train Epoch: [2140/2467]	Loss: 0.447123
     Train Epoch: [2140/2467]	Loss: 0.499902
          Train Epoch: [2160/2467]	Loss: 0.074940
Train Epoch: [2160/2467]	Loss: 0.197612
     Train Epoch: [2160/2467]	Loss: 0.453995
     Train Epoch: [2160/2467]	Loss: 0.248492
             Train Epoch: [2180/2467]	Loss: 0.361333
  Train Epoch: [2180/2467]	Loss: 0.356557Train Epoch: [2180/2467]	Loss: 0.177481

     Train Epoch: [2180/2467]	Loss: 0.203545
         Train Epoch: [2200/2467]	Loss: 0.217940     
Train Epoch: [2200/2467]	Loss: 0.184498
 Train Epoch: [2200/2467]	Loss: 0.449510
     Train Epoch: [2200/2467]	Loss: 0.200554
              Train Epoch: [2220/2467]	Loss: 0.261519Train Epoch: [2220/2467]	Loss: 0.192458
 
Train Epoch: [2220/2467]	Loss: 0.115923
     Train Epoch: [2220/2467]	Loss: 0.122367
         Train Epoch: [2240/2467]	Loss: 0.077189 
Train Epoch: [2240/2467]	Loss: 0.096078
     Train Epoch: [2240/2467]	Loss: 0.163040
     Train Epoch: [2240/2467]	Loss: 0.243253
          Train Epoch: [2260/2467]	Loss: 0.889610Train Epoch: [2260/2467]	Loss: 0.407282

     Train Epoch: [2260/2467]	Loss: 0.109660
     Train Epoch: [2260/2467]	Loss: 0.243912
     Train Epoch: [2280/2467]	Loss: 0.191241
     Train Epoch: [2280/2467]	Loss: 0.284980
     Train Epoch: [2280/2467]	Loss: 0.243805
     Train Epoch: [2280/2467]	Loss: 0.188476
     Train Epoch: [2300/2467]	Loss: 0.093639
     Train Epoch: [2300/2467]	Loss: 0.449331
     Train Epoch: [2300/2467]	Loss: 0.156526
     Train Epoch: [2300/2467]	Loss: 0.156316
         Train Epoch: [2320/2467]	Loss: 0.412973
 Train Epoch: [2320/2467]	Loss: 0.083060
     Train Epoch: [2320/2467]	Loss: 0.204143
     Train Epoch: [2320/2467]	Loss: 0.286283
              Train Epoch: [2340/2467]	Loss: 0.156134Train Epoch: [2340/2467]	Loss: 0.252939

     Train Epoch: [2340/2467]	Loss: 0.678083
 Train Epoch: [2340/2467]	Loss: 0.167303
             Train Epoch: [2360/2467]	Loss: 0.113852
  Train Epoch: [2360/2467]	Loss: 0.202346Train Epoch: [2360/2467]	Loss: 0.425074

     Train Epoch: [2360/2467]	Loss: 0.186455
         Train Epoch: [2380/2467]	Loss: 0.305512
 Train Epoch: [2380/2467]	Loss: 0.272173
     Train Epoch: [2380/2467]	Loss: 0.575217
     Train Epoch: [2380/2467]	Loss: 0.501073
         Train Epoch: [2400/2467]	Loss: 0.296523
 Train Epoch: [2400/2467]	Loss: 0.333454
     Train Epoch: [2400/2467]	Loss: 0.164454
     Train Epoch: [2400/2467]	Loss: 0.219941
         Train Epoch: [2420/2467]	Loss: 0.387669
 Train Epoch: [2420/2467]	Loss: 0.389291
     Train Epoch: [2420/2467]	Loss: 0.339727
     Train Epoch: [2420/2467]	Loss: 0.257433
     Train Epoch: [2440/2467]	Loss: 0.166727
     Train Epoch: [2440/2467]	Loss: 0.435892
     Train Epoch: [2440/2467]	Loss: 0.198522
     Train Epoch: [2440/2467]	Loss: 0.266159
     Train Epoch: [2460/2467]	Loss: 0.198831
             Train Epoch: [2460/2467]	Loss: 0.462394
  Train Epoch: [2460/2467]	Loss: 0.181942Train Epoch: [2460/2467]	Loss: 0.143421

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 10 epoch =====
     2025-05-11.03-41-28
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 10 epoch =====
     2025-05-11.03-41-29
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 10 epoch =====
     2025-05-11.03-41-29
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 10 epoch =====
     2025-05-11.03-41-29
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
         Train Epoch: [0/2467]	Loss: 0.365120
 Train Epoch: [0/2467]	Loss: 0.371201
         Train Epoch: [0/2467]	Loss: 0.218045 
Train Epoch: [0/2467]	Loss: 0.352426
          Train Epoch: [20/2467]	Loss: 0.157511Train Epoch: [20/2467]	Loss: 0.125067

          Train Epoch: [20/2467]	Loss: 0.449007Train Epoch: [20/2467]	Loss: 0.319720

         Train Epoch: [40/2467]	Loss: 0.254941
 Train Epoch: [40/2467]	Loss: 0.556516
         Train Epoch: [40/2467]	Loss: 0.286480
 Train Epoch: [40/2467]	Loss: 0.234208
     Train Epoch: [60/2467]	Loss: 0.407524
              Train Epoch: [60/2467]	Loss: 0.286347Train Epoch: [60/2467]	Loss: 0.071732

 Train Epoch: [60/2467]	Loss: 0.248118
         Train Epoch: [80/2467]	Loss: 0.160082
 Train Epoch: [80/2467]	Loss: 0.371911    
     Train Epoch: [80/2467]	Loss: 0.130033
 Train Epoch: [80/2467]	Loss: 0.293083
     Train Epoch: [100/2467]	Loss: 0.270058
         Train Epoch: [100/2467]	Loss: 0.454056
 Train Epoch: [100/2467]	Loss: 0.332404
     Train Epoch: [100/2467]	Loss: 0.453711
     Train Epoch: [120/2467]	Loss: 0.102025
         Train Epoch: [120/2467]	Loss: 0.086816 
    Train Epoch: [120/2467]	Loss: 0.432849
 Train Epoch: [120/2467]	Loss: 0.337208
         Train Epoch: [140/2467]	Loss: 0.103522
 Train Epoch: [140/2467]	Loss: 0.310211
     Train Epoch: [140/2467]	Loss: 0.240756
     Train Epoch: [140/2467]	Loss: 0.078577
     Train Epoch: [160/2467]	Loss: 0.173731
         Train Epoch: [160/2467]	Loss: 0.306112
 Train Epoch: [160/2467]	Loss: 0.419324
     Train Epoch: [160/2467]	Loss: 0.185968
          Train Epoch: [180/2467]	Loss: 0.241913Train Epoch: [180/2467]	Loss: 0.207234

     Train Epoch: [180/2467]	Loss: 0.194332
     Train Epoch: [180/2467]	Loss: 0.270873
     Train Epoch: [200/2467]	Loss: 0.179106
          Train Epoch: [200/2467]	Loss: 0.234268
Train Epoch: [200/2467]	Loss: 0.165057
     Train Epoch: [200/2467]	Loss: 0.250994
     Train Epoch: [220/2467]	Loss: 0.136037
     Train Epoch: [220/2467]	Loss: 0.266579
     Train Epoch: [220/2467]	Loss: 0.329297
     Train Epoch: [220/2467]	Loss: 0.229383
          Train Epoch: [240/2467]	Loss: 0.221862    
Train Epoch: [240/2467]	Loss: 0.042292
 Train Epoch: [240/2467]	Loss: 0.449101
     Train Epoch: [240/2467]	Loss: 0.088032
          Train Epoch: [260/2467]	Loss: 0.082237Train Epoch: [260/2467]	Loss: 0.109019

     Train Epoch: [260/2467]	Loss: 0.225953
     Train Epoch: [260/2467]	Loss: 0.085701
     Train Epoch: [280/2467]	Loss: 0.321669
         Train Epoch: [280/2467]	Loss: 0.336430
 Train Epoch: [280/2467]	Loss: 0.146359    
 Train Epoch: [280/2467]	Loss: 0.251333
         Train Epoch: [300/2467]	Loss: 0.512303
     Train Epoch: [300/2467]	Loss: 0.183351
 Train Epoch: [300/2467]	Loss: 0.080593    
 Train Epoch: [300/2467]	Loss: 0.143259
          Train Epoch: [320/2467]	Loss: 0.185169Train Epoch: [320/2467]	Loss: 0.242544    

 Train Epoch: [320/2467]	Loss: 0.386298
     Train Epoch: [320/2467]	Loss: 0.554353
     Train Epoch: [340/2467]	Loss: 0.043575
          Train Epoch: [340/2467]	Loss: 0.180920Train Epoch: [340/2467]	Loss: 0.079010

     Train Epoch: [340/2467]	Loss: 0.250964
     Train Epoch: [360/2467]	Loss: 0.097681
     Train Epoch: [360/2467]	Loss: 0.239923
         Train Epoch: [360/2467]	Loss: 0.279671
 Train Epoch: [360/2467]	Loss: 0.229726
     Train Epoch: [380/2467]	Loss: 0.289007
          Train Epoch: [380/2467]	Loss: 0.210788Train Epoch: [380/2467]	Loss: 0.662296

     Train Epoch: [380/2467]	Loss: 0.485663
         Train Epoch: [400/2467]	Loss: 0.221676
 Train Epoch: [400/2467]	Loss: 0.158604
     Train Epoch: [400/2467]	Loss: 0.544809
     Train Epoch: [400/2467]	Loss: 0.299680
     Train Epoch: [420/2467]	Loss: 0.338766
          Train Epoch: [420/2467]	Loss: 0.415318
Train Epoch: [420/2467]	Loss: 0.146634    
 Train Epoch: [420/2467]	Loss: 0.230445
         Train Epoch: [440/2467]	Loss: 0.428874
     Train Epoch: [440/2467]	Loss: 0.198112
 Train Epoch: [440/2467]	Loss: 0.151806
     Train Epoch: [440/2467]	Loss: 0.114675
     Train Epoch: [460/2467]	Loss: 0.243584
     Train Epoch: [460/2467]	Loss: 0.214694
     Train Epoch: [460/2467]	Loss: 0.179373
     Train Epoch: [460/2467]	Loss: 0.387247
     Train Epoch: [480/2467]	Loss: 0.329524    
 Train Epoch: [480/2467]	Loss: 0.137038
     Train Epoch: [480/2467]	Loss: 0.231651
     Train Epoch: [480/2467]	Loss: 0.407292
     Train Epoch: [500/2467]	Loss: 0.198187
     Train Epoch: [500/2467]	Loss: 0.131694
     Train Epoch: [500/2467]	Loss: 0.475637
     Train Epoch: [500/2467]	Loss: 0.470491
             Train Epoch: [520/2467]	Loss: 0.284555
 Train Epoch: [520/2467]	Loss: 0.264833 
Train Epoch: [520/2467]	Loss: 0.476287
     Train Epoch: [520/2467]	Loss: 0.115529
              Train Epoch: [540/2467]	Loss: 0.313521
Train Epoch: [540/2467]	Loss: 0.159683
 Train Epoch: [540/2467]	Loss: 0.086421
     Train Epoch: [540/2467]	Loss: 0.209567
         Train Epoch: [560/2467]	Loss: 0.262087     
Train Epoch: [560/2467]	Loss: 0.254995
     Train Epoch: [560/2467]	Loss: 0.366963
 Train Epoch: [560/2467]	Loss: 0.240773
     Train Epoch: [580/2467]	Loss: 0.411102    
          Train Epoch: [580/2467]	Loss: 0.185925Train Epoch: [580/2467]	Loss: 0.110542

 Train Epoch: [580/2467]	Loss: 0.193884
             Train Epoch: [600/2467]	Loss: 0.332963
  Train Epoch: [600/2467]	Loss: 0.238895
Train Epoch: [600/2467]	Loss: 0.161501
     Train Epoch: [600/2467]	Loss: 0.270591
             Train Epoch: [620/2467]	Loss: 0.489572
  Train Epoch: [620/2467]	Loss: 0.204709
Train Epoch: [620/2467]	Loss: 0.079568    
 Train Epoch: [620/2467]	Loss: 0.363109
         Train Epoch: [640/2467]	Loss: 0.180941
     Train Epoch: [640/2467]	Loss: 0.298861
 Train Epoch: [640/2467]	Loss: 0.337773
     Train Epoch: [640/2467]	Loss: 0.090663
              Train Epoch: [660/2467]	Loss: 0.064070Train Epoch: [660/2467]	Loss: 0.457621

     Train Epoch: [660/2467]	Loss: 0.062408
 Train Epoch: [660/2467]	Loss: 0.336810
          Train Epoch: [680/2467]	Loss: 0.183308
Train Epoch: [680/2467]	Loss: 0.084534
     Train Epoch: [680/2467]	Loss: 0.139341
     Train Epoch: [680/2467]	Loss: 0.184020
         Train Epoch: [700/2467]	Loss: 0.144625
          Train Epoch: [700/2467]	Loss: 0.403499 Train Epoch: [700/2467]	Loss: 0.135134
Train Epoch: [700/2467]	Loss: 0.517398

     Train Epoch: [720/2467]	Loss: 0.221273
     Train Epoch: [720/2467]	Loss: 0.282224
     Train Epoch: [720/2467]	Loss: 0.198898
     Train Epoch: [720/2467]	Loss: 0.135904
     Train Epoch: [740/2467]	Loss: 0.344115
         Train Epoch: [740/2467]	Loss: 0.074352
 Train Epoch: [740/2467]	Loss: 0.123301
     Train Epoch: [740/2467]	Loss: 0.098221
     Train Epoch: [760/2467]	Loss: 0.601653     Train Epoch: [760/2467]	Loss: 0.281682
    
 Train Epoch: [760/2467]	Loss: 0.429696
     Train Epoch: [760/2467]	Loss: 0.105908
         Train Epoch: [780/2467]	Loss: 0.380657
 Train Epoch: [780/2467]	Loss: 0.387616
     Train Epoch: [780/2467]	Loss: 0.414271
     Train Epoch: [780/2467]	Loss: 0.409239
     Train Epoch: [800/2467]	Loss: 0.342474
         Train Epoch: [800/2467]	Loss: 0.234186
 Train Epoch: [800/2467]	Loss: 0.335748
     Train Epoch: [800/2467]	Loss: 0.206428
         Train Epoch: [820/2467]	Loss: 0.078201 
Train Epoch: [820/2467]	Loss: 0.302974
         Train Epoch: [820/2467]	Loss: 0.084599
 Train Epoch: [820/2467]	Loss: 0.526076
         Train Epoch: [840/2467]	Loss: 0.387780     
Train Epoch: [840/2467]	Loss: 0.231110
     Train Epoch: [840/2467]	Loss: 0.142174
 Train Epoch: [840/2467]	Loss: 0.161736
     Train Epoch: [860/2467]	Loss: 0.168877
     Train Epoch: [860/2467]	Loss: 0.140221
     Train Epoch: [860/2467]	Loss: 0.408216
     Train Epoch: [860/2467]	Loss: 0.232511
     Train Epoch: [880/2467]	Loss: 0.222113
             Train Epoch: [880/2467]	Loss: 0.312868  
Train Epoch: [880/2467]	Loss: 0.194304Train Epoch: [880/2467]	Loss: 0.420100

         Train Epoch: [900/2467]	Loss: 0.223495
     Train Epoch: [900/2467]	Loss: 0.220675
 Train Epoch: [900/2467]	Loss: 0.187946
     Train Epoch: [900/2467]	Loss: 0.229495
               Train Epoch: [920/2467]	Loss: 0.525720Train Epoch: [920/2467]	Loss: 0.438103Train Epoch: [920/2467]	Loss: 0.344410


     Train Epoch: [920/2467]	Loss: 0.153057
         Train Epoch: [940/2467]	Loss: 0.071201
 Train Epoch: [940/2467]	Loss: 0.209371
     Train Epoch: [940/2467]	Loss: 0.340471
     Train Epoch: [940/2467]	Loss: 0.085071
         Train Epoch: [960/2467]	Loss: 0.084963
 Train Epoch: [960/2467]	Loss: 0.476929
         Train Epoch: [960/2467]	Loss: 0.123467
 Train Epoch: [960/2467]	Loss: 0.107694
         Train Epoch: [980/2467]	Loss: 0.284181
     Train Epoch: [980/2467]	Loss: 0.326733
     Train Epoch: [980/2467]	Loss: 0.126139
 Train Epoch: [980/2467]	Loss: 0.256778
     Train Epoch: [1000/2467]	Loss: 0.096153
         Train Epoch: [1000/2467]	Loss: 0.333325
 Train Epoch: [1000/2467]	Loss: 0.061655
     Train Epoch: [1000/2467]	Loss: 0.398782
          Train Epoch: [1020/2467]	Loss: 0.396187    
Train Epoch: [1020/2467]	Loss: 0.203003
     Train Epoch: [1020/2467]	Loss: 0.442080
 Train Epoch: [1020/2467]	Loss: 0.101609
     Train Epoch: [1040/2467]	Loss: 0.364988
         Train Epoch: [1040/2467]	Loss: 0.244720 
Train Epoch: [1040/2467]	Loss: 0.123718
     Train Epoch: [1040/2467]	Loss: 0.215801
     Train Epoch: [1060/2467]	Loss: 0.523478    
     Train Epoch: [1060/2467]	Loss: 0.418990 
Train Epoch: [1060/2467]	Loss: 0.163730
     Train Epoch: [1060/2467]	Loss: 0.325010
     Train Epoch: [1080/2467]	Loss: 0.193634    
     Train Epoch: [1080/2467]	Loss: 0.341405     
Train Epoch: [1080/2467]	Loss: 0.200642
 Train Epoch: [1080/2467]	Loss: 0.399323
     Train Epoch: [1100/2467]	Loss: 0.381636
         Train Epoch: [1100/2467]	Loss: 0.325311
 Train Epoch: [1100/2467]	Loss: 0.453964
     Train Epoch: [1100/2467]	Loss: 0.352918
         Train Epoch: [1120/2467]	Loss: 0.154247 
Train Epoch: [1120/2467]	Loss: 0.385665
         Train Epoch: [1120/2467]	Loss: 0.174493
 Train Epoch: [1120/2467]	Loss: 0.227543
         Train Epoch: [1140/2467]	Loss: 0.330820
     Train Epoch: [1140/2467]	Loss: 0.169512
 Train Epoch: [1140/2467]	Loss: 0.564815
     Train Epoch: [1140/2467]	Loss: 0.095024
     Train Epoch: [1160/2467]	Loss: 0.256416
         Train Epoch: [1160/2467]	Loss: 0.158146 
Train Epoch: [1160/2467]	Loss: 0.202772
     Train Epoch: [1160/2467]	Loss: 0.130353
         Train Epoch: [1180/2467]	Loss: 0.106663
 Train Epoch: [1180/2467]	Loss: 0.525512
     Train Epoch: [1180/2467]	Loss: 0.357047
     Train Epoch: [1180/2467]	Loss: 0.208245
     Train Epoch: [1200/2467]	Loss: 0.331954
     Train Epoch: [1200/2467]	Loss: 0.215141
     Train Epoch: [1200/2467]	Loss: 0.328707
     Train Epoch: [1200/2467]	Loss: 0.230151
         Train Epoch: [1220/2467]	Loss: 0.452036    
 Train Epoch: [1220/2467]	Loss: 0.278323
     Train Epoch: [1220/2467]	Loss: 0.146070
 Train Epoch: [1220/2467]	Loss: 0.447452
         Train Epoch: [1240/2467]	Loss: 0.351843
 Train Epoch: [1240/2467]	Loss: 0.261606
     Train Epoch: [1240/2467]	Loss: 0.345686
     Train Epoch: [1240/2467]	Loss: 0.473210
         Train Epoch: [1260/2467]	Loss: 0.175467
 Train Epoch: [1260/2467]	Loss: 0.105777
     Train Epoch: [1260/2467]	Loss: 0.121876
     Train Epoch: [1260/2467]	Loss: 0.484833
         Train Epoch: [1280/2467]	Loss: 0.103710
     Train Epoch: [1280/2467]	Loss: 1.063732
     Train Epoch: [1280/2467]	Loss: 0.365637
 Train Epoch: [1280/2467]	Loss: 0.469010
         Train Epoch: [1300/2467]	Loss: 0.107681
 Train Epoch: [1300/2467]	Loss: 0.393161
     Train Epoch: [1300/2467]	Loss: 0.129810
     Train Epoch: [1300/2467]	Loss: 0.390943
     Train Epoch: [1320/2467]	Loss: 0.198122
         Train Epoch: [1320/2467]	Loss: 0.308718
 Train Epoch: [1320/2467]	Loss: 0.337856
     Train Epoch: [1320/2467]	Loss: 0.104464
         Train Epoch: [1340/2467]	Loss: 0.387661
     Train Epoch: [1340/2467]	Loss: 0.278398
 Train Epoch: [1340/2467]	Loss: 0.134123
     Train Epoch: [1340/2467]	Loss: 0.255142
     Train Epoch: [1360/2467]	Loss: 0.263534
         Train Epoch: [1360/2467]	Loss: 0.344094
     Train Epoch: [1360/2467]	Loss: 0.174632
 Train Epoch: [1360/2467]	Loss: 0.124925
     Train Epoch: [1380/2467]	Loss: 0.572110
     Train Epoch: [1380/2467]	Loss: 0.368803    
 Train Epoch: [1380/2467]	Loss: 0.275346
     Train Epoch: [1380/2467]	Loss: 0.318744
          Train Epoch: [1400/2467]	Loss: 0.247335Train Epoch: [1400/2467]	Loss: 0.355695

     Train Epoch: [1400/2467]	Loss: 0.282053
     Train Epoch: [1400/2467]	Loss: 0.151071
         Train Epoch: [1420/2467]	Loss: 0.248530
 Train Epoch: [1420/2467]	Loss: 0.474362
         Train Epoch: [1420/2467]	Loss: 0.211977
 Train Epoch: [1420/2467]	Loss: 0.201162
     Train Epoch: [1440/2467]	Loss: 0.032475
     Train Epoch: [1440/2467]	Loss: 0.077779
     Train Epoch: [1440/2467]	Loss: 0.214958
     Train Epoch: [1440/2467]	Loss: 0.288121
              Train Epoch: [1460/2467]	Loss: 0.236806Train Epoch: [1460/2467]	Loss: 0.056113

 Train Epoch: [1460/2467]	Loss: 0.145084
     Train Epoch: [1460/2467]	Loss: 0.279513
     Train Epoch: [1480/2467]	Loss: 0.369804
         Train Epoch: [1480/2467]	Loss: 0.085973
 Train Epoch: [1480/2467]	Loss: 0.033888
     Train Epoch: [1480/2467]	Loss: 0.240526
     Train Epoch: [1500/2467]	Loss: 0.282840
              Train Epoch: [1500/2467]	Loss: 0.217975Train Epoch: [1500/2467]	Loss: 0.144072

 Train Epoch: [1500/2467]	Loss: 0.030815
         Train Epoch: [1520/2467]	Loss: 0.459568
     Train Epoch: [1520/2467]	Loss: 0.261823 
Train Epoch: [1520/2467]	Loss: 0.167585
     Train Epoch: [1520/2467]	Loss: 0.418645
     Train Epoch: [1540/2467]	Loss: 0.121668
     Train Epoch: [1540/2467]	Loss: 1.065640
     Train Epoch: [1540/2467]	Loss: 0.323441
     Train Epoch: [1540/2467]	Loss: 0.373553
     Train Epoch: [1560/2467]	Loss: 0.416937
              Train Epoch: [1560/2467]	Loss: 0.419272Train Epoch: [1560/2467]	Loss: 0.649862

 Train Epoch: [1560/2467]	Loss: 0.423834
         Train Epoch: [1580/2467]	Loss: 0.334644
     Train Epoch: [1580/2467]	Loss: 0.218817
 Train Epoch: [1580/2467]	Loss: 0.323718
     Train Epoch: [1580/2467]	Loss: 0.382724
          Train Epoch: [1600/2467]	Loss: 0.312576    
Train Epoch: [1600/2467]	Loss: 0.111465
 Train Epoch: [1600/2467]	Loss: 0.412563
     Train Epoch: [1600/2467]	Loss: 0.159601
     Train Epoch: [1620/2467]	Loss: 0.120792
         Train Epoch: [1620/2467]	Loss: 0.251773
 Train Epoch: [1620/2467]	Loss: 0.054157
     Train Epoch: [1620/2467]	Loss: 0.244746
         Train Epoch: [1640/2467]	Loss: 0.295685 
    Train Epoch: [1640/2467]	Loss: 0.282094
     Train Epoch: [1640/2467]	Loss: 0.236484
 Train Epoch: [1640/2467]	Loss: 0.204506
     Train Epoch: [1660/2467]	Loss: 0.304699
     Train Epoch: [1660/2467]	Loss: 0.281416
     Train Epoch: [1660/2467]	Loss: 0.162158
     Train Epoch: [1660/2467]	Loss: 0.201129
     Train Epoch: [1680/2467]	Loss: 0.304197
     Train Epoch: [1680/2467]	Loss: 0.094794
     Train Epoch: [1680/2467]	Loss: 0.227220
     Train Epoch: [1680/2467]	Loss: 0.318391
         Train Epoch: [1700/2467]	Loss: 0.295909
 Train Epoch: [1700/2467]	Loss: 0.306840    
      Train Epoch: [1700/2467]	Loss: 0.224245Train Epoch: [1700/2467]	Loss: 0.088904

              Train Epoch: [1720/2467]	Loss: 0.198348Train Epoch: [1720/2467]	Loss: 0.308761

     Train Epoch: [1720/2467]	Loss: 0.301086
 Train Epoch: [1720/2467]	Loss: 0.298730
         Train Epoch: [1740/2467]	Loss: 0.099171 
Train Epoch: [1740/2467]	Loss: 0.128326
     Train Epoch: [1740/2467]	Loss: 0.246942
     Train Epoch: [1740/2467]	Loss: 0.501682
         Train Epoch: [1760/2467]	Loss: 0.141650
     Train Epoch: [1760/2467]	Loss: 0.102274
 Train Epoch: [1760/2467]	Loss: 0.160559
     Train Epoch: [1760/2467]	Loss: 0.193755
     Train Epoch: [1780/2467]	Loss: 0.299442
     Train Epoch: [1780/2467]	Loss: 0.143975
         Train Epoch: [1780/2467]	Loss: 0.467952
 Train Epoch: [1780/2467]	Loss: 0.364564
     Train Epoch: [1800/2467]	Loss: 0.456499
             Train Epoch: [1800/2467]	Loss: 0.305247
 Train Epoch: [1800/2467]	Loss: 0.415261 
Train Epoch: [1800/2467]	Loss: 0.256128
         Train Epoch: [1820/2467]	Loss: 0.166755
 Train Epoch: [1820/2467]	Loss: 0.391316
         Train Epoch: [1820/2467]	Loss: 0.273405
 Train Epoch: [1820/2467]	Loss: 0.191031
     Train Epoch: [1840/2467]	Loss: 0.381595
         Train Epoch: [1840/2467]	Loss: 0.198671
 Train Epoch: [1840/2467]	Loss: 0.091590
     Train Epoch: [1840/2467]	Loss: 0.340093
          Train Epoch: [1860/2467]	Loss: 0.297475Train Epoch: [1860/2467]	Loss: 0.230591
    
     Train Epoch: [1860/2467]	Loss: 0.353547
 Train Epoch: [1860/2467]	Loss: 0.142433
     Train Epoch: [1880/2467]	Loss: 0.102336    
 Train Epoch: [1880/2467]	Loss: 0.183532
     Train Epoch: [1880/2467]	Loss: 0.160700
     Train Epoch: [1880/2467]	Loss: 0.077693
          Train Epoch: [1900/2467]	Loss: 0.399425Train Epoch: [1900/2467]	Loss: 0.263274

     Train Epoch: [1900/2467]	Loss: 0.342011
     Train Epoch: [1900/2467]	Loss: 0.173429
         Train Epoch: [1920/2467]	Loss: 0.317065    
 Train Epoch: [1920/2467]	Loss: 0.347418 
Train Epoch: [1920/2467]	Loss: 0.027101
     Train Epoch: [1920/2467]	Loss: 0.309949
          Train Epoch: [1940/2467]	Loss: 0.161030Train Epoch: [1940/2467]	Loss: 0.500985

     Train Epoch: [1940/2467]	Loss: 0.141576
     Train Epoch: [1940/2467]	Loss: 0.542441
     Train Epoch: [1960/2467]	Loss: 0.260686
         Train Epoch: [1960/2467]	Loss: 0.152937 
Train Epoch: [1960/2467]	Loss: 0.206608
     Train Epoch: [1960/2467]	Loss: 0.210373
     Train Epoch: [1980/2467]	Loss: 0.380483
         Train Epoch: [1980/2467]	Loss: 0.284333
     Train Epoch: [1980/2467]	Loss: 0.409442
 Train Epoch: [1980/2467]	Loss: 0.176660
         Train Epoch: [2000/2467]	Loss: 0.115151
 Train Epoch: [2000/2467]	Loss: 0.074942
          Train Epoch: [2000/2467]	Loss: 0.198051Train Epoch: [2000/2467]	Loss: 0.396189

     Train Epoch: [2020/2467]	Loss: 0.461215
     Train Epoch: [2020/2467]	Loss: 0.320065
          Train Epoch: [2020/2467]	Loss: 0.157533Train Epoch: [2020/2467]	Loss: 0.281560

              Train Epoch: [2040/2467]	Loss: 0.229559Train Epoch: [2040/2467]	Loss: 0.407705

     Train Epoch: [2040/2467]	Loss: 0.304598
 Train Epoch: [2040/2467]	Loss: 0.116499
     Train Epoch: [2060/2467]	Loss: 0.240473
         Train Epoch: [2060/2467]	Loss: 0.223627    
 Train Epoch: [2060/2467]	Loss: 0.389774 
Train Epoch: [2060/2467]	Loss: 0.263555
         Train Epoch: [2080/2467]	Loss: 0.194507
      Train Epoch: [2080/2467]	Loss: 0.328492
Train Epoch: [2080/2467]	Loss: 0.355584
     Train Epoch: [2080/2467]	Loss: 0.169434
             Train Epoch: [2100/2467]	Loss: 0.143159
 Train Epoch: [2100/2467]	Loss: 0.164992
 Train Epoch: [2100/2467]	Loss: 0.154172
     Train Epoch: [2100/2467]	Loss: 0.688756
         Train Epoch: [2120/2467]	Loss: 0.144833
 Train Epoch: [2120/2467]	Loss: 0.261509
     Train Epoch: [2120/2467]	Loss: 0.149016
     Train Epoch: [2120/2467]	Loss: 0.352118
     Train Epoch: [2140/2467]	Loss: 0.238892    
 Train Epoch: [2140/2467]	Loss: 0.635566
         Train Epoch: [2140/2467]	Loss: 0.530533
 Train Epoch: [2140/2467]	Loss: 0.392159
     Train Epoch: [2160/2467]	Loss: 0.200355
         Train Epoch: [2160/2467]	Loss: 0.416033
      Train Epoch: [2160/2467]	Loss: 0.071690Train Epoch: [2160/2467]	Loss: 0.241886

          Train Epoch: [2180/2467]	Loss: 0.244550Train Epoch: [2180/2467]	Loss: 0.186147

     Train Epoch: [2180/2467]	Loss: 0.349114
     Train Epoch: [2180/2467]	Loss: 0.214638
     Train Epoch: [2200/2467]	Loss: 0.156395
         Train Epoch: [2200/2467]	Loss: 0.433232
 Train Epoch: [2200/2467]	Loss: 0.164588
     Train Epoch: [2200/2467]	Loss: 0.144984
         Train Epoch: [2220/2467]	Loss: 0.168676
 Train Epoch: [2220/2467]	Loss: 0.107430
     Train Epoch: [2220/2467]	Loss: 0.137420
     Train Epoch: [2220/2467]	Loss: 0.185008
     Train Epoch: [2240/2467]	Loss: 0.056659
              Train Epoch: [2240/2467]	Loss: 0.235532Train Epoch: [2240/2467]	Loss: 0.149665

 Train Epoch: [2240/2467]	Loss: 0.093295
     Train Epoch: [2260/2467]	Loss: 0.518656
         Train Epoch: [2260/2467]	Loss: 0.357322 
Train Epoch: [2260/2467]	Loss: 0.128015
     Train Epoch: [2260/2467]	Loss: 0.250561
         Train Epoch: [2280/2467]	Loss: 0.255666
     Train Epoch: [2280/2467]	Loss: 0.170686
 Train Epoch: [2280/2467]	Loss: 0.126249    
 Train Epoch: [2280/2467]	Loss: 0.193662
         Train Epoch: [2300/2467]	Loss: 0.496588
 Train Epoch: [2300/2467]	Loss: 0.151371
         Train Epoch: [2300/2467]	Loss: 0.084133
 Train Epoch: [2300/2467]	Loss: 0.182936
     Train Epoch: [2320/2467]	Loss: 0.468919
             Train Epoch: [2320/2467]	Loss: 0.169046 
Train Epoch: [2320/2467]	Loss: 0.188331 
Train Epoch: [2320/2467]	Loss: 0.281295
         Train Epoch: [2340/2467]	Loss: 0.147972
 Train Epoch: [2340/2467]	Loss: 0.167713
     Train Epoch: [2340/2467]	Loss: 0.233844
     Train Epoch: [2340/2467]	Loss: 0.580594
         Train Epoch: [2360/2467]	Loss: 0.229693
     Train Epoch: [2360/2467]	Loss: 0.408711
     Train Epoch: [2360/2467]	Loss: 0.129194
 Train Epoch: [2360/2467]	Loss: 0.198085
         Train Epoch: [2380/2467]	Loss: 0.278995
 Train Epoch: [2380/2467]	Loss: 0.529761
     Train Epoch: [2380/2467]	Loss: 0.286205
     Train Epoch: [2380/2467]	Loss: 0.303846
     Train Epoch: [2400/2467]	Loss: 0.358400    
      Train Epoch: [2400/2467]	Loss: 0.160616Train Epoch: [2400/2467]	Loss: 0.303318

     Train Epoch: [2400/2467]	Loss: 0.198513
             Train Epoch: [2420/2467]	Loss: 0.335295
 Train Epoch: [2420/2467]	Loss: 0.463969 
    Train Epoch: [2420/2467]	Loss: 0.373947
 Train Epoch: [2420/2467]	Loss: 0.357012
          Train Epoch: [2440/2467]	Loss: 0.163370Train Epoch: [2440/2467]	Loss: 0.086275

     Train Epoch: [2440/2467]	Loss: 0.423369
     Train Epoch: [2440/2467]	Loss: 0.196369
         Train Epoch: [2460/2467]	Loss: 0.173281
 Train Epoch: [2460/2467]	Loss: 0.143432
     Train Epoch: [2460/2467]	Loss: 0.396175
     Train Epoch: [2460/2467]	Loss: 0.191630
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 11 epoch =====
     2025-05-11.04-03-19
after set grad
after prog
start loop
     ===== running 11 epoch =====
     2025-05-11.04-03-19
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 11 epoch =====
     2025-05-11.04-03-19
after set grad
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 11 epoch =====
     2025-05-11.04-03-19
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
             Train Epoch: [0/2467]	Loss: 0.375674 
Train Epoch: [0/2467]	Loss: 0.271595
 Train Epoch: [0/2467]	Loss: 0.296652
     Train Epoch: [0/2467]	Loss: 0.392458
          Train Epoch: [20/2467]	Loss: 0.162285
Train Epoch: [20/2467]	Loss: 0.164667
          Train Epoch: [20/2467]	Loss: 0.404751Train Epoch: [20/2467]	Loss: 0.288581

     Train Epoch: [40/2467]	Loss: 0.280228
     Train Epoch: [40/2467]	Loss: 0.403326
     Train Epoch: [40/2467]	Loss: 0.185974
     Train Epoch: [40/2467]	Loss: 0.287919
         Train Epoch: [60/2467]	Loss: 0.226189
 Train Epoch: [60/2467]	Loss: 0.341104
     Train Epoch: [60/2467]	Loss: 0.064711
     Train Epoch: [60/2467]	Loss: 0.191349
             Train Epoch: [80/2467]	Loss: 0.157313
 Train Epoch: [80/2467]	Loss: 0.131556
 Train Epoch: [80/2467]	Loss: 0.274938
     Train Epoch: [80/2467]	Loss: 0.432830
     Train Epoch: [100/2467]	Loss: 0.278719    
     Train Epoch: [100/2467]	Loss: 0.363486 
Train Epoch: [100/2467]	Loss: 0.349223
     Train Epoch: [100/2467]	Loss: 0.266759
     Train Epoch: [120/2467]	Loss: 0.098643
          Train Epoch: [120/2467]	Loss: 0.088183Train Epoch: [120/2467]	Loss: 0.407988    

 Train Epoch: [120/2467]	Loss: 0.359590
              Train Epoch: [140/2467]	Loss: 0.086322
Train Epoch: [140/2467]	Loss: 0.196300
 Train Epoch: [140/2467]	Loss: 0.297112
     Train Epoch: [140/2467]	Loss: 0.072501
     Train Epoch: [160/2467]	Loss: 0.226505
     Train Epoch: [160/2467]	Loss: 0.272462
         Train Epoch: [160/2467]	Loss: 0.184537 
Train Epoch: [160/2467]	Loss: 0.372606
         Train Epoch: [180/2467]	Loss: 0.169648
 Train Epoch: [180/2467]	Loss: 0.217285
     Train Epoch: [180/2467]	Loss: 0.224480
     Train Epoch: [180/2467]	Loss: 0.181418
         Train Epoch: [200/2467]	Loss: 0.140229    
 Train Epoch: [200/2467]	Loss: 0.164478
 Train Epoch: [200/2467]	Loss: 0.160309
     Train Epoch: [200/2467]	Loss: 0.264142
         Train Epoch: [220/2467]	Loss: 0.124513
     Train Epoch: [220/2467]	Loss: 0.265855
 Train Epoch: [220/2467]	Loss: 0.284175
     Train Epoch: [220/2467]	Loss: 0.231409
         Train Epoch: [240/2467]	Loss: 0.026437
 Train Epoch: [240/2467]	Loss: 0.225109    
      Train Epoch: [240/2467]	Loss: 0.336072
Train Epoch: [240/2467]	Loss: 0.063110
     Train Epoch: [260/2467]	Loss: 0.080092
     Train Epoch: [260/2467]	Loss: 0.095134
     Train Epoch: [260/2467]	Loss: 0.141125
     Train Epoch: [260/2467]	Loss: 0.173847
         Train Epoch: [280/2467]	Loss: 0.273647
     Train Epoch: [280/2467]	Loss: 0.107810
 Train Epoch: [280/2467]	Loss: 0.308808
     Train Epoch: [280/2467]	Loss: 0.292691
          Train Epoch: [300/2467]	Loss: 0.500979Train Epoch: [300/2467]	Loss: 0.174596

         Train Epoch: [300/2467]	Loss: 0.130843
 Train Epoch: [300/2467]	Loss: 0.059029
         Train Epoch: [320/2467]	Loss: 0.187046
     Train Epoch: [320/2467]	Loss: 0.452091
 Train Epoch: [320/2467]	Loss: 0.522138
     Train Epoch: [320/2467]	Loss: 0.230600
              Train Epoch: [340/2467]	Loss: 0.041795
Train Epoch: [340/2467]	Loss: 0.194301
 Train Epoch: [340/2467]	Loss: 0.088544
     Train Epoch: [340/2467]	Loss: 0.268006
             Train Epoch: [360/2467]	Loss: 0.146098
  Train Epoch: [360/2467]	Loss: 0.237438Train Epoch: [360/2467]	Loss: 0.263065

     Train Epoch: [360/2467]	Loss: 0.204573
              Train Epoch: [380/2467]	Loss: 0.225430 
Train Epoch: [380/2467]	Loss: 0.214349Train Epoch: [380/2467]	Loss: 0.492679

     Train Epoch: [380/2467]	Loss: 0.615156
         Train Epoch: [400/2467]	Loss: 0.208594 
Train Epoch: [400/2467]	Loss: 0.121172
     Train Epoch: [400/2467]	Loss: 0.407588
     Train Epoch: [400/2467]	Loss: 0.238672
             Train Epoch: [420/2467]	Loss: 0.339616
      Train Epoch: [420/2467]	Loss: 0.313683Train Epoch: [420/2467]	Loss: 0.145656

 Train Epoch: [420/2467]	Loss: 0.189921
          Train Epoch: [440/2467]	Loss: 0.157766Train Epoch: [440/2467]	Loss: 0.185816

         Train Epoch: [440/2467]	Loss: 0.098728
 Train Epoch: [440/2467]	Loss: 0.373028
         Train Epoch: [460/2467]	Loss: 0.321973
 Train Epoch: [460/2467]	Loss: 0.183480
     Train Epoch: [460/2467]	Loss: 0.205700
     Train Epoch: [460/2467]	Loss: 0.185919
     Train Epoch: [480/2467]	Loss: 0.418137
     Train Epoch: [480/2467]	Loss: 0.149848
         Train Epoch: [480/2467]	Loss: 0.254968
 Train Epoch: [480/2467]	Loss: 0.407277
          Train Epoch: [500/2467]	Loss: 0.370920
Train Epoch: [500/2467]	Loss: 0.130641
         Train Epoch: [500/2467]	Loss: 0.431452
 Train Epoch: [500/2467]	Loss: 0.179934
         Train Epoch: [520/2467]	Loss: 0.301341
     Train Epoch: [520/2467]	Loss: 0.285842
 Train Epoch: [520/2467]	Loss: 0.409596
     Train Epoch: [520/2467]	Loss: 0.102329
          Train Epoch: [540/2467]	Loss: 0.305250
Train Epoch: [540/2467]	Loss: 0.087977
     Train Epoch: [540/2467]	Loss: 0.168894
     Train Epoch: [540/2467]	Loss: 0.225764
          Train Epoch: [560/2467]	Loss: 0.194797
Train Epoch: [560/2467]	Loss: 0.248088
     Train Epoch: [560/2467]	Loss: 0.363742
     Train Epoch: [560/2467]	Loss: 0.265257
         Train Epoch: [580/2467]	Loss: 0.395594
     Train Epoch: [580/2467]	Loss: 0.193275
     Train Epoch: [580/2467]	Loss: 0.178297
 Train Epoch: [580/2467]	Loss: 0.081438
     Train Epoch: [600/2467]	Loss: 0.216971
         Train Epoch: [600/2467]	Loss: 0.152738
 Train Epoch: [600/2467]	Loss: 0.250586
     Train Epoch: [600/2467]	Loss: 0.230275
         Train Epoch: [620/2467]	Loss: 0.187563
     Train Epoch: [620/2467]	Loss: 0.328694
 Train Epoch: [620/2467]	Loss: 0.049578
     Train Epoch: [620/2467]	Loss: 0.316074
         Train Epoch: [640/2467]	Loss: 0.083508
 Train Epoch: [640/2467]	Loss: 0.122459
         Train Epoch: [640/2467]	Loss: 0.378638 
Train Epoch: [640/2467]	Loss: 0.224601
         Train Epoch: [660/2467]	Loss: 0.483719 
Train Epoch: [660/2467]	Loss: 0.047448
     Train Epoch: [660/2467]	Loss: 0.312991
     Train Epoch: [660/2467]	Loss: 0.064251
         Train Epoch: [680/2467]	Loss: 0.173874
 Train Epoch: [680/2467]	Loss: 0.423538
     Train Epoch: [680/2467]	Loss: 0.081880
     Train Epoch: [680/2467]	Loss: 0.142812
         Train Epoch: [700/2467]	Loss: 0.095538
 Train Epoch: [700/2467]	Loss: 0.103241
     Train Epoch: [700/2467]	Loss: 0.379974
     Train Epoch: [700/2467]	Loss: 0.508612
     Train Epoch: [720/2467]	Loss: 0.258439
     Train Epoch: [720/2467]	Loss: 0.249889    
 Train Epoch: [720/2467]	Loss: 0.185793
     Train Epoch: [720/2467]	Loss: 0.048662
         Train Epoch: [740/2467]	Loss: 0.090120 
Train Epoch: [740/2467]	Loss: 0.327428
     Train Epoch: [740/2467]	Loss: 0.088529    
 Train Epoch: [740/2467]	Loss: 0.116730
     Train Epoch: [760/2467]	Loss: 0.593892
     Train Epoch: [760/2467]	Loss: 0.300468
     Train Epoch: [760/2467]	Loss: 0.429491
     Train Epoch: [760/2467]	Loss: 0.096264
         Train Epoch: [780/2467]	Loss: 0.403465
         Train Epoch: [780/2467]	Loss: 0.395240
 Train Epoch: [780/2467]	Loss: 0.314548
 Train Epoch: [780/2467]	Loss: 0.377006
         Train Epoch: [800/2467]	Loss: 0.265633
     Train Epoch: [800/2467]	Loss: 0.369653
 Train Epoch: [800/2467]	Loss: 0.244477
     Train Epoch: [800/2467]	Loss: 0.128971
     Train Epoch: [820/2467]	Loss: 0.339079
     Train Epoch: [820/2467]	Loss: 0.065277
     Train Epoch: [820/2467]	Loss: 0.488597
     Train Epoch: [820/2467]	Loss: 0.064167
          Train Epoch: [840/2467]	Loss: 0.201042    Train Epoch: [840/2467]	Loss: 0.371568

 Train Epoch: [840/2467]	Loss: 0.153408
     Train Epoch: [840/2467]	Loss: 0.142615
          Train Epoch: [860/2467]	Loss: 0.331660Train Epoch: [860/2467]	Loss: 0.160452

         Train Epoch: [860/2467]	Loss: 0.198269
 Train Epoch: [860/2467]	Loss: 0.115923
     Train Epoch: [880/2467]	Loss: 0.206725
         Train Epoch: [880/2467]	Loss: 0.232175
     Train Epoch: [880/2467]	Loss: 0.286192
 Train Epoch: [880/2467]	Loss: 0.200832
     Train Epoch: [900/2467]	Loss: 0.158858
         Train Epoch: [900/2467]	Loss: 0.221732
 Train Epoch: [900/2467]	Loss: 0.213885
     Train Epoch: [900/2467]	Loss: 0.222201
         Train Epoch: [920/2467]	Loss: 0.436504
 Train Epoch: [920/2467]	Loss: 0.342297
     Train Epoch: [920/2467]	Loss: 0.080626
     Train Epoch: [920/2467]	Loss: 0.532925
     Train Epoch: [940/2467]	Loss: 0.047762
     Train Epoch: [940/2467]	Loss: 0.199772
     Train Epoch: [940/2467]	Loss: 0.079656
     Train Epoch: [940/2467]	Loss: 0.237653
              Train Epoch: [960/2467]	Loss: 0.450315Train Epoch: [960/2467]	Loss: 0.081787
 
Train Epoch: [960/2467]	Loss: 0.104055
     Train Epoch: [960/2467]	Loss: 0.127830
             Train Epoch: [980/2467]	Loss: 0.240096
 Train Epoch: [980/2467]	Loss: 0.112624 
Train Epoch: [980/2467]	Loss: 0.324638
     Train Epoch: [980/2467]	Loss: 0.207156
             Train Epoch: [1000/2467]	Loss: 0.106269
  Train Epoch: [1000/2467]	Loss: 0.405627Train Epoch: [1000/2467]	Loss: 0.312364

     Train Epoch: [1000/2467]	Loss: 0.057840
     Train Epoch: [1020/2467]	Loss: 0.191811
             Train Epoch: [1020/2467]	Loss: 0.429909
  Train Epoch: [1020/2467]	Loss: 0.095776
Train Epoch: [1020/2467]	Loss: 0.394204
         Train Epoch: [1040/2467]	Loss: 0.372596
     Train Epoch: [1040/2467]	Loss: 0.351494
 Train Epoch: [1040/2467]	Loss: 0.137495
     Train Epoch: [1040/2467]	Loss: 0.268816
         Train Epoch: [1060/2467]	Loss: 0.474673
 Train Epoch: [1060/2467]	Loss: 0.521798
          Train Epoch: [1060/2467]	Loss: 0.238490Train Epoch: [1060/2467]	Loss: 0.415346

     Train Epoch: [1080/2467]	Loss: 0.211129
         Train Epoch: [1080/2467]	Loss: 0.400992
 Train Epoch: [1080/2467]	Loss: 0.384844
     Train Epoch: [1080/2467]	Loss: 0.190343
     Train Epoch: [1100/2467]	Loss: 0.325630
     Train Epoch: [1100/2467]	Loss: 0.283991
     Train Epoch: [1100/2467]	Loss: 0.481275
     Train Epoch: [1100/2467]	Loss: 0.374123
     Train Epoch: [1120/2467]	Loss: 0.166168
     Train Epoch: [1120/2467]	Loss: 0.193988
         Train Epoch: [1120/2467]	Loss: 0.227678 
Train Epoch: [1120/2467]	Loss: 0.303998
         Train Epoch: [1140/2467]	Loss: 0.139858
 Train Epoch: [1140/2467]	Loss: 0.317916
     Train Epoch: [1140/2467]	Loss: 0.453577
     Train Epoch: [1140/2467]	Loss: 0.078063
             Train Epoch: [1160/2467]	Loss: 0.249387  
Train Epoch: [1160/2467]	Loss: 0.301516Train Epoch: [1160/2467]	Loss: 0.114749

     Train Epoch: [1160/2467]	Loss: 0.175952
     Train Epoch: [1180/2467]	Loss: 0.474792
         Train Epoch: [1180/2467]	Loss: 0.301356 
Train Epoch: [1180/2467]	Loss: 0.217807
     Train Epoch: [1180/2467]	Loss: 0.110360
         Train Epoch: [1200/2467]	Loss: 0.350617     
Train Epoch: [1200/2467]	Loss: 0.141169
     Train Epoch: [1200/2467]	Loss: 0.217896
 Train Epoch: [1200/2467]	Loss: 0.293596
         Train Epoch: [1220/2467]	Loss: 0.398699
 Train Epoch: [1220/2467]	Loss: 0.282159
     Train Epoch: [1220/2467]	Loss: 0.464413
     Train Epoch: [1220/2467]	Loss: 0.142332
             Train Epoch: [1240/2467]	Loss: 0.357108
 Train Epoch: [1240/2467]	Loss: 0.344871 
Train Epoch: [1240/2467]	Loss: 0.359268
     Train Epoch: [1240/2467]	Loss: 0.374086
     Train Epoch: [1260/2467]	Loss: 0.101917
             Train Epoch: [1260/2467]	Loss: 0.468756
 Train Epoch: [1260/2467]	Loss: 0.141927
 Train Epoch: [1260/2467]	Loss: 0.155900
          Train Epoch: [1280/2467]	Loss: 1.062491    
Train Epoch: [1280/2467]	Loss: 0.455702
 Train Epoch: [1280/2467]	Loss: 0.441843
     Train Epoch: [1280/2467]	Loss: 0.119326
     Train Epoch: [1300/2467]	Loss: 0.122037
         Train Epoch: [1300/2467]	Loss: 0.421619    
  Train Epoch: [1300/2467]	Loss: 0.130934Train Epoch: [1300/2467]	Loss: 0.410288

         Train Epoch: [1320/2467]	Loss: 0.237242
         Train Epoch: [1320/2467]	Loss: 0.320749
 Train Epoch: [1320/2467]	Loss: 0.077412 
Train Epoch: [1320/2467]	Loss: 0.303405
          Train Epoch: [1340/2467]	Loss: 0.413991    Train Epoch: [1340/2467]	Loss: 0.269670

 Train Epoch: [1340/2467]	Loss: 0.112637
     Train Epoch: [1340/2467]	Loss: 0.207716
         Train Epoch: [1360/2467]	Loss: 0.155245
 Train Epoch: [1360/2467]	Loss: 0.198338
     Train Epoch: [1360/2467]	Loss: 0.113895
     Train Epoch: [1360/2467]	Loss: 0.573944
     Train Epoch: [1380/2467]	Loss: 0.386862
     Train Epoch: [1380/2467]	Loss: 0.352726
     Train Epoch: [1380/2467]	Loss: 0.219805
     Train Epoch: [1380/2467]	Loss: 0.302353
     Train Epoch: [1400/2467]	Loss: 0.149577
          Train Epoch: [1400/2467]	Loss: 0.213896Train Epoch: [1400/2467]	Loss: 0.256803

     Train Epoch: [1400/2467]	Loss: 0.319738
             Train Epoch: [1420/2467]	Loss: 0.215369 
Train Epoch: [1420/2467]	Loss: 0.475504
 Train Epoch: [1420/2467]	Loss: 0.168612
     Train Epoch: [1420/2467]	Loss: 0.178817
     Train Epoch: [1440/2467]	Loss: 0.047522
         Train Epoch: [1440/2467]	Loss: 0.078177 
Train Epoch: [1440/2467]	Loss: 0.289419
     Train Epoch: [1440/2467]	Loss: 0.109454
     Train Epoch: [1460/2467]	Loss: 0.220593
         Train Epoch: [1460/2467]	Loss: 0.139087
 Train Epoch: [1460/2467]	Loss: 0.060550
     Train Epoch: [1460/2467]	Loss: 0.303572
              Train Epoch: [1480/2467]	Loss: 0.022675Train Epoch: [1480/2467]	Loss: 0.062373

 Train Epoch: [1480/2467]	Loss: 0.327405
     Train Epoch: [1480/2467]	Loss: 0.216394
     Train Epoch: [1500/2467]	Loss: 0.233182
              Train Epoch: [1500/2467]	Loss: 0.145637 
Train Epoch: [1500/2467]	Loss: 0.214676
Train Epoch: [1500/2467]	Loss: 0.033740
         Train Epoch: [1520/2467]	Loss: 0.183050
     Train Epoch: [1520/2467]	Loss: 0.282519
 Train Epoch: [1520/2467]	Loss: 0.334365
     Train Epoch: [1520/2467]	Loss: 0.373956
         Train Epoch: [1540/2467]	Loss: 0.230751 
Train Epoch: [1540/2467]	Loss: 0.123863
     Train Epoch: [1540/2467]	Loss: 0.342404
     Train Epoch: [1540/2467]	Loss: 0.904027
     Train Epoch: [1560/2467]	Loss: 0.352772
         Train Epoch: [1560/2467]	Loss: 0.376084 
Train Epoch: [1560/2467]	Loss: 0.438048
     Train Epoch: [1560/2467]	Loss: 0.596462
             Train Epoch: [1580/2467]	Loss: 0.212876
      Train Epoch: [1580/2467]	Loss: 0.291336
Train Epoch: [1580/2467]	Loss: 0.360460
 Train Epoch: [1580/2467]	Loss: 0.385269
     Train Epoch: [1600/2467]	Loss: 0.333392
         Train Epoch: [1600/2467]	Loss: 0.101005 
Train Epoch: [1600/2467]	Loss: 0.372640
     Train Epoch: [1600/2467]	Loss: 0.214465
     Train Epoch: [1620/2467]	Loss: 0.109919
     Train Epoch: [1620/2467]	Loss: 0.236410
     Train Epoch: [1620/2467]	Loss: 0.072366
     Train Epoch: [1620/2467]	Loss: 0.258582
             Train Epoch: [1640/2467]	Loss: 0.278597
 Train Epoch: [1640/2467]	Loss: 0.291990
 Train Epoch: [1640/2467]	Loss: 0.253845
     Train Epoch: [1640/2467]	Loss: 0.167387
         Train Epoch: [1660/2467]	Loss: 0.267578
     Train Epoch: [1660/2467]	Loss: 0.196127
     Train Epoch: [1660/2467]	Loss: 0.318569
 Train Epoch: [1660/2467]	Loss: 0.162826
     Train Epoch: [1680/2467]	Loss: 0.369541
     Train Epoch: [1680/2467]	Loss: 0.085223
     Train Epoch: [1680/2467]	Loss: 0.283603
     Train Epoch: [1680/2467]	Loss: 0.360904
         Train Epoch: [1700/2467]	Loss: 0.228424
 Train Epoch: [1700/2467]	Loss: 0.298517
         Train Epoch: [1700/2467]	Loss: 0.131422
 Train Epoch: [1700/2467]	Loss: 0.246151
         Train Epoch: [1720/2467]	Loss: 0.243213
     Train Epoch: [1720/2467]	Loss: 0.151109
     Train Epoch: [1720/2467]	Loss: 0.311955
 Train Epoch: [1720/2467]	Loss: 0.257255
              Train Epoch: [1740/2467]	Loss: 0.059793Train Epoch: [1740/2467]	Loss: 0.130655

 Train Epoch: [1740/2467]	Loss: 0.499740
     Train Epoch: [1740/2467]	Loss: 0.223430
     Train Epoch: [1760/2467]	Loss: 0.186211
     Train Epoch: [1760/2467]	Loss: 0.091212
         Train Epoch: [1760/2467]	Loss: 0.211951
 Train Epoch: [1760/2467]	Loss: 0.135408
     Train Epoch: [1780/2467]	Loss: 0.353855
         Train Epoch: [1780/2467]	Loss: 0.141588
 Train Epoch: [1780/2467]	Loss: 0.399142
     Train Epoch: [1780/2467]	Loss: 0.401232
     Train Epoch: [1800/2467]	Loss: 0.364401
          Train Epoch: [1800/2467]	Loss: 0.266946    Train Epoch: [1800/2467]	Loss: 0.326812

 Train Epoch: [1800/2467]	Loss: 0.258159
             Train Epoch: [1820/2467]	Loss: 0.248836 
Train Epoch: [1820/2467]	Loss: 0.141308
 Train Epoch: [1820/2467]	Loss: 0.402296
     Train Epoch: [1820/2467]	Loss: 0.220382
         Train Epoch: [1840/2467]	Loss: 0.362607
 Train Epoch: [1840/2467]	Loss: 0.186173
         Train Epoch: [1840/2467]	Loss: 0.290858 
Train Epoch: [1840/2467]	Loss: 0.094666
         Train Epoch: [1860/2467]	Loss: 0.217609
 Train Epoch: [1860/2467]	Loss: 0.202980
     Train Epoch: [1860/2467]	Loss: 0.125651
     Train Epoch: [1860/2467]	Loss: 0.222591
     Train Epoch: [1880/2467]	Loss: 0.065249
     Train Epoch: [1880/2467]	Loss: 0.245573
          Train Epoch: [1880/2467]	Loss: 0.126937
Train Epoch: [1880/2467]	Loss: 0.135929
     Train Epoch: [1900/2467]	Loss: 0.166588
     Train Epoch: [1900/2467]	Loss: 0.183539
     Train Epoch: [1900/2467]	Loss: 0.365633
     Train Epoch: [1900/2467]	Loss: 0.340978
             Train Epoch: [1920/2467]	Loss: 0.185560
  Train Epoch: [1920/2467]	Loss: 0.019903Train Epoch: [1920/2467]	Loss: 0.317217

     Train Epoch: [1920/2467]	Loss: 0.369365
     Train Epoch: [1940/2467]	Loss: 0.311290
     Train Epoch: [1940/2467]	Loss: 0.159750
     Train Epoch: [1940/2467]	Loss: 0.120351
     Train Epoch: [1940/2467]	Loss: 0.481771
     Train Epoch: [1960/2467]	Loss: 0.327657
             Train Epoch: [1960/2467]	Loss: 0.149920
  Train Epoch: [1960/2467]	Loss: 0.284404
Train Epoch: [1960/2467]	Loss: 0.186228
         Train Epoch: [1980/2467]	Loss: 0.316767
     Train Epoch: [1980/2467]	Loss: 0.365684
 Train Epoch: [1980/2467]	Loss: 0.225097
     Train Epoch: [1980/2467]	Loss: 0.356518
         Train Epoch: [2000/2467]	Loss: 0.219580
 Train Epoch: [2000/2467]	Loss: 0.097764
         Train Epoch: [2000/2467]	Loss: 0.053808
 Train Epoch: [2000/2467]	Loss: 0.458474
          Train Epoch: [2020/2467]	Loss: 0.380683Train Epoch: [2020/2467]	Loss: 0.267822

     Train Epoch: [2020/2467]	Loss: 0.130866
     Train Epoch: [2020/2467]	Loss: 0.363370
         Train Epoch: [2040/2467]	Loss: 0.191334    
 Train Epoch: [2040/2467]	Loss: 0.328905
 Train Epoch: [2040/2467]	Loss: 0.110451
     Train Epoch: [2040/2467]	Loss: 0.401713
     Train Epoch: [2060/2467]	Loss: 0.326486    
 Train Epoch: [2060/2467]	Loss: 0.219284
     Train Epoch: [2060/2467]	Loss: 0.653314
     Train Epoch: [2060/2467]	Loss: 0.280770
         Train Epoch: [2080/2467]	Loss: 0.285105
 Train Epoch: [2080/2467]	Loss: 0.181734
     Train Epoch: [2080/2467]	Loss: 0.134964
     Train Epoch: [2080/2467]	Loss: 0.124349
     Train Epoch: [2100/2467]	Loss: 0.122488
     Train Epoch: [2100/2467]	Loss: 0.170916    
 Train Epoch: [2100/2467]	Loss: 0.157096
     Train Epoch: [2100/2467]	Loss: 0.646161
     Train Epoch: [2120/2467]	Loss: 0.114295
     Train Epoch: [2120/2467]	Loss: 0.339540
     Train Epoch: [2120/2467]	Loss: 0.236180
     Train Epoch: [2120/2467]	Loss: 0.129806
             Train Epoch: [2140/2467]	Loss: 0.326018
  Train Epoch: [2140/2467]	Loss: 0.228721Train Epoch: [2140/2467]	Loss: 0.663340

     Train Epoch: [2140/2467]	Loss: 0.430727
         Train Epoch: [2160/2467]	Loss: 0.060069
 Train Epoch: [2160/2467]	Loss: 0.200638
     Train Epoch: [2160/2467]	Loss: 0.340451
     Train Epoch: [2160/2467]	Loss: 0.220876
          Train Epoch: [2180/2467]	Loss: 0.226580    Train Epoch: [2180/2467]	Loss: 0.162080

 Train Epoch: [2180/2467]	Loss: 0.349248
     Train Epoch: [2180/2467]	Loss: 0.224807
         Train Epoch: [2200/2467]	Loss: 0.158265 
Train Epoch: [2200/2467]	Loss: 0.191600
     Train Epoch: [2200/2467]	Loss: 0.428877
     Train Epoch: [2200/2467]	Loss: 0.139202
     Train Epoch: [2220/2467]	Loss: 0.241891
             Train Epoch: [2220/2467]	Loss: 0.192489
  Train Epoch: [2220/2467]	Loss: 0.110664Train Epoch: [2220/2467]	Loss: 0.136522

              Train Epoch: [2240/2467]	Loss: 0.066963Train Epoch: [2240/2467]	Loss: 0.094672

 Train Epoch: [2240/2467]	Loss: 0.203316
     Train Epoch: [2240/2467]	Loss: 0.170068
     Train Epoch: [2260/2467]	Loss: 0.699337
     Train Epoch: [2260/2467]	Loss: 0.378268
     Train Epoch: [2260/2467]	Loss: 0.238168
     Train Epoch: [2260/2467]	Loss: 0.074873
     Train Epoch: [2280/2467]	Loss: 0.099857
     Train Epoch: [2280/2467]	Loss: 0.114607
     Train Epoch: [2280/2467]	Loss: 0.202789
     Train Epoch: [2280/2467]	Loss: 0.188230
     Train Epoch: [2300/2467]	Loss: 0.092668
         Train Epoch: [2300/2467]	Loss: 0.448656
 Train Epoch: [2300/2467]	Loss: 0.191932
     Train Epoch: [2300/2467]	Loss: 0.178820
     Train Epoch: [2320/2467]	Loss: 0.373707
         Train Epoch: [2320/2467]	Loss: 0.060663
 Train Epoch: [2320/2467]	Loss: 0.187522
     Train Epoch: [2320/2467]	Loss: 0.270727
             Train Epoch: [2340/2467]	Loss: 0.148462 
Train Epoch: [2340/2467]	Loss: 0.149224
 Train Epoch: [2340/2467]	Loss: 0.204136
     Train Epoch: [2340/2467]	Loss: 0.599668
             Train Epoch: [2360/2467]	Loss: 0.080682
  Train Epoch: [2360/2467]	Loss: 0.187611Train Epoch: [2360/2467]	Loss: 0.399163

     Train Epoch: [2360/2467]	Loss: 0.166423
         Train Epoch: [2380/2467]	Loss: 0.541395
 Train Epoch: [2380/2467]	Loss: 0.627124
     Train Epoch: [2380/2467]	Loss: 0.261433
     Train Epoch: [2380/2467]	Loss: 0.270348
         Train Epoch: [2400/2467]	Loss: 0.309245
     Train Epoch: [2400/2467]	Loss: 0.152842
 Train Epoch: [2400/2467]	Loss: 0.283630
     Train Epoch: [2400/2467]	Loss: 0.155677
         Train Epoch: [2420/2467]	Loss: 0.239523
           Train Epoch: [2420/2467]	Loss: 0.365826Train Epoch: [2420/2467]	Loss: 0.309662Train Epoch: [2420/2467]	Loss: 0.390029


     Train Epoch: [2440/2467]	Loss: 0.222180
             Train Epoch: [2440/2467]	Loss: 0.385658
 Train Epoch: [2440/2467]	Loss: 0.172638
 Train Epoch: [2440/2467]	Loss: 0.176593
             Train Epoch: [2460/2467]	Loss: 0.123153
 Train Epoch: [2460/2467]	Loss: 0.408671
 Train Epoch: [2460/2467]	Loss: 0.145735
     Train Epoch: [2460/2467]	Loss: 0.157668
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 12 epoch =====
     2025-05-11.04-25-07
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 12 epoch =====
     2025-05-11.04-25-07
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 12 epoch =====
     2025-05-11.04-25-07
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 12 epoch =====
     2025-05-11.04-25-07
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
         Train Epoch: [0/2467]	Loss: 0.310977
     Train Epoch: [0/2467]	Loss: 0.225134
 Train Epoch: [0/2467]	Loss: 0.272284    
 Train Epoch: [0/2467]	Loss: 0.348801
         Train Epoch: [20/2467]	Loss: 0.133297 
Train Epoch: [20/2467]	Loss: 0.158988
     Train Epoch: [20/2467]	Loss: 0.383624
     Train Epoch: [20/2467]	Loss: 0.301290
              Train Epoch: [40/2467]	Loss: 0.267073Train Epoch: [40/2467]	Loss: 0.217849

 Train Epoch: [40/2467]	Loss: 0.491296
     Train Epoch: [40/2467]	Loss: 0.207689
     Train Epoch: [60/2467]	Loss: 0.088290
     Train Epoch: [60/2467]	Loss: 0.233892
     Train Epoch: [60/2467]	Loss: 0.312992
     Train Epoch: [60/2467]	Loss: 0.339030
         Train Epoch: [80/2467]	Loss: 0.149047
 Train Epoch: [80/2467]	Loss: 0.142544
     Train Epoch: [80/2467]	Loss: 0.336196
     Train Epoch: [80/2467]	Loss: 0.182646
         Train Epoch: [100/2467]	Loss: 0.240474
     Train Epoch: [100/2467]	Loss: 0.196786
 Train Epoch: [100/2467]	Loss: 0.248183
     Train Epoch: [100/2467]	Loss: 0.323535
     Train Epoch: [120/2467]	Loss: 0.099333
     Train Epoch: [120/2467]	Loss: 0.082789
          Train Epoch: [120/2467]	Loss: 0.408785Train Epoch: [120/2467]	Loss: 0.437259

         Train Epoch: [140/2467]	Loss: 0.329978
 Train Epoch: [140/2467]	Loss: 0.091065
         Train Epoch: [140/2467]	Loss: 0.090820
 Train Epoch: [140/2467]	Loss: 0.180026
          Train Epoch: [160/2467]	Loss: 0.191175
Train Epoch: [160/2467]	Loss: 0.281456
         Train Epoch: [160/2467]	Loss: 0.396904
 Train Epoch: [160/2467]	Loss: 0.180614
     Train Epoch: [180/2467]	Loss: 0.180753
         Train Epoch: [180/2467]	Loss: 0.167981 
Train Epoch: [180/2467]	Loss: 0.229104
     Train Epoch: [180/2467]	Loss: 0.154961
     Train Epoch: [200/2467]	Loss: 0.145864
     Train Epoch: [200/2467]	Loss: 0.221752
     Train Epoch: [200/2467]	Loss: 0.171566
     Train Epoch: [200/2467]	Loss: 0.238838
     Train Epoch: [220/2467]	Loss: 0.118763
         Train Epoch: [220/2467]	Loss: 0.253155
 Train Epoch: [220/2467]	Loss: 0.247101
     Train Epoch: [220/2467]	Loss: 0.251126
         Train Epoch: [240/2467]	Loss: 0.259573 
Train Epoch: [240/2467]	Loss: 0.029632
         Train Epoch: [240/2467]	Loss: 0.407902
 Train Epoch: [240/2467]	Loss: 0.056503
              Train Epoch: [260/2467]	Loss: 0.078526
Train Epoch: [260/2467]	Loss: 0.072400
 Train Epoch: [260/2467]	Loss: 0.094509
     Train Epoch: [260/2467]	Loss: 0.178047
     Train Epoch: [280/2467]	Loss: 0.211978
         Train Epoch: [280/2467]	Loss: 0.107678
 Train Epoch: [280/2467]	Loss: 0.307176
     Train Epoch: [280/2467]	Loss: 0.265835
         Train Epoch: [300/2467]	Loss: 0.460494
 Train Epoch: [300/2467]	Loss: 0.186305
         Train Epoch: [300/2467]	Loss: 0.066709 
Train Epoch: [300/2467]	Loss: 0.114400
          Train Epoch: [320/2467]	Loss: 0.177225Train Epoch: [320/2467]	Loss: 0.211505

         Train Epoch: [320/2467]	Loss: 0.477094
 Train Epoch: [320/2467]	Loss: 0.514889
     Train Epoch: [340/2467]	Loss: 0.044966    
     Train Epoch: [340/2467]	Loss: 0.079851
 Train Epoch: [340/2467]	Loss: 0.183708
     Train Epoch: [340/2467]	Loss: 0.274220
     Train Epoch: [360/2467]	Loss: 0.097072    
      Train Epoch: [360/2467]	Loss: 0.255819Train Epoch: [360/2467]	Loss: 0.176617

     Train Epoch: [360/2467]	Loss: 0.214118
     Train Epoch: [380/2467]	Loss: 0.601582
     Train Epoch: [380/2467]	Loss: 0.180641
     Train Epoch: [380/2467]	Loss: 0.934565
     Train Epoch: [380/2467]	Loss: 0.273263
          Train Epoch: [400/2467]	Loss: 0.080800    Train Epoch: [400/2467]	Loss: 0.194610

 Train Epoch: [400/2467]	Loss: 0.210352
     Train Epoch: [400/2467]	Loss: 0.331439
     Train Epoch: [420/2467]	Loss: 0.309462
         Train Epoch: [420/2467]	Loss: 0.163760
 Train Epoch: [420/2467]	Loss: 0.148047
     Train Epoch: [420/2467]	Loss: 0.311825
         Train Epoch: [440/2467]	Loss: 0.198552 
Train Epoch: [440/2467]	Loss: 0.351046    
     Train Epoch: [440/2467]	Loss: 0.135102
 Train Epoch: [440/2467]	Loss: 0.080083
         Train Epoch: [460/2467]	Loss: 0.314679 
Train Epoch: [460/2467]	Loss: 0.214839
         Train Epoch: [460/2467]	Loss: 0.233541
 Train Epoch: [460/2467]	Loss: 0.174323
          Train Epoch: [480/2467]	Loss: 0.368295Train Epoch: [480/2467]	Loss: 0.168777

     Train Epoch: [480/2467]	Loss: 0.367370
     Train Epoch: [480/2467]	Loss: 0.209872
         Train Epoch: [500/2467]	Loss: 0.174862
 Train Epoch: [500/2467]	Loss: 0.302033
         Train Epoch: [500/2467]	Loss: 0.144339
 Train Epoch: [500/2467]	Loss: 0.322705
         Train Epoch: [520/2467]	Loss: 0.210280
 Train Epoch: [520/2467]	Loss: 0.291475
          Train Epoch: [520/2467]	Loss: 0.413829
Train Epoch: [520/2467]	Loss: 0.088234
         Train Epoch: [540/2467]	Loss: 0.090903 
Train Epoch: [540/2467]	Loss: 0.202872    
 Train Epoch: [540/2467]	Loss: 0.311404
     Train Epoch: [540/2467]	Loss: 0.130759
         Train Epoch: [560/2467]	Loss: 0.286735
     Train Epoch: [560/2467]	Loss: 0.183995
 Train Epoch: [560/2467]	Loss: 0.465728
     Train Epoch: [560/2467]	Loss: 0.268049
         Train Epoch: [580/2467]	Loss: 0.410187
 Train Epoch: [580/2467]	Loss: 0.204314
         Train Epoch: [580/2467]	Loss: 0.092403
 Train Epoch: [580/2467]	Loss: 0.178220
     Train Epoch: [600/2467]	Loss: 0.272994
          Train Epoch: [600/2467]	Loss: 0.255127Train Epoch: [600/2467]	Loss: 0.136547

     Train Epoch: [600/2467]	Loss: 0.239082
         Train Epoch: [620/2467]	Loss: 0.171040    
 Train Epoch: [620/2467]	Loss: 0.286769
 Train Epoch: [620/2467]	Loss: 0.053112
     Train Epoch: [620/2467]	Loss: 0.382181
         Train Epoch: [640/2467]	Loss: 0.229661
     Train Epoch: [640/2467]	Loss: 0.107148
     Train Epoch: [640/2467]	Loss: 0.221234
 Train Epoch: [640/2467]	Loss: 0.332916
         Train Epoch: [660/2467]	Loss: 0.460716
     Train Epoch: [660/2467]	Loss: 0.045764
     Train Epoch: [660/2467]	Loss: 0.100753
 Train Epoch: [660/2467]	Loss: 0.322268
             Train Epoch: [680/2467]	Loss: 0.141021
  Train Epoch: [680/2467]	Loss: 0.079600Train Epoch: [680/2467]	Loss: 0.384096

     Train Epoch: [680/2467]	Loss: 0.167899
     Train Epoch: [700/2467]	Loss: 0.082069
              Train Epoch: [700/2467]	Loss: 0.170521Train Epoch: [700/2467]	Loss: 0.358398

 Train Epoch: [700/2467]	Loss: 0.484494
     Train Epoch: [720/2467]	Loss: 0.231928
         Train Epoch: [720/2467]	Loss: 0.230692
 Train Epoch: [720/2467]	Loss: 0.224480
     Train Epoch: [720/2467]	Loss: 0.190394
     Train Epoch: [740/2467]	Loss: 0.317853
         Train Epoch: [740/2467]	Loss: 0.075494
 Train Epoch: [740/2467]	Loss: 0.133605
     Train Epoch: [740/2467]	Loss: 0.081370
              Train Epoch: [760/2467]	Loss: 0.549427
    Train Epoch: [760/2467]	Loss: 0.166150 
Train Epoch: [760/2467]	Loss: 0.287532
 Train Epoch: [760/2467]	Loss: 0.350682
     Train Epoch: [780/2467]	Loss: 0.391877
          Train Epoch: [780/2467]	Loss: 0.390257Train Epoch: [780/2467]	Loss: 0.403344

     Train Epoch: [780/2467]	Loss: 0.207601
         Train Epoch: [800/2467]	Loss: 0.423947    
 Train Epoch: [800/2467]	Loss: 0.229226 
Train Epoch: [800/2467]	Loss: 0.396934
     Train Epoch: [800/2467]	Loss: 0.123777
         Train Epoch: [820/2467]	Loss: 0.076839
     Train Epoch: [820/2467]	Loss: 0.309597
 Train Epoch: [820/2467]	Loss: 0.480287
     Train Epoch: [820/2467]	Loss: 0.075107
          Train Epoch: [840/2467]	Loss: 0.365345
Train Epoch: [840/2467]	Loss: 0.213231
     Train Epoch: [840/2467]	Loss: 0.336042
     Train Epoch: [840/2467]	Loss: 0.116161
         Train Epoch: [860/2467]	Loss: 0.338209
 Train Epoch: [860/2467]	Loss: 0.104422    
 Train Epoch: [860/2467]	Loss: 0.192954
     Train Epoch: [860/2467]	Loss: 0.091783
         Train Epoch: [880/2467]	Loss: 0.321542
 Train Epoch: [880/2467]	Loss: 0.201106
     Train Epoch: [880/2467]	Loss: 0.179530
     Train Epoch: [880/2467]	Loss: 0.230611
         Train Epoch: [900/2467]	Loss: 0.244353
     Train Epoch: [900/2467]	Loss: 0.197355
     Train Epoch: [900/2467]	Loss: 0.140015
 Train Epoch: [900/2467]	Loss: 0.215601
             Train Epoch: [920/2467]	Loss: 0.525569
 Train Epoch: [920/2467]	Loss: 0.502229 
Train Epoch: [920/2467]	Loss: 0.353754
     Train Epoch: [920/2467]	Loss: 0.060976
     Train Epoch: [940/2467]	Loss: 0.043307
         Train Epoch: [940/2467]	Loss: 0.238371
 Train Epoch: [940/2467]	Loss: 0.182094
     Train Epoch: [940/2467]	Loss: 0.068102
          Train Epoch: [960/2467]	Loss: 0.399293    
Train Epoch: [960/2467]	Loss: 0.070982
 Train Epoch: [960/2467]	Loss: 0.111223
     Train Epoch: [960/2467]	Loss: 0.127572
         Train Epoch: [980/2467]	Loss: 0.244210
     Train Epoch: [980/2467]	Loss: 0.318762
     Train Epoch: [980/2467]	Loss: 0.184565
 Train Epoch: [980/2467]	Loss: 0.221075
             Train Epoch: [1000/2467]	Loss: 0.093536    
 Train Epoch: [1000/2467]	Loss: 0.285622 
Train Epoch: [1000/2467]	Loss: 0.401618
 Train Epoch: [1000/2467]	Loss: 0.049597
     Train Epoch: [1020/2467]	Loss: 0.206187
         Train Epoch: [1020/2467]	Loss: 0.569165 
Train Epoch: [1020/2467]	Loss: 0.074046
     Train Epoch: [1020/2467]	Loss: 0.421563
     Train Epoch: [1040/2467]	Loss: 0.478536
         Train Epoch: [1040/2467]	Loss: 0.113952 
Train Epoch: [1040/2467]	Loss: 0.224600
     Train Epoch: [1040/2467]	Loss: 0.290217
         Train Epoch: [1060/2467]	Loss: 0.290499
 Train Epoch: [1060/2467]	Loss: 0.521384
     Train Epoch: [1060/2467]	Loss: 0.155637
     Train Epoch: [1060/2467]	Loss: 0.440237
          Train Epoch: [1080/2467]	Loss: 0.201709Train Epoch: [1080/2467]	Loss: 0.142438

         Train Epoch: [1080/2467]	Loss: 0.322269
 Train Epoch: [1080/2467]	Loss: 0.414292
     Train Epoch: [1100/2467]	Loss: 0.359461
         Train Epoch: [1100/2467]	Loss: 0.409038
 Train Epoch: [1100/2467]	Loss: 0.314723
     Train Epoch: [1100/2467]	Loss: 0.354118
     Train Epoch: [1120/2467]	Loss: 0.142120    
 Train Epoch: [1120/2467]	Loss: 0.149993
     Train Epoch: [1120/2467]	Loss: 0.187919
     Train Epoch: [1120/2467]	Loss: 0.323144
     Train Epoch: [1140/2467]	Loss: 0.069385
     Train Epoch: [1140/2467]	Loss: 0.298811
     Train Epoch: [1140/2467]	Loss: 0.518919
     Train Epoch: [1140/2467]	Loss: 0.144849
             Train Epoch: [1160/2467]	Loss: 0.258660
  Train Epoch: [1160/2467]	Loss: 0.245256Train Epoch: [1160/2467]	Loss: 0.093051

     Train Epoch: [1160/2467]	Loss: 0.240254
     Train Epoch: [1180/2467]	Loss: 0.118223
     Train Epoch: [1180/2467]	Loss: 0.284115
     Train Epoch: [1180/2467]	Loss: 0.185421
     Train Epoch: [1180/2467]	Loss: 0.343561
         Train Epoch: [1200/2467]	Loss: 0.335147 
Train Epoch: [1200/2467]	Loss: 0.141180
     Train Epoch: [1200/2467]	Loss: 0.211736    
 Train Epoch: [1200/2467]	Loss: 0.306696
          Train Epoch: [1220/2467]	Loss: 0.695063Train Epoch: [1220/2467]	Loss: 0.262684

         Train Epoch: [1220/2467]	Loss: 0.118792
 Train Epoch: [1220/2467]	Loss: 0.397885
     Train Epoch: [1240/2467]	Loss: 0.345913
         Train Epoch: [1240/2467]	Loss: 0.238742
 Train Epoch: [1240/2467]	Loss: 0.283866
     Train Epoch: [1240/2467]	Loss: 0.432125
     Train Epoch: [1260/2467]	Loss: 0.155032    
 Train Epoch: [1260/2467]	Loss: 0.062089
     Train Epoch: [1260/2467]	Loss: 0.101348
     Train Epoch: [1260/2467]	Loss: 0.488215
         Train Epoch: [1280/2467]	Loss: 0.103361
 Train Epoch: [1280/2467]	Loss: 0.927179
         Train Epoch: [1280/2467]	Loss: 0.358446
 Train Epoch: [1280/2467]	Loss: 0.438268
         Train Epoch: [1300/2467]	Loss: 0.119415
     Train Epoch: [1300/2467]	Loss: 0.096491
 Train Epoch: [1300/2467]	Loss: 0.401634
     Train Epoch: [1300/2467]	Loss: 0.383771
         Train Epoch: [1320/2467]	Loss: 0.096734
         Train Epoch: [1320/2467]	Loss: 0.209695 
Train Epoch: [1320/2467]	Loss: 0.267894 
Train Epoch: [1320/2467]	Loss: 0.408901
         Train Epoch: [1340/2467]	Loss: 0.361126    
 Train Epoch: [1340/2467]	Loss: 0.253979
 Train Epoch: [1340/2467]	Loss: 0.152297
     Train Epoch: [1340/2467]	Loss: 0.247509
             Train Epoch: [1360/2467]	Loss: 0.423260
 Train Epoch: [1360/2467]	Loss: 0.117000 
Train Epoch: [1360/2467]	Loss: 0.285153
     Train Epoch: [1360/2467]	Loss: 0.095810
     Train Epoch: [1380/2467]	Loss: 0.817838
         Train Epoch: [1380/2467]	Loss: 0.254331 
Train Epoch: [1380/2467]	Loss: 0.420154
     Train Epoch: [1380/2467]	Loss: 0.269677
     Train Epoch: [1400/2467]	Loss: 0.143544
         Train Epoch: [1400/2467]	Loss: 0.178160
 Train Epoch: [1400/2467]	Loss: 0.276798
     Train Epoch: [1400/2467]	Loss: 0.236782
          Train Epoch: [1420/2467]	Loss: 0.465187Train Epoch: [1420/2467]	Loss: 0.187962

     Train Epoch: [1420/2467]	Loss: 0.113663
     Train Epoch: [1420/2467]	Loss: 0.189571
     Train Epoch: [1440/2467]	Loss: 0.223466    
      Train Epoch: [1440/2467]	Loss: 0.065580
Train Epoch: [1440/2467]	Loss: 0.060866
     Train Epoch: [1440/2467]	Loss: 0.106352
     Train Epoch: [1460/2467]	Loss: 0.231618
     Train Epoch: [1460/2467]	Loss: 0.059459
     Train Epoch: [1460/2467]	Loss: 0.328101
     Train Epoch: [1460/2467]	Loss: 0.126461
     Train Epoch: [1480/2467]	Loss: 0.230722
          Train Epoch: [1480/2467]	Loss: 0.030839Train Epoch: [1480/2467]	Loss: 0.311315

     Train Epoch: [1480/2467]	Loss: 0.036331
     Train Epoch: [1500/2467]	Loss: 0.341489
         Train Epoch: [1500/2467]	Loss: 0.153256
 Train Epoch: [1500/2467]	Loss: 0.244453
     Train Epoch: [1500/2467]	Loss: 0.021768
     Train Epoch: [1520/2467]	Loss: 0.170501
         Train Epoch: [1520/2467]	Loss: 0.239458
 Train Epoch: [1520/2467]	Loss: 0.324744
     Train Epoch: [1520/2467]	Loss: 0.351494
     Train Epoch: [1540/2467]	Loss: 0.074097
     Train Epoch: [1540/2467]	Loss: 0.395214
     Train Epoch: [1540/2467]	Loss: 0.323270
     Train Epoch: [1540/2467]	Loss: 0.182779
         Train Epoch: [1560/2467]	Loss: 0.372119
     Train Epoch: [1560/2467]	Loss: 0.376393
 Train Epoch: [1560/2467]	Loss: 0.440008
     Train Epoch: [1560/2467]	Loss: 0.625726
         Train Epoch: [1580/2467]	Loss: 0.346629
 Train Epoch: [1580/2467]	Loss: 0.272131
     Train Epoch: [1580/2467]	Loss: 0.300599
     Train Epoch: [1580/2467]	Loss: 0.279851
     Train Epoch: [1600/2467]	Loss: 0.322881
          Train Epoch: [1600/2467]	Loss: 0.174614
Train Epoch: [1600/2467]	Loss: 0.291201
     Train Epoch: [1600/2467]	Loss: 0.106182
     Train Epoch: [1620/2467]	Loss: 0.117494
     Train Epoch: [1620/2467]	Loss: 0.268597
         Train Epoch: [1620/2467]	Loss: 0.059842 
Train Epoch: [1620/2467]	Loss: 0.232307
     Train Epoch: [1640/2467]	Loss: 0.253321
     Train Epoch: [1640/2467]	Loss: 0.280264
         Train Epoch: [1640/2467]	Loss: 0.224589
 Train Epoch: [1640/2467]	Loss: 0.137967
          Train Epoch: [1660/2467]	Loss: 0.242920Train Epoch: [1660/2467]	Loss: 0.214872
    
 Train Epoch: [1660/2467]	Loss: 0.285048
     Train Epoch: [1660/2467]	Loss: 0.195849
         Train Epoch: [1680/2467]	Loss: 0.342484
 Train Epoch: [1680/2467]	Loss: 0.653543
          Train Epoch: [1680/2467]	Loss: 0.085037
Train Epoch: [1680/2467]	Loss: 0.255713
     Train Epoch: [1700/2467]	Loss: 0.226311
              Train Epoch: [1700/2467]	Loss: 0.322841Train Epoch: [1700/2467]	Loss: 0.132804
 
Train Epoch: [1700/2467]	Loss: 0.255917
         Train Epoch: [1720/2467]	Loss: 0.283434
 Train Epoch: [1720/2467]	Loss: 0.178499    
     Train Epoch: [1720/2467]	Loss: 0.251136
 Train Epoch: [1720/2467]	Loss: 0.310866
     Train Epoch: [1740/2467]	Loss: 0.046400
     Train Epoch: [1740/2467]	Loss: 0.408555
     Train Epoch: [1740/2467]	Loss: 0.220435
     Train Epoch: [1740/2467]	Loss: 0.143053
     Train Epoch: [1760/2467]	Loss: 0.172652
          Train Epoch: [1760/2467]	Loss: 0.073326Train Epoch: [1760/2467]	Loss: 0.136247

     Train Epoch: [1760/2467]	Loss: 0.188997
     Train Epoch: [1780/2467]	Loss: 0.333789
     Train Epoch: [1780/2467]	Loss: 0.390882    
     Train Epoch: [1780/2467]	Loss: 0.413329
 Train Epoch: [1780/2467]	Loss: 0.139792
     Train Epoch: [1800/2467]	Loss: 0.365948
               Train Epoch: [1800/2467]	Loss: 0.283884Train Epoch: [1800/2467]	Loss: 0.316420Train Epoch: [1800/2467]	Loss: 0.292394


             Train Epoch: [1820/2467]	Loss: 0.156504 
Train Epoch: [1820/2467]	Loss: 0.132673 
Train Epoch: [1820/2467]	Loss: 0.397660
     Train Epoch: [1820/2467]	Loss: 0.184906
          Train Epoch: [1840/2467]	Loss: 0.413015Train Epoch: [1840/2467]	Loss: 0.273336

     Train Epoch: [1840/2467]	Loss: 0.329933    
 Train Epoch: [1840/2467]	Loss: 0.088009
     Train Epoch: [1860/2467]	Loss: 0.213847
             Train Epoch: [1860/2467]	Loss: 0.196484 
Train Epoch: [1860/2467]	Loss: 0.232740
 Train Epoch: [1860/2467]	Loss: 0.130048
     Train Epoch: [1880/2467]	Loss: 0.081493
             Train Epoch: [1880/2467]	Loss: 0.186588
 Train Epoch: [1880/2467]	Loss: 0.169852 
Train Epoch: [1880/2467]	Loss: 0.061576
         Train Epoch: [1900/2467]	Loss: 0.155376 
Train Epoch: [1900/2467]	Loss: 0.399946    
     Train Epoch: [1900/2467]	Loss: 0.442435
 Train Epoch: [1900/2467]	Loss: 0.315957
     Train Epoch: [1920/2467]	Loss: 0.194914
         Train Epoch: [1920/2467]	Loss: 0.273809
 Train Epoch: [1920/2467]	Loss: 0.020273    
 Train Epoch: [1920/2467]	Loss: 0.285188
     Train Epoch: [1940/2467]	Loss: 0.212618
     Train Epoch: [1940/2467]	Loss: 0.479547
     Train Epoch: [1940/2467]	Loss: 0.115570
     Train Epoch: [1940/2467]	Loss: 0.143686
              Train Epoch: [1960/2467]	Loss: 0.268473    Train Epoch: [1960/2467]	Loss: 0.292885

 Train Epoch: [1960/2467]	Loss: 0.260341 
Train Epoch: [1960/2467]	Loss: 0.144156
     Train Epoch: [1980/2467]	Loss: 0.354269
         Train Epoch: [1980/2467]	Loss: 0.449913
 Train Epoch: [1980/2467]	Loss: 0.208957
     Train Epoch: [1980/2467]	Loss: 0.369428
          Train Epoch: [2000/2467]	Loss: 0.049986
Train Epoch: [2000/2467]	Loss: 0.083589
     Train Epoch: [2000/2467]	Loss: 0.328557
     Train Epoch: [2000/2467]	Loss: 0.207636
     Train Epoch: [2020/2467]	Loss: 0.294667    
      Train Epoch: [2020/2467]	Loss: 0.294085Train Epoch: [2020/2467]	Loss: 0.135680

     Train Epoch: [2020/2467]	Loss: 0.416574
     Train Epoch: [2040/2467]	Loss: 0.218527
         Train Epoch: [2040/2467]	Loss: 0.127129
 Train Epoch: [2040/2467]	Loss: 0.314816
     Train Epoch: [2040/2467]	Loss: 0.411459
             Train Epoch: [2060/2467]	Loss: 0.237942
 Train Epoch: [2060/2467]	Loss: 0.289336
 Train Epoch: [2060/2467]	Loss: 0.233947    
 Train Epoch: [2060/2467]	Loss: 0.375088
     Train Epoch: [2080/2467]	Loss: 0.085146
     Train Epoch: [2080/2467]	Loss: 0.069850    
     Train Epoch: [2080/2467]	Loss: 0.155939
 Train Epoch: [2080/2467]	Loss: 0.298097
         Train Epoch: [2100/2467]	Loss: 0.137432
     Train Epoch: [2100/2467]	Loss: 0.642059
     Train Epoch: [2100/2467]	Loss: 0.176151 Train Epoch: [2100/2467]	Loss: 0.199959

         Train Epoch: [2120/2467]	Loss: 0.297065
         Train Epoch: [2120/2467]	Loss: 0.118893 
Train Epoch: [2120/2467]	Loss: 0.076044 
Train Epoch: [2120/2467]	Loss: 0.299137
     Train Epoch: [2140/2467]	Loss: 0.223886
     Train Epoch: [2140/2467]	Loss: 0.645763
     Train Epoch: [2140/2467]	Loss: 0.532000
     Train Epoch: [2140/2467]	Loss: 0.278001
         Train Epoch: [2160/2467]	Loss: 0.163391
 Train Epoch: [2160/2467]	Loss: 0.442708
     Train Epoch: [2160/2467]	Loss: 0.208785
     Train Epoch: [2160/2467]	Loss: 0.071838
     Train Epoch: [2180/2467]	Loss: 0.129682
              Train Epoch: [2180/2467]	Loss: 0.273857Train Epoch: [2180/2467]	Loss: 0.312704

 Train Epoch: [2180/2467]	Loss: 0.223405
     Train Epoch: [2200/2467]	Loss: 0.175678
         Train Epoch: [2200/2467]	Loss: 0.162107
 Train Epoch: [2200/2467]	Loss: 0.423258
     Train Epoch: [2200/2467]	Loss: 0.156639
         Train Epoch: [2220/2467]	Loss: 0.142676 
Train Epoch: [2220/2467]	Loss: 0.083322
         Train Epoch: [2220/2467]	Loss: 0.185777
 Train Epoch: [2220/2467]	Loss: 0.117308
         Train Epoch: [2240/2467]	Loss: 0.036566
 Train Epoch: [2240/2467]	Loss: 0.106258
     Train Epoch: [2240/2467]	Loss: 0.074465
     Train Epoch: [2240/2467]	Loss: 0.219275
     Train Epoch: [2260/2467]	Loss: 0.468011
     Train Epoch: [2260/2467]	Loss: 0.348845
     Train Epoch: [2260/2467]	Loss: 0.054987
     Train Epoch: [2260/2467]	Loss: 0.232386
     Train Epoch: [2280/2467]	Loss: 0.097245    
      Train Epoch: [2280/2467]	Loss: 0.108290Train Epoch: [2280/2467]	Loss: 0.138180

     Train Epoch: [2280/2467]	Loss: 0.184958
     Train Epoch: [2300/2467]	Loss: 0.091383
         Train Epoch: [2300/2467]	Loss: 0.113259
 Train Epoch: [2300/2467]	Loss: 0.382548
     Train Epoch: [2300/2467]	Loss: 0.153715
     Train Epoch: [2320/2467]	Loss: 0.087428
         Train Epoch: [2320/2467]	Loss: 0.259662
 Train Epoch: [2320/2467]	Loss: 0.184793
     Train Epoch: [2320/2467]	Loss: 0.366767
     Train Epoch: [2340/2467]	Loss: 0.148390
         Train Epoch: [2340/2467]	Loss: 0.575184 
Train Epoch: [2340/2467]	Loss: 0.235070
     Train Epoch: [2340/2467]	Loss: 0.161256
     Train Epoch: [2360/2467]	Loss: 0.081343
     Train Epoch: [2360/2467]	Loss: 0.177897
     Train Epoch: [2360/2467]	Loss: 0.168210
     Train Epoch: [2360/2467]	Loss: 0.371045
         Train Epoch: [2380/2467]	Loss: 0.285397
 Train Epoch: [2380/2467]	Loss: 0.518702
     Train Epoch: [2380/2467]	Loss: 0.238219
     Train Epoch: [2380/2467]	Loss: 0.361564
     Train Epoch: [2400/2467]	Loss: 0.349494    
     Train Epoch: [2400/2467]	Loss: 0.306671 
Train Epoch: [2400/2467]	Loss: 0.133028
     Train Epoch: [2400/2467]	Loss: 0.215238
          Train Epoch: [2420/2467]	Loss: 0.155657Train Epoch: [2420/2467]	Loss: 0.350198

         Train Epoch: [2420/2467]	Loss: 0.351555
 Train Epoch: [2420/2467]	Loss: 0.260249
         Train Epoch: [2440/2467]	Loss: 0.164864
 Train Epoch: [2440/2467]	Loss: 0.093657
          Train Epoch: [2440/2467]	Loss: 0.382725Train Epoch: [2440/2467]	Loss: 0.176529

     Train Epoch: [2460/2467]	Loss: 0.164002
         Train Epoch: [2460/2467]	Loss: 0.125101 
Train Epoch: [2460/2467]	Loss: 0.383703
     Train Epoch: [2460/2467]	Loss: 0.154082
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 13 epoch =====
     2025-05-11.04-46-54
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 13 epoch =====
     2025-05-11.04-46-54
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 13 epoch =====
     2025-05-11.04-46-55
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 13 epoch =====
     2025-05-11.04-46-56
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
          Train Epoch: [0/2467]	Loss: 0.233519
Train Epoch: [0/2467]	Loss: 0.359514
     Train Epoch: [0/2467]	Loss: 0.370610
     Train Epoch: [0/2467]	Loss: 0.303637
     Train Epoch: [20/2467]	Loss: 0.123532
     Train Epoch: [20/2467]	Loss: 0.376459
         Train Epoch: [20/2467]	Loss: 0.165867
 Train Epoch: [20/2467]	Loss: 0.242286
         Train Epoch: [40/2467]	Loss: 0.185315
 Train Epoch: [40/2467]	Loss: 0.207807
     Train Epoch: [40/2467]	Loss: 0.460575
     Train Epoch: [40/2467]	Loss: 0.266251
     Train Epoch: [60/2467]	Loss: 0.198162
         Train Epoch: [60/2467]	Loss: 0.062359
     Train Epoch: [60/2467]	Loss: 0.197542
 Train Epoch: [60/2467]	Loss: 0.285780
     Train Epoch: [80/2467]	Loss: 0.214927
     Train Epoch: [80/2467]	Loss: 0.140164
         Train Epoch: [80/2467]	Loss: 0.424390
 Train Epoch: [80/2467]	Loss: 0.146587
     Train Epoch: [100/2467]	Loss: 0.315062    
 Train Epoch: [100/2467]	Loss: 0.358528
         Train Epoch: [100/2467]	Loss: 0.284000
 Train Epoch: [100/2467]	Loss: 0.174576
          Train Epoch: [120/2467]	Loss: 0.317326Train Epoch: [120/2467]	Loss: 0.359679

     Train Epoch: [120/2467]	Loss: 0.085402
     Train Epoch: [120/2467]	Loss: 0.123226
     Train Epoch: [140/2467]	Loss: 0.289834
     Train Epoch: [140/2467]	Loss: 0.210496
     Train Epoch: [140/2467]	Loss: 0.047285
     Train Epoch: [140/2467]	Loss: 0.090650
             Train Epoch: [160/2467]	Loss: 0.152900
 Train Epoch: [160/2467]	Loss: 0.269104 
Train Epoch: [160/2467]	Loss: 0.277530
     Train Epoch: [160/2467]	Loss: 0.155424
              Train Epoch: [180/2467]	Loss: 0.200402Train Epoch: [180/2467]	Loss: 0.165743
 
Train Epoch: [180/2467]	Loss: 0.244600
     Train Epoch: [180/2467]	Loss: 0.251762
     Train Epoch: [200/2467]	Loss: 0.139509
          Train Epoch: [200/2467]	Loss: 0.151486Train Epoch: [200/2467]	Loss: 0.191894

     Train Epoch: [200/2467]	Loss: 0.134891
         Train Epoch: [220/2467]	Loss: 0.102542
     Train Epoch: [220/2467]	Loss: 0.238521
 Train Epoch: [220/2467]	Loss: 0.230977    
 Train Epoch: [220/2467]	Loss: 0.217282
     Train Epoch: [240/2467]	Loss: 0.205003
     Train Epoch: [240/2467]	Loss: 0.381734
     Train Epoch: [240/2467]	Loss: 0.044888
     Train Epoch: [240/2467]	Loss: 0.027712
          Train Epoch: [260/2467]	Loss: 0.068160Train Epoch: [260/2467]	Loss: 0.071516

     Train Epoch: [260/2467]	Loss: 0.090999
     Train Epoch: [260/2467]	Loss: 0.136325
     Train Epoch: [280/2467]	Loss: 0.308499    
      Train Epoch: [280/2467]	Loss: 0.106762Train Epoch: [280/2467]	Loss: 0.273437

     Train Epoch: [280/2467]	Loss: 0.201917
         Train Epoch: [300/2467]	Loss: 0.440143 
Train Epoch: [300/2467]	Loss: 0.177365
     Train Epoch: [300/2467]	Loss: 0.122784
     Train Epoch: [300/2467]	Loss: 0.059495
          Train Epoch: [320/2467]	Loss: 0.177441Train Epoch: [320/2467]	Loss: 0.209275

         Train Epoch: [320/2467]	Loss: 0.415634
 Train Epoch: [320/2467]	Loss: 0.519229
               Train Epoch: [340/2467]	Loss: 0.028358Train Epoch: [340/2467]	Loss: 0.097397
Train Epoch: [340/2467]	Loss: 0.150400

     Train Epoch: [340/2467]	Loss: 0.228838
         Train Epoch: [360/2467]	Loss: 0.085541 
Train Epoch: [360/2467]	Loss: 0.260323    
 Train Epoch: [360/2467]	Loss: 0.186164    
 Train Epoch: [360/2467]	Loss: 0.204711
         Train Epoch: [380/2467]	Loss: 0.151456 
Train Epoch: [380/2467]	Loss: 0.222665
         Train Epoch: [380/2467]	Loss: 0.469678
 Train Epoch: [380/2467]	Loss: 0.762953
         Train Epoch: [400/2467]	Loss: 0.160138
 Train Epoch: [400/2467]	Loss: 0.082925
     Train Epoch: [400/2467]	Loss: 0.400932
     Train Epoch: [400/2467]	Loss: 0.215001
         Train Epoch: [420/2467]	Loss: 0.280553
 Train Epoch: [420/2467]	Loss: 0.305829
          Train Epoch: [420/2467]	Loss: 0.183065
Train Epoch: [420/2467]	Loss: 0.113630
          Train Epoch: [440/2467]	Loss: 0.165463
Train Epoch: [440/2467]	Loss: 0.366042    
     Train Epoch: [440/2467]	Loss: 0.117977
 Train Epoch: [440/2467]	Loss: 0.097909
     Train Epoch: [460/2467]	Loss: 0.314378
              Train Epoch: [460/2467]	Loss: 0.198455
Train Epoch: [460/2467]	Loss: 0.236538
 Train Epoch: [460/2467]	Loss: 0.154043
         Train Epoch: [480/2467]	Loss: 0.195881
 Train Epoch: [480/2467]	Loss: 0.102969
     Train Epoch: [480/2467]	Loss: 0.329326
     Train Epoch: [480/2467]	Loss: 0.383406
         Train Epoch: [500/2467]	Loss: 0.170061
     Train Epoch: [500/2467]	Loss: 0.439025 
Train Epoch: [500/2467]	Loss: 0.125093
     Train Epoch: [500/2467]	Loss: 0.305203
              Train Epoch: [520/2467]	Loss: 0.240666
Train Epoch: [520/2467]	Loss: 0.203585
 Train Epoch: [520/2467]	Loss: 0.442335
     Train Epoch: [520/2467]	Loss: 0.099961
     Train Epoch: [540/2467]	Loss: 0.086377
         Train Epoch: [540/2467]	Loss: 0.292322
 Train Epoch: [540/2467]	Loss: 0.139740
     Train Epoch: [540/2467]	Loss: 0.201863
     Train Epoch: [560/2467]	Loss: 0.220064
         Train Epoch: [560/2467]	Loss: 0.266274
 Train Epoch: [560/2467]	Loss: 0.328726
     Train Epoch: [560/2467]	Loss: 0.221761
             Train Epoch: [580/2467]	Loss: 0.080174  
Train Epoch: [580/2467]	Loss: 0.194978Train Epoch: [580/2467]	Loss: 0.175166

     Train Epoch: [580/2467]	Loss: 0.436647
     Train Epoch: [600/2467]	Loss: 0.209840
         Train Epoch: [600/2467]	Loss: 0.323025
     Train Epoch: [600/2467]	Loss: 0.135090
 Train Epoch: [600/2467]	Loss: 0.243415
             Train Epoch: [620/2467]	Loss: 0.201922
 Train Epoch: [620/2467]	Loss: 0.045355 
Train Epoch: [620/2467]	Loss: 0.229911
     Train Epoch: [620/2467]	Loss: 0.258524
         Train Epoch: [640/2467]	Loss: 0.096431
 Train Epoch: [640/2467]	Loss: 0.173701
     Train Epoch: [640/2467]	Loss: 0.329922
     Train Epoch: [640/2467]	Loss: 0.219632
     Train Epoch: [660/2467]	Loss: 0.037171
     Train Epoch: [660/2467]	Loss: 0.432404
     Train Epoch: [660/2467]	Loss: 0.057672
     Train Epoch: [660/2467]	Loss: 0.309278
         Train Epoch: [680/2467]	Loss: 0.156645
 Train Epoch: [680/2467]	Loss: 0.119704
         Train Epoch: [680/2467]	Loss: 0.121350 
Train Epoch: [680/2467]	Loss: 0.098330
         Train Epoch: [700/2467]	Loss: 0.100680
 Train Epoch: [700/2467]	Loss: 0.111280
     Train Epoch: [700/2467]	Loss: 0.424246    
 Train Epoch: [700/2467]	Loss: 0.338538
          Train Epoch: [720/2467]	Loss: 0.105359
Train Epoch: [720/2467]	Loss: 0.199864
     Train Epoch: [720/2467]	Loss: 0.293999    
 Train Epoch: [720/2467]	Loss: 0.182634
         Train Epoch: [740/2467]	Loss: 0.067827
     Train Epoch: [740/2467]	Loss: 0.147812
 Train Epoch: [740/2467]	Loss: 0.377971
     Train Epoch: [740/2467]	Loss: 0.092704
     Train Epoch: [760/2467]	Loss: 0.570829    
      Train Epoch: [760/2467]	Loss: 0.318513Train Epoch: [760/2467]	Loss: 0.193464

     Train Epoch: [760/2467]	Loss: 0.426872
         Train Epoch: [780/2467]	Loss: 0.377381    
 Train Epoch: [780/2467]	Loss: 0.408651 
    Train Epoch: [780/2467]	Loss: 0.462820
 Train Epoch: [780/2467]	Loss: 0.427041
     Train Epoch: [800/2467]	Loss: 0.339037    
 Train Epoch: [800/2467]	Loss: 0.275259
     Train Epoch: [800/2467]	Loss: 0.275317
     Train Epoch: [800/2467]	Loss: 0.135541
         Train Epoch: [820/2467]	Loss: 0.318366    
 Train Epoch: [820/2467]	Loss: 0.064134
 Train Epoch: [820/2467]	Loss: 0.445953
     Train Epoch: [820/2467]	Loss: 0.084843
         Train Epoch: [840/2467]	Loss: 0.187773
     Train Epoch: [840/2467]	Loss: 0.134401
 Train Epoch: [840/2467]	Loss: 0.146708
     Train Epoch: [840/2467]	Loss: 0.360500
         Train Epoch: [860/2467]	Loss: 0.235888
 Train Epoch: [860/2467]	Loss: 0.326130
     Train Epoch: [860/2467]	Loss: 0.235631
     Train Epoch: [860/2467]	Loss: 0.088517
          Train Epoch: [880/2467]	Loss: 0.163004Train Epoch: [880/2467]	Loss: 0.319391
    
     Train Epoch: [880/2467]	Loss: 0.203109
 Train Epoch: [880/2467]	Loss: 0.242672
     Train Epoch: [900/2467]	Loss: 0.223736
     Train Epoch: [900/2467]	Loss: 0.228646
     Train Epoch: [900/2467]	Loss: 0.126470
     Train Epoch: [900/2467]	Loss: 0.228465
             Train Epoch: [920/2467]	Loss: 0.372647
 Train Epoch: [920/2467]	Loss: 0.348758 
    Train Epoch: [920/2467]	Loss: 0.061258
 Train Epoch: [920/2467]	Loss: 0.528315
         Train Epoch: [940/2467]	Loss: 0.048884    
 Train Epoch: [940/2467]	Loss: 0.197113
     Train Epoch: [940/2467]	Loss: 0.249074
 Train Epoch: [940/2467]	Loss: 0.103571
     Train Epoch: [960/2467]	Loss: 0.067094    
      Train Epoch: [960/2467]	Loss: 0.423915Train Epoch: [960/2467]	Loss: 0.101896

     Train Epoch: [960/2467]	Loss: 0.115222
         Train Epoch: [980/2467]	Loss: 0.297280 
Train Epoch: [980/2467]	Loss: 0.200934
         Train Epoch: [980/2467]	Loss: 0.182883
 Train Epoch: [980/2467]	Loss: 0.153472
         Train Epoch: [1000/2467]	Loss: 0.061104
 Train Epoch: [1000/2467]	Loss: 0.268664
     Train Epoch: [1000/2467]	Loss: 0.098093
     Train Epoch: [1000/2467]	Loss: 0.358464
     Train Epoch: [1020/2467]	Loss: 0.185515
              Train Epoch: [1020/2467]	Loss: 0.459954
Train Epoch: [1020/2467]	Loss: 0.261352
 Train Epoch: [1020/2467]	Loss: 0.105879
     Train Epoch: [1040/2467]	Loss: 0.322176    
     Train Epoch: [1040/2467]	Loss: 0.269533
 Train Epoch: [1040/2467]	Loss: 0.106893
     Train Epoch: [1040/2467]	Loss: 0.200927
             Train Epoch: [1060/2467]	Loss: 0.154239
  Train Epoch: [1060/2467]	Loss: 0.308627Train Epoch: [1060/2467]	Loss: 0.506916

     Train Epoch: [1060/2467]	Loss: 0.302908
     Train Epoch: [1080/2467]	Loss: 0.186661
     Train Epoch: [1080/2467]	Loss: 0.308052
     Train Epoch: [1080/2467]	Loss: 0.335828
     Train Epoch: [1080/2467]	Loss: 0.142814
         Train Epoch: [1100/2467]	Loss: 0.395197    
 Train Epoch: [1100/2467]	Loss: 0.455954 
Train Epoch: [1100/2467]	Loss: 0.323793
     Train Epoch: [1100/2467]	Loss: 0.301049
              Train Epoch: [1120/2467]	Loss: 0.142297
Train Epoch: [1120/2467]	Loss: 0.451410
 Train Epoch: [1120/2467]	Loss: 0.174324
     Train Epoch: [1120/2467]	Loss: 0.188883
              Train Epoch: [1140/2467]	Loss: 0.065026Train Epoch: [1140/2467]	Loss: 0.292499

 Train Epoch: [1140/2467]	Loss: 0.125051
     Train Epoch: [1140/2467]	Loss: 0.489498
             Train Epoch: [1160/2467]	Loss: 0.266080
  Train Epoch: [1160/2467]	Loss: 0.234246Train Epoch: [1160/2467]	Loss: 0.080066

     Train Epoch: [1160/2467]	Loss: 0.128202
     Train Epoch: [1180/2467]	Loss: 0.096373
     Train Epoch: [1180/2467]	Loss: 0.193859
     Train Epoch: [1180/2467]	Loss: 0.272890
     Train Epoch: [1180/2467]	Loss: 0.359566
     Train Epoch: [1200/2467]	Loss: 0.337423
         Train Epoch: [1200/2467]	Loss: 0.187098
 Train Epoch: [1200/2467]	Loss: 0.250475
     Train Epoch: [1200/2467]	Loss: 0.145900
          Train Epoch: [1220/2467]	Loss: 0.336754Train Epoch: [1220/2467]	Loss: 0.261991

         Train Epoch: [1220/2467]	Loss: 0.141771
 Train Epoch: [1220/2467]	Loss: 0.382644
     Train Epoch: [1240/2467]	Loss: 0.365145
          Train Epoch: [1240/2467]	Loss: 0.304862
Train Epoch: [1240/2467]	Loss: 0.297843
     Train Epoch: [1240/2467]	Loss: 0.383341
     Train Epoch: [1260/2467]	Loss: 0.143017    
 Train Epoch: [1260/2467]	Loss: 0.075423
     Train Epoch: [1260/2467]	Loss: 0.095171
     Train Epoch: [1260/2467]	Loss: 0.468598
         Train Epoch: [1280/2467]	Loss: 0.101835
     Train Epoch: [1280/2467]	Loss: 0.874769
 Train Epoch: [1280/2467]	Loss: 0.332815
     Train Epoch: [1280/2467]	Loss: 0.495041
         Train Epoch: [1300/2467]	Loss: 0.106338
      Train Epoch: [1300/2467]	Loss: 0.095038
Train Epoch: [1300/2467]	Loss: 0.397606
     Train Epoch: [1300/2467]	Loss: 0.317271
         Train Epoch: [1320/2467]	Loss: 0.089783
     Train Epoch: [1320/2467]	Loss: 0.275744 
    Train Epoch: [1320/2467]	Loss: 0.290861
 Train Epoch: [1320/2467]	Loss: 0.306639
     Train Epoch: [1340/2467]	Loss: 0.410904    
 Train Epoch: [1340/2467]	Loss: 0.120261
     Train Epoch: [1340/2467]	Loss: 0.258814
     Train Epoch: [1340/2467]	Loss: 0.250199
         Train Epoch: [1360/2467]	Loss: 0.145067
 Train Epoch: [1360/2467]	Loss: 0.140962
         Train Epoch: [1360/2467]	Loss: 0.071645 
Train Epoch: [1360/2467]	Loss: 0.745597
         Train Epoch: [1380/2467]	Loss: 0.285643
 Train Epoch: [1380/2467]	Loss: 0.252779
     Train Epoch: [1380/2467]	Loss: 0.383856
     Train Epoch: [1380/2467]	Loss: 0.374506
     Train Epoch: [1400/2467]	Loss: 0.136280
     Train Epoch: [1400/2467]	Loss: 0.178198
         Train Epoch: [1400/2467]	Loss: 0.245649
 Train Epoch: [1400/2467]	Loss: 0.263321
         Train Epoch: [1420/2467]	Loss: 0.124467
 Train Epoch: [1420/2467]	Loss: 0.281738
     Train Epoch: [1420/2467]	Loss: 0.166919
     Train Epoch: [1420/2467]	Loss: 0.454122
     Train Epoch: [1440/2467]	Loss: 0.075478
         Train Epoch: [1440/2467]	Loss: 0.073707 
Train Epoch: [1440/2467]	Loss: 0.062282
     Train Epoch: [1440/2467]	Loss: 0.283266
         Train Epoch: [1460/2467]	Loss: 0.201431    
 Train Epoch: [1460/2467]	Loss: 0.138003 
Train Epoch: [1460/2467]	Loss: 0.059423
     Train Epoch: [1460/2467]	Loss: 0.241474
         Train Epoch: [1480/2467]	Loss: 0.224310
     Train Epoch: [1480/2467]	Loss: 0.290996 
Train Epoch: [1480/2467]	Loss: 0.021543
     Train Epoch: [1480/2467]	Loss: 0.073060
     Train Epoch: [1500/2467]	Loss: 0.191764
     Train Epoch: [1500/2467]	Loss: 0.152429
     Train Epoch: [1500/2467]	Loss: 0.029670
     Train Epoch: [1500/2467]	Loss: 0.197192
     Train Epoch: [1520/2467]	Loss: 0.394267
             Train Epoch: [1520/2467]	Loss: 0.216976
  Train Epoch: [1520/2467]	Loss: 0.301203Train Epoch: [1520/2467]	Loss: 0.189274

     Train Epoch: [1540/2467]	Loss: 0.101739
         Train Epoch: [1540/2467]	Loss: 0.550631
 Train Epoch: [1540/2467]	Loss: 0.326299
     Train Epoch: [1540/2467]	Loss: 0.221723
     Train Epoch: [1560/2467]	Loss: 0.340864
         Train Epoch: [1560/2467]	Loss: 0.489088
 Train Epoch: [1560/2467]	Loss: 0.453196    
 Train Epoch: [1560/2467]	Loss: 0.597408
     Train Epoch: [1580/2467]	Loss: 0.347132
         Train Epoch: [1580/2467]	Loss: 0.295437
 Train Epoch: [1580/2467]	Loss: 0.396908
     Train Epoch: [1580/2467]	Loss: 0.217190
     Train Epoch: [1600/2467]	Loss: 0.087731
         Train Epoch: [1600/2467]	Loss: 0.270850 
Train Epoch: [1600/2467]	Loss: 0.297946
     Train Epoch: [1600/2467]	Loss: 0.138122
         Train Epoch: [1620/2467]	Loss: 0.114459
 Train Epoch: [1620/2467]	Loss: 0.220629
     Train Epoch: [1620/2467]	Loss: 0.066442
     Train Epoch: [1620/2467]	Loss: 0.247894
          Train Epoch: [1640/2467]	Loss: 0.296895Train Epoch: [1640/2467]	Loss: 0.278730

     Train Epoch: [1640/2467]	Loss: 0.278294
     Train Epoch: [1640/2467]	Loss: 0.168192
          Train Epoch: [1660/2467]	Loss: 0.253555    Train Epoch: [1660/2467]	Loss: 0.160309

 Train Epoch: [1660/2467]	Loss: 0.233226
     Train Epoch: [1660/2467]	Loss: 0.119139
     Train Epoch: [1680/2467]	Loss: 0.262112
     Train Epoch: [1680/2467]	Loss: 0.630289
     Train Epoch: [1680/2467]	Loss: 0.081061
     Train Epoch: [1680/2467]	Loss: 0.295187
     Train Epoch: [1700/2467]	Loss: 0.293930
          Train Epoch: [1700/2467]	Loss: 0.214735Train Epoch: [1700/2467]	Loss: 0.102396

     Train Epoch: [1700/2467]	Loss: 0.275582
         Train Epoch: [1720/2467]	Loss: 0.288364 
Train Epoch: [1720/2467]	Loss: 0.138955
     Train Epoch: [1720/2467]	Loss: 0.267841
     Train Epoch: [1720/2467]	Loss: 0.220051
         Train Epoch: [1740/2467]	Loss: 0.044665
     Train Epoch: [1740/2467]	Loss: 0.115501
 Train Epoch: [1740/2467]	Loss: 0.415844
     Train Epoch: [1740/2467]	Loss: 0.228013
     Train Epoch: [1760/2467]	Loss: 0.083882
         Train Epoch: [1760/2467]	Loss: 0.135370
 Train Epoch: [1760/2467]	Loss: 0.203551
     Train Epoch: [1760/2467]	Loss: 0.196126
     Train Epoch: [1780/2467]	Loss: 0.277706
     Train Epoch: [1780/2467]	Loss: 0.110125
     Train Epoch: [1780/2467]	Loss: 0.393432
     Train Epoch: [1780/2467]	Loss: 0.322489
     Train Epoch: [1800/2467]	Loss: 0.290131
             Train Epoch: [1800/2467]	Loss: 0.221427
 Train Epoch: [1800/2467]	Loss: 0.372504 
Train Epoch: [1800/2467]	Loss: 0.353761
         Train Epoch: [1820/2467]	Loss: 0.160325
     Train Epoch: [1820/2467]	Loss: 0.134771
 Train Epoch: [1820/2467]	Loss: 0.361272
     Train Epoch: [1820/2467]	Loss: 0.379162
         Train Epoch: [1840/2467]	Loss: 0.321965
     Train Epoch: [1840/2467]	Loss: 0.067025
 Train Epoch: [1840/2467]	Loss: 0.349884
     Train Epoch: [1840/2467]	Loss: 0.157218
         Train Epoch: [1860/2467]	Loss: 0.260467
 Train Epoch: [1860/2467]	Loss: 0.238002
     Train Epoch: [1860/2467]	Loss: 0.121916
     Train Epoch: [1860/2467]	Loss: 0.214578
         Train Epoch: [1880/2467]	Loss: 0.092675    
 Train Epoch: [1880/2467]	Loss: 0.147168
 Train Epoch: [1880/2467]	Loss: 0.050988
     Train Epoch: [1880/2467]	Loss: 0.062282
         Train Epoch: [1900/2467]	Loss: 0.167809
     Train Epoch: [1900/2467]	Loss: 0.376778 
Train Epoch: [1900/2467]	Loss: 0.401883
     Train Epoch: [1900/2467]	Loss: 0.206676
             Train Epoch: [1920/2467]	Loss: 0.275438
 Train Epoch: [1920/2467]	Loss: 0.308492
     Train Epoch: [1920/2467]	Loss: 0.018913
 Train Epoch: [1920/2467]	Loss: 0.274619
     Train Epoch: [1940/2467]	Loss: 0.323658
         Train Epoch: [1940/2467]	Loss: 0.453961 
Train Epoch: [1940/2467]	Loss: 0.121420
     Train Epoch: [1940/2467]	Loss: 0.089429
     Train Epoch: [1960/2467]	Loss: 0.257530    
     Train Epoch: [1960/2467]	Loss: 0.269976
     Train Epoch: [1960/2467]	Loss: 0.135579
 Train Epoch: [1960/2467]	Loss: 0.193144
     Train Epoch: [1980/2467]	Loss: 0.367189
         Train Epoch: [1980/2467]	Loss: 0.237401
 Train Epoch: [1980/2467]	Loss: 0.306943
     Train Epoch: [1980/2467]	Loss: 0.190378
     Train Epoch: [2000/2467]	Loss: 0.155253
     Train Epoch: [2000/2467]	Loss: 0.051659    
 Train Epoch: [2000/2467]	Loss: 0.316762
     Train Epoch: [2000/2467]	Loss: 0.092779
             Train Epoch: [2020/2467]	Loss: 0.456453
  Train Epoch: [2020/2467]	Loss: 0.260482Train Epoch: [2020/2467]	Loss: 0.342108

     Train Epoch: [2020/2467]	Loss: 0.087683
     Train Epoch: [2040/2467]	Loss: 0.187177    
     Train Epoch: [2040/2467]	Loss: 0.309626
 Train Epoch: [2040/2467]	Loss: 0.420003
     Train Epoch: [2040/2467]	Loss: 0.107913
         Train Epoch: [2060/2467]	Loss: 0.267509
 Train Epoch: [2060/2467]	Loss: 0.257930    
     Train Epoch: [2060/2467]	Loss: 0.152613
 Train Epoch: [2060/2467]	Loss: 0.285547
     Train Epoch: [2080/2467]	Loss: 0.071579
         Train Epoch: [2080/2467]	Loss: 0.280099
 Train Epoch: [2080/2467]	Loss: 0.075856
     Train Epoch: [2080/2467]	Loss: 0.120467
         Train Epoch: [2100/2467]	Loss: 0.600836
         Train Epoch: [2100/2467]	Loss: 0.107319
 Train Epoch: [2100/2467]	Loss: 0.160262
 Train Epoch: [2100/2467]	Loss: 0.141832
     Train Epoch: [2120/2467]	Loss: 0.064412
     Train Epoch: [2120/2467]	Loss: 0.125660
     Train Epoch: [2120/2467]	Loss: 0.229805
     Train Epoch: [2120/2467]	Loss: 0.292695
         Train Epoch: [2140/2467]	Loss: 0.224467
     Train Epoch: [2140/2467]	Loss: 0.636557
 Train Epoch: [2140/2467]	Loss: 0.310817
     Train Epoch: [2140/2467]	Loss: 0.443724
     Train Epoch: [2160/2467]	Loss: 0.154553
          Train Epoch: [2160/2467]	Loss: 0.358237Train Epoch: [2160/2467]	Loss: 0.065217

     Train Epoch: [2160/2467]	Loss: 0.203828
          Train Epoch: [2180/2467]	Loss: 0.208299Train Epoch: [2180/2467]	Loss: 0.148691

         Train Epoch: [2180/2467]	Loss: 0.319843
 Train Epoch: [2180/2467]	Loss: 0.230729
             Train Epoch: [2200/2467]	Loss: 0.141331
 Train Epoch: [2200/2467]	Loss: 0.323263
 Train Epoch: [2200/2467]	Loss: 0.131335
     Train Epoch: [2200/2467]	Loss: 0.123632
     Train Epoch: [2220/2467]	Loss: 0.160247    
     Train Epoch: [2220/2467]	Loss: 0.207046
 Train Epoch: [2220/2467]	Loss: 0.088971
     Train Epoch: [2220/2467]	Loss: 0.130274
         Train Epoch: [2240/2467]	Loss: 0.039899
 Train Epoch: [2240/2467]	Loss: 0.081616
     Train Epoch: [2240/2467]	Loss: 0.199982
     Train Epoch: [2240/2467]	Loss: 0.057884
     Train Epoch: [2260/2467]	Loss: 0.426437
         Train Epoch: [2260/2467]	Loss: 0.331343
 Train Epoch: [2260/2467]	Loss: 0.064833
     Train Epoch: [2260/2467]	Loss: 0.237972
     Train Epoch: [2280/2467]	Loss: 0.101878
         Train Epoch: [2280/2467]	Loss: 0.090707
 Train Epoch: [2280/2467]	Loss: 0.137198
     Train Epoch: [2280/2467]	Loss: 0.186170
     Train Epoch: [2300/2467]	Loss: 0.076493
     Train Epoch: [2300/2467]	Loss: 0.130617
          Train Epoch: [2300/2467]	Loss: 0.145285Train Epoch: [2300/2467]	Loss: 0.401559

     Train Epoch: [2320/2467]	Loss: 0.360079    
     Train Epoch: [2320/2467]	Loss: 0.042094 
    Train Epoch: [2320/2467]	Loss: 0.296839
 Train Epoch: [2320/2467]	Loss: 0.182073
          Train Epoch: [2340/2467]	Loss: 0.157054Train Epoch: [2340/2467]	Loss: 0.148036

         Train Epoch: [2340/2467]	Loss: 0.399254
 Train Epoch: [2340/2467]	Loss: 0.557204
             Train Epoch: [2360/2467]	Loss: 0.412273 
Train Epoch: [2360/2467]	Loss: 0.217496
     Train Epoch: [2360/2467]	Loss: 0.174771
 Train Epoch: [2360/2467]	Loss: 0.200256
             Train Epoch: [2380/2467]	Loss: 0.374932
  Train Epoch: [2380/2467]	Loss: 0.542646Train Epoch: [2380/2467]	Loss: 0.153103

     Train Epoch: [2380/2467]	Loss: 0.331014
          Train Epoch: [2400/2467]	Loss: 0.125838Train Epoch: [2400/2467]	Loss: 0.358416

          Train Epoch: [2400/2467]	Loss: 0.284775
Train Epoch: [2400/2467]	Loss: 0.194617
         Train Epoch: [2420/2467]	Loss: 0.147256
 Train Epoch: [2420/2467]	Loss: 0.328658
     Train Epoch: [2420/2467]	Loss: 0.451647
     Train Epoch: [2420/2467]	Loss: 0.267496
             Train Epoch: [2440/2467]	Loss: 0.050874
 Train Epoch: [2440/2467]	Loss: 0.262842 
Train Epoch: [2440/2467]	Loss: 0.364968
     Train Epoch: [2440/2467]	Loss: 0.151204
         Train Epoch: [2460/2467]	Loss: 0.163931
     Train Epoch: [2460/2467]	Loss: 0.168776
     Train Epoch: [2460/2467]	Loss: 0.114166 
Train Epoch: [2460/2467]	Loss: 0.282433
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 14 epoch =====
     2025-05-11.05-08-42
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 14 epoch =====
     2025-05-11.05-08-43
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 14 epoch =====
     2025-05-11.05-08-43
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 14 epoch =====
     2025-05-11.05-08-43
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
         Train Epoch: [0/2467]	Loss: 0.199523
     Train Epoch: [0/2467]	Loss: 0.336100
     Train Epoch: [0/2467]	Loss: 0.240847
 Train Epoch: [0/2467]	Loss: 0.335982
         Train Epoch: [20/2467]	Loss: 0.171073
 Train Epoch: [20/2467]	Loss: 0.165459
         Train Epoch: [20/2467]	Loss: 0.373952 
Train Epoch: [20/2467]	Loss: 0.301037
          Train Epoch: [40/2467]	Loss: 0.468083Train Epoch: [40/2467]	Loss: 0.194666

     Train Epoch: [40/2467]	Loss: 0.269863
     Train Epoch: [40/2467]	Loss: 0.178656
     Train Epoch: [60/2467]	Loss: 0.222981
         Train Epoch: [60/2467]	Loss: 0.167865
 Train Epoch: [60/2467]	Loss: 0.328183
     Train Epoch: [60/2467]	Loss: 0.088857
     Train Epoch: [80/2467]	Loss: 0.182111    
 Train Epoch: [80/2467]	Loss: 0.135182
     Train Epoch: [80/2467]	Loss: 0.124471
     Train Epoch: [80/2467]	Loss: 0.396481
     Train Epoch: [100/2467]	Loss: 0.321448
          Train Epoch: [100/2467]	Loss: 0.147661Train Epoch: [100/2467]	Loss: 0.344066

     Train Epoch: [100/2467]	Loss: 0.295310
          Train Epoch: [120/2467]	Loss: 0.082709Train Epoch: [120/2467]	Loss: 0.308663

         Train Epoch: [120/2467]	Loss: 0.086245 
Train Epoch: [120/2467]	Loss: 0.428874
         Train Epoch: [140/2467]	Loss: 0.319156
 Train Epoch: [140/2467]	Loss: 0.288759    
 Train Epoch: [140/2467]	Loss: 0.079604
     Train Epoch: [140/2467]	Loss: 0.091209
     Train Epoch: [160/2467]	Loss: 0.265565
     Train Epoch: [160/2467]	Loss: 0.251733
     Train Epoch: [160/2467]	Loss: 0.445513
     Train Epoch: [160/2467]	Loss: 0.187549
     Train Epoch: [180/2467]	Loss: 0.218469
     Train Epoch: [180/2467]	Loss: 0.126294
     Train Epoch: [180/2467]	Loss: 0.174196
     Train Epoch: [180/2467]	Loss: 0.165231
     Train Epoch: [200/2467]	Loss: 0.147328
         Train Epoch: [200/2467]	Loss: 0.164524
 Train Epoch: [200/2467]	Loss: 0.131600
     Train Epoch: [200/2467]	Loss: 0.143975
     Train Epoch: [220/2467]	Loss: 0.079303
             Train Epoch: [220/2467]	Loss: 0.230933
  Train Epoch: [220/2467]	Loss: 0.346987
Train Epoch: [220/2467]	Loss: 0.263745
              Train Epoch: [240/2467]	Loss: 0.209183Train Epoch: [240/2467]	Loss: 0.406918

 Train Epoch: [240/2467]	Loss: 0.042958    
 Train Epoch: [240/2467]	Loss: 0.028407
     Train Epoch: [260/2467]	Loss: 0.120810
     Train Epoch: [260/2467]	Loss: 0.057055
     Train Epoch: [260/2467]	Loss: 0.170333
     Train Epoch: [260/2467]	Loss: 0.049302
     Train Epoch: [280/2467]	Loss: 0.337835    
     Train Epoch: [280/2467]	Loss: 0.288797
 Train Epoch: [280/2467]	Loss: 0.266675
     Train Epoch: [280/2467]	Loss: 0.385767
     Train Epoch: [300/2467]	Loss: 0.470229    
 Train Epoch: [300/2467]	Loss: 0.048894
         Train Epoch: [300/2467]	Loss: 0.177124
 Train Epoch: [300/2467]	Loss: 0.105532
     Train Epoch: [320/2467]	Loss: 0.169768
         Train Epoch: [320/2467]	Loss: 0.396892 
Train Epoch: [320/2467]	Loss: 0.516418
     Train Epoch: [320/2467]	Loss: 0.193305
         Train Epoch: [340/2467]	Loss: 0.079245 
Train Epoch: [340/2467]	Loss: 0.019264
         Train Epoch: [340/2467]	Loss: 0.167628
 Train Epoch: [340/2467]	Loss: 0.230219
         Train Epoch: [360/2467]	Loss: 0.084693
 Train Epoch: [360/2467]	Loss: 0.289868
          Train Epoch: [360/2467]	Loss: 0.272917Train Epoch: [360/2467]	Loss: 0.198701

     Train Epoch: [380/2467]	Loss: 0.598628
         Train Epoch: [380/2467]	Loss: 0.590111
     Train Epoch: [380/2467]	Loss: 0.135079
 Train Epoch: [380/2467]	Loss: 0.259966
         Train Epoch: [400/2467]	Loss: 0.119836
 Train Epoch: [400/2467]	Loss: 0.213446
          Train Epoch: [400/2467]	Loss: 0.171288
Train Epoch: [400/2467]	Loss: 0.338811
             Train Epoch: [420/2467]	Loss: 0.310927 
Train Epoch: [420/2467]	Loss: 0.170606 
Train Epoch: [420/2467]	Loss: 0.132402    
 Train Epoch: [420/2467]	Loss: 0.239483
         Train Epoch: [440/2467]	Loss: 0.323606
 Train Epoch: [440/2467]	Loss: 0.122891
         Train Epoch: [440/2467]	Loss: 0.186041
 Train Epoch: [440/2467]	Loss: 0.081906
         Train Epoch: [460/2467]	Loss: 0.165906
 Train Epoch: [460/2467]	Loss: 0.186716
         Train Epoch: [460/2467]	Loss: 0.306301
 Train Epoch: [460/2467]	Loss: 0.153973
     Train Epoch: [480/2467]	Loss: 0.322030    
 Train Epoch: [480/2467]	Loss: 0.162099
         Train Epoch: [480/2467]	Loss: 0.239903
 Train Epoch: [480/2467]	Loss: 0.367473
     Train Epoch: [500/2467]	Loss: 0.154970
         Train Epoch: [500/2467]	Loss: 0.308212
 Train Epoch: [500/2467]	Loss: 0.315138    
 Train Epoch: [500/2467]	Loss: 0.136140
         Train Epoch: [520/2467]	Loss: 0.183990
 Train Epoch: [520/2467]	Loss: 0.340264
     Train Epoch: [520/2467]	Loss: 0.232223
     Train Epoch: [520/2467]	Loss: 0.084408
     Train Epoch: [540/2467]	Loss: 0.287367
             Train Epoch: [540/2467]	Loss: 0.197531
 Train Epoch: [540/2467]	Loss: 0.138594
 Train Epoch: [540/2467]	Loss: 0.081375
     Train Epoch: [560/2467]	Loss: 0.173059
     Train Epoch: [560/2467]	Loss: 0.364629
         Train Epoch: [560/2467]	Loss: 0.177702
 Train Epoch: [560/2467]	Loss: 0.226951
         Train Epoch: [580/2467]	Loss: 0.434578    
 Train Epoch: [580/2467]	Loss: 0.145915
 Train Epoch: [580/2467]	Loss: 0.178698
     Train Epoch: [580/2467]	Loss: 0.079702
         Train Epoch: [600/2467]	Loss: 0.140486
     Train Epoch: [600/2467]	Loss: 0.212885    
  Train Epoch: [600/2467]	Loss: 0.227742
Train Epoch: [600/2467]	Loss: 0.247129
         Train Epoch: [620/2467]	Loss: 0.036230
     Train Epoch: [620/2467]	Loss: 0.346666
 Train Epoch: [620/2467]	Loss: 0.224313
     Train Epoch: [620/2467]	Loss: 0.166819
     Train Epoch: [640/2467]	Loss: 0.107620
         Train Epoch: [640/2467]	Loss: 0.206611
 Train Epoch: [640/2467]	Loss: 0.259350
     Train Epoch: [640/2467]	Loss: 0.270378
     Train Epoch: [660/2467]	Loss: 0.047586    
     Train Epoch: [660/2467]	Loss: 0.041906
 Train Epoch: [660/2467]	Loss: 0.310516
     Train Epoch: [660/2467]	Loss: 0.436623
         Train Epoch: [680/2467]	Loss: 0.081778    
 Train Epoch: [680/2467]	Loss: 0.101244
 Train Epoch: [680/2467]	Loss: 0.137157
     Train Epoch: [680/2467]	Loss: 0.100128
         Train Epoch: [700/2467]	Loss: 0.382313    
     Train Epoch: [700/2467]	Loss: 0.507250
 Train Epoch: [700/2467]	Loss: 0.094192
 Train Epoch: [700/2467]	Loss: 0.062580
             Train Epoch: [720/2467]	Loss: 0.239807 
Train Epoch: [720/2467]	Loss: 0.055927
 Train Epoch: [720/2467]	Loss: 0.143709
     Train Epoch: [720/2467]	Loss: 0.177375
              Train Epoch: [740/2467]	Loss: 0.091993    Train Epoch: [740/2467]	Loss: 0.342273

 Train Epoch: [740/2467]	Loss: 0.134993
 Train Epoch: [740/2467]	Loss: 0.073308
         Train Epoch: [760/2467]	Loss: 0.286524 
Train Epoch: [760/2467]	Loss: 0.406517
     Train Epoch: [760/2467]	Loss: 0.102717
     Train Epoch: [760/2467]	Loss: 0.554284
         Train Epoch: [780/2467]	Loss: 0.376176
 Train Epoch: [780/2467]	Loss: 0.358710
     Train Epoch: [780/2467]	Loss: 0.393194
     Train Epoch: [780/2467]	Loss: 0.246424
     Train Epoch: [800/2467]	Loss: 0.228282
              Train Epoch: [800/2467]	Loss: 0.148419Train Epoch: [800/2467]	Loss: 0.291643

 Train Epoch: [800/2467]	Loss: 0.348552
     Train Epoch: [820/2467]	Loss: 0.451092
             Train Epoch: [820/2467]	Loss: 0.062003
  Train Epoch: [820/2467]	Loss: 0.060983Train Epoch: [820/2467]	Loss: 0.346829

              Train Epoch: [840/2467]	Loss: 0.422616
 Train Epoch: [840/2467]	Loss: 0.127784
Train Epoch: [840/2467]	Loss: 0.167458
     Train Epoch: [840/2467]	Loss: 0.166211
     Train Epoch: [860/2467]	Loss: 0.208103
     Train Epoch: [860/2467]	Loss: 0.078368
     Train Epoch: [860/2467]	Loss: 0.357315
     Train Epoch: [860/2467]	Loss: 0.125718
     Train Epoch: [880/2467]	Loss: 0.218591
         Train Epoch: [880/2467]	Loss: 0.162236
 Train Epoch: [880/2467]	Loss: 0.264727    
 Train Epoch: [880/2467]	Loss: 0.566926
              Train Epoch: [900/2467]	Loss: 0.217565
Train Epoch: [900/2467]	Loss: 0.185370
 Train Epoch: [900/2467]	Loss: 0.239161
     Train Epoch: [900/2467]	Loss: 0.214360
         Train Epoch: [920/2467]	Loss: 0.302902
 Train Epoch: [920/2467]	Loss: 0.187899
     Train Epoch: [920/2467]	Loss: 0.360880
     Train Epoch: [920/2467]	Loss: 0.537624
     Train Epoch: [940/2467]	Loss: 0.084352
             Train Epoch: [940/2467]	Loss: 0.053609
 Train Epoch: [940/2467]	Loss: 0.203233 
Train Epoch: [940/2467]	Loss: 0.369455
     Train Epoch: [960/2467]	Loss: 0.126716    
 Train Epoch: [960/2467]	Loss: 0.122933
     Train Epoch: [960/2467]	Loss: 0.382874
     Train Epoch: [960/2467]	Loss: 0.077888
     Train Epoch: [980/2467]	Loss: 0.102696
             Train Epoch: [980/2467]	Loss: 0.224092 
 Train Epoch: [980/2467]	Loss: 0.292897
Train Epoch: [980/2467]	Loss: 0.192562
         Train Epoch: [1000/2467]	Loss: 0.257289
 Train Epoch: [1000/2467]	Loss: 0.056381
     Train Epoch: [1000/2467]	Loss: 0.076622
     Train Epoch: [1000/2467]	Loss: 0.357129
             Train Epoch: [1020/2467]	Loss: 0.173165
 Train Epoch: [1020/2467]	Loss: 0.072712
 Train Epoch: [1020/2467]	Loss: 0.265760
     Train Epoch: [1020/2467]	Loss: 0.430819
         Train Epoch: [1040/2467]	Loss: 0.279266
     Train Epoch: [1040/2467]	Loss: 0.165735
 Train Epoch: [1040/2467]	Loss: 0.110595
     Train Epoch: [1040/2467]	Loss: 0.324688
     Train Epoch: [1060/2467]	Loss: 0.126544
              Train Epoch: [1060/2467]	Loss: 0.338351Train Epoch: [1060/2467]	Loss: 0.326146

 Train Epoch: [1060/2467]	Loss: 0.497393
         Train Epoch: [1080/2467]	Loss: 0.328977    
 Train Epoch: [1080/2467]	Loss: 0.321049
 Train Epoch: [1080/2467]	Loss: 0.162088
     Train Epoch: [1080/2467]	Loss: 0.205945
     Train Epoch: [1100/2467]	Loss: 0.298684
         Train Epoch: [1100/2467]	Loss: 0.268716
     Train Epoch: [1100/2467]	Loss: 0.414715
 Train Epoch: [1100/2467]	Loss: 0.289797
         Train Epoch: [1120/2467]	Loss: 0.165154
     Train Epoch: [1120/2467]	Loss: 0.119066
 Train Epoch: [1120/2467]	Loss: 0.344585
     Train Epoch: [1120/2467]	Loss: 0.200285
              Train Epoch: [1140/2467]	Loss: 0.057333Train Epoch: [1140/2467]	Loss: 0.476645

     Train Epoch: [1140/2467]	Loss: 0.126188
 Train Epoch: [1140/2467]	Loss: 0.287116
             Train Epoch: [1160/2467]	Loss: 0.076169    
 Train Epoch: [1160/2467]	Loss: 0.130952
 Train Epoch: [1160/2467]	Loss: 0.248977
 Train Epoch: [1160/2467]	Loss: 0.241321
          Train Epoch: [1180/2467]	Loss: 0.362726Train Epoch: [1180/2467]	Loss: 0.184714

         Train Epoch: [1180/2467]	Loss: 0.304621
 Train Epoch: [1180/2467]	Loss: 0.106437
         Train Epoch: [1200/2467]	Loss: 0.167246 
Train Epoch: [1200/2467]	Loss: 0.161040
     Train Epoch: [1200/2467]	Loss: 0.263215
     Train Epoch: [1200/2467]	Loss: 0.351004
     Train Epoch: [1220/2467]	Loss: 0.415997
     Train Epoch: [1220/2467]	Loss: 0.121301
     Train Epoch: [1220/2467]	Loss: 0.360721
     Train Epoch: [1220/2467]	Loss: 0.266786
     Train Epoch: [1240/2467]	Loss: 0.458640
         Train Epoch: [1240/2467]	Loss: 0.328262
 Train Epoch: [1240/2467]	Loss: 0.199044
     Train Epoch: [1240/2467]	Loss: 0.325662
     Train Epoch: [1260/2467]	Loss: 0.077126
     Train Epoch: [1260/2467]	Loss: 0.492652
     Train Epoch: [1260/2467]	Loss: 0.094264
     Train Epoch: [1260/2467]	Loss: 0.182828
         Train Epoch: [1280/2467]	Loss: 0.330791
     Train Epoch: [1280/2467]	Loss: 0.130357
 Train Epoch: [1280/2467]	Loss: 0.848707
     Train Epoch: [1280/2467]	Loss: 0.469640
     Train Epoch: [1300/2467]	Loss: 0.367666
         Train Epoch: [1300/2467]	Loss: 0.092239
 Train Epoch: [1300/2467]	Loss: 0.433875
     Train Epoch: [1300/2467]	Loss: 0.091154
     Train Epoch: [1320/2467]	Loss: 0.236993    
     Train Epoch: [1320/2467]	Loss: 0.180043
 Train Epoch: [1320/2467]	Loss: 0.080862
     Train Epoch: [1320/2467]	Loss: 0.316358
         Train Epoch: [1340/2467]	Loss: 0.120281
 Train Epoch: [1340/2467]	Loss: 0.415578
         Train Epoch: [1340/2467]	Loss: 0.268683
 Train Epoch: [1340/2467]	Loss: 0.183196
         Train Epoch: [1360/2467]	Loss: 0.394969
         Train Epoch: [1360/2467]	Loss: 0.096968
  Train Epoch: [1360/2467]	Loss: 0.096311
Train Epoch: [1360/2467]	Loss: 0.385916
     Train Epoch: [1380/2467]	Loss: 0.233075
     Train Epoch: [1380/2467]	Loss: 0.301188
     Train Epoch: [1380/2467]	Loss: 0.299158
     Train Epoch: [1380/2467]	Loss: 0.248693
             Train Epoch: [1400/2467]	Loss: 0.202356
 Train Epoch: [1400/2467]	Loss: 0.228687
 Train Epoch: [1400/2467]	Loss: 0.192982
     Train Epoch: [1400/2467]	Loss: 0.117440
     Train Epoch: [1420/2467]	Loss: 0.112830    
     Train Epoch: [1420/2467]	Loss: 0.166030
     Train Epoch: [1420/2467]	Loss: 0.445371
 Train Epoch: [1420/2467]	Loss: 0.144766
     Train Epoch: [1440/2467]	Loss: 0.081587
         Train Epoch: [1440/2467]	Loss: 0.030395 
Train Epoch: [1440/2467]	Loss: 0.189127
     Train Epoch: [1440/2467]	Loss: 0.054059
     Train Epoch: [1460/2467]	Loss: 0.119810
         Train Epoch: [1460/2467]	Loss: 0.055511
 Train Epoch: [1460/2467]	Loss: 0.292801
     Train Epoch: [1460/2467]	Loss: 0.179307
             Train Epoch: [1480/2467]	Loss: 0.256725
 Train Epoch: [1480/2467]	Loss: 0.055078
 Train Epoch: [1480/2467]	Loss: 0.019300
     Train Epoch: [1480/2467]	Loss: 0.212076
     Train Epoch: [1500/2467]	Loss: 0.022153
     Train Epoch: [1500/2467]	Loss: 0.153083
     Train Epoch: [1500/2467]	Loss: 0.148680
     Train Epoch: [1500/2467]	Loss: 0.189788
             Train Epoch: [1520/2467]	Loss: 0.168848 
 Train Epoch: [1520/2467]	Loss: 0.193567
Train Epoch: [1520/2467]	Loss: 0.359230
     Train Epoch: [1520/2467]	Loss: 0.275995
     Train Epoch: [1540/2467]	Loss: 0.308679
          Train Epoch: [1540/2467]	Loss: 0.192660Train Epoch: [1540/2467]	Loss: 0.113609

     Train Epoch: [1540/2467]	Loss: 0.264210
             Train Epoch: [1560/2467]	Loss: 0.422866    
 Train Epoch: [1560/2467]	Loss: 0.521644
  Train Epoch: [1560/2467]	Loss: 0.132097
Train Epoch: [1560/2467]	Loss: 0.358300
                 Train Epoch: [1580/2467]	Loss: 0.311544 
Train Epoch: [1580/2467]	Loss: 0.293594
 Train Epoch: [1580/2467]	Loss: 0.245923 
Train Epoch: [1580/2467]	Loss: 0.176110
         Train Epoch: [1600/2467]	Loss: 0.126714
 Train Epoch: [1600/2467]	Loss: 0.416069
     Train Epoch: [1600/2467]	Loss: 0.287710
     Train Epoch: [1600/2467]	Loss: 0.224094
              Train Epoch: [1620/2467]	Loss: 0.114013Train Epoch: [1620/2467]	Loss: 0.223538

 Train Epoch: [1620/2467]	Loss: 0.215722
     Train Epoch: [1620/2467]	Loss: 0.070756
             Train Epoch: [1640/2467]	Loss: 0.275737
 Train Epoch: [1640/2467]	Loss: 0.265640
 Train Epoch: [1640/2467]	Loss: 0.257758
     Train Epoch: [1640/2467]	Loss: 0.143344
         Train Epoch: [1660/2467]	Loss: 0.395791
          Train Epoch: [1660/2467]	Loss: 0.349353Train Epoch: [1660/2467]	Loss: 0.140629

 Train Epoch: [1660/2467]	Loss: 0.167886
             Train Epoch: [1680/2467]	Loss: 0.480666    
 Train Epoch: [1680/2467]	Loss: 0.250514 
Train Epoch: [1680/2467]	Loss: 0.077177
 Train Epoch: [1680/2467]	Loss: 0.239718
         Train Epoch: [1700/2467]	Loss: 0.085638    
      Train Epoch: [1700/2467]	Loss: 0.245360Train Epoch: [1700/2467]	Loss: 0.283032
 
Train Epoch: [1700/2467]	Loss: 0.195505
         Train Epoch: [1720/2467]	Loss: 0.273655
         Train Epoch: [1720/2467]	Loss: 0.186191
 Train Epoch: [1720/2467]	Loss: 0.122636
 Train Epoch: [1720/2467]	Loss: 0.266197
     Train Epoch: [1740/2467]	Loss: 0.404709
     Train Epoch: [1740/2467]	Loss: 0.136974    
      Train Epoch: [1740/2467]	Loss: 0.042449Train Epoch: [1740/2467]	Loss: 0.208893

         Train Epoch: [1760/2467]	Loss: 0.170822
 Train Epoch: [1760/2467]	Loss: 0.076821    
 Train Epoch: [1760/2467]	Loss: 0.120096
     Train Epoch: [1760/2467]	Loss: 0.187437
             Train Epoch: [1780/2467]	Loss: 0.350943
 Train Epoch: [1780/2467]	Loss: 0.295351 
Train Epoch: [1780/2467]	Loss: 0.363059
     Train Epoch: [1780/2467]	Loss: 0.116867
         Train Epoch: [1800/2467]	Loss: 0.318107
 Train Epoch: [1800/2467]	Loss: 0.260529    
 Train Epoch: [1800/2467]	Loss: 0.186572
     Train Epoch: [1800/2467]	Loss: 0.329133
     Train Epoch: [1820/2467]	Loss: 0.376648
             Train Epoch: [1820/2467]	Loss: 0.144558
  Train Epoch: [1820/2467]	Loss: 0.167903Train Epoch: [1820/2467]	Loss: 0.171333

         Train Epoch: [1840/2467]	Loss: 0.074997
      Train Epoch: [1840/2467]	Loss: 0.314106Train Epoch: [1840/2467]	Loss: 0.168088

     Train Epoch: [1840/2467]	Loss: 0.351695
         Train Epoch: [1860/2467]	Loss: 0.193624    
  Train Epoch: [1860/2467]	Loss: 0.118371Train Epoch: [1860/2467]	Loss: 0.253330

     Train Epoch: [1860/2467]	Loss: 0.198142
         Train Epoch: [1880/2467]	Loss: 0.127264    
      Train Epoch: [1880/2467]	Loss: 0.075640
Train Epoch: [1880/2467]	Loss: 0.068813
 Train Epoch: [1880/2467]	Loss: 0.056145
         Train Epoch: [1900/2467]	Loss: 0.384773
     Train Epoch: [1900/2467]	Loss: 0.199111
     Train Epoch: [1900/2467]	Loss: 0.303177
 Train Epoch: [1900/2467]	Loss: 0.156064
     Train Epoch: [1920/2467]	Loss: 0.252875
         Train Epoch: [1920/2467]	Loss: 0.327749
 Train Epoch: [1920/2467]	Loss: 0.021771
     Train Epoch: [1920/2467]	Loss: 0.135631
     Train Epoch: [1940/2467]	Loss: 0.094372
     Train Epoch: [1940/2467]	Loss: 0.581587
         Train Epoch: [1940/2467]	Loss: 0.408470
 Train Epoch: [1940/2467]	Loss: 0.133828
         Train Epoch: [1960/2467]	Loss: 0.155589
 Train Epoch: [1960/2467]	Loss: 0.248846
     Train Epoch: [1960/2467]	Loss: 0.282440    
 Train Epoch: [1960/2467]	Loss: 0.212371
     Train Epoch: [1980/2467]	Loss: 0.162262
     Train Epoch: [1980/2467]	Loss: 0.371330
         Train Epoch: [1980/2467]	Loss: 0.280429 
Train Epoch: [1980/2467]	Loss: 0.333758
         Train Epoch: [2000/2467]	Loss: 0.030995
         Train Epoch: [2000/2467]	Loss: 0.337701
 Train Epoch: [2000/2467]	Loss: 0.187456 
Train Epoch: [2000/2467]	Loss: 0.085157
         Train Epoch: [2020/2467]	Loss: 0.537079
     Train Epoch: [2020/2467]	Loss: 0.100577
 Train Epoch: [2020/2467]	Loss: 0.208333
     Train Epoch: [2020/2467]	Loss: 0.324610
         Train Epoch: [2040/2467]	Loss: 0.323075 
    Train Epoch: [2040/2467]	Loss: 0.121903
     Train Epoch: [2040/2467]	Loss: 0.389883
 Train Epoch: [2040/2467]	Loss: 0.190429
             Train Epoch: [2060/2467]	Loss: 0.132075
     Train Epoch: [2060/2467]	Loss: 0.234294 
Train Epoch: [2060/2467]	Loss: 0.266257
 Train Epoch: [2060/2467]	Loss: 0.252750
             Train Epoch: [2080/2467]	Loss: 0.316076
      Train Epoch: [2080/2467]	Loss: 0.080103Train Epoch: [2080/2467]	Loss: 0.102806

 Train Epoch: [2080/2467]	Loss: 0.099924
     Train Epoch: [2100/2467]	Loss: 0.152106
             Train Epoch: [2100/2467]	Loss: 0.124049
 Train Epoch: [2100/2467]	Loss: 0.565658
 Train Epoch: [2100/2467]	Loss: 0.110809
     Train Epoch: [2120/2467]	Loss: 0.199809    
         Train Epoch: [2120/2467]	Loss: 0.066999
 Train Epoch: [2120/2467]	Loss: 0.117705 
Train Epoch: [2120/2467]	Loss: 0.322571
             Train Epoch: [2140/2467]	Loss: 0.458443
 Train Epoch: [2140/2467]	Loss: 0.608644 
Train Epoch: [2140/2467]	Loss: 0.216128
     Train Epoch: [2140/2467]	Loss: 0.367756
     Train Epoch: [2160/2467]	Loss: 0.384567
     Train Epoch: [2160/2467]	Loss: 0.082322
     Train Epoch: [2160/2467]	Loss: 0.199698
     Train Epoch: [2160/2467]	Loss: 0.157183
                 Train Epoch: [2180/2467]	Loss: 0.213414
  Train Epoch: [2180/2467]	Loss: 0.320448Train Epoch: [2180/2467]	Loss: 0.140778 

Train Epoch: [2180/2467]	Loss: 0.174828
     Train Epoch: [2200/2467]	Loss: 0.392332
              Train Epoch: [2200/2467]	Loss: 0.135511Train Epoch: [2200/2467]	Loss: 0.127734

 Train Epoch: [2200/2467]	Loss: 0.132697
         Train Epoch: [2220/2467]	Loss: 0.181072
     Train Epoch: [2220/2467]	Loss: 0.153663    
 Train Epoch: [2220/2467]	Loss: 0.123802
 Train Epoch: [2220/2467]	Loss: 0.164264
              Train Epoch: [2240/2467]	Loss: 0.195725Train Epoch: [2240/2467]	Loss: 0.065826    

  Train Epoch: [2240/2467]	Loss: 0.039216Train Epoch: [2240/2467]	Loss: 0.096969

         Train Epoch: [2260/2467]	Loss: 0.523457
     Train Epoch: [2260/2467]	Loss: 0.306420
 Train Epoch: [2260/2467]	Loss: 0.220714
     Train Epoch: [2260/2467]	Loss: 0.043590
         Train Epoch: [2280/2467]	Loss: 0.180364
 Train Epoch: [2280/2467]	Loss: 0.160763
     Train Epoch: [2280/2467]	Loss: 0.102789
     Train Epoch: [2280/2467]	Loss: 0.133602
          Train Epoch: [2300/2467]	Loss: 0.334817    
Train Epoch: [2300/2467]	Loss: 0.126425
 Train Epoch: [2300/2467]	Loss: 0.068102
     Train Epoch: [2300/2467]	Loss: 0.118646
              Train Epoch: [2320/2467]	Loss: 0.383530 Train Epoch: [2320/2467]	Loss: 0.188485

Train Epoch: [2320/2467]	Loss: 0.046216
     Train Epoch: [2320/2467]	Loss: 0.243193
          Train Epoch: [2340/2467]	Loss: 0.221162Train Epoch: [2340/2467]	Loss: 0.168727

     Train Epoch: [2340/2467]	Loss: 0.143096
     Train Epoch: [2340/2467]	Loss: 0.557902
              Train Epoch: [2360/2467]	Loss: 0.388273Train Epoch: [2360/2467]	Loss: 0.182083

 Train Epoch: [2360/2467]	Loss: 0.156232
     Train Epoch: [2360/2467]	Loss: 0.065731
             Train Epoch: [2380/2467]	Loss: 0.332470
 Train Epoch: [2380/2467]	Loss: 0.150944
 Train Epoch: [2380/2467]	Loss: 0.481325
     Train Epoch: [2380/2467]	Loss: 0.515199
     Train Epoch: [2400/2467]	Loss: 0.122825
     Train Epoch: [2400/2467]	Loss: 0.127699
     Train Epoch: [2400/2467]	Loss: 0.229210
     Train Epoch: [2400/2467]	Loss: 0.319984
     Train Epoch: [2420/2467]	Loss: 0.385740
             Train Epoch: [2420/2467]	Loss: 0.369607
 Train Epoch: [2420/2467]	Loss: 0.256785 
Train Epoch: [2420/2467]	Loss: 0.320297
             Train Epoch: [2440/2467]	Loss: 0.175090
 Train Epoch: [2440/2467]	Loss: 0.333676 
Train Epoch: [2440/2467]	Loss: 0.077859
     Train Epoch: [2440/2467]	Loss: 0.185828
     Train Epoch: [2460/2467]	Loss: 0.113364
         Train Epoch: [2460/2467]	Loss: 0.322525
 Train Epoch: [2460/2467]	Loss: 0.162400
     Train Epoch: [2460/2467]	Loss: 0.161139
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)
=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 15 epoch =====
     2025-05-11.05-30-34
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 15 epoch =====
     2025-05-11.05-30-34
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 15 epoch =====
     2025-05-11.05-30-34
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 15 epoch =====
     2025-05-11.05-30-35
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
             Train Epoch: [0/2467]	Loss: 0.207852
 Train Epoch: [0/2467]	Loss: 0.323451
 Train Epoch: [0/2467]	Loss: 0.217543
     Train Epoch: [0/2467]	Loss: 0.386697
     Train Epoch: [20/2467]	Loss: 0.383530
         Train Epoch: [20/2467]	Loss: 0.150411
 Train Epoch: [20/2467]	Loss: 0.347015
     Train Epoch: [20/2467]	Loss: 0.118732
     Train Epoch: [40/2467]	Loss: 0.162565
         Train Epoch: [40/2467]	Loss: 0.441808
 Train Epoch: [40/2467]	Loss: 0.192138
     Train Epoch: [40/2467]	Loss: 0.142768
     Train Epoch: [60/2467]	Loss: 0.046879
         Train Epoch: [60/2467]	Loss: 0.165821
 Train Epoch: [60/2467]	Loss: 0.235252
     Train Epoch: [60/2467]	Loss: 0.283789
     Train Epoch: [80/2467]	Loss: 0.183464
             Train Epoch: [80/2467]	Loss: 0.150468  
Train Epoch: [80/2467]	Loss: 0.477645Train Epoch: [80/2467]	Loss: 0.121540

             Train Epoch: [100/2467]	Loss: 0.336852
 Train Epoch: [100/2467]	Loss: 0.309925
 Train Epoch: [100/2467]	Loss: 0.254348
     Train Epoch: [100/2467]	Loss: 0.152165
         Train Epoch: [120/2467]	Loss: 0.085118
 Train Epoch: [120/2467]	Loss: 0.328058
     Train Epoch: [120/2467]	Loss: 0.409434
     Train Epoch: [120/2467]	Loss: 0.083838
         Train Epoch: [140/2467]	Loss: 0.207352
     Train Epoch: [140/2467]	Loss: 0.087892
     Train Epoch: [140/2467]	Loss: 0.036688
 Train Epoch: [140/2467]	Loss: 0.329073
     Train Epoch: [160/2467]	Loss: 0.436411
     Train Epoch: [160/2467]	Loss: 0.196524
     Train Epoch: [160/2467]	Loss: 0.155792
     Train Epoch: [160/2467]	Loss: 0.290850
             Train Epoch: [180/2467]	Loss: 0.184169
 Train Epoch: [180/2467]	Loss: 0.303725
 Train Epoch: [180/2467]	Loss: 0.201263
     Train Epoch: [180/2467]	Loss: 0.185108
     Train Epoch: [200/2467]	Loss: 0.160416    
     Train Epoch: [200/2467]	Loss: 0.129233
 Train Epoch: [200/2467]	Loss: 0.099904
     Train Epoch: [200/2467]	Loss: 0.165913
     Train Epoch: [220/2467]	Loss: 0.231377
     Train Epoch: [220/2467]	Loss: 0.221164
     Train Epoch: [220/2467]	Loss: 0.108819
     Train Epoch: [220/2467]	Loss: 0.225142
         Train Epoch: [240/2467]	Loss: 0.326531
     Train Epoch: [240/2467]	Loss: 0.052812
 Train Epoch: [240/2467]	Loss: 0.026791
     Train Epoch: [240/2467]	Loss: 0.231521
                  Train Epoch: [260/2467]	Loss: 0.088849Train Epoch: [260/2467]	Loss: 0.205874
 
Train Epoch: [260/2467]	Loss: 0.063049
 Train Epoch: [260/2467]	Loss: 0.066225
     Train Epoch: [280/2467]	Loss: 0.341085
          Train Epoch: [280/2467]	Loss: 0.241839
Train Epoch: [280/2467]	Loss: 0.099105
     Train Epoch: [280/2467]	Loss: 0.178496
     Train Epoch: [300/2467]	Loss: 0.057118    
         Train Epoch: [300/2467]	Loss: 0.182413  
Train Epoch: [300/2467]	Loss: 0.103513Train Epoch: [300/2467]	Loss: 0.416259

         Train Epoch: [320/2467]	Loss: 0.484011    
 Train Epoch: [320/2467]	Loss: 0.540161
 Train Epoch: [320/2467]	Loss: 0.188343
     Train Epoch: [320/2467]	Loss: 0.203167
                  Train Epoch: [340/2467]	Loss: 0.064710Train Epoch: [340/2467]	Loss: 0.135119  

Train Epoch: [340/2467]	Loss: 0.015680Train Epoch: [340/2467]	Loss: 0.244293

     Train Epoch: [360/2467]	Loss: 0.162365
         Train Epoch: [360/2467]	Loss: 0.265407 
Train Epoch: [360/2467]	Loss: 0.092397
     Train Epoch: [360/2467]	Loss: 0.194736
     Train Epoch: [380/2467]	Loss: 0.135799
         Train Epoch: [380/2467]	Loss: 0.550726 
Train Epoch: [380/2467]	Loss: 1.006044
     Train Epoch: [380/2467]	Loss: 0.236441
     Train Epoch: [400/2467]	Loss: 0.187981    
 Train Epoch: [400/2467]	Loss: 0.160429
     Train Epoch: [400/2467]	Loss: 0.361816
     Train Epoch: [400/2467]	Loss: 0.128672
         Train Epoch: [420/2467]	Loss: 0.297047 
Train Epoch: [420/2467]	Loss: 0.178464
     Train Epoch: [420/2467]	Loss: 0.246262
     Train Epoch: [420/2467]	Loss: 0.105645
     Train Epoch: [440/2467]	Loss: 0.110733
          Train Epoch: [440/2467]	Loss: 0.178384Train Epoch: [440/2467]	Loss: 0.081216

     Train Epoch: [440/2467]	Loss: 0.365345
     Train Epoch: [460/2467]	Loss: 0.202739
         Train Epoch: [460/2467]	Loss: 0.149380
 Train Epoch: [460/2467]	Loss: 0.300288
     Train Epoch: [460/2467]	Loss: 0.158464
     Train Epoch: [480/2467]	Loss: 0.120223
         Train Epoch: [480/2467]	Loss: 0.194648
 Train Epoch: [480/2467]	Loss: 0.307126
     Train Epoch: [480/2467]	Loss: 0.390827
     Train Epoch: [500/2467]	Loss: 0.154766
         Train Epoch: [500/2467]	Loss: 0.563306    
 Train Epoch: [500/2467]	Loss: 0.165955
 Train Epoch: [500/2467]	Loss: 0.333081
     Train Epoch: [520/2467]	Loss: 0.330834
     Train Epoch: [520/2467]	Loss: 0.185772
         Train Epoch: [520/2467]	Loss: 0.221159
 Train Epoch: [520/2467]	Loss: 0.081482
     Train Epoch: [540/2467]	Loss: 0.282774
         Train Epoch: [540/2467]	Loss: 0.126257
 Train Epoch: [540/2467]	Loss: 0.187987
     Train Epoch: [540/2467]	Loss: 0.085418
         Train Epoch: [560/2467]	Loss: 0.311937
         Train Epoch: [560/2467]	Loss: 0.166340 
Train Epoch: [560/2467]	Loss: 0.168750
 Train Epoch: [560/2467]	Loss: 0.253491
         Train Epoch: [580/2467]	Loss: 0.166270
     Train Epoch: [580/2467]	Loss: 0.073059
 Train Epoch: [580/2467]	Loss: 0.391271
     Train Epoch: [580/2467]	Loss: 0.189136
     Train Epoch: [600/2467]	Loss: 0.132097
         Train Epoch: [600/2467]	Loss: 0.253547
 Train Epoch: [600/2467]	Loss: 0.225800
     Train Epoch: [600/2467]	Loss: 0.263326
         Train Epoch: [620/2467]	Loss: 0.040570
 Train Epoch: [620/2467]	Loss: 0.365312    
 Train Epoch: [620/2467]	Loss: 0.304940
     Train Epoch: [620/2467]	Loss: 0.183360
     Train Epoch: [640/2467]	Loss: 0.066090
         Train Epoch: [640/2467]	Loss: 0.223572    
 Train Epoch: [640/2467]	Loss: 0.131196
 Train Epoch: [640/2467]	Loss: 0.354492
             Train Epoch: [660/2467]	Loss: 0.210751
 Train Epoch: [660/2467]	Loss: 0.283038
 Train Epoch: [660/2467]	Loss: 0.039482
     Train Epoch: [660/2467]	Loss: 0.460105
         Train Epoch: [680/2467]	Loss: 0.237778
     Train Epoch: [680/2467]	Loss: 0.069548
 Train Epoch: [680/2467]	Loss: 0.528481
     Train Epoch: [680/2467]	Loss: 0.160160
     Train Epoch: [700/2467]	Loss: 0.368318
          Train Epoch: [700/2467]	Loss: 0.069371Train Epoch: [700/2467]	Loss: 0.114615

     Train Epoch: [700/2467]	Loss: 0.458376
         Train Epoch: [720/2467]	Loss: 0.122273
          Train Epoch: [720/2467]	Loss: 0.231460Train Epoch: [720/2467]	Loss: 0.052287

 Train Epoch: [720/2467]	Loss: 0.231013
                  Train Epoch: [740/2467]	Loss: 0.139363
Train Epoch: [740/2467]	Loss: 0.066728 
Train Epoch: [740/2467]	Loss: 0.330811
 Train Epoch: [740/2467]	Loss: 0.075757
         Train Epoch: [760/2467]	Loss: 0.275336
     Train Epoch: [760/2467]	Loss: 0.532432 
Train Epoch: [760/2467]	Loss: 0.099124
     Train Epoch: [760/2467]	Loss: 0.356588
         Train Epoch: [780/2467]	Loss: 0.193988
     Train Epoch: [780/2467]	Loss: 0.379730
 Train Epoch: [780/2467]	Loss: 0.382797
     Train Epoch: [780/2467]	Loss: 0.347286
              Train Epoch: [800/2467]	Loss: 0.118525Train Epoch: [800/2467]	Loss: 0.221092

     Train Epoch: [800/2467]	Loss: 0.211785
 Train Epoch: [800/2467]	Loss: 0.286864
     Train Epoch: [820/2467]	Loss: 0.446501
         Train Epoch: [820/2467]	Loss: 0.059865
 Train Epoch: [820/2467]	Loss: 0.289120
     Train Epoch: [820/2467]	Loss: 0.072534
     Train Epoch: [840/2467]	Loss: 0.108919
          Train Epoch: [840/2467]	Loss: 0.359285Train Epoch: [840/2467]	Loss: 0.189248    

 Train Epoch: [840/2467]	Loss: 0.164213
         Train Epoch: [860/2467]	Loss: 0.098330
     Train Epoch: [860/2467]	Loss: 0.307488
     Train Epoch: [860/2467]	Loss: 0.197805
 Train Epoch: [860/2467]	Loss: 0.124104
     Train Epoch: [880/2467]	Loss: 0.198593    
 Train Epoch: [880/2467]	Loss: 0.266105
     Train Epoch: [880/2467]	Loss: 0.168571
     Train Epoch: [880/2467]	Loss: 0.241468
     Train Epoch: [900/2467]	Loss: 0.115511
              Train Epoch: [900/2467]	Loss: 0.209976Train Epoch: [900/2467]	Loss: 0.205763

 Train Epoch: [900/2467]	Loss: 0.211477
         Train Epoch: [920/2467]	Loss: 0.329494
     Train Epoch: [920/2467]	Loss: 0.413413
 Train Epoch: [920/2467]	Loss: 0.368023
     Train Epoch: [920/2467]	Loss: 0.061268
     Train Epoch: [940/2467]	Loss: 0.063651    
 Train Epoch: [940/2467]	Loss: 0.051040    
 Train Epoch: [940/2467]	Loss: 0.190991
     Train Epoch: [940/2467]	Loss: 0.157332
     Train Epoch: [960/2467]	Loss: 0.083656
              Train Epoch: [960/2467]	Loss: 0.076155Train Epoch: [960/2467]	Loss: 0.099653

 Train Epoch: [960/2467]	Loss: 0.369994
     Train Epoch: [980/2467]	Loss: 0.180723
         Train Epoch: [980/2467]	Loss: 0.263472
 Train Epoch: [980/2467]	Loss: 0.096044
     Train Epoch: [980/2467]	Loss: 0.153850
     Train Epoch: [1000/2467]	Loss: 0.261123
     Train Epoch: [1000/2467]	Loss: 0.044708
     Train Epoch: [1000/2467]	Loss: 0.066181
     Train Epoch: [1000/2467]	Loss: 0.374195
             Train Epoch: [1020/2467]	Loss: 0.168413
 Train Epoch: [1020/2467]	Loss: 0.090380 
Train Epoch: [1020/2467]	Loss: 0.164281    
 Train Epoch: [1020/2467]	Loss: 0.429618
                   Train Epoch: [1040/2467]	Loss: 0.234361Train Epoch: [1040/2467]	Loss: 0.294070Train Epoch: [1040/2467]	Loss: 0.113713


 Train Epoch: [1040/2467]	Loss: 0.198995
     Train Epoch: [1060/2467]	Loss: 0.148445
          Train Epoch: [1060/2467]	Loss: 0.300343Train Epoch: [1060/2467]	Loss: 0.342652

     Train Epoch: [1060/2467]	Loss: 0.496192
             Train Epoch: [1080/2467]	Loss: 0.295332
  Train Epoch: [1080/2467]	Loss: 0.380915Train Epoch: [1080/2467]	Loss: 0.138269

     Train Epoch: [1080/2467]	Loss: 0.203616
     Train Epoch: [1100/2467]	Loss: 0.278050
             Train Epoch: [1100/2467]	Loss: 0.284804
 Train Epoch: [1100/2467]	Loss: 0.272126 
Train Epoch: [1100/2467]	Loss: 0.300895
               Train Epoch: [1120/2467]	Loss: 0.140792Train Epoch: [1120/2467]	Loss: 0.305461Train Epoch: [1120/2467]	Loss: 0.172488


     Train Epoch: [1120/2467]	Loss: 0.132555
                   Train Epoch: [1140/2467]	Loss: 0.435220Train Epoch: [1140/2467]	Loss: 0.077422 

Train Epoch: [1140/2467]	Loss: 0.099966Train Epoch: [1140/2467]	Loss: 0.269502

              Train Epoch: [1160/2467]	Loss: 0.121882 Train Epoch: [1160/2467]	Loss: 0.083217

Train Epoch: [1160/2467]	Loss: 0.227861
     Train Epoch: [1160/2467]	Loss: 0.222134
             Train Epoch: [1180/2467]	Loss: 0.395166 
Train Epoch: [1180/2467]	Loss: 0.177390
 Train Epoch: [1180/2467]	Loss: 0.259755
     Train Epoch: [1180/2467]	Loss: 0.095943
     Train Epoch: [1200/2467]	Loss: 0.188618
         Train Epoch: [1200/2467]	Loss: 0.191178
 Train Epoch: [1200/2467]	Loss: 0.372942
     Train Epoch: [1200/2467]	Loss: 0.249580
     Train Epoch: [1220/2467]	Loss: 0.135397    
      Train Epoch: [1220/2467]	Loss: 0.253344Train Epoch: [1220/2467]	Loss: 0.313288    

 Train Epoch: [1220/2467]	Loss: 0.371955
         Train Epoch: [1240/2467]	Loss: 0.325867
     Train Epoch: [1240/2467]	Loss: 0.261898
 Train Epoch: [1240/2467]	Loss: 0.254248
     Train Epoch: [1240/2467]	Loss: 0.398629
     Train Epoch: [1260/2467]	Loss: 0.191613    
 Train Epoch: [1260/2467]	Loss: 0.466686
     Train Epoch: [1260/2467]	Loss: 0.086495
     Train Epoch: [1260/2467]	Loss: 0.077249
     Train Epoch: [1280/2467]	Loss: 0.097222
              Train Epoch: [1280/2467]	Loss: 0.461181Train Epoch: [1280/2467]	Loss: 0.840753

 Train Epoch: [1280/2467]	Loss: 0.304323
                 Train Epoch: [1300/2467]	Loss: 0.279642
  Train Epoch: [1300/2467]	Loss: 0.109824
Train Epoch: [1300/2467]	Loss: 0.087276
 Train Epoch: [1300/2467]	Loss: 0.370082
         Train Epoch: [1320/2467]	Loss: 0.311877
 Train Epoch: [1320/2467]	Loss: 0.075473
          Train Epoch: [1320/2467]	Loss: 0.176805Train Epoch: [1320/2467]	Loss: 0.311967

             Train Epoch: [1340/2467]	Loss: 0.121022
 Train Epoch: [1340/2467]	Loss: 0.401839
 Train Epoch: [1340/2467]	Loss: 0.182663
     Train Epoch: [1340/2467]	Loss: 0.275350
         Train Epoch: [1360/2467]	Loss: 0.208302    
     Train Epoch: [1360/2467]	Loss: 0.125953 
Train Epoch: [1360/2467]	Loss: 0.083346
 Train Epoch: [1360/2467]	Loss: 0.123557
     Train Epoch: [1380/2467]	Loss: 0.234063    
 Train Epoch: [1380/2467]	Loss: 0.380257
          Train Epoch: [1380/2467]	Loss: 0.451718Train Epoch: [1380/2467]	Loss: 0.350430

         Train Epoch: [1400/2467]	Loss: 0.324417
     Train Epoch: [1400/2467]	Loss: 0.209050
 Train Epoch: [1400/2467]	Loss: 0.224876
     Train Epoch: [1400/2467]	Loss: 0.119082
         Train Epoch: [1420/2467]	Loss: 0.176117
 Train Epoch: [1420/2467]	Loss: 0.098386
         Train Epoch: [1420/2467]	Loss: 0.181647
 Train Epoch: [1420/2467]	Loss: 0.439051
             Train Epoch: [1440/2467]	Loss: 0.253583
 Train Epoch: [1440/2467]	Loss: 0.053397
     Train Epoch: [1440/2467]	Loss: 0.077930
 Train Epoch: [1440/2467]	Loss: 0.039614
     Train Epoch: [1460/2467]	Loss: 0.183719    
 Train Epoch: [1460/2467]	Loss: 0.115860
         Train Epoch: [1460/2467]	Loss: 0.049900
 Train Epoch: [1460/2467]	Loss: 0.301458
         Train Epoch: [1480/2467]	Loss: 0.303790 
Train Epoch: [1480/2467]	Loss: 0.058115
     Train Epoch: [1480/2467]	Loss: 0.020002
     Train Epoch: [1480/2467]	Loss: 0.208983
         Train Epoch: [1500/2467]	Loss: 0.108085    
  Train Epoch: [1500/2467]	Loss: 0.036254Train Epoch: [1500/2467]	Loss: 0.223997

     Train Epoch: [1500/2467]	Loss: 0.142564
         Train Epoch: [1520/2467]	Loss: 0.176077
     Train Epoch: [1520/2467]	Loss: 0.253623
     Train Epoch: [1520/2467]	Loss: 0.274477
 Train Epoch: [1520/2467]	Loss: 0.234368
     Train Epoch: [1540/2467]	Loss: 0.306723
     Train Epoch: [1540/2467]	Loss: 0.083514
     Train Epoch: [1540/2467]	Loss: 0.178367
     Train Epoch: [1540/2467]	Loss: 0.348260
                 Train Epoch: [1560/2467]	Loss: 0.542509
   Train Epoch: [1560/2467]	Loss: 0.369888Train Epoch: [1560/2467]	Loss: 0.116461
Train Epoch: [1560/2467]	Loss: 0.481530

         Train Epoch: [1580/2467]	Loss: 0.218379
         Train Epoch: [1580/2467]	Loss: 0.243951
  Train Epoch: [1580/2467]	Loss: 0.166913
Train Epoch: [1580/2467]	Loss: 0.308122
     Train Epoch: [1600/2467]	Loss: 0.381983
         Train Epoch: [1600/2467]	Loss: 0.096139 
Train Epoch: [1600/2467]	Loss: 0.267977
     Train Epoch: [1600/2467]	Loss: 0.130044
             Train Epoch: [1620/2467]	Loss: 0.246286
 Train Epoch: [1620/2467]	Loss: 0.102233
 Train Epoch: [1620/2467]	Loss: 0.242312    
 Train Epoch: [1620/2467]	Loss: 0.054148
     Train Epoch: [1640/2467]	Loss: 0.205087
         Train Epoch: [1640/2467]	Loss: 0.292960
 Train Epoch: [1640/2467]	Loss: 0.260744
     Train Epoch: [1640/2467]	Loss: 0.133669
     Train Epoch: [1660/2467]	Loss: 0.308540
              Train Epoch: [1660/2467]	Loss: 0.186825
Train Epoch: [1660/2467]	Loss: 0.241130
 Train Epoch: [1660/2467]	Loss: 0.148945
         Train Epoch: [1680/2467]	Loss: 0.075842
         Train Epoch: [1680/2467]	Loss: 0.242021
 Train Epoch: [1680/2467]	Loss: 0.239858
 Train Epoch: [1680/2467]	Loss: 0.217798
             Train Epoch: [1700/2467]	Loss: 0.108240
  Train Epoch: [1700/2467]	Loss: 0.240662
Train Epoch: [1700/2467]	Loss: 0.315701
     Train Epoch: [1700/2467]	Loss: 0.201218
             Train Epoch: [1720/2467]	Loss: 0.192515
 Train Epoch: [1720/2467]	Loss: 0.243281
 Train Epoch: [1720/2467]	Loss: 0.247103
     Train Epoch: [1720/2467]	Loss: 0.092087
     Train Epoch: [1740/2467]	Loss: 0.516871
         Train Epoch: [1740/2467]	Loss: 0.144133
 Train Epoch: [1740/2467]	Loss: 0.212587
     Train Epoch: [1740/2467]	Loss: 0.042252
         Train Epoch: [1760/2467]	Loss: 0.191128
 Train Epoch: [1760/2467]	Loss: 0.170829
          Train Epoch: [1760/2467]	Loss: 0.067183Train Epoch: [1760/2467]	Loss: 0.119366

     Train Epoch: [1780/2467]	Loss: 0.395154
     Train Epoch: [1780/2467]	Loss: 0.327547
          Train Epoch: [1780/2467]	Loss: 0.366844Train Epoch: [1780/2467]	Loss: 0.112068

     Train Epoch: [1800/2467]	Loss: 0.291192
             Train Epoch: [1800/2467]	Loss: 0.307544
  Train Epoch: [1800/2467]	Loss: 0.196983
Train Epoch: [1800/2467]	Loss: 0.272792
             Train Epoch: [1820/2467]	Loss: 0.194263 
Train Epoch: [1820/2467]	Loss: 0.378238
     Train Epoch: [1820/2467]	Loss: 0.158528
 Train Epoch: [1820/2467]	Loss: 0.134048
              Train Epoch: [1840/2467]	Loss: 0.073052
Train Epoch: [1840/2467]	Loss: 0.294621 
Train Epoch: [1840/2467]	Loss: 0.314443    
 Train Epoch: [1840/2467]	Loss: 0.144317
     Train Epoch: [1860/2467]	Loss: 0.264520    
     Train Epoch: [1860/2467]	Loss: 0.240439 
Train Epoch: [1860/2467]	Loss: 0.227276
     Train Epoch: [1860/2467]	Loss: 0.166305
     Train Epoch: [1880/2467]	Loss: 0.124327
              Train Epoch: [1880/2467]	Loss: 0.052238
Train Epoch: [1880/2467]	Loss: 0.100214
 Train Epoch: [1880/2467]	Loss: 0.082963
             Train Epoch: [1900/2467]	Loss: 0.281208 
    Train Epoch: [1900/2467]	Loss: 0.354717 
Train Epoch: [1900/2467]	Loss: 0.203372
 Train Epoch: [1900/2467]	Loss: 0.151922
     Train Epoch: [1920/2467]	Loss: 0.269122
     Train Epoch: [1920/2467]	Loss: 0.015436    
 Train Epoch: [1920/2467]	Loss: 0.267758
     Train Epoch: [1920/2467]	Loss: 0.182869
     Train Epoch: [1940/2467]	Loss: 0.095267
     Train Epoch: [1940/2467]	Loss: 0.308286
     Train Epoch: [1940/2467]	Loss: 0.450807
     Train Epoch: [1940/2467]	Loss: 0.121761
         Train Epoch: [1960/2467]	Loss: 0.127750
     Train Epoch: [1960/2467]	Loss: 0.260468
 Train Epoch: [1960/2467]	Loss: 0.255058
     Train Epoch: [1960/2467]	Loss: 0.250659
     Train Epoch: [1980/2467]	Loss: 0.146689
             Train Epoch: [1980/2467]	Loss: 0.251088
  Train Epoch: [1980/2467]	Loss: 0.304378Train Epoch: [1980/2467]	Loss: 0.302957

         Train Epoch: [2000/2467]	Loss: 0.288712    
      Train Epoch: [2000/2467]	Loss: 0.159215Train Epoch: [2000/2467]	Loss: 0.051740

 Train Epoch: [2000/2467]	Loss: 0.090981
         Train Epoch: [2020/2467]	Loss: 0.270093
         Train Epoch: [2020/2467]	Loss: 0.204945
  Train Epoch: [2020/2467]	Loss: 0.356902Train Epoch: [2020/2467]	Loss: 0.098097

     Train Epoch: [2040/2467]	Loss: 0.231030
         Train Epoch: [2040/2467]	Loss: 0.146781 
Train Epoch: [2040/2467]	Loss: 0.407181
     Train Epoch: [2040/2467]	Loss: 0.105715
             Train Epoch: [2060/2467]	Loss: 0.158367
     Train Epoch: [2060/2467]	Loss: 0.185956
 Train Epoch: [2060/2467]	Loss: 0.242873
 Train Epoch: [2060/2467]	Loss: 0.226210
     Train Epoch: [2080/2467]	Loss: 0.324373    
     Train Epoch: [2080/2467]	Loss: 0.099416
 Train Epoch: [2080/2467]	Loss: 0.065025
     Train Epoch: [2080/2467]	Loss: 0.128574
         Train Epoch: [2100/2467]	Loss: 0.157872    
     Train Epoch: [2100/2467]	Loss: 0.199370 
Train Epoch: [2100/2467]	Loss: 0.130096
 Train Epoch: [2100/2467]	Loss: 0.544661
     Train Epoch: [2120/2467]	Loss: 0.183299
     Train Epoch: [2120/2467]	Loss: 0.061827
     Train Epoch: [2120/2467]	Loss: 0.271292
     Train Epoch: [2120/2467]	Loss: 0.118622
          Train Epoch: [2140/2467]	Loss: 0.597507Train Epoch: [2140/2467]	Loss: 0.375550

     Train Epoch: [2140/2467]	Loss: 0.410658
     Train Epoch: [2140/2467]	Loss: 0.208992
          Train Epoch: [2160/2467]	Loss: 0.315931Train Epoch: [2160/2467]	Loss: 0.184601

     Train Epoch: [2160/2467]	Loss: 0.069920
     Train Epoch: [2160/2467]	Loss: 0.237585
         Train Epoch: [2180/2467]	Loss: 0.346088    
     Train Epoch: [2180/2467]	Loss: 0.242403
  Train Epoch: [2180/2467]	Loss: 0.166878
Train Epoch: [2180/2467]	Loss: 0.130813
              Train Epoch: [2200/2467]	Loss: 0.289842
Train Epoch: [2200/2467]	Loss: 0.144197    
 Train Epoch: [2200/2467]	Loss: 0.132992 
Train Epoch: [2200/2467]	Loss: 0.153298
         Train Epoch: [2220/2467]	Loss: 0.222129
         Train Epoch: [2220/2467]	Loss: 0.124735
  Train Epoch: [2220/2467]	Loss: 0.083354
Train Epoch: [2220/2467]	Loss: 0.162802
             Train Epoch: [2240/2467]	Loss: 0.215174
 Train Epoch: [2240/2467]	Loss: 0.039765 
Train Epoch: [2240/2467]	Loss: 0.091619    
 Train Epoch: [2240/2467]	Loss: 0.078600
     Train Epoch: [2260/2467]	Loss: 0.245075
         Train Epoch: [2260/2467]	Loss: 0.272662
 Train Epoch: [2260/2467]	Loss: 0.435157    
 Train Epoch: [2260/2467]	Loss: 0.054529
     Train Epoch: [2280/2467]	Loss: 0.095854    
         Train Epoch: [2280/2467]	Loss: 0.180847
  Train Epoch: [2280/2467]	Loss: 0.108243Train Epoch: [2280/2467]	Loss: 0.113284

          Train Epoch: [2300/2467]	Loss: 0.060472
Train Epoch: [2300/2467]	Loss: 0.328234
     Train Epoch: [2300/2467]	Loss: 0.113002
     Train Epoch: [2300/2467]	Loss: 0.131005
         Train Epoch: [2320/2467]	Loss: 0.231506
 Train Epoch: [2320/2467]	Loss: 0.031633    
      Train Epoch: [2320/2467]	Loss: 0.289933
Train Epoch: [2320/2467]	Loss: 0.186106
         Train Epoch: [2340/2467]	Loss: 0.230587     
Train Epoch: [2340/2467]	Loss: 0.538771
 Train Epoch: [2340/2467]	Loss: 0.139144
     Train Epoch: [2340/2467]	Loss: 0.153399
              Train Epoch: [2360/2467]	Loss: 0.156503Train Epoch: [2360/2467]	Loss: 0.361795

 Train Epoch: [2360/2467]	Loss: 0.079444
     Train Epoch: [2360/2467]	Loss: 0.162150
     Train Epoch: [2380/2467]	Loss: 0.514737
              Train Epoch: [2380/2467]	Loss: 0.274777Train Epoch: [2380/2467]	Loss: 0.208901
 
Train Epoch: [2380/2467]	Loss: 0.132652
     Train Epoch: [2400/2467]	Loss: 0.115805
             Train Epoch: [2400/2467]	Loss: 0.166079
  Train Epoch: [2400/2467]	Loss: 0.189769
Train Epoch: [2400/2467]	Loss: 0.287901
     Train Epoch: [2420/2467]	Loss: 0.249939
              Train Epoch: [2420/2467]	Loss: 0.154007Train Epoch: [2420/2467]	Loss: 0.323846
 
Train Epoch: [2420/2467]	Loss: 0.288024
             Train Epoch: [2440/2467]	Loss: 0.157319
  Train Epoch: [2440/2467]	Loss: 0.309464Train Epoch: [2440/2467]	Loss: 0.053783

     Train Epoch: [2440/2467]	Loss: 0.154848
         Train Epoch: [2460/2467]	Loss: 0.116688
     Train Epoch: [2460/2467]	Loss: 0.393768 
Train Epoch: [2460/2467]	Loss: 0.122351
     Train Epoch: [2460/2467]	Loss: 0.148347
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 16 epoch =====
     2025-05-11.05-52-27
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 16 epoch =====
     2025-05-11.05-52-27
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 16 epoch =====
     2025-05-11.05-52-27
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 16 epoch =====
     2025-05-11.05-52-27
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.200613
          Train Epoch: [0/2467]	Loss: 0.310574Train Epoch: [0/2467]	Loss: 0.153201

     Train Epoch: [0/2467]	Loss: 0.281484
         Train Epoch: [20/2467]	Loss: 0.380756
 Train Epoch: [20/2467]	Loss: 0.261797    
     Train Epoch: [20/2467]	Loss: 0.115311
 Train Epoch: [20/2467]	Loss: 0.247004
         Train Epoch: [40/2467]	Loss: 0.404545     
Train Epoch: [40/2467]	Loss: 0.156135
 Train Epoch: [40/2467]	Loss: 0.221088
     Train Epoch: [40/2467]	Loss: 0.187975
     Train Epoch: [60/2467]	Loss: 0.058853
         Train Epoch: [60/2467]	Loss: 0.265269
 Train Epoch: [60/2467]	Loss: 0.204419
     Train Epoch: [60/2467]	Loss: 0.164946
          Train Epoch: [80/2467]	Loss: 0.119087Train Epoch: [80/2467]	Loss: 0.130992
    
      Train Epoch: [80/2467]	Loss: 0.349542Train Epoch: [80/2467]	Loss: 0.126669

         Train Epoch: [100/2467]	Loss: 0.250782
 Train Epoch: [100/2467]	Loss: 0.207072
     Train Epoch: [100/2467]	Loss: 0.134726    
 Train Epoch: [100/2467]	Loss: 0.281371
         Train Epoch: [120/2467]	Loss: 0.404513
 Train Epoch: [120/2467]	Loss: 0.325432
     Train Epoch: [120/2467]	Loss: 0.087752
     Train Epoch: [120/2467]	Loss: 0.092907
         Train Epoch: [140/2467]	Loss: 0.271290 
    Train Epoch: [140/2467]	Loss: 0.316236
 Train Epoch: [140/2467]	Loss: 0.052262
     Train Epoch: [140/2467]	Loss: 0.088870
              Train Epoch: [160/2467]	Loss: 0.350553Train Epoch: [160/2467]	Loss: 0.140793 

Train Epoch: [160/2467]	Loss: 0.268541
     Train Epoch: [160/2467]	Loss: 0.209777
              Train Epoch: [180/2467]	Loss: 0.139964
Train Epoch: [180/2467]	Loss: 0.197101
     Train Epoch: [180/2467]	Loss: 0.171788
 Train Epoch: [180/2467]	Loss: 0.164677
     Train Epoch: [200/2467]	Loss: 0.136345    
         Train Epoch: [200/2467]	Loss: 0.132603 
Train Epoch: [200/2467]	Loss: 0.120760
 Train Epoch: [200/2467]	Loss: 0.202434
          Train Epoch: [220/2467]	Loss: 0.110365    Train Epoch: [220/2467]	Loss: 0.219394

      Train Epoch: [220/2467]	Loss: 0.231965
Train Epoch: [220/2467]	Loss: 0.213690
     Train Epoch: [240/2467]	Loss: 0.425047    
         Train Epoch: [240/2467]	Loss: 0.044860
 Train Epoch: [240/2467]	Loss: 0.027812
 Train Epoch: [240/2467]	Loss: 0.200990
         Train Epoch: [260/2467]	Loss: 0.082341    
 Train Epoch: [260/2467]	Loss: 0.143640
 Train Epoch: [260/2467]	Loss: 0.059222
     Train Epoch: [260/2467]	Loss: 0.051147
     Train Epoch: [280/2467]	Loss: 0.292272
         Train Epoch: [280/2467]	Loss: 0.102442    
 Train Epoch: [280/2467]	Loss: 0.179849
 Train Epoch: [280/2467]	Loss: 0.234159
     Train Epoch: [300/2467]	Loss: 0.045177
         Train Epoch: [300/2467]	Loss: 0.415018
 Train Epoch: [300/2467]	Loss: 0.171019
     Train Epoch: [300/2467]	Loss: 0.132755
         Train Epoch: [320/2467]	Loss: 0.299679    
 Train Epoch: [320/2467]	Loss: 0.500834
 Train Epoch: [320/2467]	Loss: 0.182247
     Train Epoch: [320/2467]	Loss: 0.183584
     Train Epoch: [340/2467]	Loss: 0.066767
         Train Epoch: [340/2467]	Loss: 0.158723
 Train Epoch: [340/2467]	Loss: 0.017330
     Train Epoch: [340/2467]	Loss: 0.222503
     Train Epoch: [360/2467]	Loss: 0.113882
         Train Epoch: [360/2467]	Loss: 0.254777
 Train Epoch: [360/2467]	Loss: 0.203859
     Train Epoch: [360/2467]	Loss: 0.077030
     Train Epoch: [380/2467]	Loss: 0.114135
         Train Epoch: [380/2467]	Loss: 0.382123
 Train Epoch: [380/2467]	Loss: 0.411870
     Train Epoch: [380/2467]	Loss: 0.223251
     Train Epoch: [400/2467]	Loss: 0.200249    
     Train Epoch: [400/2467]	Loss: 0.102014 
Train Epoch: [400/2467]	Loss: 0.104509
     Train Epoch: [400/2467]	Loss: 0.398672
     Train Epoch: [420/2467]	Loss: 0.151476
     Train Epoch: [420/2467]	Loss: 0.227889
     Train Epoch: [420/2467]	Loss: 0.141299    
 Train Epoch: [420/2467]	Loss: 0.288004
         Train Epoch: [440/2467]	Loss: 0.074982 
Train Epoch: [440/2467]	Loss: 0.179097
     Train Epoch: [440/2467]	Loss: 0.096941
     Train Epoch: [440/2467]	Loss: 0.378112
         Train Epoch: [460/2467]	Loss: 0.197936
      Train Epoch: [460/2467]	Loss: 0.152705Train Epoch: [460/2467]	Loss: 0.216502

     Train Epoch: [460/2467]	Loss: 0.290942
     Train Epoch: [480/2467]	Loss: 0.249551
             Train Epoch: [480/2467]	Loss: 0.180630 
Train Epoch: [480/2467]	Loss: 0.306416 
Train Epoch: [480/2467]	Loss: 0.349633
     Train Epoch: [500/2467]	Loss: 0.125055
         Train Epoch: [500/2467]	Loss: 0.278021    
 Train Epoch: [500/2467]	Loss: 0.232992 
Train Epoch: [500/2467]	Loss: 0.145531
     Train Epoch: [520/2467]	Loss: 0.325464
             Train Epoch: [520/2467]	Loss: 0.160348
  Train Epoch: [520/2467]	Loss: 0.173445
Train Epoch: [520/2467]	Loss: 0.083224
     Train Epoch: [540/2467]	Loss: 0.300367
          Train Epoch: [540/2467]	Loss: 0.181259
Train Epoch: [540/2467]	Loss: 0.206449
     Train Epoch: [540/2467]	Loss: 0.068816
             Train Epoch: [560/2467]	Loss: 0.324840
      Train Epoch: [560/2467]	Loss: 0.155465Train Epoch: [560/2467]	Loss: 0.223255

 Train Epoch: [560/2467]	Loss: 0.149554
         Train Epoch: [580/2467]	Loss: 0.145094
     Train Epoch: [580/2467]	Loss: 0.066714 
Train Epoch: [580/2467]	Loss: 0.376160
     Train Epoch: [580/2467]	Loss: 0.191825
     Train Epoch: [600/2467]	Loss: 0.143731
         Train Epoch: [600/2467]	Loss: 0.220706
 Train Epoch: [600/2467]	Loss: 0.205463
     Train Epoch: [600/2467]	Loss: 0.230565
         Train Epoch: [620/2467]	Loss: 0.031769
     Train Epoch: [620/2467]	Loss: 0.268038
 Train Epoch: [620/2467]	Loss: 0.159087
     Train Epoch: [620/2467]	Loss: 0.309662
             Train Epoch: [640/2467]	Loss: 0.232380  
Train Epoch: [640/2467]	Loss: 0.291837Train Epoch: [640/2467]	Loss: 0.066690

     Train Epoch: [640/2467]	Loss: 0.142403
             Train Epoch: [660/2467]	Loss: 0.064465
 Train Epoch: [660/2467]	Loss: 0.259700    
 Train Epoch: [660/2467]	Loss: 0.498698
 Train Epoch: [660/2467]	Loss: 0.038537
                  Train Epoch: [680/2467]	Loss: 0.060253
Train Epoch: [680/2467]	Loss: 0.174914
  Train Epoch: [680/2467]	Loss: 0.145568Train Epoch: [680/2467]	Loss: 0.070792

     Train Epoch: [700/2467]	Loss: 0.361726
              Train Epoch: [700/2467]	Loss: 0.454235Train Epoch: [700/2467]	Loss: 0.056442

 Train Epoch: [700/2467]	Loss: 0.100947
             Train Epoch: [720/2467]	Loss: 0.243404
      Train Epoch: [720/2467]	Loss: 0.149769Train Epoch: [720/2467]	Loss: 0.192618

 Train Epoch: [720/2467]	Loss: 0.028700
         Train Epoch: [740/2467]	Loss: 0.059281
 Train Epoch: [740/2467]	Loss: 0.189906
     Train Epoch: [740/2467]	Loss: 0.069805
     Train Epoch: [740/2467]	Loss: 0.341365
              Train Epoch: [760/2467]	Loss: 0.390427 
Train Epoch: [760/2467]	Loss: 0.540389Train Epoch: [760/2467]	Loss: 0.274083

     Train Epoch: [760/2467]	Loss: 0.080779
             Train Epoch: [780/2467]	Loss: 0.385509
 Train Epoch: [780/2467]	Loss: 0.369340
 Train Epoch: [780/2467]	Loss: 0.390417
     Train Epoch: [780/2467]	Loss: 0.192002
             Train Epoch: [800/2467]	Loss: 0.118146 
Train Epoch: [800/2467]	Loss: 0.212403
 Train Epoch: [800/2467]	Loss: 0.279066
     Train Epoch: [800/2467]	Loss: 0.247926
     Train Epoch: [820/2467]	Loss: 0.449763
         Train Epoch: [820/2467]	Loss: 0.049832
 Train Epoch: [820/2467]	Loss: 0.066959
     Train Epoch: [820/2467]	Loss: 0.353113
     Train Epoch: [840/2467]	Loss: 0.122080
         Train Epoch: [840/2467]	Loss: 0.387760
 Train Epoch: [840/2467]	Loss: 0.163684
     Train Epoch: [840/2467]	Loss: 0.082704
         Train Epoch: [860/2467]	Loss: 0.222690
     Train Epoch: [860/2467]	Loss: 0.088508
 Train Epoch: [860/2467]	Loss: 0.289242
     Train Epoch: [860/2467]	Loss: 0.081391
         Train Epoch: [880/2467]	Loss: 0.237125
         Train Epoch: [880/2467]	Loss: 0.251754
 Train Epoch: [880/2467]	Loss: 0.260954 
Train Epoch: [880/2467]	Loss: 0.150548
         Train Epoch: [900/2467]	Loss: 0.199339
 Train Epoch: [900/2467]	Loss: 0.112134
     Train Epoch: [900/2467]	Loss: 0.204087
     Train Epoch: [900/2467]	Loss: 0.205089
          Train Epoch: [920/2467]	Loss: 0.335009
Train Epoch: [920/2467]	Loss: 0.050331
     Train Epoch: [920/2467]	Loss: 0.388710
     Train Epoch: [920/2467]	Loss: 0.438388
     Train Epoch: [940/2467]	Loss: 0.100158
         Train Epoch: [940/2467]	Loss: 0.048469 
    Train Epoch: [940/2467]	Loss: 0.240197
 Train Epoch: [940/2467]	Loss: 0.138366
             Train Epoch: [960/2467]	Loss: 0.083027
  Train Epoch: [960/2467]	Loss: 0.082584Train Epoch: [960/2467]	Loss: 0.095025

     Train Epoch: [960/2467]	Loss: 0.402171
             Train Epoch: [980/2467]	Loss: 0.113975
      Train Epoch: [980/2467]	Loss: 0.167508
Train Epoch: [980/2467]	Loss: 0.132367
 Train Epoch: [980/2467]	Loss: 0.268309
         Train Epoch: [1000/2467]	Loss: 0.241822
 Train Epoch: [1000/2467]	Loss: 0.322655
         Train Epoch: [1000/2467]	Loss: 0.039464 
Train Epoch: [1000/2467]	Loss: 0.061244
         Train Epoch: [1020/2467]	Loss: 0.456891
         Train Epoch: [1020/2467]	Loss: 0.141157
 Train Epoch: [1020/2467]	Loss: 0.086619 
Train Epoch: [1020/2467]	Loss: 0.167185
     Train Epoch: [1040/2467]	Loss: 0.112947
          Train Epoch: [1040/2467]	Loss: 0.164697Train Epoch: [1040/2467]	Loss: 0.259297

     Train Epoch: [1040/2467]	Loss: 0.285646
         Train Epoch: [1060/2467]	Loss: 0.233756
     Train Epoch: [1060/2467]	Loss: 0.113674    
  Train Epoch: [1060/2467]	Loss: 0.321749Train Epoch: [1060/2467]	Loss: 0.461404

         Train Epoch: [1080/2467]	Loss: 0.330392
         Train Epoch: [1080/2467]	Loss: 0.314621
 Train Epoch: [1080/2467]	Loss: 0.140330 
Train Epoch: [1080/2467]	Loss: 0.157163
         Train Epoch: [1100/2467]	Loss: 0.275445
 Train Epoch: [1100/2467]	Loss: 0.227488
     Train Epoch: [1100/2467]	Loss: 0.293910
     Train Epoch: [1100/2467]	Loss: 0.336427
     Train Epoch: [1120/2467]	Loss: 0.144737
              Train Epoch: [1120/2467]	Loss: 0.156752
Train Epoch: [1120/2467]	Loss: 0.269036
 Train Epoch: [1120/2467]	Loss: 0.132514
     Train Epoch: [1140/2467]	Loss: 0.462558
              Train Epoch: [1140/2467]	Loss: 0.074423Train Epoch: [1140/2467]	Loss: 0.091746

 Train Epoch: [1140/2467]	Loss: 0.284936
          Train Epoch: [1160/2467]	Loss: 0.112621    
Train Epoch: [1160/2467]	Loss: 0.079223
     Train Epoch: [1160/2467]	Loss: 0.224411
 Train Epoch: [1160/2467]	Loss: 0.201517
         Train Epoch: [1180/2467]	Loss: 0.259319    
     Train Epoch: [1180/2467]	Loss: 0.422494 
 Train Epoch: [1180/2467]	Loss: 0.067446
Train Epoch: [1180/2467]	Loss: 0.169699
     Train Epoch: [1200/2467]	Loss: 0.182222    
     Train Epoch: [1200/2467]	Loss: 0.289903
     Train Epoch: [1200/2467]	Loss: 0.142622
 Train Epoch: [1200/2467]	Loss: 0.346015
         Train Epoch: [1220/2467]	Loss: 0.115319
     Train Epoch: [1220/2467]	Loss: 0.399465
 Train Epoch: [1220/2467]	Loss: 0.247356
     Train Epoch: [1220/2467]	Loss: 0.302779
             Train Epoch: [1240/2467]	Loss: 0.328929 
Train Epoch: [1240/2467]	Loss: 0.248580 
Train Epoch: [1240/2467]	Loss: 0.210692
     Train Epoch: [1240/2467]	Loss: 0.338439
              Train Epoch: [1260/2467]	Loss: 0.086489
Train Epoch: [1260/2467]	Loss: 0.058962
 Train Epoch: [1260/2467]	Loss: 0.476553
     Train Epoch: [1260/2467]	Loss: 0.143626
         Train Epoch: [1280/2467]	Loss: 0.282379
     Train Epoch: [1280/2467]	Loss: 0.853641
 Train Epoch: [1280/2467]	Loss: 0.092529
     Train Epoch: [1280/2467]	Loss: 0.463024
         Train Epoch: [1300/2467]	Loss: 0.363860    
 Train Epoch: [1300/2467]	Loss: 0.330785
 Train Epoch: [1300/2467]	Loss: 0.082872
     Train Epoch: [1300/2467]	Loss: 0.088685
     Train Epoch: [1320/2467]	Loss: 0.239796
          Train Epoch: [1320/2467]	Loss: 0.363807    Train Epoch: [1320/2467]	Loss: 0.081050

 Train Epoch: [1320/2467]	Loss: 0.153252
         Train Epoch: [1340/2467]	Loss: 0.144812    
 Train Epoch: [1340/2467]	Loss: 0.189671 
    Train Epoch: [1340/2467]	Loss: 0.248532
 Train Epoch: [1340/2467]	Loss: 0.347171
          Train Epoch: [1360/2467]	Loss: 0.188292Train Epoch: [1360/2467]	Loss: 0.099584

         Train Epoch: [1360/2467]	Loss: 0.105300
 Train Epoch: [1360/2467]	Loss: 0.154407
     Train Epoch: [1380/2467]	Loss: 0.335473    
 Train Epoch: [1380/2467]	Loss: 0.263658
     Train Epoch: [1380/2467]	Loss: 0.336818
     Train Epoch: [1380/2467]	Loss: 0.237232
         Train Epoch: [1400/2467]	Loss: 0.255267
     Train Epoch: [1400/2467]	Loss: 0.135364
     Train Epoch: [1400/2467]	Loss: 0.221702
 Train Epoch: [1400/2467]	Loss: 0.094825
     Train Epoch: [1420/2467]	Loss: 0.155474
             Train Epoch: [1420/2467]	Loss: 0.158954
 Train Epoch: [1420/2467]	Loss: 0.178562 
Train Epoch: [1420/2467]	Loss: 0.431002
              Train Epoch: [1440/2467]	Loss: 0.185979Train Epoch: [1440/2467]	Loss: 0.078786

 Train Epoch: [1440/2467]	Loss: 0.049529
     Train Epoch: [1440/2467]	Loss: 0.028821
     Train Epoch: [1460/2467]	Loss: 0.114233
     Train Epoch: [1460/2467]	Loss: 0.052015
     Train Epoch: [1460/2467]	Loss: 0.167978
     Train Epoch: [1460/2467]	Loss: 0.317961
          Train Epoch: [1480/2467]	Loss: 0.282561Train Epoch: [1480/2467]	Loss: 0.228705

     Train Epoch: [1480/2467]	Loss: 0.045342
     Train Epoch: [1480/2467]	Loss: 0.016934
               Train Epoch: [1500/2467]	Loss: 0.021762Train Epoch: [1500/2467]	Loss: 0.183591
Train Epoch: [1500/2467]	Loss: 0.182910

     Train Epoch: [1500/2467]	Loss: 0.150790
     Train Epoch: [1520/2467]	Loss: 0.184122
             Train Epoch: [1520/2467]	Loss: 0.392339
  Train Epoch: [1520/2467]	Loss: 0.276641Train Epoch: [1520/2467]	Loss: 0.185295

         Train Epoch: [1540/2467]	Loss: 0.298911
     Train Epoch: [1540/2467]	Loss: 0.184034
 Train Epoch: [1540/2467]	Loss: 0.093358
     Train Epoch: [1540/2467]	Loss: 0.155156
         Train Epoch: [1560/2467]	Loss: 0.431746    
      Train Epoch: [1560/2467]	Loss: 0.337221Train Epoch: [1560/2467]	Loss: 0.423012

 Train Epoch: [1560/2467]	Loss: 0.525562
         Train Epoch: [1580/2467]	Loss: 0.505426
     Train Epoch: [1580/2467]	Loss: 0.293064
 Train Epoch: [1580/2467]	Loss: 0.247012
     Train Epoch: [1580/2467]	Loss: 0.208298
         Train Epoch: [1600/2467]	Loss: 0.288388
     Train Epoch: [1600/2467]	Loss: 0.203835 
Train Epoch: [1600/2467]	Loss: 0.369835    
 Train Epoch: [1600/2467]	Loss: 0.075532
          Train Epoch: [1620/2467]	Loss: 0.213035
Train Epoch: [1620/2467]	Loss: 0.108507
     Train Epoch: [1620/2467]	Loss: 0.044919
     Train Epoch: [1620/2467]	Loss: 0.242676
     Train Epoch: [1640/2467]	Loss: 0.170105
         Train Epoch: [1640/2467]	Loss: 0.285560 
Train Epoch: [1640/2467]	Loss: 0.281164
     Train Epoch: [1640/2467]	Loss: 0.132231
     Train Epoch: [1660/2467]	Loss: 0.255863
         Train Epoch: [1660/2467]	Loss: 0.155015
 Train Epoch: [1660/2467]	Loss: 0.249691
     Train Epoch: [1660/2467]	Loss: 0.148474
     Train Epoch: [1680/2467]	Loss: 0.245101
         Train Epoch: [1680/2467]	Loss: 0.055376
 Train Epoch: [1680/2467]	Loss: 0.451416
     Train Epoch: [1680/2467]	Loss: 0.230433
         Train Epoch: [1700/2467]	Loss: 0.081973
 Train Epoch: [1700/2467]	Loss: 0.278943
          Train Epoch: [1700/2467]	Loss: 0.237093
Train Epoch: [1700/2467]	Loss: 0.242419
             Train Epoch: [1720/2467]	Loss: 0.096534
 Train Epoch: [1720/2467]	Loss: 0.172691
 Train Epoch: [1720/2467]	Loss: 0.236231
     Train Epoch: [1720/2467]	Loss: 0.230064
             Train Epoch: [1740/2467]	Loss: 0.447505
 Train Epoch: [1740/2467]	Loss: 0.040271
 Train Epoch: [1740/2467]	Loss: 0.206490
     Train Epoch: [1740/2467]	Loss: 0.099883
     Train Epoch: [1760/2467]	Loss: 0.157709
         Train Epoch: [1760/2467]	Loss: 0.155968
 Train Epoch: [1760/2467]	Loss: 0.071687
     Train Epoch: [1760/2467]	Loss: 0.122345
              Train Epoch: [1780/2467]	Loss: 0.292576Train Epoch: [1780/2467]	Loss: 0.336320

 Train Epoch: [1780/2467]	Loss: 0.392699
     Train Epoch: [1780/2467]	Loss: 0.138555
     Train Epoch: [1800/2467]	Loss: 0.256180
         Train Epoch: [1800/2467]	Loss: 0.191523    
 Train Epoch: [1800/2467]	Loss: 0.374641 
Train Epoch: [1800/2467]	Loss: 0.437344
             Train Epoch: [1820/2467]	Loss: 0.379458
 Train Epoch: [1820/2467]	Loss: 0.144457    
 Train Epoch: [1820/2467]	Loss: 0.183126
 Train Epoch: [1820/2467]	Loss: 0.133360
         Train Epoch: [1840/2467]	Loss: 0.244790
     Train Epoch: [1840/2467]	Loss: 0.330296
 Train Epoch: [1840/2467]	Loss: 0.173530
     Train Epoch: [1840/2467]	Loss: 0.064577
         Train Epoch: [1860/2467]	Loss: 0.278673
 Train Epoch: [1860/2467]	Loss: 0.117766
         Train Epoch: [1860/2467]	Loss: 0.232663
 Train Epoch: [1860/2467]	Loss: 0.190381
             Train Epoch: [1880/2467]	Loss: 0.145826
 Train Epoch: [1880/2467]	Loss: 0.072261
 Train Epoch: [1880/2467]	Loss: 0.054213
     Train Epoch: [1880/2467]	Loss: 0.048687
          Train Epoch: [1900/2467]	Loss: 0.202021
Train Epoch: [1900/2467]	Loss: 0.295931
     Train Epoch: [1900/2467]	Loss: 0.152605
     Train Epoch: [1900/2467]	Loss: 0.369552
     Train Epoch: [1920/2467]	Loss: 0.251693
             Train Epoch: [1920/2467]	Loss: 0.020075
 Train Epoch: [1920/2467]	Loss: 0.271976 
Train Epoch: [1920/2467]	Loss: 0.191953
     Train Epoch: [1940/2467]	Loss: 0.079362
     Train Epoch: [1940/2467]	Loss: 0.241703
         Train Epoch: [1940/2467]	Loss: 0.132020
 Train Epoch: [1940/2467]	Loss: 0.446831
              Train Epoch: [1960/2467]	Loss: 0.198498Train Epoch: [1960/2467]	Loss: 0.177810

     Train Epoch: [1960/2467]	Loss: 0.216999
 Train Epoch: [1960/2467]	Loss: 0.366901
     Train Epoch: [1980/2467]	Loss: 0.258518    
 Train Epoch: [1980/2467]	Loss: 0.309028
     Train Epoch: [1980/2467]	Loss: 0.341417
     Train Epoch: [1980/2467]	Loss: 0.307808
         Train Epoch: [2000/2467]	Loss: 0.030507
         Train Epoch: [2000/2467]	Loss: 0.335124
  Train Epoch: [2000/2467]	Loss: 0.177675Train Epoch: [2000/2467]	Loss: 0.081194

     Train Epoch: [2020/2467]	Loss: 0.297662    
     Train Epoch: [2020/2467]	Loss: 0.076173 
Train Epoch: [2020/2467]	Loss: 0.195385
     Train Epoch: [2020/2467]	Loss: 0.277162
         Train Epoch: [2040/2467]	Loss: 0.275539
     Train Epoch: [2040/2467]	Loss: 0.107805
     Train Epoch: [2040/2467]	Loss: 0.124357
 Train Epoch: [2040/2467]	Loss: 0.386366
         Train Epoch: [2060/2467]	Loss: 0.182753    
      Train Epoch: [2060/2467]	Loss: 0.226221Train Epoch: [2060/2467]	Loss: 0.264568

 Train Epoch: [2060/2467]	Loss: 0.219723
     Train Epoch: [2080/2467]	Loss: 0.271923
          Train Epoch: [2080/2467]	Loss: 0.141686
Train Epoch: [2080/2467]	Loss: 0.065549
     Train Epoch: [2080/2467]	Loss: 0.121441
     Train Epoch: [2100/2467]	Loss: 0.127808    
         Train Epoch: [2100/2467]	Loss: 0.603282
 Train Epoch: [2100/2467]	Loss: 0.153588 
Train Epoch: [2100/2467]	Loss: 0.108627
     Train Epoch: [2120/2467]	Loss: 0.237592
             Train Epoch: [2120/2467]	Loss: 0.105431
  Train Epoch: [2120/2467]	Loss: 0.245131Train Epoch: [2120/2467]	Loss: 0.141181

             Train Epoch: [2140/2467]	Loss: 0.648877
 Train Epoch: [2140/2467]	Loss: 0.331338
     Train Epoch: [2140/2467]	Loss: 0.183360
 Train Epoch: [2140/2467]	Loss: 0.382198
         Train Epoch: [2160/2467]	Loss: 0.360349    
  Train Epoch: [2160/2467]	Loss: 0.056699
Train Epoch: [2160/2467]	Loss: 0.159922
     Train Epoch: [2160/2467]	Loss: 0.193675
         Train Epoch: [2180/2467]	Loss: 0.285475
      Train Epoch: [2180/2467]	Loss: 0.150311Train Epoch: [2180/2467]	Loss: 0.124849

     Train Epoch: [2180/2467]	Loss: 0.223447
         Train Epoch: [2200/2467]	Loss: 0.302704 
Train Epoch: [2200/2467]	Loss: 0.154024
     Train Epoch: [2200/2467]	Loss: 0.134979
     Train Epoch: [2200/2467]	Loss: 0.158647
               Train Epoch: [2220/2467]	Loss: 0.119751Train Epoch: [2220/2467]	Loss: 0.162555Train Epoch: [2220/2467]	Loss: 0.194172


     Train Epoch: [2220/2467]	Loss: 0.062943
             Train Epoch: [2240/2467]	Loss: 0.238053
  Train Epoch: [2240/2467]	Loss: 0.049492Train Epoch: [2240/2467]	Loss: 0.034421

     Train Epoch: [2240/2467]	Loss: 0.102629
         Train Epoch: [2260/2467]	Loss: 0.228050
 Train Epoch: [2260/2467]	Loss: 0.583605
         Train Epoch: [2260/2467]	Loss: 0.292387
 Train Epoch: [2260/2467]	Loss: 0.117755
     Train Epoch: [2280/2467]	Loss: 0.177435
     Train Epoch: [2280/2467]	Loss: 0.107446
     Train Epoch: [2280/2467]	Loss: 0.135978
     Train Epoch: [2280/2467]	Loss: 0.080546
          Train Epoch: [2300/2467]	Loss: 0.066828Train Epoch: [2300/2467]	Loss: 0.332362

     Train Epoch: [2300/2467]	Loss: 0.108600
     Train Epoch: [2300/2467]	Loss: 0.126177
     Train Epoch: [2320/2467]	Loss: 0.173391
          Train Epoch: [2320/2467]	Loss: 0.234174Train Epoch: [2320/2467]	Loss: 0.273815

     Train Epoch: [2320/2467]	Loss: 0.033139
         Train Epoch: [2340/2467]	Loss: 0.206991
 Train Epoch: [2340/2467]	Loss: 0.141322
         Train Epoch: [2340/2467]	Loss: 0.149894
 Train Epoch: [2340/2467]	Loss: 0.541438
     Train Epoch: [2360/2467]	Loss: 0.340985
             Train Epoch: [2360/2467]	Loss: 0.162091
  Train Epoch: [2360/2467]	Loss: 0.156604Train Epoch: [2360/2467]	Loss: 0.090168

         Train Epoch: [2380/2467]	Loss: 0.502892    
 Train Epoch: [2380/2467]	Loss: 0.194174
     Train Epoch: [2380/2467]	Loss: 0.194419
 Train Epoch: [2380/2467]	Loss: 0.231858
                 Train Epoch: [2400/2467]	Loss: 0.123714
 Train Epoch: [2400/2467]	Loss: 0.124726 
 Train Epoch: [2400/2467]	Loss: 0.186073Train Epoch: [2400/2467]	Loss: 0.259372

             Train Epoch: [2420/2467]	Loss: 0.245979
 Train Epoch: [2420/2467]	Loss: 0.364079
 Train Epoch: [2420/2467]	Loss: 0.108308
     Train Epoch: [2420/2467]	Loss: 0.275358
             Train Epoch: [2440/2467]	Loss: 0.298094
 Train Epoch: [2440/2467]	Loss: 0.156422 
Train Epoch: [2440/2467]	Loss: 0.163780
     Train Epoch: [2440/2467]	Loss: 0.054182
             Train Epoch: [2460/2467]	Loss: 0.110543    
   Train Epoch: [2460/2467]	Loss: 0.270811Train Epoch: [2460/2467]	Loss: 0.137137
Train Epoch: [2460/2467]	Loss: 0.154033

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 17 epoch =====
     2025-05-11.06-14-18
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 17 epoch =====
     2025-05-11.06-14-19
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 17 epoch =====
     2025-05-11.06-14-19
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 17 epoch =====
     2025-05-11.06-14-19
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
         Train Epoch: [0/2467]	Loss: 0.293042
 Train Epoch: [0/2467]	Loss: 0.204927
     Train Epoch: [0/2467]	Loss: 0.190001
     Train Epoch: [0/2467]	Loss: 0.292387
         Train Epoch: [20/2467]	Loss: 0.339252
 Train Epoch: [20/2467]	Loss: 0.227861
     Train Epoch: [20/2467]	Loss: 0.244502
     Train Epoch: [20/2467]	Loss: 0.121863
     Train Epoch: [40/2467]	Loss: 0.422362
          Train Epoch: [40/2467]	Loss: 0.192746Train Epoch: [40/2467]	Loss: 0.160907

     Train Epoch: [40/2467]	Loss: 0.337142
     Train Epoch: [60/2467]	Loss: 0.068145
     Train Epoch: [60/2467]	Loss: 0.178730
     Train Epoch: [60/2467]	Loss: 0.246263
     Train Epoch: [60/2467]	Loss: 0.189065
     Train Epoch: [80/2467]	Loss: 0.138858
              Train Epoch: [80/2467]	Loss: 0.322367 Train Epoch: [80/2467]	Loss: 0.129252

Train Epoch: [80/2467]	Loss: 0.185521
     Train Epoch: [100/2467]	Loss: 0.245398
         Train Epoch: [100/2467]	Loss: 0.266053
 Train Epoch: [100/2467]	Loss: 0.105973    
 Train Epoch: [100/2467]	Loss: 0.308060
         Train Epoch: [120/2467]	Loss: 0.087140
         Train Epoch: [120/2467]	Loss: 0.435816
  Train Epoch: [120/2467]	Loss: 0.073012
Train Epoch: [120/2467]	Loss: 0.344393
             Train Epoch: [140/2467]	Loss: 0.176819
     Train Epoch: [140/2467]	Loss: 0.079107 
Train Epoch: [140/2467]	Loss: 0.281839 
Train Epoch: [140/2467]	Loss: 0.039166
         Train Epoch: [160/2467]	Loss: 0.202088
 Train Epoch: [160/2467]	Loss: 0.177543
     Train Epoch: [160/2467]	Loss: 0.161330
     Train Epoch: [160/2467]	Loss: 0.287371
         Train Epoch: [180/2467]	Loss: 0.229179
 Train Epoch: [180/2467]	Loss: 0.138276    
 Train Epoch: [180/2467]	Loss: 0.167718
     Train Epoch: [180/2467]	Loss: 0.237896
     Train Epoch: [200/2467]	Loss: 0.133293
     Train Epoch: [200/2467]	Loss: 0.121346
     Train Epoch: [200/2467]	Loss: 0.130300
     Train Epoch: [200/2467]	Loss: 0.194818
     Train Epoch: [220/2467]	Loss: 0.214530
     Train Epoch: [220/2467]	Loss: 0.179082
         Train Epoch: [220/2467]	Loss: 0.228275
 Train Epoch: [220/2467]	Loss: 0.094991
     Train Epoch: [240/2467]	Loss: 0.368318    
     Train Epoch: [240/2467]	Loss: 0.226736 
Train Epoch: [240/2467]	Loss: 0.025581
     Train Epoch: [240/2467]	Loss: 0.043951
     Train Epoch: [260/2467]	Loss: 0.071212
     Train Epoch: [260/2467]	Loss: 0.059865    
 Train Epoch: [260/2467]	Loss: 0.039001    
 Train Epoch: [260/2467]	Loss: 0.116490
                 Train Epoch: [280/2467]	Loss: 0.197264
  Train Epoch: [280/2467]	Loss: 0.326462 
Train Epoch: [280/2467]	Loss: 0.241998Train Epoch: [280/2467]	Loss: 0.107572

     Train Epoch: [300/2467]	Loss: 0.039201
              Train Epoch: [300/2467]	Loss: 0.408887Train Epoch: [300/2467]	Loss: 0.168388
 
Train Epoch: [300/2467]	Loss: 0.146141
                  Train Epoch: [320/2467]	Loss: 0.185670Train Epoch: [320/2467]	Loss: 0.470808

  Train Epoch: [320/2467]	Loss: 0.382410Train Epoch: [320/2467]	Loss: 0.186431

         Train Epoch: [340/2467]	Loss: 0.062122 
Train Epoch: [340/2467]	Loss: 0.237275
     Train Epoch: [340/2467]	Loss: 0.133242
     Train Epoch: [340/2467]	Loss: 0.021643
     Train Epoch: [360/2467]	Loss: 0.087198
     Train Epoch: [360/2467]	Loss: 0.242215
         Train Epoch: [360/2467]	Loss: 0.110725
 Train Epoch: [360/2467]	Loss: 0.218393
     Train Epoch: [380/2467]	Loss: 0.096477
     Train Epoch: [380/2467]	Loss: 0.199531
     Train Epoch: [380/2467]	Loss: 0.409107
     Train Epoch: [380/2467]	Loss: 1.317860
             Train Epoch: [400/2467]	Loss: 0.175363
 Train Epoch: [400/2467]	Loss: 0.186760 
Train Epoch: [400/2467]	Loss: 0.342448
     Train Epoch: [400/2467]	Loss: 0.071021
     Train Epoch: [420/2467]	Loss: 0.164676
     Train Epoch: [420/2467]	Loss: 0.255167
     Train Epoch: [420/2467]	Loss: 0.156349
     Train Epoch: [420/2467]	Loss: 0.279637
         Train Epoch: [440/2467]	Loss: 0.112825
 Train Epoch: [440/2467]	Loss: 0.067667    
 Train Epoch: [440/2467]	Loss: 0.163505
     Train Epoch: [440/2467]	Loss: 0.370784
     Train Epoch: [460/2467]	Loss: 0.204881
         Train Epoch: [460/2467]	Loss: 0.294769
     Train Epoch: [460/2467]	Loss: 0.154039
 Train Epoch: [460/2467]	Loss: 0.147689
     Train Epoch: [480/2467]	Loss: 0.142025
         Train Epoch: [480/2467]	Loss: 0.310987 
Train Epoch: [480/2467]	Loss: 0.175550
     Train Epoch: [480/2467]	Loss: 0.341326
                   Train Epoch: [500/2467]	Loss: 0.122531Train Epoch: [500/2467]	Loss: 0.160270Train Epoch: [500/2467]	Loss: 0.234004


 Train Epoch: [500/2467]	Loss: 0.228685
     Train Epoch: [520/2467]	Loss: 0.323323
     Train Epoch: [520/2467]	Loss: 0.096056
     Train Epoch: [520/2467]	Loss: 0.152207
     Train Epoch: [520/2467]	Loss: 0.243636
     Train Epoch: [540/2467]	Loss: 0.266162
     Train Epoch: [540/2467]	Loss: 0.167556
     Train Epoch: [540/2467]	Loss: 0.076641
     Train Epoch: [540/2467]	Loss: 0.112419
     Train Epoch: [560/2467]	Loss: 0.279653
     Train Epoch: [560/2467]	Loss: 0.237423
     Train Epoch: [560/2467]	Loss: 0.162660
     Train Epoch: [560/2467]	Loss: 0.182255
          Train Epoch: [580/2467]	Loss: 0.082207
Train Epoch: [580/2467]	Loss: 0.182621    
 Train Epoch: [580/2467]	Loss: 0.198385
     Train Epoch: [580/2467]	Loss: 0.383499
     Train Epoch: [600/2467]	Loss: 0.135398
         Train Epoch: [600/2467]	Loss: 0.205401
 Train Epoch: [600/2467]	Loss: 0.202665
     Train Epoch: [600/2467]	Loss: 0.450012
             Train Epoch: [620/2467]	Loss: 0.060406
     Train Epoch: [620/2467]	Loss: 0.168068
  Train Epoch: [620/2467]	Loss: 0.335563Train Epoch: [620/2467]	Loss: 0.224115

     Train Epoch: [640/2467]	Loss: 0.194901
         Train Epoch: [640/2467]	Loss: 0.201558
 Train Epoch: [640/2467]	Loss: 0.241261
     Train Epoch: [640/2467]	Loss: 0.296038
     Train Epoch: [660/2467]	Loss: 0.044743
              Train Epoch: [660/2467]	Loss: 0.247977Train Epoch: [660/2467]	Loss: 0.032680 

Train Epoch: [660/2467]	Loss: 0.450393
             Train Epoch: [680/2467]	Loss: 0.057756
  Train Epoch: [680/2467]	Loss: 0.142205Train Epoch: [680/2467]	Loss: 0.145415

     Train Epoch: [680/2467]	Loss: 0.115650
     Train Epoch: [700/2467]	Loss: 0.357174    
 Train Epoch: [700/2467]	Loss: 0.406410    
     Train Epoch: [700/2467]	Loss: 0.088409
 Train Epoch: [700/2467]	Loss: 0.072438
         Train Epoch: [720/2467]	Loss: 0.058827
 Train Epoch: [720/2467]	Loss: 0.248234
     Train Epoch: [720/2467]	Loss: 0.133548
     Train Epoch: [720/2467]	Loss: 0.112617
     Train Epoch: [740/2467]	Loss: 0.076454
     Train Epoch: [740/2467]	Loss: 0.334193
     Train Epoch: [740/2467]	Loss: 0.126963
     Train Epoch: [740/2467]	Loss: 0.066812
     Train Epoch: [760/2467]	Loss: 0.253926
             Train Epoch: [760/2467]	Loss: 0.334781
 Train Epoch: [760/2467]	Loss: 0.543275
 Train Epoch: [760/2467]	Loss: 0.072639
         Train Epoch: [780/2467]	Loss: 0.339996    
 Train Epoch: [780/2467]	Loss: 0.342736 
Train Epoch: [780/2467]	Loss: 0.411811
     Train Epoch: [780/2467]	Loss: 0.204427
         Train Epoch: [800/2467]	Loss: 0.212815
     Train Epoch: [800/2467]	Loss: 0.118755
 Train Epoch: [800/2467]	Loss: 0.309102
     Train Epoch: [800/2467]	Loss: 0.276861
     Train Epoch: [820/2467]	Loss: 0.461096
     Train Epoch: [820/2467]	Loss: 0.290282
     Train Epoch: [820/2467]	Loss: 0.049486
     Train Epoch: [820/2467]	Loss: 0.063288
               Train Epoch: [840/2467]	Loss: 0.148388Train Epoch: [840/2467]	Loss: 0.109103
Train Epoch: [840/2467]	Loss: 0.322605

     Train Epoch: [840/2467]	Loss: 0.184145
             Train Epoch: [860/2467]	Loss: 0.186507
 Train Epoch: [860/2467]	Loss: 0.258759
 Train Epoch: [860/2467]	Loss: 0.291539
     Train Epoch: [860/2467]	Loss: 0.090190
     Train Epoch: [880/2467]	Loss: 0.220906
     Train Epoch: [880/2467]	Loss: 0.172142
     Train Epoch: [880/2467]	Loss: 0.213598
     Train Epoch: [880/2467]	Loss: 0.286234
              Train Epoch: [900/2467]	Loss: 0.211229Train Epoch: [900/2467]	Loss: 0.105076
    
 Train Epoch: [900/2467]	Loss: 0.207126
 Train Epoch: [900/2467]	Loss: 0.218506
                 Train Epoch: [920/2467]	Loss: 0.351721  
Train Epoch: [920/2467]	Loss: 0.379386 Train Epoch: [920/2467]	Loss: 0.361944

Train Epoch: [920/2467]	Loss: 0.070588
     Train Epoch: [940/2467]	Loss: 0.074672
     Train Epoch: [940/2467]	Loss: 0.040610
     Train Epoch: [940/2467]	Loss: 0.140083
     Train Epoch: [940/2467]	Loss: 0.185342
     Train Epoch: [960/2467]	Loss: 0.096916
         Train Epoch: [960/2467]	Loss: 0.395558
     Train Epoch: [960/2467]	Loss: 0.061441
 Train Epoch: [960/2467]	Loss: 0.119870
         Train Epoch: [980/2467]	Loss: 0.123170    
 Train Epoch: [980/2467]	Loss: 0.262191 
Train Epoch: [980/2467]	Loss: 0.146116
     Train Epoch: [980/2467]	Loss: 0.176427
             Train Epoch: [1000/2467]	Loss: 0.262936
  Train Epoch: [1000/2467]	Loss: 0.041462
Train Epoch: [1000/2467]	Loss: 0.345372
     Train Epoch: [1000/2467]	Loss: 0.054349
         Train Epoch: [1020/2467]	Loss: 0.161204    
     Train Epoch: [1020/2467]	Loss: 0.085174
  Train Epoch: [1020/2467]	Loss: 0.421987
Train Epoch: [1020/2467]	Loss: 0.080989
     Train Epoch: [1040/2467]	Loss: 0.187132
     Train Epoch: [1040/2467]	Loss: 0.118555
         Train Epoch: [1040/2467]	Loss: 0.268940
 Train Epoch: [1040/2467]	Loss: 0.141846
                 Train Epoch: [1060/2467]	Loss: 0.113586
   Train Epoch: [1060/2467]	Loss: 0.312720Train Epoch: [1060/2467]	Loss: 0.189962

Train Epoch: [1060/2467]	Loss: 0.435417
         Train Epoch: [1080/2467]	Loss: 0.299878
         Train Epoch: [1080/2467]	Loss: 0.353141
  Train Epoch: [1080/2467]	Loss: 0.126602Train Epoch: [1080/2467]	Loss: 0.174596

             Train Epoch: [1100/2467]	Loss: 0.281118 
Train Epoch: [1100/2467]	Loss: 0.225183
 Train Epoch: [1100/2467]	Loss: 0.276867
     Train Epoch: [1100/2467]	Loss: 0.274229
                 Train Epoch: [1120/2467]	Loss: 0.138732
 Train Epoch: [1120/2467]	Loss: 0.384082 
Train Epoch: [1120/2467]	Loss: 0.175247
 Train Epoch: [1120/2467]	Loss: 0.120686
     Train Epoch: [1140/2467]	Loss: 0.412205
     Train Epoch: [1140/2467]	Loss: 0.072305
     Train Epoch: [1140/2467]	Loss: 0.108679
     Train Epoch: [1140/2467]	Loss: 0.291004
         Train Epoch: [1160/2467]	Loss: 0.156520
         Train Epoch: [1160/2467]	Loss: 0.084688
 Train Epoch: [1160/2467]	Loss: 0.176246
 Train Epoch: [1160/2467]	Loss: 0.253874
     Train Epoch: [1180/2467]	Loss: 0.257750
     Train Epoch: [1180/2467]	Loss: 0.077007
     Train Epoch: [1180/2467]	Loss: 0.169447
     Train Epoch: [1180/2467]	Loss: 0.317153
         Train Epoch: [1200/2467]	Loss: 0.245985
     Train Epoch: [1200/2467]	Loss: 0.160149
 Train Epoch: [1200/2467]	Loss: 0.200922
     Train Epoch: [1200/2467]	Loss: 0.323965
              Train Epoch: [1220/2467]	Loss: 0.284515Train Epoch: [1220/2467]	Loss: 0.254921

 Train Epoch: [1220/2467]	Loss: 0.115829
     Train Epoch: [1220/2467]	Loss: 0.305769
              Train Epoch: [1240/2467]	Loss: 0.355524Train Epoch: [1240/2467]	Loss: 0.305048

     Train Epoch: [1240/2467]	Loss: 0.253328
 Train Epoch: [1240/2467]	Loss: 0.323378
     Train Epoch: [1260/2467]	Loss: 0.067471    
     Train Epoch: [1260/2467]	Loss: 0.069295
 Train Epoch: [1260/2467]	Loss: 0.433088
     Train Epoch: [1260/2467]	Loss: 0.155387
                 Train Epoch: [1280/2467]	Loss: 0.445656
  Train Epoch: [1280/2467]	Loss: 0.687454Train Epoch: [1280/2467]	Loss: 0.307036 

Train Epoch: [1280/2467]	Loss: 0.102560
     Train Epoch: [1300/2467]	Loss: 0.091255
     Train Epoch: [1300/2467]	Loss: 0.286994
     Train Epoch: [1300/2467]	Loss: 0.111264
     Train Epoch: [1300/2467]	Loss: 0.355007
             Train Epoch: [1320/2467]	Loss: 0.055419
 Train Epoch: [1320/2467]	Loss: 0.276854
 Train Epoch: [1320/2467]	Loss: 0.260420
     Train Epoch: [1320/2467]	Loss: 0.182785
     Train Epoch: [1340/2467]	Loss: 0.133387    
         Train Epoch: [1340/2467]	Loss: 0.199949 
 Train Epoch: [1340/2467]	Loss: 0.393008Train Epoch: [1340/2467]	Loss: 0.269287

         Train Epoch: [1360/2467]	Loss: 0.301919
         Train Epoch: [1360/2467]	Loss: 0.161889
  Train Epoch: [1360/2467]	Loss: 0.086587Train Epoch: [1360/2467]	Loss: 0.122014

     Train Epoch: [1380/2467]	Loss: 0.201045
     Train Epoch: [1380/2467]	Loss: 0.315951
          Train Epoch: [1380/2467]	Loss: 0.313968
Train Epoch: [1380/2467]	Loss: 0.228450
             Train Epoch: [1400/2467]	Loss: 0.194632
 Train Epoch: [1400/2467]	Loss: 0.155017
 Train Epoch: [1400/2467]	Loss: 0.196886
     Train Epoch: [1400/2467]	Loss: 0.100862
         Train Epoch: [1420/2467]	Loss: 0.109329
         Train Epoch: [1420/2467]	Loss: 0.155869
 Train Epoch: [1420/2467]	Loss: 0.417840
 Train Epoch: [1420/2467]	Loss: 0.140137
     Train Epoch: [1440/2467]	Loss: 0.055426
               Train Epoch: [1440/2467]	Loss: 0.115757Train Epoch: [1440/2467]	Loss: 0.197467Train Epoch: [1440/2467]	Loss: 0.055913


     Train Epoch: [1460/2467]	Loss: 0.133749
     Train Epoch: [1460/2467]	Loss: 0.384788
     Train Epoch: [1460/2467]	Loss: 0.051233
     Train Epoch: [1460/2467]	Loss: 0.149047
     Train Epoch: [1480/2467]	Loss: 0.048983
     Train Epoch: [1480/2467]	Loss: 0.211015
     Train Epoch: [1480/2467]	Loss: 0.015380
     Train Epoch: [1480/2467]	Loss: 0.284142
             Train Epoch: [1500/2467]	Loss: 0.016736  
Train Epoch: [1500/2467]	Loss: 0.170022Train Epoch: [1500/2467]	Loss: 0.128716

     Train Epoch: [1500/2467]	Loss: 0.136675
             Train Epoch: [1520/2467]	Loss: 0.272195 
Train Epoch: [1520/2467]	Loss: 0.287717
 Train Epoch: [1520/2467]	Loss: 0.204160    
 Train Epoch: [1520/2467]	Loss: 0.186758
     Train Epoch: [1540/2467]	Loss: 0.292734
             Train Epoch: [1540/2467]	Loss: 0.134348
 Train Epoch: [1540/2467]	Loss: 0.110825
 Train Epoch: [1540/2467]	Loss: 0.285236
         Train Epoch: [1560/2467]	Loss: 0.418684    
 Train Epoch: [1560/2467]	Loss: 0.379407
 Train Epoch: [1560/2467]	Loss: 0.123965
     Train Epoch: [1560/2467]	Loss: 0.491063
     Train Epoch: [1580/2467]	Loss: 0.259766
     Train Epoch: [1580/2467]	Loss: 0.324201
     Train Epoch: [1580/2467]	Loss: 0.144709
     Train Epoch: [1580/2467]	Loss: 0.273794
         Train Epoch: [1600/2467]	Loss: 0.326042
          Train Epoch: [1600/2467]	Loss: 0.138270Train Epoch: [1600/2467]	Loss: 0.310887

 Train Epoch: [1600/2467]	Loss: 0.105541
         Train Epoch: [1620/2467]	Loss: 0.199970
 Train Epoch: [1620/2467]	Loss: 0.106389
     Train Epoch: [1620/2467]	Loss: 0.051529
     Train Epoch: [1620/2467]	Loss: 0.233413
         Train Epoch: [1640/2467]	Loss: 0.180328
 Train Epoch: [1640/2467]	Loss: 0.257941
     Train Epoch: [1640/2467]	Loss: 0.198668
     Train Epoch: [1640/2467]	Loss: 0.283522
              Train Epoch: [1660/2467]	Loss: 0.233087    Train Epoch: [1660/2467]	Loss: 0.171048
 
Train Epoch: [1660/2467]	Loss: 0.143997
 Train Epoch: [1660/2467]	Loss: 0.232673
               Train Epoch: [1680/2467]	Loss: 0.084789Train Epoch: [1680/2467]	Loss: 0.199557
Train Epoch: [1680/2467]	Loss: 0.199365

     Train Epoch: [1680/2467]	Loss: 0.193454
     Train Epoch: [1700/2467]	Loss: 0.089601
         Train Epoch: [1700/2467]	Loss: 0.243537 
Train Epoch: [1700/2467]	Loss: 0.293053
     Train Epoch: [1700/2467]	Loss: 0.239569
              Train Epoch: [1720/2467]	Loss: 0.071517Train Epoch: [1720/2467]	Loss: 0.154042 

Train Epoch: [1720/2467]	Loss: 0.256016
     Train Epoch: [1720/2467]	Loss: 0.234321
         Train Epoch: [1740/2467]	Loss: 0.420592
         Train Epoch: [1740/2467]	Loss: 0.199210
 Train Epoch: [1740/2467]	Loss: 0.162911
 Train Epoch: [1740/2467]	Loss: 0.033206
     Train Epoch: [1760/2467]	Loss: 0.183868
             Train Epoch: [1760/2467]	Loss: 0.166710
 Train Epoch: [1760/2467]	Loss: 0.063890
 Train Epoch: [1760/2467]	Loss: 0.128433
              Train Epoch: [1780/2467]	Loss: 0.325426Train Epoch: [1780/2467]	Loss: 0.225859

 Train Epoch: [1780/2467]	Loss: 0.279058
     Train Epoch: [1780/2467]	Loss: 0.108750
         Train Epoch: [1800/2467]	Loss: 0.238497    
     Train Epoch: [1800/2467]	Loss: 0.198584 
 Train Epoch: [1800/2467]	Loss: 0.360906
Train Epoch: [1800/2467]	Loss: 0.258259
     Train Epoch: [1820/2467]	Loss: 0.357663
     Train Epoch: [1820/2467]	Loss: 0.142838
         Train Epoch: [1820/2467]	Loss: 0.116961
 Train Epoch: [1820/2467]	Loss: 0.150598
              Train Epoch: [1840/2467]	Loss: 0.069541Train Epoch: [1840/2467]	Loss: 0.279982

 Train Epoch: [1840/2467]	Loss: 0.306125    
 Train Epoch: [1840/2467]	Loss: 0.157564
         Train Epoch: [1860/2467]	Loss: 0.191380
 Train Epoch: [1860/2467]	Loss: 0.181148    
 Train Epoch: [1860/2467]	Loss: 0.160174
     Train Epoch: [1860/2467]	Loss: 0.225756
     Train Epoch: [1880/2467]	Loss: 0.132802
          Train Epoch: [1880/2467]	Loss: 0.041445Train Epoch: [1880/2467]	Loss: 0.048705

     Train Epoch: [1880/2467]	Loss: 0.082015
     Train Epoch: [1900/2467]	Loss: 0.141391    
           Train Epoch: [1900/2467]	Loss: 0.330861Train Epoch: [1900/2467]	Loss: 0.270806Train Epoch: [1900/2467]	Loss: 0.130131


     Train Epoch: [1920/2467]	Loss: 0.250196
     Train Epoch: [1920/2467]	Loss: 0.015838
          Train Epoch: [1920/2467]	Loss: 0.273840Train Epoch: [1920/2467]	Loss: 0.196684

     Train Epoch: [1940/2467]	Loss: 0.091510
     Train Epoch: [1940/2467]	Loss: 0.255678
     Train Epoch: [1940/2467]	Loss: 0.398624
     Train Epoch: [1940/2467]	Loss: 0.124379
         Train Epoch: [1960/2467]	Loss: 0.180872
         Train Epoch: [1960/2467]	Loss: 0.226759
  Train Epoch: [1960/2467]	Loss: 0.133453Train Epoch: [1960/2467]	Loss: 0.281275

     Train Epoch: [1980/2467]	Loss: 0.134601
     Train Epoch: [1980/2467]	Loss: 0.248848
     Train Epoch: [1980/2467]	Loss: 0.264353
     Train Epoch: [1980/2467]	Loss: 0.265641
         Train Epoch: [2000/2467]	Loss: 0.042057
     Train Epoch: [2000/2467]	Loss: 0.281758
 Train Epoch: [2000/2467]	Loss: 0.197537
     Train Epoch: [2000/2467]	Loss: 0.080588
     Train Epoch: [2020/2467]	Loss: 0.239279
     Train Epoch: [2020/2467]	Loss: 0.184240
         Train Epoch: [2020/2467]	Loss: 0.249528
 Train Epoch: [2020/2467]	Loss: 0.081043
     Train Epoch: [2040/2467]	Loss: 0.229217
             Train Epoch: [2040/2467]	Loss: 0.131083  
Train Epoch: [2040/2467]	Loss: 0.097549Train Epoch: [2040/2467]	Loss: 0.401083

         Train Epoch: [2060/2467]	Loss: 0.109928
         Train Epoch: [2060/2467]	Loss: 0.187425
  Train Epoch: [2060/2467]	Loss: 0.245461Train Epoch: [2060/2467]	Loss: 0.205982

         Train Epoch: [2080/2467]	Loss: 0.326738
 Train Epoch: [2080/2467]	Loss: 0.056639
     Train Epoch: [2080/2467]	Loss: 0.099362
     Train Epoch: [2080/2467]	Loss: 0.052576
     Train Epoch: [2100/2467]	Loss: 0.149104
             Train Epoch: [2100/2467]	Loss: 0.131795
 Train Epoch: [2100/2467]	Loss: 0.106978
 Train Epoch: [2100/2467]	Loss: 0.540470
     Train Epoch: [2120/2467]	Loss: 0.260727
     Train Epoch: [2120/2467]	Loss: 0.064918
     Train Epoch: [2120/2467]	Loss: 0.110618
     Train Epoch: [2120/2467]	Loss: 0.255347
         Train Epoch: [2140/2467]	Loss: 0.615546 
Train Epoch: [2140/2467]	Loss: 0.216476
     Train Epoch: [2140/2467]	Loss: 0.385387
     Train Epoch: [2140/2467]	Loss: 0.252881
             Train Epoch: [2160/2467]	Loss: 0.370631
 Train Epoch: [2160/2467]	Loss: 0.213066
 Train Epoch: [2160/2467]	Loss: 0.131333
     Train Epoch: [2160/2467]	Loss: 0.069685
             Train Epoch: [2180/2467]	Loss: 0.302112
     Train Epoch: [2180/2467]	Loss: 0.186367
 Train Epoch: [2180/2467]	Loss: 0.163231
 Train Epoch: [2180/2467]	Loss: 0.136577
         Train Epoch: [2200/2467]	Loss: 0.340078
 Train Epoch: [2200/2467]	Loss: 0.125530    
      Train Epoch: [2200/2467]	Loss: 0.069460
Train Epoch: [2200/2467]	Loss: 0.129304
     Train Epoch: [2220/2467]	Loss: 0.208957
         Train Epoch: [2220/2467]	Loss: 0.132123 
Train Epoch: [2220/2467]	Loss: 0.123538
     Train Epoch: [2220/2467]	Loss: 0.134199
         Train Epoch: [2240/2467]	Loss: 0.173162    
      Train Epoch: [2240/2467]	Loss: 0.057935Train Epoch: [2240/2467]	Loss: 0.031929 

Train Epoch: [2240/2467]	Loss: 0.083919
     Train Epoch: [2260/2467]	Loss: 0.223618    
     Train Epoch: [2260/2467]	Loss: 0.391898
 Train Epoch: [2260/2467]	Loss: 0.311883
     Train Epoch: [2260/2467]	Loss: 0.031817
     Train Epoch: [2280/2467]	Loss: 0.172262
     Train Epoch: [2280/2467]	Loss: 0.123679
     Train Epoch: [2280/2467]	Loss: 0.079400
     Train Epoch: [2280/2467]	Loss: 0.162980
         Train Epoch: [2300/2467]	Loss: 0.126938
 Train Epoch: [2300/2467]	Loss: 0.271721
     Train Epoch: [2300/2467]	Loss: 0.098470
     Train Epoch: [2300/2467]	Loss: 0.058096
              Train Epoch: [2320/2467]	Loss: 0.031608 Train Epoch: [2320/2467]	Loss: 0.256242

Train Epoch: [2320/2467]	Loss: 0.155261
     Train Epoch: [2320/2467]	Loss: 0.230002
         Train Epoch: [2340/2467]	Loss: 0.210597     
Train Epoch: [2340/2467]	Loss: 0.533671
     Train Epoch: [2340/2467]	Loss: 0.138637
 Train Epoch: [2340/2467]	Loss: 0.128681
     Train Epoch: [2360/2467]	Loss: 0.367911
     Train Epoch: [2360/2467]	Loss: 0.175132
     Train Epoch: [2360/2467]	Loss: 0.078651
     Train Epoch: [2360/2467]	Loss: 0.166354
         Train Epoch: [2380/2467]	Loss: 0.500033
 Train Epoch: [2380/2467]	Loss: 0.285396
     Train Epoch: [2380/2467]	Loss: 0.215348
     Train Epoch: [2380/2467]	Loss: 0.256191
     Train Epoch: [2400/2467]	Loss: 0.103108
             Train Epoch: [2400/2467]	Loss: 0.149894
  Train Epoch: [2400/2467]	Loss: 0.164876Train Epoch: [2400/2467]	Loss: 0.247422

         Train Epoch: [2420/2467]	Loss: 0.205081
      Train Epoch: [2420/2467]	Loss: 0.410824
Train Epoch: [2420/2467]	Loss: 0.102894
     Train Epoch: [2420/2467]	Loss: 0.319131
         Train Epoch: [2440/2467]	Loss: 0.326597
     Train Epoch: [2440/2467]	Loss: 0.154526
 Train Epoch: [2440/2467]	Loss: 0.050441
     Train Epoch: [2440/2467]	Loss: 0.174345
     Train Epoch: [2460/2467]	Loss: 0.112555
         Train Epoch: [2460/2467]	Loss: 0.292671
     Train Epoch: [2460/2467]	Loss: 0.132622
 Train Epoch: [2460/2467]	Loss: 0.140823
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 18 epoch =====
     2025-05-11.06-36-11
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 18 epoch =====
     2025-05-11.06-36-12
after set grad
after prog
start loop
     ===== running 18 epoch =====
     2025-05-11.06-36-12
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 18 epoch =====
     2025-05-11.06-36-12
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.193143
              Train Epoch: [0/2467]	Loss: 0.301675
Train Epoch: [0/2467]	Loss: 0.271622 
Train Epoch: [0/2467]	Loss: 0.180292
     Train Epoch: [20/2467]	Loss: 0.343928
         Train Epoch: [20/2467]	Loss: 0.238018
 Train Epoch: [20/2467]	Loss: 0.125524
     Train Epoch: [20/2467]	Loss: 0.150377
         Train Epoch: [40/2467]	Loss: 0.489713        
 Train Epoch: [40/2467]	Loss: 0.172774
 Train Epoch: [40/2467]	Loss: 0.158273
 Train Epoch: [40/2467]	Loss: 0.149410
     Train Epoch: [60/2467]	Loss: 0.056631
         Train Epoch: [60/2467]	Loss: 0.212101 
    Train Epoch: [60/2467]	Loss: 0.284384
 Train Epoch: [60/2467]	Loss: 0.148332
         Train Epoch: [80/2467]	Loss: 0.146177
 Train Epoch: [80/2467]	Loss: 0.443605    
 Train Epoch: [80/2467]	Loss: 0.117563
     Train Epoch: [80/2467]	Loss: 0.138715
             Train Epoch: [100/2467]	Loss: 0.329125
  Train Epoch: [100/2467]	Loss: 0.306724
Train Epoch: [100/2467]	Loss: 0.262904
     Train Epoch: [100/2467]	Loss: 0.124756
             Train Epoch: [120/2467]	Loss: 0.085336
 Train Epoch: [120/2467]	Loss: 0.423323
 Train Epoch: [120/2467]	Loss: 0.288063
     Train Epoch: [120/2467]	Loss: 0.098787
         Train Epoch: [140/2467]	Loss: 0.185798
     Train Epoch: [140/2467]	Loss: 0.072627 
Train Epoch: [140/2467]	Loss: 0.316692
     Train Epoch: [140/2467]	Loss: 0.045888
         Train Epoch: [160/2467]	Loss: 0.163708    
 Train Epoch: [160/2467]	Loss: 0.214599
 Train Epoch: [160/2467]	Loss: 0.132988
     Train Epoch: [160/2467]	Loss: 0.228506
          Train Epoch: [180/2467]	Loss: 0.143511Train Epoch: [180/2467]	Loss: 0.200202

     Train Epoch: [180/2467]	Loss: 0.135886    
 Train Epoch: [180/2467]	Loss: 0.149714
     Train Epoch: [200/2467]	Loss: 0.120634
         Train Epoch: [200/2467]	Loss: 0.142390
 Train Epoch: [200/2467]	Loss: 0.121925
     Train Epoch: [200/2467]	Loss: 0.131381
     Train Epoch: [220/2467]	Loss: 0.222073
             Train Epoch: [220/2467]	Loss: 0.168218 
 Train Epoch: [220/2467]	Loss: 0.234055
Train Epoch: [220/2467]	Loss: 0.121154
         Train Epoch: [240/2467]	Loss: 0.244449
 Train Epoch: [240/2467]	Loss: 0.194557
     Train Epoch: [240/2467]	Loss: 0.027156
     Train Epoch: [240/2467]	Loss: 0.043167
         Train Epoch: [260/2467]	Loss: 0.069449    
 Train Epoch: [260/2467]	Loss: 0.155900
 Train Epoch: [260/2467]	Loss: 0.048465
     Train Epoch: [260/2467]	Loss: 0.055025
     Train Epoch: [280/2467]	Loss: 0.250628
     Train Epoch: [280/2467]	Loss: 0.203283
     Train Epoch: [280/2467]	Loss: 0.101406
     Train Epoch: [280/2467]	Loss: 0.258551
         Train Epoch: [300/2467]	Loss: 0.088175
 Train Epoch: [300/2467]	Loss: 0.406764    
 Train Epoch: [300/2467]	Loss: 0.156017
     Train Epoch: [300/2467]	Loss: 0.051235
     Train Epoch: [320/2467]	Loss: 0.460478
         Train Epoch: [320/2467]	Loss: 0.298513 
Train Epoch: [320/2467]	Loss: 0.182674
     Train Epoch: [320/2467]	Loss: 0.193147
                 Train Epoch: [340/2467]	Loss: 0.215036
 Train Epoch: [340/2467]	Loss: 0.065827 
Train Epoch: [340/2467]	Loss: 0.017354
 Train Epoch: [340/2467]	Loss: 0.137547
     Train Epoch: [360/2467]	Loss: 0.091442    
          Train Epoch: [360/2467]	Loss: 0.188149
Train Epoch: [360/2467]	Loss: 0.243169
 Train Epoch: [360/2467]	Loss: 0.081013
          Train Epoch: [380/2467]	Loss: 0.229162
Train Epoch: [380/2467]	Loss: 0.120932
     Train Epoch: [380/2467]	Loss: 0.541523
     Train Epoch: [380/2467]	Loss: 0.230833
         Train Epoch: [400/2467]	Loss: 0.252781    
 Train Epoch: [400/2467]	Loss: 0.127492    
  Train Epoch: [400/2467]	Loss: 0.070725
Train Epoch: [400/2467]	Loss: 0.180833
             Train Epoch: [420/2467]	Loss: 0.135273
  Train Epoch: [420/2467]	Loss: 0.139238Train Epoch: [420/2467]	Loss: 0.291807

     Train Epoch: [420/2467]	Loss: 0.254965
         Train Epoch: [440/2467]	Loss: 0.095659
         Train Epoch: [440/2467]	Loss: 0.360732
  Train Epoch: [440/2467]	Loss: 0.070242
Train Epoch: [440/2467]	Loss: 0.166877
          Train Epoch: [460/2467]	Loss: 0.188110Train Epoch: [460/2467]	Loss: 0.182271

         Train Epoch: [460/2467]	Loss: 0.271795
 Train Epoch: [460/2467]	Loss: 0.165969
     Train Epoch: [480/2467]	Loss: 0.228146
         Train Epoch: [480/2467]	Loss: 0.343471
     Train Epoch: [480/2467]	Loss: 0.305693
 Train Epoch: [480/2467]	Loss: 0.169628
     Train Epoch: [500/2467]	Loss: 0.116607
         Train Epoch: [500/2467]	Loss: 0.222975     
Train Epoch: [500/2467]	Loss: 0.218233
 Train Epoch: [500/2467]	Loss: 0.202344
         Train Epoch: [520/2467]	Loss: 0.297250
      Train Epoch: [520/2467]	Loss: 0.135342Train Epoch: [520/2467]	Loss: 0.076485

     Train Epoch: [520/2467]	Loss: 0.242407
     Train Epoch: [540/2467]	Loss: 0.289555
             Train Epoch: [540/2467]	Loss: 0.163653 
 Train Epoch: [540/2467]	Loss: 0.124559
Train Epoch: [540/2467]	Loss: 0.078814
     Train Epoch: [560/2467]	Loss: 0.303569
         Train Epoch: [560/2467]	Loss: 0.162780
     Train Epoch: [560/2467]	Loss: 0.228274
 Train Epoch: [560/2467]	Loss: 0.196323
     Train Epoch: [580/2467]	Loss: 0.168819
         Train Epoch: [580/2467]	Loss: 0.335742
 Train Epoch: [580/2467]	Loss: 0.071299
     Train Epoch: [580/2467]	Loss: 0.182418
     Train Epoch: [600/2467]	Loss: 0.125000
     Train Epoch: [600/2467]	Loss: 0.201394
         Train Epoch: [600/2467]	Loss: 0.198129
 Train Epoch: [600/2467]	Loss: 0.201231
     Train Epoch: [620/2467]	Loss: 0.040764
             Train Epoch: [620/2467]	Loss: 0.247102 
Train Epoch: [620/2467]	Loss: 0.171854
 Train Epoch: [620/2467]	Loss: 0.322236
     Train Epoch: [640/2467]	Loss: 0.050325
     Train Epoch: [640/2467]	Loss: 0.139912
     Train Epoch: [640/2467]	Loss: 0.206765
     Train Epoch: [640/2467]	Loss: 0.277783
             Train Epoch: [660/2467]	Loss: 0.040916
     Train Epoch: [660/2467]	Loss: 0.223697 
Train Epoch: [660/2467]	Loss: 0.483091
 Train Epoch: [660/2467]	Loss: 0.034128
     Train Epoch: [680/2467]	Loss: 0.101330
         Train Epoch: [680/2467]	Loss: 0.079349 
Train Epoch: [680/2467]	Loss: 0.125355
     Train Epoch: [680/2467]	Loss: 0.101179
         Train Epoch: [700/2467]	Loss: 0.347069
      Train Epoch: [700/2467]	Loss: 0.423616Train Epoch: [700/2467]	Loss: 0.063589    

 Train Epoch: [700/2467]	Loss: 0.081153
     Train Epoch: [720/2467]	Loss: 0.233021    
 Train Epoch: [720/2467]	Loss: 0.139241
     Train Epoch: [720/2467]	Loss: 0.043979
     Train Epoch: [720/2467]	Loss: 0.132052
              Train Epoch: [740/2467]	Loss: 0.072110Train Epoch: [740/2467]	Loss: 0.113863

     Train Epoch: [740/2467]	Loss: 0.319976
 Train Epoch: [740/2467]	Loss: 0.062443
         Train Epoch: [760/2467]	Loss: 0.262133 
Train Epoch: [760/2467]	Loss: 0.480544
     Train Epoch: [760/2467]	Loss: 0.056578
     Train Epoch: [760/2467]	Loss: 0.366231
             Train Epoch: [780/2467]	Loss: 0.187536
 Train Epoch: [780/2467]	Loss: 0.344771 
Train Epoch: [780/2467]	Loss: 0.352296
     Train Epoch: [780/2467]	Loss: 0.411063
             Train Epoch: [800/2467]	Loss: 0.140818
      Train Epoch: [800/2467]	Loss: 0.209145Train Epoch: [800/2467]	Loss: 0.214423

 Train Epoch: [800/2467]	Loss: 0.204068
     Train Epoch: [820/2467]	Loss: 0.451385
         Train Epoch: [820/2467]	Loss: 0.046160
 Train Epoch: [820/2467]	Loss: 0.066200
     Train Epoch: [820/2467]	Loss: 0.293480
     Train Epoch: [840/2467]	Loss: 0.212358
          Train Epoch: [840/2467]	Loss: 0.116404Train Epoch: [840/2467]	Loss: 0.330479

     Train Epoch: [840/2467]	Loss: 0.184786
         Train Epoch: [860/2467]	Loss: 0.212376
 Train Epoch: [860/2467]	Loss: 0.286227
     Train Epoch: [860/2467]	Loss: 0.275025
     Train Epoch: [860/2467]	Loss: 0.123676
             Train Epoch: [880/2467]	Loss: 0.158986
      Train Epoch: [880/2467]	Loss: 0.239045Train Epoch: [880/2467]	Loss: 0.274108

 Train Epoch: [880/2467]	Loss: 0.168102
                  Train Epoch: [900/2467]	Loss: 0.126765
Train Epoch: [900/2467]	Loss: 0.212310
 Train Epoch: [900/2467]	Loss: 0.194587
 Train Epoch: [900/2467]	Loss: 0.232346
     Train Epoch: [920/2467]	Loss: 0.318170    
      Train Epoch: [920/2467]	Loss: 0.394898
Train Epoch: [920/2467]	Loss: 0.485800
     Train Epoch: [920/2467]	Loss: 0.060556
         Train Epoch: [940/2467]	Loss: 0.042639
 Train Epoch: [940/2467]	Loss: 0.056541
     Train Epoch: [940/2467]	Loss: 0.159921
     Train Epoch: [940/2467]	Loss: 0.221538
     Train Epoch: [960/2467]	Loss: 0.070829
     Train Epoch: [960/2467]	Loss: 0.075351
     Train Epoch: [960/2467]	Loss: 0.420292
     Train Epoch: [960/2467]	Loss: 0.095769
     Train Epoch: [980/2467]	Loss: 0.085925
         Train Epoch: [980/2467]	Loss: 0.268223
 Train Epoch: [980/2467]	Loss: 0.160376
     Train Epoch: [980/2467]	Loss: 0.197932
             Train Epoch: [1000/2467]	Loss: 0.299835
  Train Epoch: [1000/2467]	Loss: 0.356258
Train Epoch: [1000/2467]	Loss: 0.039809
     Train Epoch: [1000/2467]	Loss: 0.054071
     Train Epoch: [1020/2467]	Loss: 0.173653
         Train Epoch: [1020/2467]	Loss: 0.416605
 Train Epoch: [1020/2467]	Loss: 0.093191
     Train Epoch: [1020/2467]	Loss: 0.088917
     Train Epoch: [1040/2467]	Loss: 0.108277
          Train Epoch: [1040/2467]	Loss: 0.282368Train Epoch: [1040/2467]	Loss: 0.224100

     Train Epoch: [1040/2467]	Loss: 0.153624
     Train Epoch: [1060/2467]	Loss: 0.270643
         Train Epoch: [1060/2467]	Loss: 0.468789 
Train Epoch: [1060/2467]	Loss: 0.281471
     Train Epoch: [1060/2467]	Loss: 0.123632
             Train Epoch: [1080/2467]	Loss: 0.318254 
Train Epoch: [1080/2467]	Loss: 0.300240
 Train Epoch: [1080/2467]	Loss: 0.141268
     Train Epoch: [1080/2467]	Loss: 0.162135
     Train Epoch: [1100/2467]	Loss: 0.278221    
         Train Epoch: [1100/2467]	Loss: 0.264748
  Train Epoch: [1100/2467]	Loss: 0.279827Train Epoch: [1100/2467]	Loss: 0.277679

     Train Epoch: [1120/2467]	Loss: 0.137854
         Train Epoch: [1120/2467]	Loss: 0.285644
 Train Epoch: [1120/2467]	Loss: 0.144792
     Train Epoch: [1120/2467]	Loss: 0.169145
     Train Epoch: [1140/2467]	Loss: 0.444272
     Train Epoch: [1140/2467]	Loss: 0.270415
         Train Epoch: [1140/2467]	Loss: 0.058653
 Train Epoch: [1140/2467]	Loss: 0.092456
     Train Epoch: [1160/2467]	Loss: 0.160729
          Train Epoch: [1160/2467]	Loss: 0.081232Train Epoch: [1160/2467]	Loss: 0.217001

     Train Epoch: [1160/2467]	Loss: 0.111397
         Train Epoch: [1180/2467]	Loss: 0.065545 
Train Epoch: [1180/2467]	Loss: 0.218332
         Train Epoch: [1180/2467]	Loss: 0.180081 
Train Epoch: [1180/2467]	Loss: 0.340544
     Train Epoch: [1200/2467]	Loss: 0.168854
             Train Epoch: [1200/2467]	Loss: 0.087450 
Train Epoch: [1200/2467]	Loss: 0.271732
 Train Epoch: [1200/2467]	Loss: 0.346845
          Train Epoch: [1220/2467]	Loss: 0.101081Train Epoch: [1220/2467]	Loss: 0.262286

          Train Epoch: [1220/2467]	Loss: 0.339544Train Epoch: [1220/2467]	Loss: 0.247587

          Train Epoch: [1240/2467]	Loss: 0.338608
Train Epoch: [1240/2467]	Loss: 0.328310    
 Train Epoch: [1240/2467]	Loss: 0.197534
     Train Epoch: [1240/2467]	Loss: 0.201057
     Train Epoch: [1260/2467]	Loss: 0.073824
          Train Epoch: [1260/2467]	Loss: 0.072590Train Epoch: [1260/2467]	Loss: 0.143317

     Train Epoch: [1260/2467]	Loss: 0.452918
     Train Epoch: [1280/2467]	Loss: 0.726741
         Train Epoch: [1280/2467]	Loss: 0.472727
 Train Epoch: [1280/2467]	Loss: 0.078933
     Train Epoch: [1280/2467]	Loss: 0.318081
     Train Epoch: [1300/2467]	Loss: 0.087358
         Train Epoch: [1300/2467]	Loss: 0.364512
     Train Epoch: [1300/2467]	Loss: 0.066675
 Train Epoch: [1300/2467]	Loss: 0.316130
     Train Epoch: [1320/2467]	Loss: 0.062321
          Train Epoch: [1320/2467]	Loss: 0.150252
    Train Epoch: [1320/2467]	Loss: 0.191112
 Train Epoch: [1320/2467]	Loss: 0.272686
         Train Epoch: [1340/2467]	Loss: 0.247307
 Train Epoch: [1340/2467]	Loss: 0.127320
         Train Epoch: [1340/2467]	Loss: 0.366502
 Train Epoch: [1340/2467]	Loss: 0.173374
         Train Epoch: [1360/2467]	Loss: 0.082295
     Train Epoch: [1360/2467]	Loss: 0.180355
     Train Epoch: [1360/2467]	Loss: 0.157012
 Train Epoch: [1360/2467]	Loss: 0.078619
     Train Epoch: [1380/2467]	Loss: 0.317092
     Train Epoch: [1380/2467]	Loss: 0.309381
     Train Epoch: [1380/2467]	Loss: 0.211207
     Train Epoch: [1380/2467]	Loss: 0.388538
     Train Epoch: [1400/2467]	Loss: 0.083562
         Train Epoch: [1400/2467]	Loss: 0.125667
     Train Epoch: [1400/2467]	Loss: 0.184898
 Train Epoch: [1400/2467]	Loss: 0.185192
     Train Epoch: [1420/2467]	Loss: 0.436033
     Train Epoch: [1420/2467]	Loss: 0.103653
         Train Epoch: [1420/2467]	Loss: 0.145107 
Train Epoch: [1420/2467]	Loss: 0.152898
     Train Epoch: [1440/2467]	Loss: 0.089368
         Train Epoch: [1440/2467]	Loss: 0.048256
     Train Epoch: [1440/2467]	Loss: 0.194025
 Train Epoch: [1440/2467]	Loss: 0.030171
     Train Epoch: [1460/2467]	Loss: 0.048608
         Train Epoch: [1460/2467]	Loss: 0.125593
 Train Epoch: [1460/2467]	Loss: 0.274221
     Train Epoch: [1460/2467]	Loss: 0.150650
     Train Epoch: [1480/2467]	Loss: 0.035784    
 Train Epoch: [1480/2467]	Loss: 0.021254
     Train Epoch: [1480/2467]	Loss: 0.199015
     Train Epoch: [1480/2467]	Loss: 0.266676
     Train Epoch: [1500/2467]	Loss: 0.168048
     Train Epoch: [1500/2467]	Loss: 0.100812
     Train Epoch: [1500/2467]	Loss: 0.025176
     Train Epoch: [1500/2467]	Loss: 0.147426
     Train Epoch: [1520/2467]	Loss: 0.385138
             Train Epoch: [1520/2467]	Loss: 0.300284
 Train Epoch: [1520/2467]	Loss: 0.200315
 Train Epoch: [1520/2467]	Loss: 0.174331
     Train Epoch: [1540/2467]	Loss: 0.287841
     Train Epoch: [1540/2467]	Loss: 0.159915
     Train Epoch: [1540/2467]	Loss: 0.074305
     Train Epoch: [1540/2467]	Loss: 0.123996
         Train Epoch: [1560/2467]	Loss: 0.090996
 Train Epoch: [1560/2467]	Loss: 0.424208
     Train Epoch: [1560/2467]	Loss: 0.450986
     Train Epoch: [1560/2467]	Loss: 0.326799
     Train Epoch: [1580/2467]	Loss: 0.169438
         Train Epoch: [1580/2467]	Loss: 0.250254    
  Train Epoch: [1580/2467]	Loss: 0.198906Train Epoch: [1580/2467]	Loss: 0.297867

     Train Epoch: [1600/2467]	Loss: 0.045333
         Train Epoch: [1600/2467]	Loss: 0.262843
 Train Epoch: [1600/2467]	Loss: 0.127440    
 Train Epoch: [1600/2467]	Loss: 0.322693
     Train Epoch: [1620/2467]	Loss: 0.217883
     Train Epoch: [1620/2467]	Loss: 0.232122
     Train Epoch: [1620/2467]	Loss: 0.105014
     Train Epoch: [1620/2467]	Loss: 0.041297
     Train Epoch: [1640/2467]	Loss: 0.239810
         Train Epoch: [1640/2467]	Loss: 0.171646
 Train Epoch: [1640/2467]	Loss: 0.249356    
 Train Epoch: [1640/2467]	Loss: 0.121737
     Train Epoch: [1660/2467]	Loss: 0.146632
     Train Epoch: [1660/2467]	Loss: 0.284266    
     Train Epoch: [1660/2467]	Loss: 0.226326 
Train Epoch: [1660/2467]	Loss: 0.156647
         Train Epoch: [1680/2467]	Loss: 0.063176
     Train Epoch: [1680/2467]	Loss: 0.204562
 Train Epoch: [1680/2467]	Loss: 0.244559
     Train Epoch: [1680/2467]	Loss: 0.573157
         Train Epoch: [1700/2467]	Loss: 0.182836    
      Train Epoch: [1700/2467]	Loss: 0.090159Train Epoch: [1700/2467]	Loss: 0.314563

 Train Epoch: [1700/2467]	Loss: 0.203580
         Train Epoch: [1720/2467]	Loss: 0.251590
 Train Epoch: [1720/2467]	Loss: 0.224181    
      Train Epoch: [1720/2467]	Loss: 0.291142
Train Epoch: [1720/2467]	Loss: 0.065952
     Train Epoch: [1740/2467]	Loss: 0.082785
     Train Epoch: [1740/2467]	Loss: 0.039102
         Train Epoch: [1740/2467]	Loss: 0.203178
 Train Epoch: [1740/2467]	Loss: 0.398106
             Train Epoch: [1760/2467]	Loss: 0.219732
 Train Epoch: [1760/2467]	Loss: 0.140667
 Train Epoch: [1760/2467]	Loss: 0.059379
     Train Epoch: [1760/2467]	Loss: 0.182562
         Train Epoch: [1780/2467]	Loss: 0.339800 
Train Epoch: [1780/2467]	Loss: 0.215851
         Train Epoch: [1780/2467]	Loss: 0.116254
 Train Epoch: [1780/2467]	Loss: 0.295119
         Train Epoch: [1800/2467]	Loss: 0.323557
 Train Epoch: [1800/2467]	Loss: 0.244382
         Train Epoch: [1800/2467]	Loss: 0.177038 
Train Epoch: [1800/2467]	Loss: 0.254734
              Train Epoch: [1820/2467]	Loss: 0.164037Train Epoch: [1820/2467]	Loss: 0.118761

 Train Epoch: [1820/2467]	Loss: 0.455678
     Train Epoch: [1820/2467]	Loss: 0.167835
     Train Epoch: [1840/2467]	Loss: 0.283191
              Train Epoch: [1840/2467]	Loss: 0.361553Train Epoch: [1840/2467]	Loss: 0.059541

 Train Epoch: [1840/2467]	Loss: 0.167097
         Train Epoch: [1860/2467]	Loss: 0.154610
         Train Epoch: [1860/2467]	Loss: 0.156886 
Train Epoch: [1860/2467]	Loss: 0.115745
 Train Epoch: [1860/2467]	Loss: 0.201740
         Train Epoch: [1880/2467]	Loss: 0.032980 
Train Epoch: [1880/2467]	Loss: 0.120817
         Train Epoch: [1880/2467]	Loss: 0.077537
 Train Epoch: [1880/2467]	Loss: 0.045470
     Train Epoch: [1900/2467]	Loss: 0.117945
         Train Epoch: [1900/2467]	Loss: 0.255259 
Train Epoch: [1900/2467]	Loss: 0.327874    
 Train Epoch: [1900/2467]	Loss: 0.269475
     Train Epoch: [1920/2467]	Loss: 0.024277
     Train Epoch: [1920/2467]	Loss: 0.253370
         Train Epoch: [1920/2467]	Loss: 0.246811
 Train Epoch: [1920/2467]	Loss: 0.179789
          Train Epoch: [1940/2467]	Loss: 0.269470
Train Epoch: [1940/2467]	Loss: 0.121939    
      Train Epoch: [1940/2467]	Loss: 0.401903Train Epoch: [1940/2467]	Loss: 0.090710

     Train Epoch: [1960/2467]	Loss: 0.222285
         Train Epoch: [1960/2467]	Loss: 0.118109 
Train Epoch: [1960/2467]	Loss: 0.310047    
 Train Epoch: [1960/2467]	Loss: 0.164688
     Train Epoch: [1980/2467]	Loss: 0.132459
     Train Epoch: [1980/2467]	Loss: 0.350062
     Train Epoch: [1980/2467]	Loss: 0.312071
     Train Epoch: [1980/2467]	Loss: 0.263837
     Train Epoch: [2000/2467]	Loss: 0.168059    
 Train Epoch: [2000/2467]	Loss: 0.037899
     Train Epoch: [2000/2467]	Loss: 0.073433    
 Train Epoch: [2000/2467]	Loss: 0.247822
     Train Epoch: [2020/2467]	Loss: 0.201559
     Train Epoch: [2020/2467]	Loss: 0.329132
     Train Epoch: [2020/2467]	Loss: 0.086491
     Train Epoch: [2020/2467]	Loss: 0.181895
     Train Epoch: [2040/2467]	Loss: 0.203224
          Train Epoch: [2040/2467]	Loss: 0.417233Train Epoch: [2040/2467]	Loss: 0.099919

     Train Epoch: [2040/2467]	Loss: 0.173294
     Train Epoch: [2060/2467]	Loss: 0.101012    
     Train Epoch: [2060/2467]	Loss: 0.319179
 Train Epoch: [2060/2467]	Loss: 0.226874
     Train Epoch: [2060/2467]	Loss: 0.206657
     Train Epoch: [2080/2467]	Loss: 0.088242
         Train Epoch: [2080/2467]	Loss: 0.104361
 Train Epoch: [2080/2467]	Loss: 0.063985
     Train Epoch: [2080/2467]	Loss: 0.319147
             Train Epoch: [2100/2467]	Loss: 0.100479
 Train Epoch: [2100/2467]	Loss: 0.125174 
Train Epoch: [2100/2467]	Loss: 0.536501    
 Train Epoch: [2100/2467]	Loss: 0.141182
         Train Epoch: [2120/2467]	Loss: 0.049311
     Train Epoch: [2120/2467]	Loss: 0.194487     
Train Epoch: [2120/2467]	Loss: 0.097302
 Train Epoch: [2120/2467]	Loss: 0.265757
             Train Epoch: [2140/2467]	Loss: 0.384942
  Train Epoch: [2140/2467]	Loss: 0.302650Train Epoch: [2140/2467]	Loss: 0.601510

     Train Epoch: [2140/2467]	Loss: 0.182309
     Train Epoch: [2160/2467]	Loss: 0.340961
     Train Epoch: [2160/2467]	Loss: 0.194244
     Train Epoch: [2160/2467]	Loss: 0.103166
     Train Epoch: [2160/2467]	Loss: 0.077499
         Train Epoch: [2180/2467]	Loss: 0.100384
 Train Epoch: [2180/2467]	Loss: 0.283558    
     Train Epoch: [2180/2467]	Loss: 0.168482
 Train Epoch: [2180/2467]	Loss: 0.204828
         Train Epoch: [2200/2467]	Loss: 0.075584
 Train Epoch: [2200/2467]	Loss: 0.270600
         Train Epoch: [2200/2467]	Loss: 0.111717
 Train Epoch: [2200/2467]	Loss: 0.158858
     Train Epoch: [2220/2467]	Loss: 0.065044
         Train Epoch: [2220/2467]	Loss: 0.130020
     Train Epoch: [2220/2467]	Loss: 0.193778
 Train Epoch: [2220/2467]	Loss: 0.127551
         Train Epoch: [2240/2467]	Loss: 0.075543
 Train Epoch: [2240/2467]	Loss: 0.159074        
  Train Epoch: [2240/2467]	Loss: 0.032730Train Epoch: [2240/2467]	Loss: 0.040649

     Train Epoch: [2260/2467]	Loss: 0.243914
          Train Epoch: [2260/2467]	Loss: 0.050882Train Epoch: [2260/2467]	Loss: 0.212376

     Train Epoch: [2260/2467]	Loss: 0.406746
     Train Epoch: [2280/2467]	Loss: 0.176664
          Train Epoch: [2280/2467]	Loss: 0.105137Train Epoch: [2280/2467]	Loss: 0.094863

     Train Epoch: [2280/2467]	Loss: 0.135089
     Train Epoch: [2300/2467]	Loss: 0.287055
          Train Epoch: [2300/2467]	Loss: 0.132854Train Epoch: [2300/2467]	Loss: 0.123954

     Train Epoch: [2300/2467]	Loss: 0.057891
     Train Epoch: [2320/2467]	Loss: 0.266964
              Train Epoch: [2320/2467]	Loss: 0.183020Train Epoch: [2320/2467]	Loss: 0.051992

 Train Epoch: [2320/2467]	Loss: 0.224622
         Train Epoch: [2340/2467]	Loss: 0.142899 
Train Epoch: [2340/2467]	Loss: 0.381819
         Train Epoch: [2340/2467]	Loss: 0.545554
 Train Epoch: [2340/2467]	Loss: 0.129400
         Train Epoch: [2360/2467]	Loss: 0.341900
 Train Epoch: [2360/2467]	Loss: 0.163006
         Train Epoch: [2360/2467]	Loss: 0.098853
 Train Epoch: [2360/2467]	Loss: 0.148350
     Train Epoch: [2380/2467]	Loss: 0.126086
         Train Epoch: [2380/2467]	Loss: 0.505856
 Train Epoch: [2380/2467]	Loss: 0.232284
     Train Epoch: [2380/2467]	Loss: 0.229065
         Train Epoch: [2400/2467]	Loss: 0.111510
 Train Epoch: [2400/2467]	Loss: 0.278782
     Train Epoch: [2400/2467]	Loss: 0.145492
     Train Epoch: [2400/2467]	Loss: 0.208106
     Train Epoch: [2420/2467]	Loss: 0.230776
     Train Epoch: [2420/2467]	Loss: 0.329206
     Train Epoch: [2420/2467]	Loss: 0.260493
     Train Epoch: [2420/2467]	Loss: 0.091847
         Train Epoch: [2440/2467]	Loss: 0.037356
 Train Epoch: [2440/2467]	Loss: 0.273566
     Train Epoch: [2440/2467]	Loss: 0.165238
     Train Epoch: [2440/2467]	Loss: 0.149588
     Train Epoch: [2460/2467]	Loss: 0.152488
          Train Epoch: [2460/2467]	Loss: 0.283500Train Epoch: [2460/2467]	Loss: 0.112408

     Train Epoch: [2460/2467]	Loss: 0.130443
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 19 epoch =====
     2025-05-11.06-58-03
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 19 epoch =====
     2025-05-11.06-58-03
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 19 epoch =====
     2025-05-11.06-58-04
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 19 epoch =====
     2025-05-11.06-58-04
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
              Train Epoch: [0/2467]	Loss: 0.324778Train Epoch: [0/2467]	Loss: 0.301137

     Train Epoch: [0/2467]	Loss: 0.165138
 Train Epoch: [0/2467]	Loss: 0.214092
     Train Epoch: [20/2467]	Loss: 0.328529
     Train Epoch: [20/2467]	Loss: 0.209248
     Train Epoch: [20/2467]	Loss: 0.076083
     Train Epoch: [20/2467]	Loss: 0.149632
     Train Epoch: [40/2467]	Loss: 0.235563
         Train Epoch: [40/2467]	Loss: 0.412326 
Train Epoch: [40/2467]	Loss: 0.210402
     Train Epoch: [40/2467]	Loss: 0.163147
         Train Epoch: [60/2467]	Loss: 0.245140
 Train Epoch: [60/2467]	Loss: 0.048905
     Train Epoch: [60/2467]	Loss: 0.161582
     Train Epoch: [60/2467]	Loss: 0.198389
     Train Epoch: [80/2467]	Loss: 0.306459
         Train Epoch: [80/2467]	Loss: 0.103739
 Train Epoch: [80/2467]	Loss: 0.103367    
 Train Epoch: [80/2467]	Loss: 0.269079
     Train Epoch: [100/2467]	Loss: 0.107068
     Train Epoch: [100/2467]	Loss: 0.229896    
 Train Epoch: [100/2467]	Loss: 0.251229
     Train Epoch: [100/2467]	Loss: 0.223217
     Train Epoch: [120/2467]	Loss: 0.092234
     Train Epoch: [120/2467]	Loss: 0.404013
     Train Epoch: [120/2467]	Loss: 0.277328
     Train Epoch: [120/2467]	Loss: 0.083763
         Train Epoch: [140/2467]	Loss: 0.032197
 Train Epoch: [140/2467]	Loss: 0.160429
          Train Epoch: [140/2467]	Loss: 0.287173Train Epoch: [140/2467]	Loss: 0.067978

         Train Epoch: [160/2467]	Loss: 0.232352
     Train Epoch: [160/2467]	Loss: 0.144754
 Train Epoch: [160/2467]	Loss: 0.156862
     Train Epoch: [160/2467]	Loss: 0.217599
         Train Epoch: [180/2467]	Loss: 0.098202
 Train Epoch: [180/2467]	Loss: 0.196442        
  Train Epoch: [180/2467]	Loss: 0.142404Train Epoch: [180/2467]	Loss: 0.175886

         Train Epoch: [200/2467]	Loss: 0.142339 
Train Epoch: [200/2467]	Loss: 0.094920
         Train Epoch: [200/2467]	Loss: 0.115869
 Train Epoch: [200/2467]	Loss: 0.108351
     Train Epoch: [220/2467]	Loss: 0.175284
     Train Epoch: [220/2467]	Loss: 0.218097
     Train Epoch: [220/2467]	Loss: 0.226106
     Train Epoch: [220/2467]	Loss: 0.092924
     Train Epoch: [240/2467]	Loss: 0.021108    
 Train Epoch: [240/2467]	Loss: 0.261481
         Train Epoch: [240/2467]	Loss: 0.178101
 Train Epoch: [240/2467]	Loss: 0.045594
         Train Epoch: [260/2467]	Loss: 0.060149
 Train Epoch: [260/2467]	Loss: 0.059009
         Train Epoch: [260/2467]	Loss: 0.136893
 Train Epoch: [260/2467]	Loss: 0.042352
     Train Epoch: [280/2467]	Loss: 0.099771
         Train Epoch: [280/2467]	Loss: 0.192480 
Train Epoch: [280/2467]	Loss: 0.172855
     Train Epoch: [280/2467]	Loss: 0.144838
     Train Epoch: [300/2467]	Loss: 0.165692
              Train Epoch: [300/2467]	Loss: 0.085155Train Epoch: [300/2467]	Loss: 0.386603
 
Train Epoch: [300/2467]	Loss: 0.050300
         Train Epoch: [320/2467]	Loss: 0.192828
     Train Epoch: [320/2467]	Loss: 0.288867    
 Train Epoch: [320/2467]	Loss: 0.191942
 Train Epoch: [320/2467]	Loss: 0.470062
         Train Epoch: [340/2467]	Loss: 0.062697
         Train Epoch: [340/2467]	Loss: 0.132983
  Train Epoch: [340/2467]	Loss: 0.018692Train Epoch: [340/2467]	Loss: 0.205405

     Train Epoch: [360/2467]	Loss: 0.230039
          Train Epoch: [360/2467]	Loss: 0.093004
Train Epoch: [360/2467]	Loss: 0.061832
     Train Epoch: [360/2467]	Loss: 0.189921
     Train Epoch: [380/2467]	Loss: 0.110480
         Train Epoch: [380/2467]	Loss: 0.328328
 Train Epoch: [380/2467]	Loss: 0.365015
     Train Epoch: [380/2467]	Loss: 0.232559
     Train Epoch: [400/2467]	Loss: 0.111776
         Train Epoch: [400/2467]	Loss: 0.076628
 Train Epoch: [400/2467]	Loss: 0.191369
     Train Epoch: [400/2467]	Loss: 0.213535
     Train Epoch: [420/2467]	Loss: 0.245618
         Train Epoch: [420/2467]	Loss: 0.277861
 Train Epoch: [420/2467]	Loss: 0.160018
     Train Epoch: [420/2467]	Loss: 0.193931
     Train Epoch: [440/2467]	Loss: 0.156845
         Train Epoch: [440/2467]	Loss: 0.095150
     Train Epoch: [440/2467]	Loss: 0.360020
 Train Epoch: [440/2467]	Loss: 0.063845
         Train Epoch: [460/2467]	Loss: 0.260156
 Train Epoch: [460/2467]	Loss: 0.194052
         Train Epoch: [460/2467]	Loss: 0.157788 
Train Epoch: [460/2467]	Loss: 0.158709
     Train Epoch: [480/2467]	Loss: 0.154203
     Train Epoch: [480/2467]	Loss: 0.111129    
     Train Epoch: [480/2467]	Loss: 0.301050 
Train Epoch: [480/2467]	Loss: 0.366570
     Train Epoch: [500/2467]	Loss: 0.136016
     Train Epoch: [500/2467]	Loss: 0.162011    
 Train Epoch: [500/2467]	Loss: 0.154971
     Train Epoch: [500/2467]	Loss: 0.240569
     Train Epoch: [520/2467]	Loss: 0.196818
     Train Epoch: [520/2467]	Loss: 0.212114
     Train Epoch: [520/2467]	Loss: 0.259856
     Train Epoch: [520/2467]	Loss: 0.085629
         Train Epoch: [540/2467]	Loss: 0.153492
 Train Epoch: [540/2467]	Loss: 0.290979
     Train Epoch: [540/2467]	Loss: 0.063279
     Train Epoch: [540/2467]	Loss: 0.164542
     Train Epoch: [560/2467]	Loss: 0.147348
          Train Epoch: [560/2467]	Loss: 0.342869Train Epoch: [560/2467]	Loss: 0.205777

     Train Epoch: [560/2467]	Loss: 0.153947
     Train Epoch: [580/2467]	Loss: 0.332744
     Train Epoch: [580/2467]	Loss: 0.170804
         Train Epoch: [580/2467]	Loss: 0.195379
 Train Epoch: [580/2467]	Loss: 0.079031
         Train Epoch: [600/2467]	Loss: 0.227487
 Train Epoch: [600/2467]	Loss: 0.130095
          Train Epoch: [600/2467]	Loss: 0.222179Train Epoch: [600/2467]	Loss: 0.278221

         Train Epoch: [620/2467]	Loss: 0.195984
 Train Epoch: [620/2467]	Loss: 0.039684    
 Train Epoch: [620/2467]	Loss: 0.150805
     Train Epoch: [620/2467]	Loss: 0.237180
                  Train Epoch: [640/2467]	Loss: 0.196143Train Epoch: [640/2467]	Loss: 0.155955
 
Train Epoch: [640/2467]	Loss: 0.050576
 Train Epoch: [640/2467]	Loss: 0.277161
         Train Epoch: [660/2467]	Loss: 0.033463 
Train Epoch: [660/2467]	Loss: 0.075818    
      Train Epoch: [660/2467]	Loss: 0.209793
Train Epoch: [660/2467]	Loss: 0.401400
     Train Epoch: [680/2467]	Loss: 0.135384
         Train Epoch: [680/2467]	Loss: 0.080443    
  Train Epoch: [680/2467]	Loss: 0.076113
Train Epoch: [680/2467]	Loss: 0.043607
     Train Epoch: [700/2467]	Loss: 0.075365
     Train Epoch: [700/2467]	Loss: 0.313359
     Train Epoch: [700/2467]	Loss: 0.442891
     Train Epoch: [700/2467]	Loss: 0.060626
         Train Epoch: [720/2467]	Loss: 0.026112
 Train Epoch: [720/2467]	Loss: 0.213485
     Train Epoch: [720/2467]	Loss: 0.161431
     Train Epoch: [720/2467]	Loss: 0.137124
         Train Epoch: [740/2467]	Loss: 0.071021    
  Train Epoch: [740/2467]	Loss: 0.329077Train Epoch: [740/2467]	Loss: 0.103413

     Train Epoch: [740/2467]	Loss: 0.074723
     Train Epoch: [760/2467]	Loss: 0.080897
             Train Epoch: [760/2467]	Loss: 0.252313
 Train Epoch: [760/2467]	Loss: 0.461861 
Train Epoch: [760/2467]	Loss: 0.366059
     Train Epoch: [780/2467]	Loss: 0.387328
     Train Epoch: [780/2467]	Loss: 0.381705
          Train Epoch: [780/2467]	Loss: 0.143292Train Epoch: [780/2467]	Loss: 0.372990

     Train Epoch: [800/2467]	Loss: 0.221802
     Train Epoch: [800/2467]	Loss: 0.253784
     Train Epoch: [800/2467]	Loss: 0.131440
     Train Epoch: [800/2467]	Loss: 0.197984
     Train Epoch: [820/2467]	Loss: 0.050044
     Train Epoch: [820/2467]	Loss: 0.426778
         Train Epoch: [820/2467]	Loss: 0.064186
 Train Epoch: [820/2467]	Loss: 0.286998
     Train Epoch: [840/2467]	Loss: 0.373687
         Train Epoch: [840/2467]	Loss: 0.152090
 Train Epoch: [840/2467]	Loss: 0.168696
     Train Epoch: [840/2467]	Loss: 0.067864
     Train Epoch: [860/2467]	Loss: 0.281785
         Train Epoch: [860/2467]	Loss: 0.191441
 Train Epoch: [860/2467]	Loss: 0.106893
     Train Epoch: [860/2467]	Loss: 0.103198
     Train Epoch: [880/2467]	Loss: 0.269987    
      Train Epoch: [880/2467]	Loss: 0.182702Train Epoch: [880/2467]	Loss: 0.226819

     Train Epoch: [880/2467]	Loss: 0.152335
     Train Epoch: [900/2467]	Loss: 0.191462
     Train Epoch: [900/2467]	Loss: 0.099405
     Train Epoch: [900/2467]	Loss: 0.216757
     Train Epoch: [900/2467]	Loss: 0.222079
     Train Epoch: [920/2467]	Loss: 0.312788
     Train Epoch: [920/2467]	Loss: 0.299823
     Train Epoch: [920/2467]	Loss: 0.051539
     Train Epoch: [920/2467]	Loss: 0.414766
     Train Epoch: [940/2467]	Loss: 0.058139
              Train Epoch: [940/2467]	Loss: 0.038539Train Epoch: [940/2467]	Loss: 0.205364

 Train Epoch: [940/2467]	Loss: 0.127098
          Train Epoch: [960/2467]	Loss: 0.307148Train Epoch: [960/2467]	Loss: 0.184552

     Train Epoch: [960/2467]	Loss: 0.098685
     Train Epoch: [960/2467]	Loss: 0.076268
     Train Epoch: [980/2467]	Loss: 0.064623
              Train Epoch: [980/2467]	Loss: 0.257356
Train Epoch: [980/2467]	Loss: 0.165636
 Train Epoch: [980/2467]	Loss: 0.156594
         Train Epoch: [1000/2467]	Loss: 0.320578
 Train Epoch: [1000/2467]	Loss: 0.048236
     Train Epoch: [1000/2467]	Loss: 0.251510
     Train Epoch: [1000/2467]	Loss: 0.056097
         Train Epoch: [1020/2467]	Loss: 0.157366
 Train Epoch: [1020/2467]	Loss: 0.416101
     Train Epoch: [1020/2467]	Loss: 0.092566
     Train Epoch: [1020/2467]	Loss: 0.111567
     Train Epoch: [1040/2467]	Loss: 0.302595
     Train Epoch: [1040/2467]	Loss: 0.143845
     Train Epoch: [1040/2467]	Loss: 0.247390
     Train Epoch: [1040/2467]	Loss: 0.114646
     Train Epoch: [1060/2467]	Loss: 0.281696
         Train Epoch: [1060/2467]	Loss: 0.214107 
Train Epoch: [1060/2467]	Loss: 0.435734
     Train Epoch: [1060/2467]	Loss: 0.113677
     Train Epoch: [1080/2467]	Loss: 0.152525
               Train Epoch: [1080/2467]	Loss: 0.263323Train Epoch: [1080/2467]	Loss: 0.272240
Train Epoch: [1080/2467]	Loss: 0.163029

         Train Epoch: [1100/2467]	Loss: 0.253784 
Train Epoch: [1100/2467]	Loss: 0.260178
     Train Epoch: [1100/2467]	Loss: 0.268904
     Train Epoch: [1100/2467]	Loss: 0.263112
     Train Epoch: [1120/2467]	Loss: 0.264147
     Train Epoch: [1120/2467]	Loss: 0.152478
         Train Epoch: [1120/2467]	Loss: 0.149338
 Train Epoch: [1120/2467]	Loss: 0.130065
     Train Epoch: [1140/2467]	Loss: 0.075283
             Train Epoch: [1140/2467]	Loss: 0.101669
 Train Epoch: [1140/2467]	Loss: 0.534953 
Train Epoch: [1140/2467]	Loss: 0.271340
          Train Epoch: [1160/2467]	Loss: 0.114923
Train Epoch: [1160/2467]	Loss: 0.187871    
     Train Epoch: [1160/2467]	Loss: 0.198160
 Train Epoch: [1160/2467]	Loss: 0.252296
          Train Epoch: [1180/2467]	Loss: 0.253505Train Epoch: [1180/2467]	Loss: 0.184537

     Train Epoch: [1180/2467]	Loss: 0.282409
     Train Epoch: [1180/2467]	Loss: 0.091616
     Train Epoch: [1200/2467]	Loss: 0.149103
         Train Epoch: [1200/2467]	Loss: 0.159883    
  Train Epoch: [1200/2467]	Loss: 0.317261
Train Epoch: [1200/2467]	Loss: 0.213064
         Train Epoch: [1220/2467]	Loss: 0.237483
 Train Epoch: [1220/2467]	Loss: 0.105196
         Train Epoch: [1220/2467]	Loss: 0.296621
 Train Epoch: [1220/2467]	Loss: 0.263387
         Train Epoch: [1240/2467]	Loss: 0.409615
     Train Epoch: [1240/2467]	Loss: 0.333658
 Train Epoch: [1240/2467]	Loss: 0.252669
     Train Epoch: [1240/2467]	Loss: 0.169955
          Train Epoch: [1260/2467]	Loss: 0.073341Train Epoch: [1260/2467]	Loss: 0.064372

         Train Epoch: [1260/2467]	Loss: 0.179910 
Train Epoch: [1260/2467]	Loss: 0.440376
     Train Epoch: [1280/2467]	Loss: 0.263779
     Train Epoch: [1280/2467]	Loss: 0.089214
     Train Epoch: [1280/2467]	Loss: 0.444831
     Train Epoch: [1280/2467]	Loss: 0.853138
         Train Epoch: [1300/2467]	Loss: 0.080591
 Train Epoch: [1300/2467]	Loss: 0.366618
     Train Epoch: [1300/2467]	Loss: 0.104975
     Train Epoch: [1300/2467]	Loss: 0.315873
     Train Epoch: [1320/2467]	Loss: 0.058143
         Train Epoch: [1320/2467]	Loss: 0.152098
 Train Epoch: [1320/2467]	Loss: 0.248061
     Train Epoch: [1320/2467]	Loss: 0.255172
     Train Epoch: [1340/2467]	Loss: 0.255982
         Train Epoch: [1340/2467]	Loss: 0.110716
 Train Epoch: [1340/2467]	Loss: 0.176784
     Train Epoch: [1340/2467]	Loss: 0.378571
     Train Epoch: [1360/2467]	Loss: 0.115007
         Train Epoch: [1360/2467]	Loss: 0.200088 
Train Epoch: [1360/2467]	Loss: 0.139477
     Train Epoch: [1360/2467]	Loss: 0.076688
     Train Epoch: [1380/2467]	Loss: 0.251813    
 Train Epoch: [1380/2467]	Loss: 0.300596
     Train Epoch: [1380/2467]	Loss: 0.227891
     Train Epoch: [1380/2467]	Loss: 0.228640
     Train Epoch: [1400/2467]	Loss: 0.102100          Train Epoch: [1400/2467]	Loss: 0.192061Train Epoch: [1400/2467]	Loss: 0.181571     Train Epoch: [1400/2467]	Loss: 0.188125



         Train Epoch: [1420/2467]	Loss: 0.410491
 Train Epoch: [1420/2467]	Loss: 0.088783
         Train Epoch: [1420/2467]	Loss: 0.127966 
Train Epoch: [1420/2467]	Loss: 0.155962
     Train Epoch: [1440/2467]	Loss: 0.023773
     Train Epoch: [1440/2467]	Loss: 0.061044
     Train Epoch: [1440/2467]	Loss: 0.043914
     Train Epoch: [1440/2467]	Loss: 0.168774
         Train Epoch: [1460/2467]	Loss: 0.044526
         Train Epoch: [1460/2467]	Loss: 0.258357
  Train Epoch: [1460/2467]	Loss: 0.128464Train Epoch: [1460/2467]	Loss: 0.139088

     Train Epoch: [1480/2467]	Loss: 0.031587
         Train Epoch: [1480/2467]	Loss: 0.014135
 Train Epoch: [1480/2467]	Loss: 0.210031
     Train Epoch: [1480/2467]	Loss: 0.243594
     Train Epoch: [1500/2467]	Loss: 0.135636
     Train Epoch: [1500/2467]	Loss: 0.015956
         Train Epoch: [1500/2467]	Loss: 0.123466
 Train Epoch: [1500/2467]	Loss: 0.134340
         Train Epoch: [1520/2467]	Loss: 0.368824
 Train Epoch: [1520/2467]	Loss: 0.205134
     Train Epoch: [1520/2467]	Loss: 0.278082
     Train Epoch: [1520/2467]	Loss: 0.181212
         Train Epoch: [1540/2467]	Loss: 0.118612
 Train Epoch: [1540/2467]	Loss: 0.291936    
     Train Epoch: [1540/2467]	Loss: 0.100834
 Train Epoch: [1540/2467]	Loss: 0.165411
         Train Epoch: [1560/2467]	Loss: 0.394705    
  Train Epoch: [1560/2467]	Loss: 0.365847Train Epoch: [1560/2467]	Loss: 0.469327

     Train Epoch: [1560/2467]	Loss: 0.329074
     Train Epoch: [1580/2467]	Loss: 0.172811
     Train Epoch: [1580/2467]	Loss: 0.198350    
 Train Epoch: [1580/2467]	Loss: 0.230081
     Train Epoch: [1580/2467]	Loss: 0.344358
         Train Epoch: [1600/2467]	Loss: 0.264910 
Train Epoch: [1600/2467]	Loss: 0.079332
     Train Epoch: [1600/2467]	Loss: 0.286297
     Train Epoch: [1600/2467]	Loss: 0.153534
     Train Epoch: [1620/2467]	Loss: 0.170942
          Train Epoch: [1620/2467]	Loss: 0.261394Train Epoch: [1620/2467]	Loss: 0.048532

     Train Epoch: [1620/2467]	Loss: 0.108407
     Train Epoch: [1640/2467]	Loss: 0.255484
     Train Epoch: [1640/2467]	Loss: 0.179127
     Train Epoch: [1640/2467]	Loss: 0.139006
     Train Epoch: [1640/2467]	Loss: 0.255440
     Train Epoch: [1660/2467]	Loss: 0.179493    
 Train Epoch: [1660/2467]	Loss: 0.292569    
     Train Epoch: [1660/2467]	Loss: 0.224055 
Train Epoch: [1660/2467]	Loss: 0.117290
             Train Epoch: [1680/2467]	Loss: 0.064396 
Train Epoch: [1680/2467]	Loss: 0.183074
 Train Epoch: [1680/2467]	Loss: 0.218500    
 Train Epoch: [1680/2467]	Loss: 0.260828
         Train Epoch: [1700/2467]	Loss: 0.093838
     Train Epoch: [1700/2467]	Loss: 0.321699 
Train Epoch: [1700/2467]	Loss: 0.235736
     Train Epoch: [1700/2467]	Loss: 0.202762
     Train Epoch: [1720/2467]	Loss: 0.240012
         Train Epoch: [1720/2467]	Loss: 0.212987 
Train Epoch: [1720/2467]	Loss: 0.072478
     Train Epoch: [1720/2467]	Loss: 0.213592
         Train Epoch: [1740/2467]	Loss: 0.396449 
Train Epoch: [1740/2467]	Loss: 0.032042
     Train Epoch: [1740/2467]	Loss: 0.193340
     Train Epoch: [1740/2467]	Loss: 0.073414
          Train Epoch: [1760/2467]	Loss: 0.164608Train Epoch: [1760/2467]	Loss: 0.156238

     Train Epoch: [1760/2467]	Loss: 0.070285
     Train Epoch: [1760/2467]	Loss: 0.136913
     Train Epoch: [1780/2467]	Loss: 0.256912
             Train Epoch: [1780/2467]	Loss: 0.319489
 Train Epoch: [1780/2467]	Loss: 0.329865
 Train Epoch: [1780/2467]	Loss: 0.110234
         Train Epoch: [1800/2467]	Loss: 0.305487
     Train Epoch: [1800/2467]	Loss: 0.317065
 Train Epoch: [1800/2467]	Loss: 0.215190
     Train Epoch: [1800/2467]	Loss: 0.274023
             Train Epoch: [1820/2467]	Loss: 0.137480 
Train Epoch: [1820/2467]	Loss: 0.361094
 Train Epoch: [1820/2467]	Loss: 0.160568
     Train Epoch: [1820/2467]	Loss: 0.142084
     Train Epoch: [1840/2467]	Loss: 0.134759
     Train Epoch: [1840/2467]	Loss: 0.321978
          Train Epoch: [1840/2467]	Loss: 0.259976
Train Epoch: [1840/2467]	Loss: 0.082137
     Train Epoch: [1860/2467]	Loss: 0.154672    
     Train Epoch: [1860/2467]	Loss: 0.174919
     Train Epoch: [1860/2467]	Loss: 0.143559
 Train Epoch: [1860/2467]	Loss: 0.175773
         Train Epoch: [1880/2467]	Loss: 0.037501
 Train Epoch: [1880/2467]	Loss: 0.104202
         Train Epoch: [1880/2467]	Loss: 0.063907
 Train Epoch: [1880/2467]	Loss: 0.052960
     Train Epoch: [1900/2467]	Loss: 0.140453    
     Train Epoch: [1900/2467]	Loss: 0.180415
 Train Epoch: [1900/2467]	Loss: 0.308910
     Train Epoch: [1900/2467]	Loss: 0.517251
     Train Epoch: [1920/2467]	Loss: 0.020848
          Train Epoch: [1920/2467]	Loss: 0.226806Train Epoch: [1920/2467]	Loss: 0.119545

     Train Epoch: [1920/2467]	Loss: 0.262691
         Train Epoch: [1940/2467]	Loss: 0.086976 
Train Epoch: [1940/2467]	Loss: 0.206048
     Train Epoch: [1940/2467]	Loss: 0.396404
     Train Epoch: [1940/2467]	Loss: 0.117571
     Train Epoch: [1960/2467]	Loss: 0.206620
     Train Epoch: [1960/2467]	Loss: 0.131947
         Train Epoch: [1960/2467]	Loss: 0.100548
 Train Epoch: [1960/2467]	Loss: 0.239381
     Train Epoch: [1980/2467]	Loss: 0.150644    
 Train Epoch: [1980/2467]	Loss: 0.307215
         Train Epoch: [1980/2467]	Loss: 0.322824
 Train Epoch: [1980/2467]	Loss: 0.267075
     Train Epoch: [2000/2467]	Loss: 0.161377
     Train Epoch: [2000/2467]	Loss: 0.030515
          Train Epoch: [2000/2467]	Loss: 0.070766
Train Epoch: [2000/2467]	Loss: 0.232297
     Train Epoch: [2020/2467]	Loss: 0.163825
         Train Epoch: [2020/2467]	Loss: 0.249514
 Train Epoch: [2020/2467]	Loss: 0.175303
     Train Epoch: [2020/2467]	Loss: 0.070971
     Train Epoch: [2040/2467]	Loss: 0.105458
              Train Epoch: [2040/2467]	Loss: 0.098085 Train Epoch: [2040/2467]	Loss: 0.388081

Train Epoch: [2040/2467]	Loss: 0.192923
     Train Epoch: [2060/2467]	Loss: 0.218123
         Train Epoch: [2060/2467]	Loss: 0.264771 
Train Epoch: [2060/2467]	Loss: 0.273798
     Train Epoch: [2060/2467]	Loss: 0.109125
     Train Epoch: [2080/2467]	Loss: 0.057686
     Train Epoch: [2080/2467]	Loss: 0.274820
     Train Epoch: [2080/2467]	Loss: 0.074718
     Train Epoch: [2080/2467]	Loss: 0.059519
     Train Epoch: [2100/2467]	Loss: 0.524742
              Train Epoch: [2100/2467]	Loss: 0.134560 
Train Epoch: [2100/2467]	Loss: 0.107685Train Epoch: [2100/2467]	Loss: 0.103842

         Train Epoch: [2120/2467]	Loss: 0.051517
 Train Epoch: [2120/2467]	Loss: 0.215131    
     Train Epoch: [2120/2467]	Loss: 0.094001 
Train Epoch: [2120/2467]	Loss: 0.254491
         Train Epoch: [2140/2467]	Loss: 0.575854 
    Train Epoch: [2140/2467]	Loss: 0.254364
 Train Epoch: [2140/2467]	Loss: 0.366847
     Train Epoch: [2140/2467]	Loss: 0.198069
     Train Epoch: [2160/2467]	Loss: 0.065715
     Train Epoch: [2160/2467]	Loss: 0.330714
     Train Epoch: [2160/2467]	Loss: 0.201034
     Train Epoch: [2160/2467]	Loss: 0.118435
     Train Epoch: [2180/2467]	Loss: 0.271427
         Train Epoch: [2180/2467]	Loss: 0.163475
 Train Epoch: [2180/2467]	Loss: 0.192596
     Train Epoch: [2180/2467]	Loss: 0.118146
         Train Epoch: [2200/2467]	Loss: 0.051514
 Train Epoch: [2200/2467]	Loss: 0.200727
          Train Epoch: [2200/2467]	Loss: 0.125502Train Epoch: [2200/2467]	Loss: 0.156170

     Train Epoch: [2220/2467]	Loss: 0.065297
          Train Epoch: [2220/2467]	Loss: 0.191949Train Epoch: [2220/2467]	Loss: 0.119172

     Train Epoch: [2220/2467]	Loss: 0.136407
         Train Epoch: [2240/2467]	Loss: 0.081449
 Train Epoch: [2240/2467]	Loss: 0.171945
          Train Epoch: [2240/2467]	Loss: 0.028229
Train Epoch: [2240/2467]	Loss: 0.050484
     Train Epoch: [2260/2467]	Loss: 0.242401
     Train Epoch: [2260/2467]	Loss: 0.201594
     Train Epoch: [2260/2467]	Loss: 0.033184
     Train Epoch: [2260/2467]	Loss: 0.421583
     Train Epoch: [2280/2467]	Loss: 0.178164    
     Train Epoch: [2280/2467]	Loss: 0.086298
 Train Epoch: [2280/2467]	Loss: 0.090116
     Train Epoch: [2280/2467]	Loss: 0.107115
         Train Epoch: [2300/2467]	Loss: 0.254923
     Train Epoch: [2300/2467]	Loss: 0.105880
 Train Epoch: [2300/2467]	Loss: 0.130357
     Train Epoch: [2300/2467]	Loss: 0.053078
     Train Epoch: [2320/2467]	Loss: 0.220181    
 Train Epoch: [2320/2467]	Loss: 0.159948
     Train Epoch: [2320/2467]	Loss: 0.018874
     Train Epoch: [2320/2467]	Loss: 0.219631
     Train Epoch: [2340/2467]	Loss: 0.520506
     Train Epoch: [2340/2467]	Loss: 0.242376
     Train Epoch: [2340/2467]	Loss: 0.136970
     Train Epoch: [2340/2467]	Loss: 0.139216
     Train Epoch: [2360/2467]	Loss: 0.139974
              Train Epoch: [2360/2467]	Loss: 0.150434Train Epoch: [2360/2467]	Loss: 0.087558
 
Train Epoch: [2360/2467]	Loss: 0.355927
              Train Epoch: [2380/2467]	Loss: 0.276251Train Epoch: [2380/2467]	Loss: 0.137979 

Train Epoch: [2380/2467]	Loss: 0.469302    
 Train Epoch: [2380/2467]	Loss: 0.183355
         Train Epoch: [2400/2467]	Loss: 0.110652
     Train Epoch: [2400/2467]	Loss: 0.259492
     Train Epoch: [2400/2467]	Loss: 0.138891
 Train Epoch: [2400/2467]	Loss: 0.203940
     Train Epoch: [2420/2467]	Loss: 0.183707    
          Train Epoch: [2420/2467]	Loss: 0.264515 
Train Epoch: [2420/2467]	Loss: 0.093180Train Epoch: [2420/2467]	Loss: 0.375772

             Train Epoch: [2440/2467]	Loss: 0.185079
     Train Epoch: [2440/2467]	Loss: 0.149624 
Train Epoch: [2440/2467]	Loss: 0.281444
 Train Epoch: [2440/2467]	Loss: 0.059300
     Train Epoch: [2460/2467]	Loss: 0.140783
         Train Epoch: [2460/2467]	Loss: 0.097372
     Train Epoch: [2460/2467]	Loss: 0.124954
 Train Epoch: [2460/2467]	Loss: 0.297538
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 20 epoch =====
     2025-05-11.07-19-53
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 20 epoch =====
     2025-05-11.07-19-53
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 20 epoch =====
     2025-05-11.07-19-54
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 20 epoch =====
     2025-05-11.07-19-54
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.265950
         Train Epoch: [0/2467]	Loss: 0.332803
     Train Epoch: [0/2467]	Loss: 0.201852
 Train Epoch: [0/2467]	Loss: 0.218173
     Train Epoch: [20/2467]	Loss: 0.326358
               Train Epoch: [20/2467]	Loss: 0.112562Train Epoch: [20/2467]	Loss: 0.230796

Train Epoch: [20/2467]	Loss: 0.083358
         Train Epoch: [40/2467]	Loss: 0.141681
     Train Epoch: [40/2467]	Loss: 0.173894
 Train Epoch: [40/2467]	Loss: 0.414740
     Train Epoch: [40/2467]	Loss: 0.136121
         Train Epoch: [60/2467]	Loss: 0.255197
 Train Epoch: [60/2467]	Loss: 0.042270
         Train Epoch: [60/2467]	Loss: 0.194910
 Train Epoch: [60/2467]	Loss: 0.170095
     Train Epoch: [80/2467]	Loss: 0.388061
         Train Epoch: [80/2467]	Loss: 0.154164
 Train Epoch: [80/2467]	Loss: 0.131045
     Train Epoch: [80/2467]	Loss: 0.143663
     Train Epoch: [100/2467]	Loss: 0.091319
     Train Epoch: [100/2467]	Loss: 0.244127
     Train Epoch: [100/2467]	Loss: 0.233075
     Train Epoch: [100/2467]	Loss: 0.184463
     Train Epoch: [120/2467]	Loss: 0.060102
     Train Epoch: [120/2467]	Loss: 0.384972
         Train Epoch: [120/2467]	Loss: 0.314383
 Train Epoch: [120/2467]	Loss: 0.076862
         Train Epoch: [140/2467]	Loss: 0.071099
 Train Epoch: [140/2467]	Loss: 0.162398
     Train Epoch: [140/2467]	Loss: 0.100618    
 Train Epoch: [140/2467]	Loss: 0.321799
     Train Epoch: [160/2467]	Loss: 0.146464
     Train Epoch: [160/2467]	Loss: 0.147504
     Train Epoch: [160/2467]	Loss: 0.367972
     Train Epoch: [160/2467]	Loss: 0.150079
     Train Epoch: [180/2467]	Loss: 0.135178
         Train Epoch: [180/2467]	Loss: 0.228637 
Train Epoch: [180/2467]	Loss: 0.186847
     Train Epoch: [180/2467]	Loss: 0.115794
         Train Epoch: [200/2467]	Loss: 0.093059
 Train Epoch: [200/2467]	Loss: 0.087009
     Train Epoch: [200/2467]	Loss: 0.075732
     Train Epoch: [200/2467]	Loss: 0.136725
     Train Epoch: [220/2467]	Loss: 0.166126
     Train Epoch: [220/2467]	Loss: 0.095190
     Train Epoch: [220/2467]	Loss: 0.206601
     Train Epoch: [220/2467]	Loss: 0.225683
         Train Epoch: [240/2467]	Loss: 0.264921
     Train Epoch: [240/2467]	Loss: 0.026247
     Train Epoch: [240/2467]	Loss: 0.043443 
Train Epoch: [240/2467]	Loss: 0.185498
     Train Epoch: [260/2467]	Loss: 0.058233
         Train Epoch: [260/2467]	Loss: 0.040426
 Train Epoch: [260/2467]	Loss: 0.090973
     Train Epoch: [260/2467]	Loss: 0.123155
     Train Epoch: [280/2467]	Loss: 0.091329
     Train Epoch: [280/2467]	Loss: 0.345811
         Train Epoch: [280/2467]	Loss: 0.276215
 Train Epoch: [280/2467]	Loss: 0.281142
         Train Epoch: [300/2467]	Loss: 0.067351
         Train Epoch: [300/2467]	Loss: 0.089043
 Train Epoch: [300/2467]	Loss: 0.399041
 Train Epoch: [300/2467]	Loss: 0.163222
         Train Epoch: [320/2467]	Loss: 0.201873
         Train Epoch: [320/2467]	Loss: 0.167050
  Train Epoch: [320/2467]	Loss: 0.387454Train Epoch: [320/2467]	Loss: 0.452334

         Train Epoch: [340/2467]	Loss: 0.120242
     Train Epoch: [340/2467]	Loss: 0.057312
 Train Epoch: [340/2467]	Loss: 0.020952
     Train Epoch: [340/2467]	Loss: 0.209946
     Train Epoch: [360/2467]	Loss: 0.225088
         Train Epoch: [360/2467]	Loss: 0.095111
 Train Epoch: [360/2467]	Loss: 0.084541
     Train Epoch: [360/2467]	Loss: 0.220626
     Train Epoch: [380/2467]	Loss: 0.095133
         Train Epoch: [380/2467]	Loss: 0.364060
 Train Epoch: [380/2467]	Loss: 0.260368
     Train Epoch: [380/2467]	Loss: 0.197889
     Train Epoch: [400/2467]	Loss: 0.117329
         Train Epoch: [400/2467]	Loss: 0.182910
 Train Epoch: [400/2467]	Loss: 0.060597
     Train Epoch: [400/2467]	Loss: 0.200637
     Train Epoch: [420/2467]	Loss: 0.221805
     Train Epoch: [420/2467]	Loss: 0.273229
     Train Epoch: [420/2467]	Loss: 0.104008
     Train Epoch: [420/2467]	Loss: 0.138748
     Train Epoch: [440/2467]	Loss: 0.163355
     Train Epoch: [440/2467]	Loss: 0.095388
     Train Epoch: [440/2467]	Loss: 0.061999
     Train Epoch: [440/2467]	Loss: 0.341870
         Train Epoch: [460/2467]	Loss: 0.256948
 Train Epoch: [460/2467]	Loss: 0.176076
     Train Epoch: [460/2467]	Loss: 0.134172
     Train Epoch: [460/2467]	Loss: 0.217133
          Train Epoch: [480/2467]	Loss: 0.161628Train Epoch: [480/2467]	Loss: 0.084870

         Train Epoch: [480/2467]	Loss: 0.347115
 Train Epoch: [480/2467]	Loss: 0.298027
         Train Epoch: [500/2467]	Loss: 0.137064
 Train Epoch: [500/2467]	Loss: 0.269710
     Train Epoch: [500/2467]	Loss: 0.148255
     Train Epoch: [500/2467]	Loss: 0.124826
     Train Epoch: [520/2467]	Loss: 0.233722
     Train Epoch: [520/2467]	Loss: 0.070887
     Train Epoch: [520/2467]	Loss: 0.283630
     Train Epoch: [520/2467]	Loss: 0.192173
         Train Epoch: [540/2467]	Loss: 0.207929
 Train Epoch: [540/2467]	Loss: 0.304115
     Train Epoch: [540/2467]	Loss: 0.122345
     Train Epoch: [540/2467]	Loss: 0.068942
         Train Epoch: [560/2467]	Loss: 0.243216    
 Train Epoch: [560/2467]	Loss: 0.144321
 Train Epoch: [560/2467]	Loss: 0.231568
     Train Epoch: [560/2467]	Loss: 0.162352
     Train Epoch: [580/2467]	Loss: 0.325606
         Train Epoch: [580/2467]	Loss: 0.061206
 Train Epoch: [580/2467]	Loss: 0.146490
     Train Epoch: [580/2467]	Loss: 0.209508
         Train Epoch: [600/2467]	Loss: 0.215257
 Train Epoch: [600/2467]	Loss: 0.128202
     Train Epoch: [600/2467]	Loss: 0.320227
     Train Epoch: [600/2467]	Loss: 0.224422
         Train Epoch: [620/2467]	Loss: 0.207959
 Train Epoch: [620/2467]	Loss: 0.028024
     Train Epoch: [620/2467]	Loss: 0.150035
     Train Epoch: [620/2467]	Loss: 0.245457
     Train Epoch: [640/2467]	Loss: 0.112520
         Train Epoch: [640/2467]	Loss: 0.051223
     Train Epoch: [640/2467]	Loss: 0.182971
 Train Epoch: [640/2467]	Loss: 0.254577
     Train Epoch: [660/2467]	Loss: 0.033236
         Train Epoch: [660/2467]	Loss: 0.217692 
Train Epoch: [660/2467]	Loss: 0.050546
     Train Epoch: [660/2467]	Loss: 0.453468
         Train Epoch: [680/2467]	Loss: 0.097580
     Train Epoch: [680/2467]	Loss: 0.114886 
Train Epoch: [680/2467]	Loss: 0.053075
     Train Epoch: [680/2467]	Loss: 0.076448
              Train Epoch: [700/2467]	Loss: 0.415049Train Epoch: [700/2467]	Loss: 0.079211

 Train Epoch: [700/2467]	Loss: 0.343760
     Train Epoch: [700/2467]	Loss: 0.069647
         Train Epoch: [720/2467]	Loss: 0.226036
 Train Epoch: [720/2467]	Loss: 0.094246
     Train Epoch: [720/2467]	Loss: 0.132300
     Train Epoch: [720/2467]	Loss: 0.047013
     Train Epoch: [740/2467]	Loss: 0.058578
         Train Epoch: [740/2467]	Loss: 0.061749
 Train Epoch: [740/2467]	Loss: 0.096993
     Train Epoch: [740/2467]	Loss: 0.338136
     Train Epoch: [760/2467]	Loss: 0.074892
         Train Epoch: [760/2467]	Loss: 0.269193
 Train Epoch: [760/2467]	Loss: 0.495768
     Train Epoch: [760/2467]	Loss: 0.310781
     Train Epoch: [780/2467]	Loss: 0.375903
     Train Epoch: [780/2467]	Loss: 0.320528
     Train Epoch: [780/2467]	Loss: 0.344965
     Train Epoch: [780/2467]	Loss: 0.165902
     Train Epoch: [800/2467]	Loss: 0.174125
         Train Epoch: [800/2467]	Loss: 0.193702
 Train Epoch: [800/2467]	Loss: 0.262122
     Train Epoch: [800/2467]	Loss: 0.125625
     Train Epoch: [820/2467]	Loss: 0.045690
     Train Epoch: [820/2467]	Loss: 0.428914
         Train Epoch: [820/2467]	Loss: 0.061985
 Train Epoch: [820/2467]	Loss: 0.450412
              Train Epoch: [840/2467]	Loss: 0.137580
Train Epoch: [840/2467]	Loss: 0.113165
 Train Epoch: [840/2467]	Loss: 0.076313
     Train Epoch: [840/2467]	Loss: 0.318634
          Train Epoch: [860/2467]	Loss: 0.281282Train Epoch: [860/2467]	Loss: 0.211749

          Train Epoch: [860/2467]	Loss: 0.067320Train Epoch: [860/2467]	Loss: 0.073161

     Train Epoch: [880/2467]	Loss: 0.202968
     Train Epoch: [880/2467]	Loss: 0.229519
     Train Epoch: [880/2467]	Loss: 0.244895
     Train Epoch: [880/2467]	Loss: 0.170922
     Train Epoch: [900/2467]	Loss: 0.205427
     Train Epoch: [900/2467]	Loss: 0.211616
     Train Epoch: [900/2467]	Loss: 0.114841
     Train Epoch: [900/2467]	Loss: 0.219315
     Train Epoch: [920/2467]	Loss: 0.359205
         Train Epoch: [920/2467]	Loss: 0.054380
     Train Epoch: [920/2467]	Loss: 0.261977
 Train Epoch: [920/2467]	Loss: 0.311621
         Train Epoch: [940/2467]	Loss: 0.041690 
Train Epoch: [940/2467]	Loss: 0.365424
         Train Epoch: [940/2467]	Loss: 0.215508
 Train Epoch: [940/2467]	Loss: 0.220108
     Train Epoch: [960/2467]	Loss: 0.324731
     Train Epoch: [960/2467]	Loss: 0.092678
     Train Epoch: [960/2467]	Loss: 0.084215
     Train Epoch: [960/2467]	Loss: 0.058152
         Train Epoch: [980/2467]	Loss: 0.239483
 Train Epoch: [980/2467]	Loss: 0.074766
          Train Epoch: [980/2467]	Loss: 0.166978Train Epoch: [980/2467]	Loss: 0.138493

             Train Epoch: [1000/2467]	Loss: 0.049738
  Train Epoch: [1000/2467]	Loss: 0.059255Train Epoch: [1000/2467]	Loss: 0.309469

     Train Epoch: [1000/2467]	Loss: 0.242474
          Train Epoch: [1020/2467]	Loss: 0.437472
Train Epoch: [1020/2467]	Loss: 0.168128
     Train Epoch: [1020/2467]	Loss: 0.099683
     Train Epoch: [1020/2467]	Loss: 0.111028
     Train Epoch: [1040/2467]	Loss: 0.105525
         Train Epoch: [1040/2467]	Loss: 0.236543
     Train Epoch: [1040/2467]	Loss: 0.153827
 Train Epoch: [1040/2467]	Loss: 0.140589
         Train Epoch: [1060/2467]	Loss: 0.282036
 Train Epoch: [1060/2467]	Loss: 0.168201
     Train Epoch: [1060/2467]	Loss: 0.442694
     Train Epoch: [1060/2467]	Loss: 0.104735
     Train Epoch: [1080/2467]	Loss: 0.136538
             Train Epoch: [1080/2467]	Loss: 0.140023 
 Train Epoch: [1080/2467]	Loss: 0.275793
Train Epoch: [1080/2467]	Loss: 0.274953
         Train Epoch: [1100/2467]	Loss: 0.257475 
Train Epoch: [1100/2467]	Loss: 0.240553
     Train Epoch: [1100/2467]	Loss: 0.233858
     Train Epoch: [1100/2467]	Loss: 0.278250
     Train Epoch: [1120/2467]	Loss: 0.256800
         Train Epoch: [1120/2467]	Loss: 0.148448
 Train Epoch: [1120/2467]	Loss: 0.128898
     Train Epoch: [1120/2467]	Loss: 0.160764
              Train Epoch: [1140/2467]	Loss: 0.088336Train Epoch: [1140/2467]	Loss: 0.431698

 Train Epoch: [1140/2467]	Loss: 0.062172
     Train Epoch: [1140/2467]	Loss: 0.272952
          Train Epoch: [1160/2467]	Loss: 0.144269
Train Epoch: [1160/2467]	Loss: 0.103884
     Train Epoch: [1160/2467]	Loss: 0.228176
     Train Epoch: [1160/2467]	Loss: 0.122390
     Train Epoch: [1180/2467]	Loss: 0.069136
         Train Epoch: [1180/2467]	Loss: 0.165713
 Train Epoch: [1180/2467]	Loss: 0.210069
     Train Epoch: [1180/2467]	Loss: 0.319724
         Train Epoch: [1200/2467]	Loss: 0.084970
     Train Epoch: [1200/2467]	Loss: 0.162617
 Train Epoch: [1200/2467]	Loss: 0.216711
     Train Epoch: [1200/2467]	Loss: 0.307101
         Train Epoch: [1220/2467]	Loss: 0.284026
     Train Epoch: [1220/2467]	Loss: 0.109358
     Train Epoch: [1220/2467]	Loss: 0.252981
 Train Epoch: [1220/2467]	Loss: 0.276223
             Train Epoch: [1240/2467]	Loss: 0.260413
 Train Epoch: [1240/2467]	Loss: 0.351598
 Train Epoch: [1240/2467]	Loss: 0.316789    
 Train Epoch: [1240/2467]	Loss: 0.228727
               Train Epoch: [1260/2467]	Loss: 0.097037Train Epoch: [1260/2467]	Loss: 0.066355Train Epoch: [1260/2467]	Loss: 0.157985


     Train Epoch: [1260/2467]	Loss: 0.434352
     Train Epoch: [1280/2467]	Loss: 0.251551
     Train Epoch: [1280/2467]	Loss: 0.574503    
 Train Epoch: [1280/2467]	Loss: 0.467038
     Train Epoch: [1280/2467]	Loss: 0.101490
     Train Epoch: [1300/2467]	Loss: 0.341923    
     Train Epoch: [1300/2467]	Loss: 0.247454 
Train Epoch: [1300/2467]	Loss: 0.067047
     Train Epoch: [1300/2467]	Loss: 0.108299
     Train Epoch: [1320/2467]	Loss: 0.195804
          Train Epoch: [1320/2467]	Loss: 0.136903Train Epoch: [1320/2467]	Loss: 0.268884

     Train Epoch: [1320/2467]	Loss: 0.055319
          Train Epoch: [1340/2467]	Loss: 0.159616
Train Epoch: [1340/2467]	Loss: 0.367704    
 Train Epoch: [1340/2467]	Loss: 0.175074
     Train Epoch: [1340/2467]	Loss: 0.266220
         Train Epoch: [1360/2467]	Loss: 0.104831
 Train Epoch: [1360/2467]	Loss: 0.157535
         Train Epoch: [1360/2467]	Loss: 0.061335
 Train Epoch: [1360/2467]	Loss: 0.165263
         Train Epoch: [1380/2467]	Loss: 0.185335
     Train Epoch: [1380/2467]	Loss: 0.274490
 Train Epoch: [1380/2467]	Loss: 0.218305
     Train Epoch: [1380/2467]	Loss: 0.330716
     Train Epoch: [1400/2467]	Loss: 0.195193
          Train Epoch: [1400/2467]	Loss: 0.186967Train Epoch: [1400/2467]	Loss: 0.131086    

 Train Epoch: [1400/2467]	Loss: 0.203818
     Train Epoch: [1420/2467]	Loss: 0.139879    
     Train Epoch: [1420/2467]	Loss: 0.164064 
Train Epoch: [1420/2467]	Loss: 0.150417
     Train Epoch: [1420/2467]	Loss: 0.412684
     Train Epoch: [1440/2467]	Loss: 0.045741
     Train Epoch: [1440/2467]	Loss: 0.149425
     Train Epoch: [1440/2467]	Loss: 0.045025    
 Train Epoch: [1440/2467]	Loss: 0.062838
     Train Epoch: [1460/2467]	Loss: 0.044721    
      Train Epoch: [1460/2467]	Loss: 0.118252Train Epoch: [1460/2467]	Loss: 0.110123    

 Train Epoch: [1460/2467]	Loss: 0.244977
              Train Epoch: [1480/2467]	Loss: 0.204499Train Epoch: [1480/2467]	Loss: 0.023633

     Train Epoch: [1480/2467]	Loss: 0.014633
 Train Epoch: [1480/2467]	Loss: 0.264709
         Train Epoch: [1500/2467]	Loss: 0.017232
 Train Epoch: [1500/2467]	Loss: 0.101890
     Train Epoch: [1500/2467]	Loss: 0.145875
     Train Epoch: [1500/2467]	Loss: 0.128493
         Train Epoch: [1520/2467]	Loss: 0.259757
 Train Epoch: [1520/2467]	Loss: 0.155430
         Train Epoch: [1520/2467]	Loss: 0.242939
 Train Epoch: [1520/2467]	Loss: 0.172190
     Train Epoch: [1540/2467]	Loss: 0.119308
     Train Epoch: [1540/2467]	Loss: 0.279534
         Train Epoch: [1540/2467]	Loss: 0.068109
 Train Epoch: [1540/2467]	Loss: 0.136983
         Train Epoch: [1560/2467]	Loss: 0.102436
 Train Epoch: [1560/2467]	Loss: 0.443920
         Train Epoch: [1560/2467]	Loss: 0.314273 
Train Epoch: [1560/2467]	Loss: 0.500343
     Train Epoch: [1580/2467]	Loss: 0.097890
              Train Epoch: [1580/2467]	Loss: 0.226369Train Epoch: [1580/2467]	Loss: 0.302477
 
Train Epoch: [1580/2467]	Loss: 0.161151
     Train Epoch: [1600/2467]	Loss: 0.333319        
  Train Epoch: [1600/2467]	Loss: 0.131766Train Epoch: [1600/2467]	Loss: 0.262202

     Train Epoch: [1600/2467]	Loss: 0.050164
     Train Epoch: [1620/2467]	Loss: 0.198977
               Train Epoch: [1620/2467]	Loss: 0.228560Train Epoch: [1620/2467]	Loss: 0.041244Train Epoch: [1620/2467]	Loss: 0.102739


         Train Epoch: [1640/2467]	Loss: 0.224193
         Train Epoch: [1640/2467]	Loss: 0.147000
  Train Epoch: [1640/2467]	Loss: 0.215320Train Epoch: [1640/2467]	Loss: 0.098303

         Train Epoch: [1660/2467]	Loss: 0.208579
     Train Epoch: [1660/2467]	Loss: 0.258148
 Train Epoch: [1660/2467]	Loss: 0.133716
     Train Epoch: [1660/2467]	Loss: 0.141159
     Train Epoch: [1680/2467]	Loss: 0.186767    
 Train Epoch: [1680/2467]	Loss: 0.069290
     Train Epoch: [1680/2467]	Loss: 0.191250
     Train Epoch: [1680/2467]	Loss: 0.165766
     Train Epoch: [1700/2467]	Loss: 0.186544
     Train Epoch: [1700/2467]	Loss: 0.105947
         Train Epoch: [1700/2467]	Loss: 0.350215
 Train Epoch: [1700/2467]	Loss: 0.190162
     Train Epoch: [1720/2467]	Loss: 0.232552
          Train Epoch: [1720/2467]	Loss: 0.076877Train Epoch: [1720/2467]	Loss: 0.190039

     Train Epoch: [1720/2467]	Loss: 0.164723
     Train Epoch: [1740/2467]	Loss: 0.073103
     Train Epoch: [1740/2467]	Loss: 0.043013    
     Train Epoch: [1740/2467]	Loss: 0.193957
 Train Epoch: [1740/2467]	Loss: 0.349383
     Train Epoch: [1760/2467]	Loss: 0.148740
          Train Epoch: [1760/2467]	Loss: 0.070735Train Epoch: [1760/2467]	Loss: 0.115873

     Train Epoch: [1760/2467]	Loss: 0.138868
             Train Epoch: [1780/2467]	Loss: 0.272927
  Train Epoch: [1780/2467]	Loss: 0.307178
Train Epoch: [1780/2467]	Loss: 0.282670
     Train Epoch: [1780/2467]	Loss: 0.116669
                    Train Epoch: [1800/2467]	Loss: 0.294524
Train Epoch: [1800/2467]	Loss: 0.245837
Train Epoch: [1800/2467]	Loss: 0.293504Train Epoch: [1800/2467]	Loss: 0.210652

     Train Epoch: [1820/2467]	Loss: 0.332594    
     Train Epoch: [1820/2467]	Loss: 0.119220
 Train Epoch: [1820/2467]	Loss: 0.137890
     Train Epoch: [1820/2467]	Loss: 0.138925
     Train Epoch: [1840/2467]	Loss: 0.139816
              Train Epoch: [1840/2467]	Loss: 0.295868Train Epoch: [1840/2467]	Loss: 0.287607

 Train Epoch: [1840/2467]	Loss: 0.068009
         Train Epoch: [1860/2467]	Loss: 0.176288
     Train Epoch: [1860/2467]	Loss: 0.157799
     Train Epoch: [1860/2467]	Loss: 0.176772
 Train Epoch: [1860/2467]	Loss: 0.119179
     Train Epoch: [1880/2467]	Loss: 0.109561
     Train Epoch: [1880/2467]	Loss: 0.062865
     Train Epoch: [1880/2467]	Loss: 0.045495
     Train Epoch: [1880/2467]	Loss: 0.067698
     Train Epoch: [1900/2467]	Loss: 0.116117
          Train Epoch: [1900/2467]	Loss: 0.312561    
Train Epoch: [1900/2467]	Loss: 0.157779
 Train Epoch: [1900/2467]	Loss: 0.309926
         Train Epoch: [1920/2467]	Loss: 0.018404
     Train Epoch: [1920/2467]	Loss: 0.200378
 Train Epoch: [1920/2467]	Loss: 0.109502    
 Train Epoch: [1920/2467]	Loss: 0.196196
         Train Epoch: [1940/2467]	Loss: 0.090067
 Train Epoch: [1940/2467]	Loss: 0.118733
     Train Epoch: [1940/2467]	Loss: 0.374831
     Train Epoch: [1940/2467]	Loss: 0.215418
     Train Epoch: [1960/2467]	Loss: 0.248522
         Train Epoch: [1960/2467]	Loss: 0.121440 
Train Epoch: [1960/2467]	Loss: 0.221082
     Train Epoch: [1960/2467]	Loss: 0.135912
     Train Epoch: [1980/2467]	Loss: 0.128774
         Train Epoch: [1980/2467]	Loss: 0.273481
 Train Epoch: [1980/2467]	Loss: 0.257534
     Train Epoch: [1980/2467]	Loss: 0.252657
         Train Epoch: [2000/2467]	Loss: 0.148155
 Train Epoch: [2000/2467]	Loss: 0.037868
     Train Epoch: [2000/2467]	Loss: 0.075134    
 Train Epoch: [2000/2467]	Loss: 0.435091
     Train Epoch: [2020/2467]	Loss: 0.147786
         Train Epoch: [2020/2467]	Loss: 0.375260
     Train Epoch: [2020/2467]	Loss: 0.180386
 Train Epoch: [2020/2467]	Loss: 0.065282
         Train Epoch: [2040/2467]	Loss: 0.139400
 Train Epoch: [2040/2467]	Loss: 0.095849
     Train Epoch: [2040/2467]	Loss: 0.209557
     Train Epoch: [2040/2467]	Loss: 0.392018
     Train Epoch: [2060/2467]	Loss: 0.171384
             Train Epoch: [2060/2467]	Loss: 0.211029 
Train Epoch: [2060/2467]	Loss: 0.247950
 Train Epoch: [2060/2467]	Loss: 0.205341
         Train Epoch: [2080/2467]	Loss: 0.258766
     Train Epoch: [2080/2467]	Loss: 0.126546
 Train Epoch: [2080/2467]	Loss: 0.068070
     Train Epoch: [2080/2467]	Loss: 0.074822
                  Train Epoch: [2100/2467]	Loss: 0.135780 Train Epoch: [2100/2467]	Loss: 0.524689
 
Train Epoch: [2100/2467]	Loss: 0.145659Train Epoch: [2100/2467]	Loss: 0.101685

     Train Epoch: [2120/2467]	Loss: 0.361073
          Train Epoch: [2120/2467]	Loss: 0.099511Train Epoch: [2120/2467]	Loss: 0.049074

     Train Epoch: [2120/2467]	Loss: 0.219437
         Train Epoch: [2140/2467]	Loss: 0.199026
         Train Epoch: [2140/2467]	Loss: 0.548790
  Train Epoch: [2140/2467]	Loss: 0.306750Train Epoch: [2140/2467]	Loss: 0.377955

     Train Epoch: [2160/2467]	Loss: 0.334365
     Train Epoch: [2160/2467]	Loss: 0.159688
     Train Epoch: [2160/2467]	Loss: 0.128073
     Train Epoch: [2160/2467]	Loss: 0.047796
         Train Epoch: [2180/2467]	Loss: 0.100909
 Train Epoch: [2180/2467]	Loss: 0.280184
     Train Epoch: [2180/2467]	Loss: 0.209340
     Train Epoch: [2180/2467]	Loss: 0.152531
         Train Epoch: [2200/2467]	Loss: 0.258574     
Train Epoch: [2200/2467]	Loss: 0.050161
 Train Epoch: [2200/2467]	Loss: 0.105376
     Train Epoch: [2200/2467]	Loss: 0.133893
     Train Epoch: [2220/2467]	Loss: 0.062879
     Train Epoch: [2220/2467]	Loss: 0.123325
     Train Epoch: [2220/2467]	Loss: 0.194488
     Train Epoch: [2220/2467]	Loss: 0.115875
         Train Epoch: [2240/2467]	Loss: 0.156519 
Train Epoch: [2240/2467]	Loss: 0.072239
     Train Epoch: [2240/2467]	Loss: 0.042315
     Train Epoch: [2240/2467]	Loss: 0.031986
         Train Epoch: [2260/2467]	Loss: 0.364758
 Train Epoch: [2260/2467]	Loss: 0.046985    
 Train Epoch: [2260/2467]	Loss: 0.214177
     Train Epoch: [2260/2467]	Loss: 0.388737
     Train Epoch: [2280/2467]	Loss: 0.182567
          Train Epoch: [2280/2467]	Loss: 0.212743
Train Epoch: [2280/2467]	Loss: 0.100000
     Train Epoch: [2280/2467]	Loss: 0.093956
         Train Epoch: [2300/2467]	Loss: 0.315269
 Train Epoch: [2300/2467]	Loss: 0.085824
         Train Epoch: [2300/2467]	Loss: 0.101137
 Train Epoch: [2300/2467]	Loss: 0.047640
     Train Epoch: [2320/2467]	Loss: 0.348304
         Train Epoch: [2320/2467]	Loss: 0.030552
 Train Epoch: [2320/2467]	Loss: 0.161438
     Train Epoch: [2320/2467]	Loss: 0.198381
     Train Epoch: [2340/2467]	Loss: 0.126005
         Train Epoch: [2340/2467]	Loss: 0.285541
     Train Epoch: [2340/2467]	Loss: 0.144075
 Train Epoch: [2340/2467]	Loss: 0.529828
     Train Epoch: [2360/2467]	Loss: 0.155919
              Train Epoch: [2360/2467]	Loss: 0.336974Train Epoch: [2360/2467]	Loss: 0.143928

 Train Epoch: [2360/2467]	Loss: 0.063213
     Train Epoch: [2380/2467]	Loss: 0.158512
     Train Epoch: [2380/2467]	Loss: 0.495975
     Train Epoch: [2380/2467]	Loss: 0.151164
     Train Epoch: [2380/2467]	Loss: 0.211582
     Train Epoch: [2400/2467]	Loss: 0.098363
         Train Epoch: [2400/2467]	Loss: 0.236034 
Train Epoch: [2400/2467]	Loss: 0.145589
     Train Epoch: [2400/2467]	Loss: 0.250597
         Train Epoch: [2420/2467]	Loss: 0.050855
     Train Epoch: [2420/2467]	Loss: 0.146113
 Train Epoch: [2420/2467]	Loss: 0.322244
     Train Epoch: [2420/2467]	Loss: 0.310042
     Train Epoch: [2440/2467]	Loss: 0.045316    
 Train Epoch: [2440/2467]	Loss: 0.285677
         Train Epoch: [2440/2467]	Loss: 0.147393
 Train Epoch: [2440/2467]	Loss: 0.150749
         Train Epoch: [2460/2467]	Loss: 0.159650 
    Train Epoch: [2460/2467]	Loss: 0.108560
     Train Epoch: [2460/2467]	Loss: 0.180597
 Train Epoch: [2460/2467]	Loss: 0.258472
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 21 epoch =====
     2025-05-11.07-41-46
    after set grad ===== running 21 epoch =====
    
 2025-05-11.07-41-46
after prog
start loop
after set grad
after prog
start loop
     ===== running 21 epoch =====
     2025-05-11.07-41-46
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable 	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 21 epoch =====
     2025-05-11.07-41-46
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.135791    
 Train Epoch: [0/2467]	Loss: 0.193065
     Train Epoch: [0/2467]	Loss: 0.297848
     Train Epoch: [0/2467]	Loss: 0.419081
          Train Epoch: [20/2467]	Loss: 0.115319
Train Epoch: [20/2467]	Loss: 0.314517
     Train Epoch: [20/2467]	Loss: 0.168813
     Train Epoch: [20/2467]	Loss: 0.089175
         Train Epoch: [40/2467]	Loss: 0.125668 
Train Epoch: [40/2467]	Loss: 0.449233
         Train Epoch: [40/2467]	Loss: 0.130347
 Train Epoch: [40/2467]	Loss: 0.130874
     Train Epoch: [60/2467]	Loss: 0.276278    
 Train Epoch: [60/2467]	Loss: 0.050877
     Train Epoch: [60/2467]	Loss: 0.128694
     Train Epoch: [60/2467]	Loss: 0.169149
         Train Epoch: [80/2467]	Loss: 0.363145
 Train Epoch: [80/2467]	Loss: 0.125168
         Train Epoch: [80/2467]	Loss: 0.183678
 Train Epoch: [80/2467]	Loss: 0.193526
     Train Epoch: [100/2467]	Loss: 0.095770
     Train Epoch: [100/2467]	Loss: 0.239002
     Train Epoch: [100/2467]	Loss: 0.286038
     Train Epoch: [100/2467]	Loss: 0.222700
     Train Epoch: [120/2467]	Loss: 0.274959
         Train Epoch: [120/2467]	Loss: 0.081958
 Train Epoch: [120/2467]	Loss: 0.275328
     Train Epoch: [120/2467]	Loss: 0.379463
         Train Epoch: [140/2467]	Loss: 0.040482
 Train Epoch: [140/2467]	Loss: 0.165737
     Train Epoch: [140/2467]	Loss: 0.094098
     Train Epoch: [140/2467]	Loss: 0.275504
         Train Epoch: [160/2467]	Loss: 0.156110
 Train Epoch: [160/2467]	Loss: 0.144933
         Train Epoch: [160/2467]	Loss: 0.154787
 Train Epoch: [160/2467]	Loss: 0.259641
     Train Epoch: [180/2467]	Loss: 0.150392
         Train Epoch: [180/2467]	Loss: 0.216631
     Train Epoch: [180/2467]	Loss: 0.171344
 Train Epoch: [180/2467]	Loss: 0.153576
         Train Epoch: [200/2467]	Loss: 0.118009
 Train Epoch: [200/2467]	Loss: 0.109737    
 Train Epoch: [200/2467]	Loss: 0.135755
     Train Epoch: [200/2467]	Loss: 0.172847
     Train Epoch: [220/2467]	Loss: 0.152090
          Train Epoch: [220/2467]	Loss: 0.184461Train Epoch: [220/2467]	Loss: 0.227211

     Train Epoch: [220/2467]	Loss: 0.071320
         Train Epoch: [240/2467]	Loss: 0.246610
     Train Epoch: [240/2467]	Loss: 0.169094
 Train Epoch: [240/2467]	Loss: 0.051098
     Train Epoch: [240/2467]	Loss: 0.023379
     Train Epoch: [260/2467]	Loss: 0.058509
         Train Epoch: [260/2467]	Loss: 0.033621
 Train Epoch: [260/2467]	Loss: 0.059043
     Train Epoch: [260/2467]	Loss: 0.095804
     Train Epoch: [280/2467]	Loss: 0.196501
         Train Epoch: [280/2467]	Loss: 0.366497
 Train Epoch: [280/2467]	Loss: 0.135006
     Train Epoch: [280/2467]	Loss: 0.118965
     Train Epoch: [300/2467]	Loss: 0.161174
     Train Epoch: [300/2467]	Loss: 0.094680
         Train Epoch: [300/2467]	Loss: 0.396068
 Train Epoch: [300/2467]	Loss: 0.037247
         Train Epoch: [320/2467]	Loss: 0.171210
     Train Epoch: [320/2467]	Loss: 0.464906
     Train Epoch: [320/2467]	Loss: 0.168746
 Train Epoch: [320/2467]	Loss: 0.463604
         Train Epoch: [340/2467]	Loss: 0.052655
     Train Epoch: [340/2467]	Loss: 0.103521
 Train Epoch: [340/2467]	Loss: 0.233729
     Train Epoch: [340/2467]	Loss: 0.016674
         Train Epoch: [360/2467]	Loss: 0.235842
 Train Epoch: [360/2467]	Loss: 0.080758
     Train Epoch: [360/2467]	Loss: 0.176679
     Train Epoch: [360/2467]	Loss: 0.055952
         Train Epoch: [380/2467]	Loss: 0.192176
 Train Epoch: [380/2467]	Loss: 0.087151
         Train Epoch: [380/2467]	Loss: 0.204510 
Train Epoch: [380/2467]	Loss: 0.488950
     Train Epoch: [400/2467]	Loss: 0.098726
          Train Epoch: [400/2467]	Loss: 0.055716Train Epoch: [400/2467]	Loss: 0.175400

     Train Epoch: [400/2467]	Loss: 0.197518
     Train Epoch: [420/2467]	Loss: 0.248860
         Train Epoch: [420/2467]	Loss: 0.297575
 Train Epoch: [420/2467]	Loss: 0.164006
     Train Epoch: [420/2467]	Loss: 0.121288
     Train Epoch: [440/2467]	Loss: 0.149916
         Train Epoch: [440/2467]	Loss: 0.363477
 Train Epoch: [440/2467]	Loss: 0.088614
     Train Epoch: [440/2467]	Loss: 0.057777
     Train Epoch: [460/2467]	Loss: 0.247433
         Train Epoch: [460/2467]	Loss: 0.139224
 Train Epoch: [460/2467]	Loss: 0.140417
     Train Epoch: [460/2467]	Loss: 0.164604
         Train Epoch: [480/2467]	Loss: 0.157870
     Train Epoch: [480/2467]	Loss: 0.169301
 Train Epoch: [480/2467]	Loss: 0.312329
     Train Epoch: [480/2467]	Loss: 0.369052
     Train Epoch: [500/2467]	Loss: 0.211428
          Train Epoch: [500/2467]	Loss: 0.114614Train Epoch: [500/2467]	Loss: 0.131866

     Train Epoch: [500/2467]	Loss: 0.229670
              Train Epoch: [520/2467]	Loss: 0.182606Train Epoch: [520/2467]	Loss: 0.273840

 Train Epoch: [520/2467]	Loss: 0.084905
     Train Epoch: [520/2467]	Loss: 0.165670
         Train Epoch: [540/2467]	Loss: 0.153489
 Train Epoch: [540/2467]	Loss: 0.299489
     Train Epoch: [540/2467]	Loss: 0.108810    
 Train Epoch: [540/2467]	Loss: 0.070507
     Train Epoch: [560/2467]	Loss: 0.180166    
     Train Epoch: [560/2467]	Loss: 0.239799
     Train Epoch: [560/2467]	Loss: 0.132559
 Train Epoch: [560/2467]	Loss: 0.267580
         Train Epoch: [580/2467]	Loss: 0.290530
     Train Epoch: [580/2467]	Loss: 0.167286
 Train Epoch: [580/2467]	Loss: 0.059648
     Train Epoch: [580/2467]	Loss: 0.277869
     Train Epoch: [600/2467]	Loss: 0.203302
          Train Epoch: [600/2467]	Loss: 0.212874Train Epoch: [600/2467]	Loss: 0.128382

     Train Epoch: [600/2467]	Loss: 0.208883
         Train Epoch: [620/2467]	Loss: 0.030964 
Train Epoch: [620/2467]	Loss: 0.174837
     Train Epoch: [620/2467]	Loss: 0.206004
     Train Epoch: [620/2467]	Loss: 0.201701
     Train Epoch: [640/2467]	Loss: 0.160920
         Train Epoch: [640/2467]	Loss: 0.050520    
 Train Epoch: [640/2467]	Loss: 0.183399
 Train Epoch: [640/2467]	Loss: 0.284865
     Train Epoch: [660/2467]	Loss: 0.029076
     Train Epoch: [660/2467]	Loss: 0.202914
     Train Epoch: [660/2467]	Loss: 0.062609
     Train Epoch: [660/2467]	Loss: 0.414832
     Train Epoch: [680/2467]	Loss: 0.118749
         Train Epoch: [680/2467]	Loss: 0.064614
 Train Epoch: [680/2467]	Loss: 0.194412    
 Train Epoch: [680/2467]	Loss: 0.040990
     Train Epoch: [700/2467]	Loss: 0.065053
         Train Epoch: [700/2467]	Loss: 0.351201 
Train Epoch: [700/2467]	Loss: 0.068233
     Train Epoch: [700/2467]	Loss: 0.329899
         Train Epoch: [720/2467]	Loss: 0.032084
 Train Epoch: [720/2467]	Loss: 0.228328
         Train Epoch: [720/2467]	Loss: 0.135562
 Train Epoch: [720/2467]	Loss: 0.106199
         Train Epoch: [740/2467]	Loss: 0.056723
 Train Epoch: [740/2467]	Loss: 0.066608
         Train Epoch: [740/2467]	Loss: 0.115290
 Train Epoch: [740/2467]	Loss: 0.311114
     Train Epoch: [760/2467]	Loss: 0.055979
     Train Epoch: [760/2467]	Loss: 0.280357    
     Train Epoch: [760/2467]	Loss: 0.460382
 Train Epoch: [760/2467]	Loss: 0.300394
     Train Epoch: [780/2467]	Loss: 0.369796
     Train Epoch: [780/2467]	Loss: 0.324524
     Train Epoch: [780/2467]	Loss: 0.180265
     Train Epoch: [780/2467]	Loss: 0.334647
     Train Epoch: [800/2467]	Loss: 0.176542
     Train Epoch: [800/2467]	Loss: 0.189750
          Train Epoch: [800/2467]	Loss: 0.232282Train Epoch: [800/2467]	Loss: 0.106730

     Train Epoch: [820/2467]	Loss: 0.040714
         Train Epoch: [820/2467]	Loss: 0.220290    
 Train Epoch: [820/2467]	Loss: 0.067167
 Train Epoch: [820/2467]	Loss: 0.480275
     Train Epoch: [840/2467]	Loss: 0.306932
         Train Epoch: [840/2467]	Loss: 0.143117
 Train Epoch: [840/2467]	Loss: 0.125895
     Train Epoch: [840/2467]	Loss: 0.055736
     Train Epoch: [860/2467]	Loss: 0.349877
     Train Epoch: [860/2467]	Loss: 0.218039
     Train Epoch: [860/2467]	Loss: 0.069995    
 Train Epoch: [860/2467]	Loss: 0.147918
             Train Epoch: [880/2467]	Loss: 0.173000
     Train Epoch: [880/2467]	Loss: 0.222083
 Train Epoch: [880/2467]	Loss: 0.239005 
Train Epoch: [880/2467]	Loss: 0.144209
     Train Epoch: [900/2467]	Loss: 0.178453
          Train Epoch: [900/2467]	Loss: 0.202212Train Epoch: [900/2467]	Loss: 0.093443

     Train Epoch: [900/2467]	Loss: 0.200907
         Train Epoch: [920/2467]	Loss: 0.319589
 Train Epoch: [920/2467]	Loss: 0.324710
     Train Epoch: [920/2467]	Loss: 0.052600
     Train Epoch: [920/2467]	Loss: 0.285179
         Train Epoch: [940/2467]	Loss: 0.051881
     Train Epoch: [940/2467]	Loss: 0.028031 
Train Epoch: [940/2467]	Loss: 0.200720
     Train Epoch: [940/2467]	Loss: 0.098525
         Train Epoch: [960/2467]	Loss: 0.355930
 Train Epoch: [960/2467]	Loss: 0.106467
         Train Epoch: [960/2467]	Loss: 0.088839
 Train Epoch: [960/2467]	Loss: 0.098457
          Train Epoch: [980/2467]	Loss: 0.078230
    Train Epoch: [980/2467]	Loss: 0.239299
     Train Epoch: [980/2467]	Loss: 0.153060
 Train Epoch: [980/2467]	Loss: 0.146752
     Train Epoch: [1000/2467]	Loss: 0.059005
     Train Epoch: [1000/2467]	Loss: 0.032828
         Train Epoch: [1000/2467]	Loss: 0.245851
 Train Epoch: [1000/2467]	Loss: 0.302627
         Train Epoch: [1020/2467]	Loss: 0.180692
 Train Epoch: [1020/2467]	Loss: 0.392126
     Train Epoch: [1020/2467]	Loss: 0.069388
     Train Epoch: [1020/2467]	Loss: 0.110445
     Train Epoch: [1040/2467]	Loss: 0.161910    
 Train Epoch: [1040/2467]	Loss: 0.155325
     Train Epoch: [1040/2467]	Loss: 0.130202
     Train Epoch: [1040/2467]	Loss: 0.246743
     Train Epoch: [1060/2467]	Loss: 0.259932    
     Train Epoch: [1060/2467]	Loss: 0.190939
     Train Epoch: [1060/2467]	Loss: 0.443680
 Train Epoch: [1060/2467]	Loss: 0.130752
     Train Epoch: [1080/2467]	Loss: 0.133415
     Train Epoch: [1080/2467]	Loss: 0.300035
     Train Epoch: [1080/2467]	Loss: 0.150479
     Train Epoch: [1080/2467]	Loss: 0.314504
          Train Epoch: [1100/2467]	Loss: 0.278062Train Epoch: [1100/2467]	Loss: 0.612716

     Train Epoch: [1100/2467]	Loss: 0.282560
     Train Epoch: [1100/2467]	Loss: 0.261132
     Train Epoch: [1120/2467]	Loss: 0.288331
         Train Epoch: [1120/2467]	Loss: 0.132911
 Train Epoch: [1120/2467]	Loss: 0.129148    
 Train Epoch: [1120/2467]	Loss: 0.158666
         Train Epoch: [1140/2467]	Loss: 0.058437
         Train Epoch: [1140/2467]	Loss: 0.431800
 Train Epoch: [1140/2467]	Loss: 0.096896
 Train Epoch: [1140/2467]	Loss: 0.269161
     Train Epoch: [1160/2467]	Loss: 0.088409    
     Train Epoch: [1160/2467]	Loss: 0.182474 
Train Epoch: [1160/2467]	Loss: 0.111034
     Train Epoch: [1160/2467]	Loss: 0.219228
     Train Epoch: [1180/2467]	Loss: 0.211343
     Train Epoch: [1180/2467]	Loss: 0.270271
     Train Epoch: [1180/2467]	Loss: 0.055854
     Train Epoch: [1180/2467]	Loss: 0.144290
     Train Epoch: [1200/2467]	Loss: 0.076424
             Train Epoch: [1200/2467]	Loss: 0.323125
  Train Epoch: [1200/2467]	Loss: 0.305615Train Epoch: [1200/2467]	Loss: 0.162176

             Train Epoch: [1220/2467]	Loss: 0.110097
 Train Epoch: [1220/2467]	Loss: 0.308830 
Train Epoch: [1220/2467]	Loss: 0.245709
     Train Epoch: [1220/2467]	Loss: 0.237590
     Train Epoch: [1240/2467]	Loss: 0.314268
               Train Epoch: [1240/2467]	Loss: 0.238810Train Epoch: [1240/2467]	Loss: 0.303571
Train Epoch: [1240/2467]	Loss: 0.149706

         Train Epoch: [1260/2467]	Loss: 0.427098
     Train Epoch: [1260/2467]	Loss: 0.068968
 Train Epoch: [1260/2467]	Loss: 0.142580
     Train Epoch: [1260/2467]	Loss: 0.068161
             Train Epoch: [1280/2467]	Loss: 0.228220
 Train Epoch: [1280/2467]	Loss: 0.122904
 Train Epoch: [1280/2467]	Loss: 0.413428
     Train Epoch: [1280/2467]	Loss: 0.456830
                  Train Epoch: [1300/2467]	Loss: 0.082106Train Epoch: [1300/2467]	Loss: 0.070406 
 
Train Epoch: [1300/2467]	Loss: 0.324695Train Epoch: [1300/2467]	Loss: 0.290371

              Train Epoch: [1320/2467]	Loss: 0.203970Train Epoch: [1320/2467]	Loss: 0.133431
 
Train Epoch: [1320/2467]	Loss: 0.048466
     Train Epoch: [1320/2467]	Loss: 0.286289
     Train Epoch: [1340/2467]	Loss: 0.120672
          Train Epoch: [1340/2467]	Loss: 0.232953
Train Epoch: [1340/2467]	Loss: 0.340998
     Train Epoch: [1340/2467]	Loss: 0.231351
         Train Epoch: [1360/2467]	Loss: 0.131776    
 Train Epoch: [1360/2467]	Loss: 0.115896 
Train Epoch: [1360/2467]	Loss: 0.233868
     Train Epoch: [1360/2467]	Loss: 0.072290
     Train Epoch: [1380/2467]	Loss: 0.203265
             Train Epoch: [1380/2467]	Loss: 0.326828
 Train Epoch: [1380/2467]	Loss: 0.230332 
Train Epoch: [1380/2467]	Loss: 0.333016
             Train Epoch: [1400/2467]	Loss: 0.171933 
 Train Epoch: [1400/2467]	Loss: 0.237690Train Epoch: [1400/2467]	Loss: 0.195660
    
 Train Epoch: [1400/2467]	Loss: 0.089408
     Train Epoch: [1420/2467]	Loss: 0.136708
         Train Epoch: [1420/2467]	Loss: 0.479498 
Train Epoch: [1420/2467]	Loss: 0.254848
     Train Epoch: [1420/2467]	Loss: 0.152158
             Train Epoch: [1440/2467]	Loss: 0.072512
  Train Epoch: [1440/2467]	Loss: 0.021407
Train Epoch: [1440/2467]	Loss: 0.044245
     Train Epoch: [1440/2467]	Loss: 0.148216
     Train Epoch: [1460/2467]	Loss: 0.111658
     Train Epoch: [1460/2467]	Loss: 0.051987
         Train Epoch: [1460/2467]	Loss: 0.152473
 Train Epoch: [1460/2467]	Loss: 0.265871
     Train Epoch: [1480/2467]	Loss: 0.045191
         Train Epoch: [1480/2467]	Loss: 0.194413    
  Train Epoch: [1480/2467]	Loss: 0.016417Train Epoch: [1480/2467]	Loss: 0.243566

              Train Epoch: [1500/2467]	Loss: 0.014465Train Epoch: [1500/2467]	Loss: 0.130443

 Train Epoch: [1500/2467]	Loss: 0.326820
     Train Epoch: [1500/2467]	Loss: 0.098601
                   Train Epoch: [1520/2467]	Loss: 0.182799Train Epoch: [1520/2467]	Loss: 0.255769Train Epoch: [1520/2467]	Loss: 0.206685


 Train Epoch: [1520/2467]	Loss: 0.269821
     Train Epoch: [1540/2467]	Loss: 0.296344
         Train Epoch: [1540/2467]	Loss: 0.106328
 Train Epoch: [1540/2467]	Loss: 0.073229
     Train Epoch: [1540/2467]	Loss: 0.164238
     Train Epoch: [1560/2467]	Loss: 0.414511
     Train Epoch: [1560/2467]	Loss: 0.127133
     Train Epoch: [1560/2467]	Loss: 0.336369
     Train Epoch: [1560/2467]	Loss: 0.470327
     Train Epoch: [1580/2467]	Loss: 0.212801    
     Train Epoch: [1580/2467]	Loss: 0.110250
 Train Epoch: [1580/2467]	Loss: 0.173586
     Train Epoch: [1580/2467]	Loss: 0.263777
         Train Epoch: [1600/2467]	Loss: 0.254001    
      Train Epoch: [1600/2467]	Loss: 0.129746Train Epoch: [1600/2467]	Loss: 0.255099

 Train Epoch: [1600/2467]	Loss: 0.074435
         Train Epoch: [1620/2467]	Loss: 0.098719 
Train Epoch: [1620/2467]	Loss: 0.182708
     Train Epoch: [1620/2467]	Loss: 0.040987
     Train Epoch: [1620/2467]	Loss: 0.230741
         Train Epoch: [1640/2467]	Loss: 0.149183
         Train Epoch: [1640/2467]	Loss: 0.140860
  Train Epoch: [1640/2467]	Loss: 0.230463
Train Epoch: [1640/2467]	Loss: 0.265228
         Train Epoch: [1660/2467]	Loss: 0.251960
         Train Epoch: [1660/2467]	Loss: 0.083419
 Train Epoch: [1660/2467]	Loss: 0.140703 
Train Epoch: [1660/2467]	Loss: 0.192287
     Train Epoch: [1680/2467]	Loss: 0.049847
             Train Epoch: [1680/2467]	Loss: 0.219421
 Train Epoch: [1680/2467]	Loss: 0.221669
 Train Epoch: [1680/2467]	Loss: 0.157240
         Train Epoch: [1700/2467]	Loss: 0.089978
     Train Epoch: [1700/2467]	Loss: 0.196457
     Train Epoch: [1700/2467]	Loss: 0.322453
 Train Epoch: [1700/2467]	Loss: 0.186581
     Train Epoch: [1720/2467]	Loss: 0.236631
         Train Epoch: [1720/2467]	Loss: 0.169605
 Train Epoch: [1720/2467]	Loss: 0.064674
     Train Epoch: [1720/2467]	Loss: 0.133007
         Train Epoch: [1740/2467]	Loss: 0.089021
 Train Epoch: [1740/2467]	Loss: 0.192828
         Train Epoch: [1740/2467]	Loss: 0.405998
 Train Epoch: [1740/2467]	Loss: 0.025437
     Train Epoch: [1760/2467]	Loss: 0.121303
     Train Epoch: [1760/2467]	Loss: 0.145233
          Train Epoch: [1760/2467]	Loss: 0.060735Train Epoch: [1760/2467]	Loss: 0.132180

     Train Epoch: [1780/2467]	Loss: 0.251649
     Train Epoch: [1780/2467]	Loss: 0.270533
     Train Epoch: [1780/2467]	Loss: 0.168503
     Train Epoch: [1780/2467]	Loss: 0.326849
     Train Epoch: [1800/2467]	Loss: 0.231712
     Train Epoch: [1800/2467]	Loss: 0.277158
     Train Epoch: [1800/2467]	Loss: 0.335426
     Train Epoch: [1800/2467]	Loss: 0.185173
         Train Epoch: [1820/2467]	Loss: 0.354055
     Train Epoch: [1820/2467]	Loss: 0.148523
 Train Epoch: [1820/2467]	Loss: 0.144026
     Train Epoch: [1820/2467]	Loss: 0.135659
              Train Epoch: [1840/2467]	Loss: 0.067880 
Train Epoch: [1840/2467]	Loss: 0.287533Train Epoch: [1840/2467]	Loss: 0.317480

     Train Epoch: [1840/2467]	Loss: 0.117203
     Train Epoch: [1860/2467]	Loss: 0.174329    
           Train Epoch: [1860/2467]	Loss: 0.106677Train Epoch: [1860/2467]	Loss: 0.212764Train Epoch: [1860/2467]	Loss: 0.168446


             Train Epoch: [1880/2467]	Loss: 0.096163
  Train Epoch: [1880/2467]	Loss: 0.052273
Train Epoch: [1880/2467]	Loss: 0.117563
     Train Epoch: [1880/2467]	Loss: 0.059503
         Train Epoch: [1900/2467]	Loss: 0.181310
 Train Epoch: [1900/2467]	Loss: 0.304763    
 Train Epoch: [1900/2467]	Loss: 0.120661
     Train Epoch: [1900/2467]	Loss: 0.275717
     Train Epoch: [1920/2467]	Loss: 0.270372
         Train Epoch: [1920/2467]	Loss: 0.037487
 Train Epoch: [1920/2467]	Loss: 0.162876    
 Train Epoch: [1920/2467]	Loss: 0.224468
     Train Epoch: [1940/2467]	Loss: 0.080862
     Train Epoch: [1940/2467]	Loss: 0.200405
     Train Epoch: [1940/2467]	Loss: 0.134379    
 Train Epoch: [1940/2467]	Loss: 0.385560
     Train Epoch: [1960/2467]	Loss: 0.129262
         Train Epoch: [1960/2467]	Loss: 0.228292
     Train Epoch: [1960/2467]	Loss: 0.198201
 Train Epoch: [1960/2467]	Loss: 0.254435
     Train Epoch: [1980/2467]	Loss: 0.124535
     Train Epoch: [1980/2467]	Loss: 0.258628
     Train Epoch: [1980/2467]	Loss: 0.263717
     Train Epoch: [1980/2467]	Loss: 0.330120
     Train Epoch: [2000/2467]	Loss: 0.028417
     Train Epoch: [2000/2467]	Loss: 0.188933
     Train Epoch: [2000/2467]	Loss: 0.322123
     Train Epoch: [2000/2467]	Loss: 0.068083
     Train Epoch: [2020/2467]	Loss: 0.207609
     Train Epoch: [2020/2467]	Loss: 0.152185
     Train Epoch: [2020/2467]	Loss: 0.231176
     Train Epoch: [2020/2467]	Loss: 0.093755
                  Train Epoch: [2040/2467]	Loss: 0.213369  
Train Epoch: [2040/2467]	Loss: 0.127649Train Epoch: [2040/2467]	Loss: 0.373281
Train Epoch: [2040/2467]	Loss: 0.092519

     Train Epoch: [2060/2467]	Loss: 0.178690
     Train Epoch: [2060/2467]	Loss: 0.205358    
 Train Epoch: [2060/2467]	Loss: 0.091150
     Train Epoch: [2060/2467]	Loss: 0.246449
         Train Epoch: [2080/2467]	Loss: 0.310870
     Train Epoch: [2080/2467]	Loss: 0.062703
     Train Epoch: [2080/2467]	Loss: 0.124474
 Train Epoch: [2080/2467]	Loss: 0.073665
     Train Epoch: [2100/2467]	Loss: 0.132394
         Train Epoch: [2100/2467]	Loss: 0.105137 
Train Epoch: [2100/2467]	Loss: 0.566354    
 Train Epoch: [2100/2467]	Loss: 0.113150
     Train Epoch: [2120/2467]	Loss: 0.187532
     Train Epoch: [2120/2467]	Loss: 0.051589
         Train Epoch: [2120/2467]	Loss: 0.226521
 Train Epoch: [2120/2467]	Loss: 0.099780
         Train Epoch: [2140/2467]	Loss: 0.558037
         Train Epoch: [2140/2467]	Loss: 0.217661
  Train Epoch: [2140/2467]	Loss: 0.383497Train Epoch: [2140/2467]	Loss: 0.345232

     Train Epoch: [2160/2467]	Loss: 0.338191
          Train Epoch: [2160/2467]	Loss: 0.123841Train Epoch: [2160/2467]	Loss: 0.175278

     Train Epoch: [2160/2467]	Loss: 0.050989
     Train Epoch: [2180/2467]	Loss: 0.285031
          Train Epoch: [2180/2467]	Loss: 0.092344Train Epoch: [2180/2467]	Loss: 0.141637

     Train Epoch: [2180/2467]	Loss: 0.191773
                 Train Epoch: [2200/2467]	Loss: 0.199792
 Train Epoch: [2200/2467]	Loss: 0.130090
  Train Epoch: [2200/2467]	Loss: 0.068641Train Epoch: [2200/2467]	Loss: 0.097456

         Train Epoch: [2220/2467]	Loss: 0.186290
 Train Epoch: [2220/2467]	Loss: 0.091748    
     Train Epoch: [2220/2467]	Loss: 0.119545
 Train Epoch: [2220/2467]	Loss: 0.072896
         Train Epoch: [2240/2467]	Loss: 0.038700
 Train Epoch: [2240/2467]	Loss: 0.082080
         Train Epoch: [2240/2467]	Loss: 0.030683 
Train Epoch: [2240/2467]	Loss: 0.130494
              Train Epoch: [2260/2467]	Loss: 0.204131
Train Epoch: [2260/2467]	Loss: 0.362747 
    Train Epoch: [2260/2467]	Loss: 0.263848
 Train Epoch: [2260/2467]	Loss: 0.137704
     Train Epoch: [2280/2467]	Loss: 0.146570
     Train Epoch: [2280/2467]	Loss: 0.089983
         Train Epoch: [2280/2467]	Loss: 0.115384
 Train Epoch: [2280/2467]	Loss: 0.087401
     Train Epoch: [2300/2467]	Loss: 0.251472
     Train Epoch: [2300/2467]	Loss: 0.067084
     Train Epoch: [2300/2467]	Loss: 0.092427
     Train Epoch: [2300/2467]	Loss: 0.062130
              Train Epoch: [2320/2467]	Loss: 0.184792Train Epoch: [2320/2467]	Loss: 0.027036
 
Train Epoch: [2320/2467]	Loss: 0.149196
     Train Epoch: [2320/2467]	Loss: 0.260491
             Train Epoch: [2340/2467]	Loss: 0.231604
     Train Epoch: [2340/2467]	Loss: 0.503442 
Train Epoch: [2340/2467]	Loss: 0.134746
 Train Epoch: [2340/2467]	Loss: 0.127443
     Train Epoch: [2360/2467]	Loss: 0.327236
     Train Epoch: [2360/2467]	Loss: 0.170846
     Train Epoch: [2360/2467]	Loss: 0.137858
     Train Epoch: [2360/2467]	Loss: 0.189492
     Train Epoch: [2380/2467]	Loss: 0.475503    
         Train Epoch: [2380/2467]	Loss: 0.144971
  Train Epoch: [2380/2467]	Loss: 0.207822Train Epoch: [2380/2467]	Loss: 0.262191

     Train Epoch: [2400/2467]	Loss: 0.101004
             Train Epoch: [2400/2467]	Loss: 0.178421
 Train Epoch: [2400/2467]	Loss: 0.166738
 Train Epoch: [2400/2467]	Loss: 0.229302
     Train Epoch: [2420/2467]	Loss: 0.148545    
     Train Epoch: [2420/2467]	Loss: 0.405311 
Train Epoch: [2420/2467]	Loss: 0.041642
     Train Epoch: [2420/2467]	Loss: 0.257335
         Train Epoch: [2440/2467]	Loss: 0.264541
     Train Epoch: [2440/2467]	Loss: 0.133742
 Train Epoch: [2440/2467]	Loss: 0.050027
     Train Epoch: [2440/2467]	Loss: 0.171581
     Train Epoch: [2460/2467]	Loss: 0.094048    
          Train Epoch: [2460/2467]	Loss: 0.112670Train Epoch: [2460/2467]	Loss: 0.269745
 
Train Epoch: [2460/2467]	Loss: 0.140005
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 22 epoch =====
     2025-05-11.08-03-37
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 22 epoch =====
     2025-05-11.08-03-37
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 22 epoch =====
     2025-05-11.08-03-37
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 22 epoch =====
     2025-05-11.08-03-38
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
         Train Epoch: [0/2467]	Loss: 0.111756
 Train Epoch: [0/2467]	Loss: 0.341277
     Train Epoch: [0/2467]	Loss: 0.281559
     Train Epoch: [0/2467]	Loss: 0.191368
     Train Epoch: [20/2467]	Loss: 0.307074
             Train Epoch: [20/2467]	Loss: 0.076368
  Train Epoch: [20/2467]	Loss: 0.164870Train Epoch: [20/2467]	Loss: 0.116643

     Train Epoch: [40/2467]	Loss: 0.351825
             Train Epoch: [40/2467]	Loss: 0.158300 
Train Epoch: [40/2467]	Loss: 0.122106 
Train Epoch: [40/2467]	Loss: 0.123645
             Train Epoch: [60/2467]	Loss: 0.134496 
     Train Epoch: [60/2467]	Loss: 0.170147
Train Epoch: [60/2467]	Loss: 0.052821
 Train Epoch: [60/2467]	Loss: 0.435033
     Train Epoch: [80/2467]	Loss: 0.123168
          Train Epoch: [80/2467]	Loss: 0.107500Train Epoch: [80/2467]	Loss: 0.350557

     Train Epoch: [80/2467]	Loss: 0.109887
     Train Epoch: [100/2467]	Loss: 0.166904
         Train Epoch: [100/2467]	Loss: 0.101168
     Train Epoch: [100/2467]	Loss: 0.189156
 Train Epoch: [100/2467]	Loss: 0.203212
     Train Epoch: [120/2467]	Loss: 0.074853    
     Train Epoch: [120/2467]	Loss: 0.314270
 Train Epoch: [120/2467]	Loss: 0.057163    
 Train Epoch: [120/2467]	Loss: 0.381655
         Train Epoch: [140/2467]	Loss: 0.159233
     Train Epoch: [140/2467]	Loss: 0.067864
     Train Epoch: [140/2467]	Loss: 0.318414
 Train Epoch: [140/2467]	Loss: 0.063225
     Train Epoch: [160/2467]	Loss: 0.128673    
     Train Epoch: [160/2467]	Loss: 0.129507
 Train Epoch: [160/2467]	Loss: 0.135662
     Train Epoch: [160/2467]	Loss: 0.215279
              Train Epoch: [180/2467]	Loss: 0.112005
Train Epoch: [180/2467]	Loss: 0.183381 
Train Epoch: [180/2467]	Loss: 0.144587
     Train Epoch: [180/2467]	Loss: 0.130030
             Train Epoch: [200/2467]	Loss: 0.067251
      Train Epoch: [200/2467]	Loss: 0.126827Train Epoch: [200/2467]	Loss: 0.095582

 Train Epoch: [200/2467]	Loss: 0.110398
     Train Epoch: [220/2467]	Loss: 0.206366
         Train Epoch: [220/2467]	Loss: 0.174328
     Train Epoch: [220/2467]	Loss: 0.221784
 Train Epoch: [220/2467]	Loss: 0.075083
              Train Epoch: [240/2467]	Loss: 0.051224 Train Epoch: [240/2467]	Loss: 0.278050

Train Epoch: [240/2467]	Loss: 0.173458
     Train Epoch: [240/2467]	Loss: 0.022186
         Train Epoch: [260/2467]	Loss: 0.054037
 Train Epoch: [260/2467]	Loss: 0.110311
     Train Epoch: [260/2467]	Loss: 0.051467
     Train Epoch: [260/2467]	Loss: 0.037693
     Train Epoch: [280/2467]	Loss: 0.201299
         Train Epoch: [280/2467]	Loss: 0.113427
 Train Epoch: [280/2467]	Loss: 0.248048
     Train Epoch: [280/2467]	Loss: 0.141179
         Train Epoch: [300/2467]	Loss: 0.037732
 Train Epoch: [300/2467]	Loss: 0.092681
     Train Epoch: [300/2467]	Loss: 0.166013
     Train Epoch: [300/2467]	Loss: 0.374165
             Train Epoch: [320/2467]	Loss: 0.261594 
Train Epoch: [320/2467]	Loss: 0.425708 
    Train Epoch: [320/2467]	Loss: 0.158607
 Train Epoch: [320/2467]	Loss: 0.186930
              Train Epoch: [340/2467]	Loss: 0.194404Train Epoch: [340/2467]	Loss: 0.060934

 Train Epoch: [340/2467]	Loss: 0.163886
     Train Epoch: [340/2467]	Loss: 0.017467
     Train Epoch: [360/2467]	Loss: 0.134890
         Train Epoch: [360/2467]	Loss: 0.237365
 Train Epoch: [360/2467]	Loss: 0.063053
     Train Epoch: [360/2467]	Loss: 0.166346
     Train Epoch: [380/2467]	Loss: 0.078971    
 Train Epoch: [380/2467]	Loss: 0.395497
     Train Epoch: [380/2467]	Loss: 0.336481
     Train Epoch: [380/2467]	Loss: 0.195248
          Train Epoch: [400/2467]	Loss: 0.159282Train Epoch: [400/2467]	Loss: 0.040701

     Train Epoch: [400/2467]	Loss: 0.087937
     Train Epoch: [400/2467]	Loss: 0.392724
             Train Epoch: [420/2467]	Loss: 0.279355
 Train Epoch: [420/2467]	Loss: 0.129953
     Train Epoch: [420/2467]	Loss: 0.154002
 Train Epoch: [420/2467]	Loss: 0.224701
     Train Epoch: [440/2467]	Loss: 0.085048    
     Train Epoch: [440/2467]	Loss: 0.055379 
Train Epoch: [440/2467]	Loss: 0.165923
     Train Epoch: [440/2467]	Loss: 0.356689
     Train Epoch: [460/2467]	Loss: 0.184727
     Train Epoch: [460/2467]	Loss: 0.265842
         Train Epoch: [460/2467]	Loss: 0.131090
 Train Epoch: [460/2467]	Loss: 0.149275
     Train Epoch: [480/2467]	Loss: 0.086234
         Train Epoch: [480/2467]	Loss: 0.356364
     Train Epoch: [480/2467]	Loss: 0.150636
 Train Epoch: [480/2467]	Loss: 0.276434
     Train Epoch: [500/2467]	Loss: 0.121138
          Train Epoch: [500/2467]	Loss: 0.232538Train Epoch: [500/2467]	Loss: 0.159779

     Train Epoch: [500/2467]	Loss: 0.106405
         Train Epoch: [520/2467]	Loss: 0.337572
     Train Epoch: [520/2467]	Loss: 0.082301    
 Train Epoch: [520/2467]	Loss: 0.112054
 Train Epoch: [520/2467]	Loss: 0.176666
     Train Epoch: [540/2467]	Loss: 0.260056
              Train Epoch: [540/2467]	Loss: 0.145522 Train Epoch: [540/2467]	Loss: 0.117269

Train Epoch: [540/2467]	Loss: 0.070181
     Train Epoch: [560/2467]	Loss: 0.273880
         Train Epoch: [560/2467]	Loss: 0.137497 
Train Epoch: [560/2467]	Loss: 0.187402
     Train Epoch: [560/2467]	Loss: 0.147567
         Train Epoch: [580/2467]	Loss: 0.161462
     Train Epoch: [580/2467]	Loss: 0.060968
 Train Epoch: [580/2467]	Loss: 0.290586
     Train Epoch: [580/2467]	Loss: 0.181741
     Train Epoch: [600/2467]	Loss: 0.208559
     Train Epoch: [600/2467]	Loss: 0.126022
     Train Epoch: [600/2467]	Loss: 0.206119
     Train Epoch: [600/2467]	Loss: 0.193788
             Train Epoch: [620/2467]	Loss: 0.027426
     Train Epoch: [620/2467]	Loss: 0.157948
 Train Epoch: [620/2467]	Loss: 0.243782
 Train Epoch: [620/2467]	Loss: 0.140465
     Train Epoch: [640/2467]	Loss: 0.049080
     Train Epoch: [640/2467]	Loss: 0.174109
     Train Epoch: [640/2467]	Loss: 0.156582
     Train Epoch: [640/2467]	Loss: 0.223246
             Train Epoch: [660/2467]	Loss: 0.222667
 Train Epoch: [660/2467]	Loss: 0.418992 
Train Epoch: [660/2467]	Loss: 0.060627
     Train Epoch: [660/2467]	Loss: 0.035077
         Train Epoch: [680/2467]	Loss: 0.041599
         Train Epoch: [680/2467]	Loss: 0.088675
  Train Epoch: [680/2467]	Loss: 0.130250Train Epoch: [680/2467]	Loss: 0.079297

              Train Epoch: [700/2467]	Loss: 0.371088Train Epoch: [700/2467]	Loss: 0.340203

 Train Epoch: [700/2467]	Loss: 0.119508    
 Train Epoch: [700/2467]	Loss: 0.051298
         Train Epoch: [720/2467]	Loss: 0.218124
         Train Epoch: [720/2467]	Loss: 0.094254
  Train Epoch: [720/2467]	Loss: 0.023857
Train Epoch: [720/2467]	Loss: 0.141737
         Train Epoch: [740/2467]	Loss: 0.062441
     Train Epoch: [740/2467]	Loss: 0.092274
 Train Epoch: [740/2467]	Loss: 0.339490
     Train Epoch: [740/2467]	Loss: 0.052821
              Train Epoch: [760/2467]	Loss: 0.341486 Train Epoch: [760/2467]	Loss: 0.495518
    
Train Epoch: [760/2467]	Loss: 0.274801
 Train Epoch: [760/2467]	Loss: 0.058292
         Train Epoch: [780/2467]	Loss: 0.133634 
Train Epoch: [780/2467]	Loss: 0.319790
     Train Epoch: [780/2467]	Loss: 0.379060
     Train Epoch: [780/2467]	Loss: 0.350194
         Train Epoch: [800/2467]	Loss: 0.138313    
 Train Epoch: [800/2467]	Loss: 0.288197
 Train Epoch: [800/2467]	Loss: 0.174984
     Train Epoch: [800/2467]	Loss: 0.261201
     Train Epoch: [820/2467]	Loss: 0.448208
     Train Epoch: [820/2467]	Loss: 0.036659
     Train Epoch: [820/2467]	Loss: 0.231896
     Train Epoch: [820/2467]	Loss: 0.071656
         Train Epoch: [840/2467]	Loss: 0.104536
     Train Epoch: [840/2467]	Loss: 0.142231
     Train Epoch: [840/2467]	Loss: 0.335304
 Train Epoch: [840/2467]	Loss: 0.129128
         Train Epoch: [860/2467]	Loss: 0.187013    
      Train Epoch: [860/2467]	Loss: 0.094495Train Epoch: [860/2467]	Loss: 0.242717

 Train Epoch: [860/2467]	Loss: 0.089592
             Train Epoch: [880/2467]	Loss: 0.180465
      Train Epoch: [880/2467]	Loss: 0.156951Train Epoch: [880/2467]	Loss: 0.258933

 Train Epoch: [880/2467]	Loss: 0.200855
     Train Epoch: [900/2467]	Loss: 0.108882
         Train Epoch: [900/2467]	Loss: 0.185820
 Train Epoch: [900/2467]	Loss: 0.194136    
 Train Epoch: [900/2467]	Loss: 0.202805
     Train Epoch: [920/2467]	Loss: 0.304882
     Train Epoch: [920/2467]	Loss: 0.048996
     Train Epoch: [920/2467]	Loss: 0.329080
     Train Epoch: [920/2467]	Loss: 0.251843
     Train Epoch: [940/2467]	Loss: 0.054028
         Train Epoch: [940/2467]	Loss: 0.196061
 Train Epoch: [940/2467]	Loss: 0.029528
     Train Epoch: [940/2467]	Loss: 0.115400
         Train Epoch: [960/2467]	Loss: 0.056999
 Train Epoch: [960/2467]	Loss: 0.081964
     Train Epoch: [960/2467]	Loss: 0.296718
     Train Epoch: [960/2467]	Loss: 0.091133
     Train Epoch: [980/2467]	Loss: 0.075481
         Train Epoch: [980/2467]	Loss: 0.130603
 Train Epoch: [980/2467]	Loss: 0.227252
     Train Epoch: [980/2467]	Loss: 0.127519
             Train Epoch: [1000/2467]	Loss: 0.369021
  Train Epoch: [1000/2467]	Loss: 0.231042Train Epoch: [1000/2467]	Loss: 0.041354

     Train Epoch: [1000/2467]	Loss: 0.080568
     Train Epoch: [1020/2467]	Loss: 0.393153
         Train Epoch: [1020/2467]	Loss: 0.082368
     Train Epoch: [1020/2467]	Loss: 0.069766 
Train Epoch: [1020/2467]	Loss: 0.161265
     Train Epoch: [1040/2467]	Loss: 0.186934    
          Train Epoch: [1040/2467]	Loss: 0.120035Train Epoch: [1040/2467]	Loss: 0.117068

 Train Epoch: [1040/2467]	Loss: 0.256672
             Train Epoch: [1060/2467]	Loss: 0.095379
  Train Epoch: [1060/2467]	Loss: 0.268598Train Epoch: [1060/2467]	Loss: 0.162724

     Train Epoch: [1060/2467]	Loss: 0.427590
             Train Epoch: [1080/2467]	Loss: 0.252634
  Train Epoch: [1080/2467]	Loss: 0.118347Train Epoch: [1080/2467]	Loss: 0.161253

     Train Epoch: [1080/2467]	Loss: 0.250730
     Train Epoch: [1100/2467]	Loss: 0.267321
     Train Epoch: [1100/2467]	Loss: 0.231302
     Train Epoch: [1100/2467]	Loss: 0.271310
     Train Epoch: [1100/2467]	Loss: 0.269706
     Train Epoch: [1120/2467]	Loss: 0.132256
     Train Epoch: [1120/2467]	Loss: 0.300108
     Train Epoch: [1120/2467]	Loss: 0.133851
     Train Epoch: [1120/2467]	Loss: 0.147745
     Train Epoch: [1140/2467]	Loss: 0.376129
     Train Epoch: [1140/2467]	Loss: 0.054966
     Train Epoch: [1140/2467]	Loss: 0.277642
     Train Epoch: [1140/2467]	Loss: 0.100320
             Train Epoch: [1160/2467]	Loss: 0.098451
 Train Epoch: [1160/2467]	Loss: 0.220031 
Train Epoch: [1160/2467]	Loss: 0.109775    
 Train Epoch: [1160/2467]	Loss: 0.144573
             Train Epoch: [1180/2467]	Loss: 0.159392
  Train Epoch: [1180/2467]	Loss: 0.144979
Train Epoch: [1180/2467]	Loss: 0.059971
     Train Epoch: [1180/2467]	Loss: 0.234687
     Train Epoch: [1200/2467]	Loss: 0.122526
     Train Epoch: [1200/2467]	Loss: 0.081933
     Train Epoch: [1200/2467]	Loss: 0.303954
     Train Epoch: [1200/2467]	Loss: 0.279077
          Train Epoch: [1220/2467]	Loss: 0.103404Train Epoch: [1220/2467]	Loss: 0.250060

     Train Epoch: [1220/2467]	Loss: 0.253108
     Train Epoch: [1220/2467]	Loss: 0.289580
         Train Epoch: [1240/2467]	Loss: 0.197111     
Train Epoch: [1240/2467]	Loss: 0.322684
 Train Epoch: [1240/2467]	Loss: 0.304237
     Train Epoch: [1240/2467]	Loss: 0.225237
         Train Epoch: [1260/2467]	Loss: 0.059052
         Train Epoch: [1260/2467]	Loss: 0.442042
  Train Epoch: [1260/2467]	Loss: 0.134214Train Epoch: [1260/2467]	Loss: 0.072760

         Train Epoch: [1280/2467]	Loss: 0.239513    
 Train Epoch: [1280/2467]	Loss: 0.431604
 Train Epoch: [1280/2467]	Loss: 0.182890
     Train Epoch: [1280/2467]	Loss: 0.075596
         Train Epoch: [1300/2467]	Loss: 0.234819
     Train Epoch: [1300/2467]	Loss: 0.082694
 Train Epoch: [1300/2467]	Loss: 0.390499
     Train Epoch: [1300/2467]	Loss: 0.056782
         Train Epoch: [1320/2467]	Loss: 0.192253
         Train Epoch: [1320/2467]	Loss: 0.293040
  Train Epoch: [1320/2467]	Loss: 0.052192Train Epoch: [1320/2467]	Loss: 0.137293

             Train Epoch: [1340/2467]	Loss: 0.111178 
Train Epoch: [1340/2467]	Loss: 0.169988 
Train Epoch: [1340/2467]	Loss: 0.310232
     Train Epoch: [1340/2467]	Loss: 0.252453
          Train Epoch: [1360/2467]	Loss: 0.069847
Train Epoch: [1360/2467]	Loss: 0.214424
         Train Epoch: [1360/2467]	Loss: 0.098811
 Train Epoch: [1360/2467]	Loss: 0.132538
     Train Epoch: [1380/2467]	Loss: 0.179733
         Train Epoch: [1380/2467]	Loss: 1.799749
 Train Epoch: [1380/2467]	Loss: 0.316274
     Train Epoch: [1380/2467]	Loss: 0.216262
     Train Epoch: [1400/2467]	Loss: 0.164990    
         Train Epoch: [1400/2467]	Loss: 0.194862 
Train Epoch: [1400/2467]	Loss: 0.203013 
Train Epoch: [1400/2467]	Loss: 0.139797
     Train Epoch: [1420/2467]	Loss: 0.120254
     Train Epoch: [1420/2467]	Loss: 0.441994
     Train Epoch: [1420/2467]	Loss: 0.155722
     Train Epoch: [1420/2467]	Loss: 0.161726
              Train Epoch: [1440/2467]	Loss: 0.066612    Train Epoch: [1440/2467]	Loss: 0.043627

 Train Epoch: [1440/2467]	Loss: 0.066914 
Train Epoch: [1440/2467]	Loss: 0.210788
     Train Epoch: [1460/2467]	Loss: 0.110199
     Train Epoch: [1460/2467]	Loss: 0.059955
         Train Epoch: [1460/2467]	Loss: 0.292445
 Train Epoch: [1460/2467]	Loss: 0.144638
              Train Epoch: [1480/2467]	Loss: 0.246708Train Epoch: [1480/2467]	Loss: 0.029490

 Train Epoch: [1480/2467]	Loss: 0.195874
     Train Epoch: [1480/2467]	Loss: 0.013278
         Train Epoch: [1500/2467]	Loss: 0.089911
     Train Epoch: [1500/2467]	Loss: 0.019074
 Train Epoch: [1500/2467]	Loss: 0.132284
     Train Epoch: [1500/2467]	Loss: 0.130320
     Train Epoch: [1520/2467]	Loss: 0.187293
         Train Epoch: [1520/2467]	Loss: 0.241902
 Train Epoch: [1520/2467]	Loss: 0.184483
     Train Epoch: [1520/2467]	Loss: 0.291567
         Train Epoch: [1540/2467]	Loss: 0.288184
         Train Epoch: [1540/2467]	Loss: 0.233986
  Train Epoch: [1540/2467]	Loss: 0.082889
Train Epoch: [1540/2467]	Loss: 0.133665
                 Train Epoch: [1560/2467]	Loss: 0.502894
  Train Epoch: [1560/2467]	Loss: 0.145161Train Epoch: [1560/2467]	Loss: 0.290799 

Train Epoch: [1560/2467]	Loss: 0.389567
             Train Epoch: [1580/2467]	Loss: 0.221867
      Train Epoch: [1580/2467]	Loss: 0.143886Train Epoch: [1580/2467]	Loss: 0.293719

 Train Epoch: [1580/2467]	Loss: 0.123968
              Train Epoch: [1600/2467]	Loss: 0.158640Train Epoch: [1600/2467]	Loss: 0.255016

 Train Epoch: [1600/2467]	Loss: 0.058717
     Train Epoch: [1600/2467]	Loss: 0.248213
     Train Epoch: [1620/2467]	Loss: 0.245121
     Train Epoch: [1620/2467]	Loss: 0.220358
     Train Epoch: [1620/2467]	Loss: 0.080753
     Train Epoch: [1620/2467]	Loss: 0.127689
     Train Epoch: [1640/2467]	Loss: 0.149249
     Train Epoch: [1640/2467]	Loss: 0.226694
         Train Epoch: [1640/2467]	Loss: 0.153634
 Train Epoch: [1640/2467]	Loss: 0.236274
     Train Epoch: [1660/2467]	Loss: 0.213833
     Train Epoch: [1660/2467]	Loss: 0.184690
     Train Epoch: [1660/2467]	Loss: 0.096389
     Train Epoch: [1660/2467]	Loss: 0.242272
          Train Epoch: [1680/2467]	Loss: 0.160313Train Epoch: [1680/2467]	Loss: 0.057466

     Train Epoch: [1680/2467]	Loss: 0.187160
     Train Epoch: [1680/2467]	Loss: 0.179102
     Train Epoch: [1700/2467]	Loss: 0.088705    
     Train Epoch: [1700/2467]	Loss: 0.289029 
Train Epoch: [1700/2467]	Loss: 0.235493
     Train Epoch: [1700/2467]	Loss: 0.196151
         Train Epoch: [1720/2467]	Loss: 0.245331    
 Train Epoch: [1720/2467]	Loss: 0.169849
 Train Epoch: [1720/2467]	Loss: 0.222176
     Train Epoch: [1720/2467]	Loss: 0.079280
     Train Epoch: [1740/2467]	Loss: 0.355047
         Train Epoch: [1740/2467]	Loss: 0.199398
 Train Epoch: [1740/2467]	Loss: 0.109205
     Train Epoch: [1740/2467]	Loss: 0.039543
     Train Epoch: [1760/2467]	Loss: 0.144119
     Train Epoch: [1760/2467]	Loss: 0.141164
     Train Epoch: [1760/2467]	Loss: 0.063089
     Train Epoch: [1760/2467]	Loss: 0.136048
     Train Epoch: [1780/2467]	Loss: 0.278715
         Train Epoch: [1780/2467]	Loss: 0.292474 
Train Epoch: [1780/2467]	Loss: 0.183674
     Train Epoch: [1780/2467]	Loss: 0.121086
     Train Epoch: [1800/2467]	Loss: 0.226185
             Train Epoch: [1800/2467]	Loss: 0.282584 
Train Epoch: [1800/2467]	Loss: 0.151922
 Train Epoch: [1800/2467]	Loss: 0.234850
         Train Epoch: [1820/2467]	Loss: 0.327278 
Train Epoch: [1820/2467]	Loss: 0.113524
          Train Epoch: [1820/2467]	Loss: 0.151365
Train Epoch: [1820/2467]	Loss: 0.159750
         Train Epoch: [1840/2467]	Loss: 0.328121    
 Train Epoch: [1840/2467]	Loss: 0.062773
 Train Epoch: [1840/2467]	Loss: 0.121098
     Train Epoch: [1840/2467]	Loss: 0.229078
         Train Epoch: [1860/2467]	Loss: 0.158506
     Train Epoch: [1860/2467]	Loss: 0.191925
 Train Epoch: [1860/2467]	Loss: 0.155115
     Train Epoch: [1860/2467]	Loss: 0.104506
     Train Epoch: [1880/2467]	Loss: 0.103242    
 Train Epoch: [1880/2467]	Loss: 0.061935
     Train Epoch: [1880/2467]	Loss: 0.024334
     Train Epoch: [1880/2467]	Loss: 0.044556
              Train Epoch: [1900/2467]	Loss: 0.209521Train Epoch: [1900/2467]	Loss: 0.271487

 Train Epoch: [1900/2467]	Loss: 0.284293
     Train Epoch: [1900/2467]	Loss: 0.109967
             Train Epoch: [1920/2467]	Loss: 0.309201
 Train Epoch: [1920/2467]	Loss: 0.235762
 Train Epoch: [1920/2467]	Loss: 0.119809
     Train Epoch: [1920/2467]	Loss: 0.015750
             Train Epoch: [1940/2467]	Loss: 0.090036
 Train Epoch: [1940/2467]	Loss: 0.220153
 Train Epoch: [1940/2467]	Loss: 0.110773
     Train Epoch: [1940/2467]	Loss: 0.376978
     Train Epoch: [1960/2467]	Loss: 0.109219
         Train Epoch: [1960/2467]	Loss: 0.191995
 Train Epoch: [1960/2467]	Loss: 0.182746
     Train Epoch: [1960/2467]	Loss: 0.094954
                 Train Epoch: [1980/2467]	Loss: 0.267213 
Train Epoch: [1980/2467]	Loss: 0.129416 
Train Epoch: [1980/2467]	Loss: 0.253793
 Train Epoch: [1980/2467]	Loss: 0.271732
         Train Epoch: [2000/2467]	Loss: 0.023885
     Train Epoch: [2000/2467]	Loss: 0.242287    
 Train Epoch: [2000/2467]	Loss: 0.058809
 Train Epoch: [2000/2467]	Loss: 0.146101
     Train Epoch: [2020/2467]	Loss: 0.163388
         Train Epoch: [2020/2467]	Loss: 0.174040 
Train Epoch: [2020/2467]	Loss: 0.329544
     Train Epoch: [2020/2467]	Loss: 0.083755
     Train Epoch: [2040/2467]	Loss: 0.261679
     Train Epoch: [2040/2467]	Loss: 0.399229
         Train Epoch: [2040/2467]	Loss: 0.119570
 Train Epoch: [2040/2467]	Loss: 0.097383
             Train Epoch: [2060/2467]	Loss: 0.084793
      Train Epoch: [2060/2467]	Loss: 0.240446
Train Epoch: [2060/2467]	Loss: 0.262514
 Train Epoch: [2060/2467]	Loss: 0.320891
             Train Epoch: [2080/2467]	Loss: 0.268278
 Train Epoch: [2080/2467]	Loss: 0.056828 
Train Epoch: [2080/2467]	Loss: 0.093905    
 Train Epoch: [2080/2467]	Loss: 0.052341
     Train Epoch: [2100/2467]	Loss: 0.156660
         Train Epoch: [2100/2467]	Loss: 0.518012
 Train Epoch: [2100/2467]	Loss: 0.100199
     Train Epoch: [2100/2467]	Loss: 0.070406
     Train Epoch: [2120/2467]	Loss: 0.209275
          Train Epoch: [2120/2467]	Loss: 0.047933Train Epoch: [2120/2467]	Loss: 0.110211

     Train Epoch: [2120/2467]	Loss: 0.212031
             Train Epoch: [2140/2467]	Loss: 0.340704
 Train Epoch: [2140/2467]	Loss: 0.196352
 Train Epoch: [2140/2467]	Loss: 0.614693
     Train Epoch: [2140/2467]	Loss: 0.285640
          Train Epoch: [2160/2467]	Loss: 0.298299Train Epoch: [2160/2467]	Loss: 0.057581

     Train Epoch: [2160/2467]	Loss: 0.145297
     Train Epoch: [2160/2467]	Loss: 0.188908
              Train Epoch: [2180/2467]	Loss: 0.165768Train Epoch: [2180/2467]	Loss: 0.097763

 Train Epoch: [2180/2467]	Loss: 0.251712
     Train Epoch: [2180/2467]	Loss: 0.178618
               Train Epoch: [2200/2467]	Loss: 0.139641Train Epoch: [2200/2467]	Loss: 0.123556
Train Epoch: [2200/2467]	Loss: 0.188647

     Train Epoch: [2200/2467]	Loss: 0.063742
     Train Epoch: [2220/2467]	Loss: 0.187192
         Train Epoch: [2220/2467]	Loss: 0.069736 
Train Epoch: [2220/2467]	Loss: 0.104060
     Train Epoch: [2220/2467]	Loss: 0.160720
              Train Epoch: [2240/2467]	Loss: 0.235798
    Train Epoch: [2240/2467]	Loss: 0.118084
 Train Epoch: [2240/2467]	Loss: 0.025057
 Train Epoch: [2240/2467]	Loss: 0.092258
         Train Epoch: [2260/2467]	Loss: 0.202468 
Train Epoch: [2260/2467]	Loss: 0.490780
     Train Epoch: [2260/2467]	Loss: 0.219998
     Train Epoch: [2260/2467]	Loss: 0.028786
         Train Epoch: [2280/2467]	Loss: 0.169779
         Train Epoch: [2280/2467]	Loss: 0.221747
 Train Epoch: [2280/2467]	Loss: 0.100223
 Train Epoch: [2280/2467]	Loss: 0.099035
         Train Epoch: [2300/2467]	Loss: 0.298180
 Train Epoch: [2300/2467]	Loss: 0.098743    
     Train Epoch: [2300/2467]	Loss: 0.075977
 Train Epoch: [2300/2467]	Loss: 0.051863
         Train Epoch: [2320/2467]	Loss: 0.182162
     Train Epoch: [2320/2467]	Loss: 0.151772    
 Train Epoch: [2320/2467]	Loss: 0.040937 
Train Epoch: [2320/2467]	Loss: 0.262676
              Train Epoch: [2340/2467]	Loss: 0.204854Train Epoch: [2340/2467]	Loss: 0.150135

 Train Epoch: [2340/2467]	Loss: 0.150862
     Train Epoch: [2340/2467]	Loss: 0.509148
          Train Epoch: [2360/2467]	Loss: 0.339340Train Epoch: [2360/2467]	Loss: 0.141731    

 Train Epoch: [2360/2467]	Loss: 0.165015
     Train Epoch: [2360/2467]	Loss: 0.061177
         Train Epoch: [2380/2467]	Loss: 0.449184
 Train Epoch: [2380/2467]	Loss: 0.110200    
 Train Epoch: [2380/2467]	Loss: 0.171025
     Train Epoch: [2380/2467]	Loss: 0.270106
     Train Epoch: [2400/2467]	Loss: 0.100377    
         Train Epoch: [2400/2467]	Loss: 0.106616
  Train Epoch: [2400/2467]	Loss: 0.222637
Train Epoch: [2400/2467]	Loss: 0.139480
     Train Epoch: [2420/2467]	Loss: 0.160185
          Train Epoch: [2420/2467]	Loss: 0.032310
Train Epoch: [2420/2467]	Loss: 0.372007
     Train Epoch: [2420/2467]	Loss: 0.231294
             Train Epoch: [2440/2467]	Loss: 0.302139  
Train Epoch: [2440/2467]	Loss: 0.148691Train Epoch: [2440/2467]	Loss: 0.160379

     Train Epoch: [2440/2467]	Loss: 0.032216
             Train Epoch: [2460/2467]	Loss: 0.087142
  Train Epoch: [2460/2467]	Loss: 0.273016Train Epoch: [2460/2467]	Loss: 0.162504

     Train Epoch: [2460/2467]	Loss: 0.142985
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 23 epoch =====
     2025-05-11.08-25-30
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 23 epoch =====
     2025-05-11.08-25-30
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 23 epoch =====
     2025-05-11.08-25-30
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 23 epoch =====
     2025-05-11.08-25-30
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.313881
             Train Epoch: [0/2467]	Loss: 0.115223
  Train Epoch: [0/2467]	Loss: 0.178476Train Epoch: [0/2467]	Loss: 0.301190

     Train Epoch: [20/2467]	Loss: 0.312022
             Train Epoch: [20/2467]	Loss: 0.114215
  Train Epoch: [20/2467]	Loss: 0.154598Train Epoch: [20/2467]	Loss: 0.063341

     Train Epoch: [40/2467]	Loss: 0.418190
             Train Epoch: [40/2467]	Loss: 0.182263
 Train Epoch: [40/2467]	Loss: 0.159250
 Train Epoch: [40/2467]	Loss: 0.213227
         Train Epoch: [60/2467]	Loss: 0.052033
     Train Epoch: [60/2467]	Loss: 0.252602
 Train Epoch: [60/2467]	Loss: 0.155260
     Train Epoch: [60/2467]	Loss: 0.110885
         Train Epoch: [80/2467]	Loss: 0.112783
 Train Epoch: [80/2467]	Loss: 0.277136
     Train Epoch: [80/2467]	Loss: 0.200674
     Train Epoch: [80/2467]	Loss: 0.084518
              Train Epoch: [100/2467]	Loss: 0.075661Train Epoch: [100/2467]	Loss: 0.253630

 Train Epoch: [100/2467]	Loss: 0.205774
     Train Epoch: [100/2467]	Loss: 0.163640
     Train Epoch: [120/2467]	Loss: 0.299329    
     Train Epoch: [120/2467]	Loss: 0.081839
 Train Epoch: [120/2467]	Loss: 0.357867
     Train Epoch: [120/2467]	Loss: 0.064541
             Train Epoch: [140/2467]	Loss: 0.148938
      Train Epoch: [140/2467]	Loss: 0.213512Train Epoch: [140/2467]	Loss: 0.072546

 Train Epoch: [140/2467]	Loss: 0.029530
         Train Epoch: [160/2467]	Loss: 0.115981 
Train Epoch: [160/2467]	Loss: 0.143387
     Train Epoch: [160/2467]	Loss: 0.232711
     Train Epoch: [160/2467]	Loss: 0.133359
     Train Epoch: [180/2467]	Loss: 0.107749
               Train Epoch: [180/2467]	Loss: 0.130727Train Epoch: [180/2467]	Loss: 0.237535
Train Epoch: [180/2467]	Loss: 0.114825

             Train Epoch: [200/2467]	Loss: 0.085104     
Train Epoch: [200/2467]	Loss: 0.120224
 Train Epoch: [200/2467]	Loss: 0.079228
 Train Epoch: [200/2467]	Loss: 0.077963
     Train Epoch: [220/2467]	Loss: 0.168990
             Train Epoch: [220/2467]	Loss: 0.142685 
Train Epoch: [220/2467]	Loss: 0.223501 
Train Epoch: [220/2467]	Loss: 0.074550
     Train Epoch: [240/2467]	Loss: 0.223019
             Train Epoch: [240/2467]	Loss: 0.174824
  Train Epoch: [240/2467]	Loss: 0.041871Train Epoch: [240/2467]	Loss: 0.023345

              Train Epoch: [260/2467]	Loss: 0.058547Train Epoch: [260/2467]	Loss: 0.051305

 Train Epoch: [260/2467]	Loss: 0.120293
     Train Epoch: [260/2467]	Loss: 0.046160
         Train Epoch: [280/2467]	Loss: 0.188334
 Train Epoch: [280/2467]	Loss: 0.154974
     Train Epoch: [280/2467]	Loss: 0.107797    
 Train Epoch: [280/2467]	Loss: 0.208626
     Train Epoch: [300/2467]	Loss: 0.040153    
 Train Epoch: [300/2467]	Loss: 0.348755
          Train Epoch: [300/2467]	Loss: 0.163754Train Epoch: [300/2467]	Loss: 0.092329

             Train Epoch: [320/2467]	Loss: 0.437586
     Train Epoch: [320/2467]	Loss: 0.167319
 Train Epoch: [320/2467]	Loss: 0.457314 
Train Epoch: [320/2467]	Loss: 0.169667
     Train Epoch: [340/2467]	Loss: 0.058196
         Train Epoch: [340/2467]	Loss: 0.111355
     Train Epoch: [340/2467]	Loss: 0.203442
 Train Epoch: [340/2467]	Loss: 0.016577
     Train Epoch: [360/2467]	Loss: 0.076186
     Train Epoch: [360/2467]	Loss: 0.222059
     Train Epoch: [360/2467]	Loss: 0.176553
     Train Epoch: [360/2467]	Loss: 0.061380
     Train Epoch: [380/2467]	Loss: 0.098416
         Train Epoch: [380/2467]	Loss: 0.199984
 Train Epoch: [380/2467]	Loss: 0.332214
     Train Epoch: [380/2467]	Loss: 0.140941
         Train Epoch: [400/2467]	Loss: 0.172460
     Train Epoch: [400/2467]	Loss: 0.121427
 Train Epoch: [400/2467]	Loss: 0.128881
     Train Epoch: [400/2467]	Loss: 0.237840
             Train Epoch: [420/2467]	Loss: 0.272543 
 Train Epoch: [420/2467]	Loss: 0.159692
Train Epoch: [420/2467]	Loss: 0.119977
     Train Epoch: [420/2467]	Loss: 0.210346
     Train Epoch: [440/2467]	Loss: 0.093051    
     Train Epoch: [440/2467]	Loss: 0.051769 
Train Epoch: [440/2467]	Loss: 0.169699
     Train Epoch: [440/2467]	Loss: 0.386390
     Train Epoch: [460/2467]	Loss: 0.179522
     Train Epoch: [460/2467]	Loss: 0.128553
     Train Epoch: [460/2467]	Loss: 0.259620
     Train Epoch: [460/2467]	Loss: 0.125280
     Train Epoch: [480/2467]	Loss: 0.100521
         Train Epoch: [480/2467]	Loss: 0.357208
 Train Epoch: [480/2467]	Loss: 0.303870
     Train Epoch: [480/2467]	Loss: 0.163401
     Train Epoch: [500/2467]	Loss: 0.116499
     Train Epoch: [500/2467]	Loss: 0.104519
     Train Epoch: [500/2467]	Loss: 0.088479
     Train Epoch: [500/2467]	Loss: 0.186584
         Train Epoch: [520/2467]	Loss: 0.274313
     Train Epoch: [520/2467]	Loss: 0.080360
 Train Epoch: [520/2467]	Loss: 0.151848
     Train Epoch: [520/2467]	Loss: 0.164271
     Train Epoch: [540/2467]	Loss: 0.299935
         Train Epoch: [540/2467]	Loss: 0.147505
 Train Epoch: [540/2467]	Loss: 0.146819
     Train Epoch: [540/2467]	Loss: 0.067776
         Train Epoch: [560/2467]	Loss: 0.295804
         Train Epoch: [560/2467]	Loss: 0.180110
  Train Epoch: [560/2467]	Loss: 0.194282Train Epoch: [560/2467]	Loss: 0.169892

     Train Epoch: [580/2467]	Loss: 0.150704
         Train Epoch: [580/2467]	Loss: 0.353790 
Train Epoch: [580/2467]	Loss: 0.058840
     Train Epoch: [580/2467]	Loss: 0.198045
     Train Epoch: [600/2467]	Loss: 0.130830
         Train Epoch: [600/2467]	Loss: 0.213460 
Train Epoch: [600/2467]	Loss: 0.182271
     Train Epoch: [600/2467]	Loss: 0.168715
         Train Epoch: [620/2467]	Loss: 0.027788
         Train Epoch: [620/2467]	Loss: 0.208846
  Train Epoch: [620/2467]	Loss: 0.145430
Train Epoch: [620/2467]	Loss: 0.151089
     Train Epoch: [640/2467]	Loss: 0.205518
     Train Epoch: [640/2467]	Loss: 0.062894
     Train Epoch: [640/2467]	Loss: 0.204578
     Train Epoch: [640/2467]	Loss: 0.113233
         Train Epoch: [660/2467]	Loss: 0.072934
     Train Epoch: [660/2467]	Loss: 0.183180
 Train Epoch: [660/2467]	Loss: 0.026900
     Train Epoch: [660/2467]	Loss: 0.403840
     Train Epoch: [680/2467]	Loss: 0.073891
          Train Epoch: [680/2467]	Loss: 0.042535Train Epoch: [680/2467]	Loss: 0.100647

     Train Epoch: [680/2467]	Loss: 0.126296
     Train Epoch: [700/2467]	Loss: 0.358118
         Train Epoch: [700/2467]	Loss: 0.049454
 Train Epoch: [700/2467]	Loss: 0.068564
     Train Epoch: [700/2467]	Loss: 0.382166
     Train Epoch: [720/2467]	Loss: 0.185099
         Train Epoch: [720/2467]	Loss: 0.123413 
Train Epoch: [720/2467]	Loss: 0.045589
     Train Epoch: [720/2467]	Loss: 0.083688
         Train Epoch: [740/2467]	Loss: 0.075133
 Train Epoch: [740/2467]	Loss: 0.099472    
 Train Epoch: [740/2467]	Loss: 0.050532
     Train Epoch: [740/2467]	Loss: 0.303546
               Train Epoch: [760/2467]	Loss: 0.236134Train Epoch: [760/2467]	Loss: 0.299102Train Epoch: [760/2467]	Loss: 0.427098


     Train Epoch: [760/2467]	Loss: 0.051462
         Train Epoch: [780/2467]	Loss: 0.317046
     Train Epoch: [780/2467]	Loss: 0.137795
 Train Epoch: [780/2467]	Loss: 0.375737
     Train Epoch: [780/2467]	Loss: 0.339485
     Train Epoch: [800/2467]	Loss: 0.193232
         Train Epoch: [800/2467]	Loss: 0.168982
 Train Epoch: [800/2467]	Loss: 0.102114
     Train Epoch: [800/2467]	Loss: 0.241908
         Train Epoch: [820/2467]	Loss: 0.438399
     Train Epoch: [820/2467]	Loss: 0.049329
 Train Epoch: [820/2467]	Loss: 0.081911
     Train Epoch: [820/2467]	Loss: 0.259023
         Train Epoch: [840/2467]	Loss: 0.090423
         Train Epoch: [840/2467]	Loss: 0.125418
  Train Epoch: [840/2467]	Loss: 0.118880Train Epoch: [840/2467]	Loss: 0.287410

             Train Epoch: [860/2467]	Loss: 0.066176
  Train Epoch: [860/2467]	Loss: 0.236957Train Epoch: [860/2467]	Loss: 0.175622

     Train Epoch: [860/2467]	Loss: 0.215439
                   Train Epoch: [880/2467]	Loss: 0.153877Train Epoch: [880/2467]	Loss: 0.272029
Train Epoch: [880/2467]	Loss: 0.251723

 Train Epoch: [880/2467]	Loss: 0.146527
                 Train Epoch: [900/2467]	Loss: 0.216073
 Train Epoch: [900/2467]	Loss: 0.198628
 Train Epoch: [900/2467]	Loss: 0.198818 
Train Epoch: [900/2467]	Loss: 0.106916
             Train Epoch: [920/2467]	Loss: 0.050271
 Train Epoch: [920/2467]	Loss: 0.283323    
 Train Epoch: [920/2467]	Loss: 0.423534
 Train Epoch: [920/2467]	Loss: 0.327926
     Train Epoch: [940/2467]	Loss: 0.048073
     Train Epoch: [940/2467]	Loss: 0.036057
     Train Epoch: [940/2467]	Loss: 0.090027
     Train Epoch: [940/2467]	Loss: 0.227860
              Train Epoch: [960/2467]	Loss: 0.097271Train Epoch: [960/2467]	Loss: 0.055828

     Train Epoch: [960/2467]	Loss: 0.084546
 Train Epoch: [960/2467]	Loss: 0.289310
         Train Epoch: [980/2467]	Loss: 0.114338 
Train Epoch: [980/2467]	Loss: 0.058321
     Train Epoch: [980/2467]	Loss: 0.227439
     Train Epoch: [980/2467]	Loss: 0.137413
              Train Epoch: [1000/2467]	Loss: 0.219789
Train Epoch: [1000/2467]	Loss: 0.284572
 Train Epoch: [1000/2467]	Loss: 0.042258
     Train Epoch: [1000/2467]	Loss: 0.045208
     Train Epoch: [1020/2467]	Loss: 0.420822    
      Train Epoch: [1020/2467]	Loss: 0.084935Train Epoch: [1020/2467]	Loss: 0.178630

     Train Epoch: [1020/2467]	Loss: 0.075317
     Train Epoch: [1040/2467]	Loss: 0.203330    
     Train Epoch: [1040/2467]	Loss: 0.111035
     Train Epoch: [1040/2467]	Loss: 0.272875
 Train Epoch: [1040/2467]	Loss: 0.163066
             Train Epoch: [1060/2467]	Loss: 0.093291 
Train Epoch: [1060/2467]	Loss: 0.150692 
Train Epoch: [1060/2467]	Loss: 0.294879
     Train Epoch: [1060/2467]	Loss: 0.435369
     Train Epoch: [1080/2467]	Loss: 0.365657
         Train Epoch: [1080/2467]	Loss: 0.151587 
Train Epoch: [1080/2467]	Loss: 0.141814
     Train Epoch: [1080/2467]	Loss: 0.253589
     Train Epoch: [1100/2467]	Loss: 0.268283
         Train Epoch: [1100/2467]	Loss: 0.225774
 Train Epoch: [1100/2467]	Loss: 0.255158
     Train Epoch: [1100/2467]	Loss: 0.247418
     Train Epoch: [1120/2467]	Loss: 0.122379
             Train Epoch: [1120/2467]	Loss: 0.146332
  Train Epoch: [1120/2467]	Loss: 0.246979Train Epoch: [1120/2467]	Loss: 0.131934

     Train Epoch: [1140/2467]	Loss: 0.382527
     Train Epoch: [1140/2467]	Loss: 0.058956    
      Train Epoch: [1140/2467]	Loss: 0.076682
Train Epoch: [1140/2467]	Loss: 0.277953
          Train Epoch: [1160/2467]	Loss: 0.106296
Train Epoch: [1160/2467]	Loss: 0.125543
     Train Epoch: [1160/2467]	Loss: 0.116339
     Train Epoch: [1160/2467]	Loss: 0.243227
     Train Epoch: [1180/2467]	Loss: 0.246883
     Train Epoch: [1180/2467]	Loss: 0.147283
     Train Epoch: [1180/2467]	Loss: 0.053005
     Train Epoch: [1180/2467]	Loss: 0.153526
             Train Epoch: [1200/2467]	Loss: 0.129937
      Train Epoch: [1200/2467]	Loss: 0.090417
Train Epoch: [1200/2467]	Loss: 0.194794
 Train Epoch: [1200/2467]	Loss: 0.309711
         Train Epoch: [1220/2467]	Loss: 0.105186
     Train Epoch: [1220/2467]	Loss: 0.288541 
Train Epoch: [1220/2467]	Loss: 0.236421
     Train Epoch: [1220/2467]	Loss: 0.217329
     Train Epoch: [1240/2467]	Loss: 0.302068
         Train Epoch: [1240/2467]	Loss: 0.179090
 Train Epoch: [1240/2467]	Loss: 0.187985
     Train Epoch: [1240/2467]	Loss: 0.289487
     Train Epoch: [1260/2467]	Loss: 0.073305
         Train Epoch: [1260/2467]	Loss: 0.483822
 Train Epoch: [1260/2467]	Loss: 0.090473
     Train Epoch: [1260/2467]	Loss: 0.133586
     Train Epoch: [1280/2467]	Loss: 0.237513
              Train Epoch: [1280/2467]	Loss: 0.465770Train Epoch: [1280/2467]	Loss: 0.076890

 Train Epoch: [1280/2467]	Loss: 0.525070
     Train Epoch: [1300/2467]	Loss: 0.329742
         Train Epoch: [1300/2467]	Loss: 0.076245 
Train Epoch: [1300/2467]	Loss: 0.335982
     Train Epoch: [1300/2467]	Loss: 0.059280
     Train Epoch: [1320/2467]	Loss: 0.184130
         Train Epoch: [1320/2467]	Loss: 0.048358
 Train Epoch: [1320/2467]	Loss: 0.116903
     Train Epoch: [1320/2467]	Loss: 0.213799
             Train Epoch: [1340/2467]	Loss: 0.132523
      Train Epoch: [1340/2467]	Loss: 0.166336
Train Epoch: [1340/2467]	Loss: 0.303912
 Train Epoch: [1340/2467]	Loss: 0.217836
     Train Epoch: [1360/2467]	Loss: 0.136092
     Train Epoch: [1360/2467]	Loss: 0.100250
         Train Epoch: [1360/2467]	Loss: 0.138561
 Train Epoch: [1360/2467]	Loss: 0.079556
     Train Epoch: [1380/2467]	Loss: 0.192946
     Train Epoch: [1380/2467]	Loss: 0.278965
     Train Epoch: [1380/2467]	Loss: 0.214556
     Train Epoch: [1380/2467]	Loss: 0.315900
         Train Epoch: [1400/2467]	Loss: 0.176152
     Train Epoch: [1400/2467]	Loss: 0.163045
     Train Epoch: [1400/2467]	Loss: 0.123109 
Train Epoch: [1400/2467]	Loss: 0.091913
         Train Epoch: [1420/2467]	Loss: 0.092467    
  Train Epoch: [1420/2467]	Loss: 0.143150Train Epoch: [1420/2467]	Loss: 0.396639

     Train Epoch: [1420/2467]	Loss: 0.120113
          Train Epoch: [1440/2467]	Loss: 0.058958
Train Epoch: [1440/2467]	Loss: 0.043441
     Train Epoch: [1440/2467]	Loss: 0.027838
     Train Epoch: [1440/2467]	Loss: 0.147562
     Train Epoch: [1460/2467]	Loss: 0.106977    
 Train Epoch: [1460/2467]	Loss: 0.045959
     Train Epoch: [1460/2467]	Loss: 0.336165
     Train Epoch: [1460/2467]	Loss: 0.242920
     Train Epoch: [1480/2467]	Loss: 0.072065
     Train Epoch: [1480/2467]	Loss: 0.184925    
 Train Epoch: [1480/2467]	Loss: 0.013564
     Train Epoch: [1480/2467]	Loss: 0.259306
     Train Epoch: [1500/2467]	Loss: 0.023582
              Train Epoch: [1500/2467]	Loss: 0.139540Train Epoch: [1500/2467]	Loss: 0.142330

 Train Epoch: [1500/2467]	Loss: 0.095733
                 Train Epoch: [1520/2467]	Loss: 0.172927 
Train Epoch: [1520/2467]	Loss: 0.267179
  Train Epoch: [1520/2467]	Loss: 0.340582
Train Epoch: [1520/2467]	Loss: 0.187692
     Train Epoch: [1540/2467]	Loss: 0.283602
     Train Epoch: [1540/2467]	Loss: 0.128799    
 Train Epoch: [1540/2467]	Loss: 0.127380
     Train Epoch: [1540/2467]	Loss: 0.148362
             Train Epoch: [1560/2467]	Loss: 0.509125
     Train Epoch: [1560/2467]	Loss: 0.299575 
 Train Epoch: [1560/2467]	Loss: 0.080807
Train Epoch: [1560/2467]	Loss: 0.417645
     Train Epoch: [1580/2467]	Loss: 0.150875
             Train Epoch: [1580/2467]	Loss: 0.276196
  Train Epoch: [1580/2467]	Loss: 0.128390Train Epoch: [1580/2467]	Loss: 0.216867

              Train Epoch: [1600/2467]	Loss: 0.253241Train Epoch: [1600/2467]	Loss: 0.102094

     Train Epoch: [1600/2467]	Loss: 0.256742
 Train Epoch: [1600/2467]	Loss: 0.073302
             Train Epoch: [1620/2467]	Loss: 0.102376
      Train Epoch: [1620/2467]	Loss: 0.225949Train Epoch: [1620/2467]	Loss: 0.182882

 Train Epoch: [1620/2467]	Loss: 0.060302
     Train Epoch: [1640/2467]	Loss: 0.154276
         Train Epoch: [1640/2467]	Loss: 0.274642 
Train Epoch: [1640/2467]	Loss: 0.235941
     Train Epoch: [1640/2467]	Loss: 0.102944
         Train Epoch: [1660/2467]	Loss: 0.212044
     Train Epoch: [1660/2467]	Loss: 0.109361
 Train Epoch: [1660/2467]	Loss: 0.214276
     Train Epoch: [1660/2467]	Loss: 0.134122
         Train Epoch: [1680/2467]	Loss: 0.213819
     Train Epoch: [1680/2467]	Loss: 0.053772
 Train Epoch: [1680/2467]	Loss: 0.168154
     Train Epoch: [1680/2467]	Loss: 0.159452
         Train Epoch: [1700/2467]	Loss: 0.079538
     Train Epoch: [1700/2467]	Loss: 0.536558
 Train Epoch: [1700/2467]	Loss: 0.203392
     Train Epoch: [1700/2467]	Loss: 0.330161
                 Train Epoch: [1720/2467]	Loss: 0.237062 
Train Epoch: [1720/2467]	Loss: 0.189040
  Train Epoch: [1720/2467]	Loss: 0.151026Train Epoch: [1720/2467]	Loss: 0.070517

         Train Epoch: [1740/2467]	Loss: 0.048638
 Train Epoch: [1740/2467]	Loss: 0.028000
         Train Epoch: [1740/2467]	Loss: 0.353525
 Train Epoch: [1740/2467]	Loss: 0.196411
         Train Epoch: [1760/2467]	Loss: 0.152548
     Train Epoch: [1760/2467]	Loss: 0.130156
 Train Epoch: [1760/2467]	Loss: 0.097134
     Train Epoch: [1760/2467]	Loss: 0.052586
     Train Epoch: [1780/2467]	Loss: 0.157506    
     Train Epoch: [1780/2467]	Loss: 0.263879    
 Train Epoch: [1780/2467]	Loss: 0.104507
 Train Epoch: [1780/2467]	Loss: 0.275490
     Train Epoch: [1800/2467]	Loss: 0.204306
          Train Epoch: [1800/2467]	Loss: 0.308863Train Epoch: [1800/2467]	Loss: 0.143667

     Train Epoch: [1800/2467]	Loss: 0.245058
          Train Epoch: [1820/2467]	Loss: 0.124006Train Epoch: [1820/2467]	Loss: 0.109379    

     Train Epoch: [1820/2467]	Loss: 0.135445
 Train Epoch: [1820/2467]	Loss: 0.317373
         Train Epoch: [1840/2467]	Loss: 0.052258
 Train Epoch: [1840/2467]	Loss: 0.279505
     Train Epoch: [1840/2467]	Loss: 0.110047
     Train Epoch: [1840/2467]	Loss: 0.294156
             Train Epoch: [1860/2467]	Loss: 0.169243
  Train Epoch: [1860/2467]	Loss: 0.144345
Train Epoch: [1860/2467]	Loss: 0.127025
     Train Epoch: [1860/2467]	Loss: 0.161694
         Train Epoch: [1880/2467]	Loss: 0.107893
 Train Epoch: [1880/2467]	Loss: 0.077087
         Train Epoch: [1880/2467]	Loss: 0.024886
 Train Epoch: [1880/2467]	Loss: 0.041346
         Train Epoch: [1900/2467]	Loss: 0.144139
     Train Epoch: [1900/2467]	Loss: 0.278295
 Train Epoch: [1900/2467]	Loss: 0.293254
     Train Epoch: [1900/2467]	Loss: 0.113596
     Train Epoch: [1920/2467]	Loss: 0.204371
             Train Epoch: [1920/2467]	Loss: 0.113708
  Train Epoch: [1920/2467]	Loss: 0.016159Train Epoch: [1920/2467]	Loss: 0.166167

     Train Epoch: [1940/2467]	Loss: 0.078431
         Train Epoch: [1940/2467]	Loss: 0.106057
 Train Epoch: [1940/2467]	Loss: 0.164701
     Train Epoch: [1940/2467]	Loss: 0.395217
               Train Epoch: [1960/2467]	Loss: 0.259716Train Epoch: [1960/2467]	Loss: 0.186051
Train Epoch: [1960/2467]	Loss: 0.175909

     Train Epoch: [1960/2467]	Loss: 0.177792
     Train Epoch: [1980/2467]	Loss: 0.132125
         Train Epoch: [1980/2467]	Loss: 0.217885
      Train Epoch: [1980/2467]	Loss: 0.219314
Train Epoch: [1980/2467]	Loss: 0.270391
     Train Epoch: [2000/2467]	Loss: 0.024945
         Train Epoch: [2000/2467]	Loss: 0.233534
 Train Epoch: [2000/2467]	Loss: 0.150394
     Train Epoch: [2000/2467]	Loss: 0.053231
     Train Epoch: [2020/2467]	Loss: 0.177545
     Train Epoch: [2020/2467]	Loss: 0.170497
          Train Epoch: [2020/2467]	Loss: 0.054043Train Epoch: [2020/2467]	Loss: 0.228709

             Train Epoch: [2040/2467]	Loss: 0.092393
 Train Epoch: [2040/2467]	Loss: 0.383773
     Train Epoch: [2040/2467]	Loss: 0.123673
 Train Epoch: [2040/2467]	Loss: 0.177269
              Train Epoch: [2060/2467]	Loss: 0.088213Train Epoch: [2060/2467]	Loss: 0.218438 

Train Epoch: [2060/2467]	Loss: 0.195684
     Train Epoch: [2060/2467]	Loss: 0.213737
         Train Epoch: [2080/2467]	Loss: 0.249638
 Train Epoch: [2080/2467]	Loss: 0.091714
     Train Epoch: [2080/2467]	Loss: 0.051060
     Train Epoch: [2080/2467]	Loss: 0.056865
     Train Epoch: [2100/2467]	Loss: 0.116196
         Train Epoch: [2100/2467]	Loss: 0.510745
 Train Epoch: [2100/2467]	Loss: 0.099855
     Train Epoch: [2100/2467]	Loss: 0.094639
             Train Epoch: [2120/2467]	Loss: 0.188400
 Train Epoch: [2120/2467]	Loss: 0.039125
 Train Epoch: [2120/2467]	Loss: 0.093672
     Train Epoch: [2120/2467]	Loss: 0.202040
             Train Epoch: [2140/2467]	Loss: 0.323025
 Train Epoch: [2140/2467]	Loss: 0.178427 
Train Epoch: [2140/2467]	Loss: 0.543373
     Train Epoch: [2140/2467]	Loss: 0.326883
         Train Epoch: [2160/2467]	Loss: 0.370295
         Train Epoch: [2160/2467]	Loss: 0.165601
  Train Epoch: [2160/2467]	Loss: 0.059983Train Epoch: [2160/2467]	Loss: 0.124570

     Train Epoch: [2180/2467]	Loss: 0.273716
          Train Epoch: [2180/2467]	Loss: 0.136146Train Epoch: [2180/2467]	Loss: 0.102049

     Train Epoch: [2180/2467]	Loss: 0.169108
         Train Epoch: [2200/2467]	Loss: 0.211819
 Train Epoch: [2200/2467]	Loss: 0.083971
     Train Epoch: [2200/2467]	Loss: 0.047835
     Train Epoch: [2200/2467]	Loss: 0.118500
     Train Epoch: [2220/2467]	Loss: 0.189492    
     Train Epoch: [2220/2467]	Loss: 0.064429    
  Train Epoch: [2220/2467]	Loss: 0.121132
Train Epoch: [2220/2467]	Loss: 0.116746
          Train Epoch: [2240/2467]	Loss: 0.037452Train Epoch: [2240/2467]	Loss: 0.120814

     Train Epoch: [2240/2467]	Loss: 0.089137
     Train Epoch: [2240/2467]	Loss: 0.024723
     Train Epoch: [2260/2467]	Loss: 0.189871
     Train Epoch: [2260/2467]	Loss: 0.195089    
 Train Epoch: [2260/2467]	Loss: 0.037660
     Train Epoch: [2260/2467]	Loss: 0.414796
     Train Epoch: [2280/2467]	Loss: 0.142234
     Train Epoch: [2280/2467]	Loss: 0.093239
         Train Epoch: [2280/2467]	Loss: 0.083097
 Train Epoch: [2280/2467]	Loss: 0.109037
          Train Epoch: [2300/2467]	Loss: 0.247269Train Epoch: [2300/2467]	Loss: 0.094755

     Train Epoch: [2300/2467]	Loss: 0.047626    
 Train Epoch: [2300/2467]	Loss: 0.074381
         Train Epoch: [2320/2467]	Loss: 0.159590 
Train Epoch: [2320/2467]	Loss: 0.018433
     Train Epoch: [2320/2467]	Loss: 0.202163
     Train Epoch: [2320/2467]	Loss: 0.162749
         Train Epoch: [2340/2467]	Loss: 0.193491 
Train Epoch: [2340/2467]	Loss: 0.549055
          Train Epoch: [2340/2467]	Loss: 0.138367
Train Epoch: [2340/2467]	Loss: 0.147231
     Train Epoch: [2360/2467]	Loss: 0.339493
     Train Epoch: [2360/2467]	Loss: 0.145837
     Train Epoch: [2360/2467]	Loss: 0.063390
     Train Epoch: [2360/2467]	Loss: 0.152939
         Train Epoch: [2380/2467]	Loss: 0.459614
 Train Epoch: [2380/2467]	Loss: 0.139917
         Train Epoch: [2380/2467]	Loss: 0.196909
 Train Epoch: [2380/2467]	Loss: 0.214331
     Train Epoch: [2400/2467]	Loss: 0.105277
     Train Epoch: [2400/2467]	Loss: 0.147831
     Train Epoch: [2400/2467]	Loss: 0.183718
     Train Epoch: [2400/2467]	Loss: 0.115288
             Train Epoch: [2420/2467]	Loss: 0.032718 
Train Epoch: [2420/2467]	Loss: 0.231663
 Train Epoch: [2420/2467]	Loss: 0.339378
     Train Epoch: [2420/2467]	Loss: 0.148028
              Train Epoch: [2440/2467]	Loss: 0.132306    
Train Epoch: [2440/2467]	Loss: 0.305113 
Train Epoch: [2440/2467]	Loss: 0.161439
 Train Epoch: [2440/2467]	Loss: 0.030695
     Train Epoch: [2460/2467]	Loss: 0.093843    
          Train Epoch: [2460/2467]	Loss: 0.132024
Train Epoch: [2460/2467]	Loss: 0.126909
 Train Epoch: [2460/2467]	Loss: 0.272603
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 24 epoch =====
     2025-05-11.08-47-21
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 24 epoch =====
     2025-05-11.08-47-21
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 24 epoch =====
     2025-05-11.08-47-21
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 24 epoch =====
     2025-05-11.08-47-21
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
         Train Epoch: [0/2467]	Loss: 0.180627
     Train Epoch: [0/2467]	Loss: 0.312139
     Train Epoch: [0/2467]	Loss: 0.088723
 Train Epoch: [0/2467]	Loss: 0.259627
         Train Epoch: [20/2467]	Loss: 0.353942
         Train Epoch: [20/2467]	Loss: 0.208480
  Train Epoch: [20/2467]	Loss: 0.107322
Train Epoch: [20/2467]	Loss: 0.070017
     Train Epoch: [40/2467]	Loss: 0.145714
               Train Epoch: [40/2467]	Loss: 0.116878Train Epoch: [40/2467]	Loss: 0.151538Train Epoch: [40/2467]	Loss: 0.339348


         Train Epoch: [60/2467]	Loss: 0.034280
 Train Epoch: [60/2467]	Loss: 0.111431
     Train Epoch: [60/2467]	Loss: 0.294052
     Train Epoch: [60/2467]	Loss: 0.184784
     Train Epoch: [80/2467]	Loss: 0.079267
         Train Epoch: [80/2467]	Loss: 0.096081
 Train Epoch: [80/2467]	Loss: 0.124536
     Train Epoch: [80/2467]	Loss: 0.287315
     Train Epoch: [100/2467]	Loss: 0.073874
     Train Epoch: [100/2467]	Loss: 0.175878
     Train Epoch: [100/2467]	Loss: 0.126535
     Train Epoch: [100/2467]	Loss: 0.267264
             Train Epoch: [120/2467]	Loss: 0.089763
 Train Epoch: [120/2467]	Loss: 0.355520
 Train Epoch: [120/2467]	Loss: 0.273191
     Train Epoch: [120/2467]	Loss: 0.070286
         Train Epoch: [140/2467]	Loss: 0.075321 
Train Epoch: [140/2467]	Loss: 0.253891
         Train Epoch: [140/2467]	Loss: 0.076302
 Train Epoch: [140/2467]	Loss: 0.164635
     Train Epoch: [160/2467]	Loss: 0.134985
         Train Epoch: [160/2467]	Loss: 0.135035
 Train Epoch: [160/2467]	Loss: 0.129709
     Train Epoch: [160/2467]	Loss: 0.217329
                 Train Epoch: [180/2467]	Loss: 0.196299
 Train Epoch: [180/2467]	Loss: 0.127115
  Train Epoch: [180/2467]	Loss: 0.097895
Train Epoch: [180/2467]	Loss: 0.113280
         Train Epoch: [200/2467]	Loss: 0.116379
     Train Epoch: [200/2467]	Loss: 0.065314
     Train Epoch: [200/2467]	Loss: 0.073582
 Train Epoch: [200/2467]	Loss: 0.067928
     Train Epoch: [220/2467]	Loss: 0.113092
         Train Epoch: [220/2467]	Loss: 0.170383
 Train Epoch: [220/2467]	Loss: 0.137195
     Train Epoch: [220/2467]	Loss: 0.218632
         Train Epoch: [240/2467]	Loss: 0.184516 
    Train Epoch: [240/2467]	Loss: 0.020765
     Train Epoch: [240/2467]	Loss: 0.222625
 Train Epoch: [240/2467]	Loss: 0.041179
     Train Epoch: [260/2467]	Loss: 0.042681
     Train Epoch: [260/2467]	Loss: 0.061287
     Train Epoch: [260/2467]	Loss: 0.131799
     Train Epoch: [260/2467]	Loss: 0.048975
     Train Epoch: [280/2467]	Loss: 0.262395
         Train Epoch: [280/2467]	Loss: 0.312703
 Train Epoch: [280/2467]	Loss: 0.158659
     Train Epoch: [280/2467]	Loss: 0.109859
              Train Epoch: [300/2467]	Loss: 0.344545Train Epoch: [300/2467]	Loss: 0.167355

 Train Epoch: [300/2467]	Loss: 0.039310
     Train Epoch: [300/2467]	Loss: 0.090311
     Train Epoch: [320/2467]	Loss: 0.151527
     Train Epoch: [320/2467]	Loss: 0.165089    
 Train Epoch: [320/2467]	Loss: 0.264384
     Train Epoch: [320/2467]	Loss: 0.410326
         Train Epoch: [340/2467]	Loss: 0.018302 
Train Epoch: [340/2467]	Loss: 0.099768
     Train Epoch: [340/2467]	Loss: 0.204963
     Train Epoch: [340/2467]	Loss: 0.053626
         Train Epoch: [360/2467]	Loss: 0.087255
 Train Epoch: [360/2467]	Loss: 0.226508    
     Train Epoch: [360/2467]	Loss: 0.078273
 Train Epoch: [360/2467]	Loss: 0.212395
             Train Epoch: [380/2467]	Loss: 0.099415  
Train Epoch: [380/2467]	Loss: 0.242245Train Epoch: [380/2467]	Loss: 0.363084

     Train Epoch: [380/2467]	Loss: 0.135725
         Train Epoch: [400/2467]	Loss: 0.038261
 Train Epoch: [400/2467]	Loss: 0.081018
     Train Epoch: [400/2467]	Loss: 0.189617
     Train Epoch: [400/2467]	Loss: 0.179628
     Train Epoch: [420/2467]	Loss: 0.269679
     Train Epoch: [420/2467]	Loss: 0.140349
     Train Epoch: [420/2467]	Loss: 0.103841
     Train Epoch: [420/2467]	Loss: 0.215148
         Train Epoch: [440/2467]	Loss: 0.345299    
 Train Epoch: [440/2467]	Loss: 0.095902
 Train Epoch: [440/2467]	Loss: 0.150894
     Train Epoch: [440/2467]	Loss: 0.061818
               Train Epoch: [460/2467]	Loss: 0.241384    Train Epoch: [460/2467]	Loss: 0.188076
Train Epoch: [460/2467]	Loss: 0.125827

 Train Epoch: [460/2467]	Loss: 0.116807
         Train Epoch: [480/2467]	Loss: 0.292377
 Train Epoch: [480/2467]	Loss: 0.085891    
 Train Epoch: [480/2467]	Loss: 0.139504
     Train Epoch: [480/2467]	Loss: 0.344335
             Train Epoch: [500/2467]	Loss: 0.112918
 Train Epoch: [500/2467]	Loss: 0.102976
 Train Epoch: [500/2467]	Loss: 0.109467
     Train Epoch: [500/2467]	Loss: 0.200594
         Train Epoch: [520/2467]	Loss: 0.080640
 Train Epoch: [520/2467]	Loss: 0.128682
         Train Epoch: [520/2467]	Loss: 0.248805
 Train Epoch: [520/2467]	Loss: 0.113953
     Train Epoch: [540/2467]	Loss: 0.151413    
         Train Epoch: [540/2467]	Loss: 0.061276
  Train Epoch: [540/2467]	Loss: 0.280999Train Epoch: [540/2467]	Loss: 0.118638

         Train Epoch: [560/2467]	Loss: 0.198156
     Train Epoch: [560/2467]	Loss: 0.147149
 Train Epoch: [560/2467]	Loss: 0.137060
     Train Epoch: [560/2467]	Loss: 0.205499
     Train Epoch: [580/2467]	Loss: 0.195631
         Train Epoch: [580/2467]	Loss: 0.059594
 Train Epoch: [580/2467]	Loss: 0.284709
     Train Epoch: [580/2467]	Loss: 0.197790
         Train Epoch: [600/2467]	Loss: 0.206254
     Train Epoch: [600/2467]	Loss: 0.167572
     Train Epoch: [600/2467]	Loss: 0.117764
 Train Epoch: [600/2467]	Loss: 0.215259
     Train Epoch: [620/2467]	Loss: 0.137724
     Train Epoch: [620/2467]	Loss: 0.026139
     Train Epoch: [620/2467]	Loss: 0.253155
     Train Epoch: [620/2467]	Loss: 0.134865
         Train Epoch: [640/2467]	Loss: 0.139771    
  Train Epoch: [640/2467]	Loss: 0.186875Train Epoch: [640/2467]	Loss: 0.042201

     Train Epoch: [640/2467]	Loss: 0.193587
     Train Epoch: [660/2467]	Loss: 0.383909
     Train Epoch: [660/2467]	Loss: 0.057303
     Train Epoch: [660/2467]	Loss: 0.033036
     Train Epoch: [660/2467]	Loss: 0.169219
     Train Epoch: [680/2467]	Loss: 0.110920
     Train Epoch: [680/2467]	Loss: 0.040722
     Train Epoch: [680/2467]	Loss: 0.124626
     Train Epoch: [680/2467]	Loss: 0.062344
     Train Epoch: [700/2467]	Loss: 0.042867
         Train Epoch: [700/2467]	Loss: 0.066716
 Train Epoch: [700/2467]	Loss: 0.334779
     Train Epoch: [700/2467]	Loss: 0.445426
         Train Epoch: [720/2467]	Loss: 0.155292
 Train Epoch: [720/2467]	Loss: 0.019851
         Train Epoch: [720/2467]	Loss: 0.084314
 Train Epoch: [720/2467]	Loss: 0.181655
         Train Epoch: [740/2467]	Loss: 0.068395
 Train Epoch: [740/2467]	Loss: 0.301308
         Train Epoch: [740/2467]	Loss: 0.098177
 Train Epoch: [740/2467]	Loss: 0.081340
         Train Epoch: [760/2467]	Loss: 0.412888 
Train Epoch: [760/2467]	Loss: 0.055492
         Train Epoch: [760/2467]	Loss: 0.315011
 Train Epoch: [760/2467]	Loss: 0.250840
         Train Epoch: [780/2467]	Loss: 0.368588
 Train Epoch: [780/2467]	Loss: 0.346753
         Train Epoch: [780/2467]	Loss: 0.151546
 Train Epoch: [780/2467]	Loss: 0.335732
     Train Epoch: [800/2467]	Loss: 0.190334
     Train Epoch: [800/2467]	Loss: 0.207661
     Train Epoch: [800/2467]	Loss: 0.101248
     Train Epoch: [800/2467]	Loss: 0.159482
     Train Epoch: [820/2467]	Loss: 0.213906
             Train Epoch: [820/2467]	Loss: 0.480506
  Train Epoch: [820/2467]	Loss: 0.040273Train Epoch: [820/2467]	Loss: 0.055249

     Train Epoch: [840/2467]	Loss: 0.155821
         Train Epoch: [840/2467]	Loss: 0.108659
 Train Epoch: [840/2467]	Loss: 0.055542
     Train Epoch: [840/2467]	Loss: 0.315972
              Train Epoch: [860/2467]	Loss: 0.282407Train Epoch: [860/2467]	Loss: 0.062495

 Train Epoch: [860/2467]	Loss: 0.256493
     Train Epoch: [860/2467]	Loss: 0.046800
          Train Epoch: [880/2467]	Loss: 0.278960Train Epoch: [880/2467]	Loss: 0.246914

     Train Epoch: [880/2467]	Loss: 0.217906
     Train Epoch: [880/2467]	Loss: 0.158651
         Train Epoch: [900/2467]	Loss: 0.259584
     Train Epoch: [900/2467]	Loss: 0.127218
 Train Epoch: [900/2467]	Loss: 0.196967    
 Train Epoch: [900/2467]	Loss: 0.206376
          Train Epoch: [920/2467]	Loss: 0.292864Train Epoch: [920/2467]	Loss: 0.282022    

 Train Epoch: [920/2467]	Loss: 0.304235
     Train Epoch: [920/2467]	Loss: 0.050864
     Train Epoch: [940/2467]	Loss: 0.091056    
     Train Epoch: [940/2467]	Loss: 0.084849
 Train Epoch: [940/2467]	Loss: 0.169030
     Train Epoch: [940/2467]	Loss: 0.029302
     Train Epoch: [960/2467]	Loss: 0.057257
         Train Epoch: [960/2467]	Loss: 0.093903
 Train Epoch: [960/2467]	Loss: 0.315293
     Train Epoch: [960/2467]	Loss: 0.072711
         Train Epoch: [980/2467]	Loss: 0.118388    
 Train Epoch: [980/2467]	Loss: 0.232217 
Train Epoch: [980/2467]	Loss: 0.058322    
 Train Epoch: [980/2467]	Loss: 0.142132
     Train Epoch: [1000/2467]	Loss: 0.039888
             Train Epoch: [1000/2467]	Loss: 0.218701
 Train Epoch: [1000/2467]	Loss: 0.293946 
Train Epoch: [1000/2467]	Loss: 0.033000
              Train Epoch: [1020/2467]	Loss: 0.071038Train Epoch: [1020/2467]	Loss: 0.164042

 Train Epoch: [1020/2467]	Loss: 0.391956
     Train Epoch: [1020/2467]	Loss: 0.083522
         Train Epoch: [1040/2467]	Loss: 0.297176
 Train Epoch: [1040/2467]	Loss: 0.171998
         Train Epoch: [1040/2467]	Loss: 0.107086
 Train Epoch: [1040/2467]	Loss: 0.105908
         Train Epoch: [1060/2467]	Loss: 0.436728 
    Train Epoch: [1060/2467]	Loss: 0.228197
 Train Epoch: [1060/2467]	Loss: 0.116523
     Train Epoch: [1060/2467]	Loss: 0.107441
         Train Epoch: [1080/2467]	Loss: 0.116068
 Train Epoch: [1080/2467]	Loss: 0.236177
     Train Epoch: [1080/2467]	Loss: 0.136852
     Train Epoch: [1080/2467]	Loss: 0.254395
     Train Epoch: [1100/2467]	Loss: 0.205720
         Train Epoch: [1100/2467]	Loss: 0.221432 
Train Epoch: [1100/2467]	Loss: 0.272474    
 Train Epoch: [1100/2467]	Loss: 0.264984
             Train Epoch: [1120/2467]	Loss: 0.324940
 Train Epoch: [1120/2467]	Loss: 0.127584
     Train Epoch: [1120/2467]	Loss: 0.118986 
Train Epoch: [1120/2467]	Loss: 0.143972
     Train Epoch: [1140/2467]	Loss: 0.052907
     Train Epoch: [1140/2467]	Loss: 0.074484
         Train Epoch: [1140/2467]	Loss: 0.450897
 Train Epoch: [1140/2467]	Loss: 0.271399
     Train Epoch: [1160/2467]	Loss: 0.220664
          Train Epoch: [1160/2467]	Loss: 0.133031Train Epoch: [1160/2467]	Loss: 0.075612

     Train Epoch: [1160/2467]	Loss: 0.121733
     Train Epoch: [1180/2467]	Loss: 0.056476
         Train Epoch: [1180/2467]	Loss: 0.139612    
 Train Epoch: [1180/2467]	Loss: 0.234664 
Train Epoch: [1180/2467]	Loss: 0.168141
     Train Epoch: [1200/2467]	Loss: 0.295880
         Train Epoch: [1200/2467]	Loss: 0.138307
 Train Epoch: [1200/2467]	Loss: 0.232995
     Train Epoch: [1200/2467]	Loss: 0.099176
     Train Epoch: [1220/2467]	Loss: 0.242552
         Train Epoch: [1220/2467]	Loss: 0.325937
     Train Epoch: [1220/2467]	Loss: 0.240155
 Train Epoch: [1220/2467]	Loss: 0.134980
     Train Epoch: [1240/2467]	Loss: 0.312119
     Train Epoch: [1240/2467]	Loss: 0.187113
          Train Epoch: [1240/2467]	Loss: 0.278675
Train Epoch: [1240/2467]	Loss: 0.141743
     Train Epoch: [1260/2467]	Loss: 0.062157
     Train Epoch: [1260/2467]	Loss: 0.163900
     Train Epoch: [1260/2467]	Loss: 0.416887
     Train Epoch: [1260/2467]	Loss: 0.059406
         Train Epoch: [1280/2467]	Loss: 0.067249 
Train Epoch: [1280/2467]	Loss: 0.236046
         Train Epoch: [1280/2467]	Loss: 0.438109
 Train Epoch: [1280/2467]	Loss: 0.199119
         Train Epoch: [1300/2467]	Loss: 0.053073
 Train Epoch: [1300/2467]	Loss: 0.119244
         Train Epoch: [1300/2467]	Loss: 0.318492
 Train Epoch: [1300/2467]	Loss: 0.213840
          Train Epoch: [1320/2467]	Loss: 0.149416Train Epoch: [1320/2467]	Loss: 0.046158

     Train Epoch: [1320/2467]	Loss: 0.288644
     Train Epoch: [1320/2467]	Loss: 0.195748
             Train Epoch: [1340/2467]	Loss: 0.349700
 Train Epoch: [1340/2467]	Loss: 0.257298
 Train Epoch: [1340/2467]	Loss: 0.114230
     Train Epoch: [1340/2467]	Loss: 0.157050
     Train Epoch: [1360/2467]	Loss: 0.095648
     Train Epoch: [1360/2467]	Loss: 0.056080
     Train Epoch: [1360/2467]	Loss: 0.116494
     Train Epoch: [1360/2467]	Loss: 0.124125
     Train Epoch: [1380/2467]	Loss: 0.354119
     Train Epoch: [1380/2467]	Loss: 0.297858
     Train Epoch: [1380/2467]	Loss: 0.198962    
 Train Epoch: [1380/2467]	Loss: 0.222238
     Train Epoch: [1400/2467]	Loss: 0.107285
         Train Epoch: [1400/2467]	Loss: 0.160346 
Train Epoch: [1400/2467]	Loss: 0.115439
     Train Epoch: [1400/2467]	Loss: 0.158174
               Train Epoch: [1420/2467]	Loss: 0.401324Train Epoch: [1420/2467]	Loss: 0.136724Train Epoch: [1420/2467]	Loss: 0.077775


     Train Epoch: [1420/2467]	Loss: 0.152861
     Train Epoch: [1440/2467]	Loss: 0.057360
         Train Epoch: [1440/2467]	Loss: 0.026413
 Train Epoch: [1440/2467]	Loss: 0.143553
     Train Epoch: [1440/2467]	Loss: 0.054106
     Train Epoch: [1460/2467]	Loss: 0.114323
          Train Epoch: [1460/2467]	Loss: 0.042816Train Epoch: [1460/2467]	Loss: 0.414886

     Train Epoch: [1460/2467]	Loss: 0.134250
             Train Epoch: [1480/2467]	Loss: 0.266229
 Train Epoch: [1480/2467]	Loss: 0.037570
 Train Epoch: [1480/2467]	Loss: 0.014515
     Train Epoch: [1480/2467]	Loss: 0.209068
         Train Epoch: [1500/2467]	Loss: 0.096447
 Train Epoch: [1500/2467]	Loss: 0.135622
          Train Epoch: [1500/2467]	Loss: 0.014432Train Epoch: [1500/2467]	Loss: 0.107688

     Train Epoch: [1520/2467]	Loss: 0.298947
         Train Epoch: [1520/2467]	Loss: 0.159365
 Train Epoch: [1520/2467]	Loss: 0.146301
     Train Epoch: [1520/2467]	Loss: 0.260684
         Train Epoch: [1540/2467]	Loss: 0.034718 
Train Epoch: [1540/2467]	Loss: 0.102762
         Train Epoch: [1540/2467]	Loss: 0.286757
 Train Epoch: [1540/2467]	Loss: 0.126860
         Train Epoch: [1560/2467]	Loss: 0.282179    
 Train Epoch: [1560/2467]	Loss: 0.366360 
Train Epoch: [1560/2467]	Loss: 0.077199
     Train Epoch: [1560/2467]	Loss: 0.481847
             Train Epoch: [1580/2467]	Loss: 0.120240
      Train Epoch: [1580/2467]	Loss: 0.309640Train Epoch: [1580/2467]	Loss: 0.218945

 Train Epoch: [1580/2467]	Loss: 0.143748
          Train Epoch: [1600/2467]	Loss: 0.050807Train Epoch: [1600/2467]	Loss: 0.227720
    
     Train Epoch: [1600/2467]	Loss: 0.224981
 Train Epoch: [1600/2467]	Loss: 0.114980
     Train Epoch: [1620/2467]	Loss: 0.102748
     Train Epoch: [1620/2467]	Loss: 0.172196
     Train Epoch: [1620/2467]	Loss: 0.253469
     Train Epoch: [1620/2467]	Loss: 0.051370
     Train Epoch: [1640/2467]	Loss: 0.226778
     Train Epoch: [1640/2467]	Loss: 0.126455
     Train Epoch: [1640/2467]	Loss: 0.159046
     Train Epoch: [1640/2467]	Loss: 0.225175
             Train Epoch: [1660/2467]	Loss: 0.126925
      Train Epoch: [1660/2467]	Loss: 0.205957Train Epoch: [1660/2467]	Loss: 0.174933

 Train Epoch: [1660/2467]	Loss: 0.074472
             Train Epoch: [1680/2467]	Loss: 0.164907 
Train Epoch: [1680/2467]	Loss: 0.156601
 Train Epoch: [1680/2467]	Loss: 0.054315
     Train Epoch: [1680/2467]	Loss: 0.193096
             Train Epoch: [1700/2467]	Loss: 0.162544 
Train Epoch: [1700/2467]	Loss: 0.330547 
Train Epoch: [1700/2467]	Loss: 0.078125    
 Train Epoch: [1700/2467]	Loss: 0.169248
     Train Epoch: [1720/2467]	Loss: 0.174098
              Train Epoch: [1720/2467]	Loss: 0.051042Train Epoch: [1720/2467]	Loss: 0.126485

 Train Epoch: [1720/2467]	Loss: 0.228802
         Train Epoch: [1740/2467]	Loss: 0.056518    
  Train Epoch: [1740/2467]	Loss: 0.336702Train Epoch: [1740/2467]	Loss: 0.025828
    
 Train Epoch: [1740/2467]	Loss: 0.202607
     Train Epoch: [1760/2467]	Loss: 0.043973
          Train Epoch: [1760/2467]	Loss: 0.109199Train Epoch: [1760/2467]	Loss: 0.143712

     Train Epoch: [1760/2467]	Loss: 0.141528
     Train Epoch: [1780/2467]	Loss: 0.117751
     Train Epoch: [1780/2467]	Loss: 0.324344
     Train Epoch: [1780/2467]	Loss: 0.328802
     Train Epoch: [1780/2467]	Loss: 0.283316
     Train Epoch: [1800/2467]	Loss: 0.269606    
     Train Epoch: [1800/2467]	Loss: 0.167930 
Train Epoch: [1800/2467]	Loss: 0.163680
     Train Epoch: [1800/2467]	Loss: 0.294599
             Train Epoch: [1820/2467]	Loss: 0.139561 
Train Epoch: [1820/2467]	Loss: 0.303110
 Train Epoch: [1820/2467]	Loss: 0.161935    
 Train Epoch: [1820/2467]	Loss: 0.116899
     Train Epoch: [1840/2467]	Loss: 0.297071    
     Train Epoch: [1840/2467]	Loss: 0.115028
 Train Epoch: [1840/2467]	Loss: 0.231648
     Train Epoch: [1840/2467]	Loss: 0.051629
     Train Epoch: [1860/2467]	Loss: 0.158864    
 Train Epoch: [1860/2467]	Loss: 0.131877
     Train Epoch: [1860/2467]	Loss: 0.123338
     Train Epoch: [1860/2467]	Loss: 0.140557
              Train Epoch: [1880/2467]	Loss: 0.022566
Train Epoch: [1880/2467]	Loss: 0.042549
 Train Epoch: [1880/2467]	Loss: 0.085319
     Train Epoch: [1880/2467]	Loss: 0.059536
          Train Epoch: [1900/2467]	Loss: 0.109897Train Epoch: [1900/2467]	Loss: 0.295283

     Train Epoch: [1900/2467]	Loss: 0.276353
     Train Epoch: [1900/2467]	Loss: 0.138689
              Train Epoch: [1920/2467]	Loss: 0.020414Train Epoch: [1920/2467]	Loss: 0.137336

 Train Epoch: [1920/2467]	Loss: 0.193065
     Train Epoch: [1920/2467]	Loss: 0.316936
     Train Epoch: [1940/2467]	Loss: 0.174512
     Train Epoch: [1940/2467]	Loss: 0.331319
         Train Epoch: [1940/2467]	Loss: 0.112052
 Train Epoch: [1940/2467]	Loss: 0.063249
         Train Epoch: [1960/2467]	Loss: 0.182355
 Train Epoch: [1960/2467]	Loss: 0.195799
     Train Epoch: [1960/2467]	Loss: 0.097931
     Train Epoch: [1960/2467]	Loss: 0.093752
     Train Epoch: [1980/2467]	Loss: 0.321452
         Train Epoch: [1980/2467]	Loss: 0.269007
 Train Epoch: [1980/2467]	Loss: 0.109300
     Train Epoch: [1980/2467]	Loss: 0.240869
         Train Epoch: [2000/2467]	Loss: 0.152286
 Train Epoch: [2000/2467]	Loss: 0.057222
         Train Epoch: [2000/2467]	Loss: 0.021041
 Train Epoch: [2000/2467]	Loss: 0.229876
         Train Epoch: [2020/2467]	Loss: 0.189196
     Train Epoch: [2020/2467]	Loss: 0.384463
 Train Epoch: [2020/2467]	Loss: 0.154315
     Train Epoch: [2020/2467]	Loss: 0.056486
         Train Epoch: [2040/2467]	Loss: 0.384470
     Train Epoch: [2040/2467]	Loss: 0.096086
 Train Epoch: [2040/2467]	Loss: 0.188829
     Train Epoch: [2040/2467]	Loss: 0.082954
     Train Epoch: [2060/2467]	Loss: 0.222012
     Train Epoch: [2060/2467]	Loss: 0.069627
     Train Epoch: [2060/2467]	Loss: 0.228339
     Train Epoch: [2060/2467]	Loss: 0.201638
         Train Epoch: [2080/2467]	Loss: 0.041030
     Train Epoch: [2080/2467]	Loss: 0.077925
     Train Epoch: [2080/2467]	Loss: 0.241970
 Train Epoch: [2080/2467]	Loss: 0.098214
     Train Epoch: [2100/2467]	Loss: 0.098585
         Train Epoch: [2100/2467]	Loss: 0.117392
 Train Epoch: [2100/2467]	Loss: 0.088272
     Train Epoch: [2100/2467]	Loss: 0.475585
         Train Epoch: [2120/2467]	Loss: 0.201392    
 Train Epoch: [2120/2467]	Loss: 0.107045
 Train Epoch: [2120/2467]	Loss: 0.123915
     Train Epoch: [2120/2467]	Loss: 0.048947
             Train Epoch: [2140/2467]	Loss: 0.185609 
Train Epoch: [2140/2467]	Loss: 0.545159 
Train Epoch: [2140/2467]	Loss: 0.243704
     Train Epoch: [2140/2467]	Loss: 0.357645
             Train Epoch: [2160/2467]	Loss: 0.062573
      Train Epoch: [2160/2467]	Loss: 0.323770Train Epoch: [2160/2467]	Loss: 0.114573

 Train Epoch: [2160/2467]	Loss: 0.240318
         Train Epoch: [2180/2467]	Loss: 0.084709    
      Train Epoch: [2180/2467]	Loss: 0.249462Train Epoch: [2180/2467]	Loss: 0.144358

 Train Epoch: [2180/2467]	Loss: 0.169245
              Train Epoch: [2200/2467]	Loss: 0.039188Train Epoch: [2200/2467]	Loss: 0.107074

     Train Epoch: [2200/2467]	Loss: 0.194317
 Train Epoch: [2200/2467]	Loss: 0.102932
         Train Epoch: [2220/2467]	Loss: 0.097524
     Train Epoch: [2220/2467]	Loss: 0.167379
 Train Epoch: [2220/2467]	Loss: 0.112638
     Train Epoch: [2220/2467]	Loss: 0.068360
         Train Epoch: [2240/2467]	Loss: 0.033533
      Train Epoch: [2240/2467]	Loss: 0.105986Train Epoch: [2240/2467]	Loss: 0.130251

     Train Epoch: [2240/2467]	Loss: 0.078453
              Train Epoch: [2260/2467]	Loss: 0.312690Train Epoch: [2260/2467]	Loss: 0.508701

 Train Epoch: [2260/2467]	Loss: 0.032228
     Train Epoch: [2260/2467]	Loss: 0.199717
     Train Epoch: [2280/2467]	Loss: 0.092711
         Train Epoch: [2280/2467]	Loss: 0.071649
     Train Epoch: [2280/2467]	Loss: 0.140420
 Train Epoch: [2280/2467]	Loss: 0.156977
         Train Epoch: [2300/2467]	Loss: 0.051583
     Train Epoch: [2300/2467]	Loss: 0.076526
 Train Epoch: [2300/2467]	Loss: 0.249610
     Train Epoch: [2300/2467]	Loss: 0.105294
     Train Epoch: [2320/2467]	Loss: 0.249742    
          Train Epoch: [2320/2467]	Loss: 0.025168
Train Epoch: [2320/2467]	Loss: 0.146231
 Train Epoch: [2320/2467]	Loss: 0.201588
         Train Epoch: [2340/2467]	Loss: 0.106343 
Train Epoch: [2340/2467]	Loss: 0.135756    
     Train Epoch: [2340/2467]	Loss: 0.189888
 Train Epoch: [2340/2467]	Loss: 0.455702
     Train Epoch: [2360/2467]	Loss: 0.053167
     Train Epoch: [2360/2467]	Loss: 0.410173    
 Train Epoch: [2360/2467]	Loss: 0.159739
     Train Epoch: [2360/2467]	Loss: 0.151164
         Train Epoch: [2380/2467]	Loss: 0.085996 
Train Epoch: [2380/2467]	Loss: 0.406030
     Train Epoch: [2380/2467]	Loss: 0.145665
     Train Epoch: [2380/2467]	Loss: 0.180571
          Train Epoch: [2400/2467]	Loss: 0.188528Train Epoch: [2400/2467]	Loss: 0.100090

     Train Epoch: [2400/2467]	Loss: 0.119130
     Train Epoch: [2400/2467]	Loss: 0.156319
         Train Epoch: [2420/2467]	Loss: 0.056738
          Train Epoch: [2420/2467]	Loss: 0.169389
Train Epoch: [2420/2467]	Loss: 0.266937
 Train Epoch: [2420/2467]	Loss: 0.326022
         Train Epoch: [2440/2467]	Loss: 0.024182
 Train Epoch: [2440/2467]	Loss: 0.253706
     Train Epoch: [2440/2467]	Loss: 0.115786
     Train Epoch: [2440/2467]	Loss: 0.142141
     Train Epoch: [2460/2467]	Loss: 0.138091    
     Train Epoch: [2460/2467]	Loss: 0.081161
 Train Epoch: [2460/2467]	Loss: 0.135843
     Train Epoch: [2460/2467]	Loss: 0.232857
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 25 epoch =====
     2025-05-11.09-09-09
     ===== running 25 epoch =====
     2025-05-11.09-09-09
after set grad
after set grad
after prog
start loop
after prog
start loop
     ===== running 25 epoch =====
     2025-05-11.09-09-10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 25 epoch =====
     2025-05-11.09-09-10
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.283223
         Train Epoch: [0/2467]	Loss: 0.170540
 Train Epoch: [0/2467]	Loss: 0.098434
     Train Epoch: [0/2467]	Loss: 0.333417
         Train Epoch: [20/2467]	Loss: 0.063487
     Train Epoch: [20/2467]	Loss: 0.193879
 Train Epoch: [20/2467]	Loss: 0.315594
     Train Epoch: [20/2467]	Loss: 0.177039
     Train Epoch: [40/2467]	Loss: 0.139423
         Train Epoch: [40/2467]	Loss: 0.594840
 Train Epoch: [40/2467]	Loss: 0.125909
     Train Epoch: [40/2467]	Loss: 0.170666
     Train Epoch: [60/2467]	Loss: 0.208389
              Train Epoch: [60/2467]	Loss: 0.176494Train Epoch: [60/2467]	Loss: 0.118143

 Train Epoch: [60/2467]	Loss: 0.182779
              Train Epoch: [80/2467]	Loss: 0.175072Train Epoch: [80/2467]	Loss: 0.264738

 Train Epoch: [80/2467]	Loss: 0.113018
     Train Epoch: [80/2467]	Loss: 0.101991
     Train Epoch: [100/2467]	Loss: 0.095800
             Train Epoch: [100/2467]	Loss: 0.223410
 Train Epoch: [100/2467]	Loss: 0.159161 
Train Epoch: [100/2467]	Loss: 0.155386
         Train Epoch: [120/2467]	Loss: 0.376970
 Train Epoch: [120/2467]	Loss: 0.211416
     Train Epoch: [120/2467]	Loss: 0.082490
     Train Epoch: [120/2467]	Loss: 0.099851
         Train Epoch: [140/2467]	Loss: 0.209989
     Train Epoch: [140/2467]	Loss: 0.136074
 Train Epoch: [140/2467]	Loss: 0.078436
     Train Epoch: [140/2467]	Loss: 0.102694
     Train Epoch: [160/2467]	Loss: 0.141797
              Train Epoch: [160/2467]	Loss: 0.156954 Train Epoch: [160/2467]	Loss: 0.159643

Train Epoch: [160/2467]	Loss: 0.219683
         Train Epoch: [180/2467]	Loss: 0.161556
 Train Epoch: [180/2467]	Loss: 0.263685
         Train Epoch: [180/2467]	Loss: 0.198098
 Train Epoch: [180/2467]	Loss: 0.168578
         Train Epoch: [200/2467]	Loss: 0.055220 
Train Epoch: [200/2467]	Loss: 0.117173
         Train Epoch: [200/2467]	Loss: 0.051801 
Train Epoch: [200/2467]	Loss: 0.076779
             Train Epoch: [220/2467]	Loss: 0.203149 
Train Epoch: [220/2467]	Loss: 0.074890
 Train Epoch: [220/2467]	Loss: 0.154485
     Train Epoch: [220/2467]	Loss: 0.221647
         Train Epoch: [240/2467]	Loss: 0.166232
          Train Epoch: [240/2467]	Loss: 0.275345Train Epoch: [240/2467]	Loss: 0.018263

 Train Epoch: [240/2467]	Loss: 0.043435
         Train Epoch: [260/2467]	Loss: 0.036363
 Train Epoch: [260/2467]	Loss: 0.064280
     Train Epoch: [260/2467]	Loss: 0.110417
     Train Epoch: [260/2467]	Loss: 0.060689
         Train Epoch: [280/2467]	Loss: 0.183045
 Train Epoch: [280/2467]	Loss: 0.226839
     Train Epoch: [280/2467]	Loss: 0.148619
     Train Epoch: [280/2467]	Loss: 0.096537
         Train Epoch: [300/2467]	Loss: 0.399728
 Train Epoch: [300/2467]	Loss: 0.036974
     Train Epoch: [300/2467]	Loss: 0.087542
     Train Epoch: [300/2467]	Loss: 0.161932
         Train Epoch: [320/2467]	Loss: 0.176299 
Train Epoch: [320/2467]	Loss: 0.171840    
     Train Epoch: [320/2467]	Loss: 0.265116
 Train Epoch: [320/2467]	Loss: 0.418879
         Train Epoch: [340/2467]	Loss: 0.018487
      Train Epoch: [340/2467]	Loss: 0.045618Train Epoch: [340/2467]	Loss: 0.093637

     Train Epoch: [340/2467]	Loss: 0.173888
         Train Epoch: [360/2467]	Loss: 0.084415    
  Train Epoch: [360/2467]	Loss: 0.221695Train Epoch: [360/2467]	Loss: 0.071946

     Train Epoch: [360/2467]	Loss: 0.167626
     Train Epoch: [380/2467]	Loss: 0.343873    
     Train Epoch: [380/2467]	Loss: 0.076994    
 Train Epoch: [380/2467]	Loss: 0.191327 
Train Epoch: [380/2467]	Loss: 0.193749
         Train Epoch: [400/2467]	Loss: 0.047232 
Train Epoch: [400/2467]	Loss: 0.064520
     Train Epoch: [400/2467]	Loss: 0.164429
     Train Epoch: [400/2467]	Loss: 0.199825
     Train Epoch: [420/2467]	Loss: 0.243329    
      Train Epoch: [420/2467]	Loss: 0.266958Train Epoch: [420/2467]	Loss: 0.143371

     Train Epoch: [420/2467]	Loss: 0.094622
          Train Epoch: [440/2467]	Loss: 0.369528Train Epoch: [440/2467]	Loss: 0.160163
    
     Train Epoch: [440/2467]	Loss: 0.081484
 Train Epoch: [440/2467]	Loss: 0.059773
          Train Epoch: [460/2467]	Loss: 0.160057Train Epoch: [460/2467]	Loss: 0.262500

         Train Epoch: [460/2467]	Loss: 0.149525
 Train Epoch: [460/2467]	Loss: 0.229592
             Train Epoch: [480/2467]	Loss: 0.291889
  Train Epoch: [480/2467]	Loss: 0.145500Train Epoch: [480/2467]	Loss: 0.036778

     Train Epoch: [480/2467]	Loss: 0.335290
             Train Epoch: [500/2467]	Loss: 0.080079
 Train Epoch: [500/2467]	Loss: 0.105687
 Train Epoch: [500/2467]	Loss: 0.099226
     Train Epoch: [500/2467]	Loss: 0.189494
     Train Epoch: [520/2467]	Loss: 0.152736
             Train Epoch: [520/2467]	Loss: 0.276050
 Train Epoch: [520/2467]	Loss: 0.077732 
Train Epoch: [520/2467]	Loss: 0.104942
     Train Epoch: [540/2467]	Loss: 0.147648    
     Train Epoch: [540/2467]	Loss: 0.284869
 Train Epoch: [540/2467]	Loss: 0.066200
     Train Epoch: [540/2467]	Loss: 0.134015
              Train Epoch: [560/2467]	Loss: 0.160421
Train Epoch: [560/2467]	Loss: 0.162908
 Train Epoch: [560/2467]	Loss: 0.254135
     Train Epoch: [560/2467]	Loss: 0.217108
          Train Epoch: [580/2467]	Loss: 0.277747Train Epoch: [580/2467]	Loss: 0.212086

     Train Epoch: [580/2467]	Loss: 0.060264    
 Train Epoch: [580/2467]	Loss: 0.131819
     Train Epoch: [600/2467]	Loss: 0.199304
         Train Epoch: [600/2467]	Loss: 0.171991
 Train Epoch: [600/2467]	Loss: 0.172094
     Train Epoch: [600/2467]	Loss: 0.122027
             Train Epoch: [620/2467]	Loss: 0.141489 
Train Epoch: [620/2467]	Loss: 0.153023
 Train Epoch: [620/2467]	Loss: 0.039211
     Train Epoch: [620/2467]	Loss: 0.229225
          Train Epoch: [640/2467]	Loss: 0.053515    Train Epoch: [640/2467]	Loss: 0.120479

     Train Epoch: [640/2467]	Loss: 0.177310
 Train Epoch: [640/2467]	Loss: 0.198988
             Train Epoch: [660/2467]	Loss: 0.031418
 Train Epoch: [660/2467]	Loss: 0.056913     
Train Epoch: [660/2467]	Loss: 0.397451
 Train Epoch: [660/2467]	Loss: 0.187610
          Train Epoch: [680/2467]	Loss: 0.045635
Train Epoch: [680/2467]	Loss: 0.177195
     Train Epoch: [680/2467]	Loss: 0.067566
     Train Epoch: [680/2467]	Loss: 0.115222
     Train Epoch: [700/2467]	Loss: 0.120764
         Train Epoch: [700/2467]	Loss: 0.317359 
Train Epoch: [700/2467]	Loss: 0.371375
     Train Epoch: [700/2467]	Loss: 0.051758
             Train Epoch: [720/2467]	Loss: 0.021634 
Train Epoch: [720/2467]	Loss: 0.124736
     Train Epoch: [720/2467]	Loss: 0.064129
 Train Epoch: [720/2467]	Loss: 0.218826
         Train Epoch: [740/2467]	Loss: 0.339170
 Train Epoch: [740/2467]	Loss: 0.055016
     Train Epoch: [740/2467]	Loss: 0.086135
     Train Epoch: [740/2467]	Loss: 0.095137
     Train Epoch: [760/2467]	Loss: 0.396059
         Train Epoch: [760/2467]	Loss: 0.259977
 Train Epoch: [760/2467]	Loss: 0.275439
     Train Epoch: [760/2467]	Loss: 0.047747
          Train Epoch: [780/2467]	Loss: 0.295585Train Epoch: [780/2467]	Loss: 0.358470

         Train Epoch: [780/2467]	Loss: 0.154324
 Train Epoch: [780/2467]	Loss: 0.310561
         Train Epoch: [800/2467]	Loss: 0.216697
 Train Epoch: [800/2467]	Loss: 0.183468
     Train Epoch: [800/2467]	Loss: 0.197322
     Train Epoch: [800/2467]	Loss: 0.180288
     Train Epoch: [820/2467]	Loss: 0.203557
         Train Epoch: [820/2467]	Loss: 0.059062
 Train Epoch: [820/2467]	Loss: 0.414777
     Train Epoch: [820/2467]	Loss: 0.043187
         Train Epoch: [840/2467]	Loss: 0.339407
     Train Epoch: [840/2467]	Loss: 0.140235
 Train Epoch: [840/2467]	Loss: 0.142928
     Train Epoch: [840/2467]	Loss: 0.053175
     Train Epoch: [860/2467]	Loss: 0.055276
         Train Epoch: [860/2467]	Loss: 0.209591
 Train Epoch: [860/2467]	Loss: 0.053812
     Train Epoch: [860/2467]	Loss: 0.290269
              Train Epoch: [880/2467]	Loss: 0.162265Train Epoch: [880/2467]	Loss: 0.202384

 Train Epoch: [880/2467]	Loss: 0.130651
     Train Epoch: [880/2467]	Loss: 0.206852
     Train Epoch: [900/2467]	Loss: 0.092681
     Train Epoch: [900/2467]	Loss: 0.208773
     Train Epoch: [900/2467]	Loss: 0.197574
     Train Epoch: [900/2467]	Loss: 0.195428
     Train Epoch: [920/2467]	Loss: 0.466319
     Train Epoch: [920/2467]	Loss: 0.298258
     Train Epoch: [920/2467]	Loss: 0.042851
     Train Epoch: [920/2467]	Loss: 0.290221
          Train Epoch: [940/2467]	Loss: 0.047184Train Epoch: [940/2467]	Loss: 0.080085

          Train Epoch: [940/2467]	Loss: 0.184585Train Epoch: [940/2467]	Loss: 0.046121

         Train Epoch: [960/2467]	Loss: 0.061721
 Train Epoch: [960/2467]	Loss: 0.296151
         Train Epoch: [960/2467]	Loss: 0.085431
 Train Epoch: [960/2467]	Loss: 0.067974
     Train Epoch: [980/2467]	Loss: 0.217308    
 Train Epoch: [980/2467]	Loss: 0.110269
     Train Epoch: [980/2467]	Loss: 0.170961
     Train Epoch: [980/2467]	Loss: 0.060502
         Train Epoch: [1000/2467]	Loss: 0.033788
     Train Epoch: [1000/2467]	Loss: 0.262696
     Train Epoch: [1000/2467]	Loss: 0.246783
 Train Epoch: [1000/2467]	Loss: 0.032295
     Train Epoch: [1020/2467]	Loss: 0.141232
          Train Epoch: [1020/2467]	Loss: 0.067454Train Epoch: [1020/2467]	Loss: 0.395643

     Train Epoch: [1020/2467]	Loss: 0.069414
         Train Epoch: [1040/2467]	Loss: 0.250122
 Train Epoch: [1040/2467]	Loss: 0.106937
         Train Epoch: [1040/2467]	Loss: 0.108947
 Train Epoch: [1040/2467]	Loss: 0.143861
     Train Epoch: [1060/2467]	Loss: 0.206020
     Train Epoch: [1060/2467]	Loss: 0.141651
     Train Epoch: [1060/2467]	Loss: 0.425678
     Train Epoch: [1060/2467]	Loss: 0.085315
              Train Epoch: [1080/2467]	Loss: 0.123389Train Epoch: [1080/2467]	Loss: 0.194108

      Train Epoch: [1080/2467]	Loss: 0.299328
Train Epoch: [1080/2467]	Loss: 0.280891
             Train Epoch: [1100/2467]	Loss: 0.213244 
 Train Epoch: [1100/2467]	Loss: 0.263053Train Epoch: [1100/2467]	Loss: 0.246868

     Train Epoch: [1100/2467]	Loss: 0.250619
         Train Epoch: [1120/2467]	Loss: 0.268141
 Train Epoch: [1120/2467]	Loss: 0.115992
     Train Epoch: [1120/2467]	Loss: 0.145547
     Train Epoch: [1120/2467]	Loss: 0.120524
         Train Epoch: [1140/2467]	Loss: 0.299835
 Train Epoch: [1140/2467]	Loss: 0.095672
     Train Epoch: [1140/2467]	Loss: 0.421901
     Train Epoch: [1140/2467]	Loss: 0.050383
     Train Epoch: [1160/2467]	Loss: 0.241205
     Train Epoch: [1160/2467]	Loss: 0.096489
     Train Epoch: [1160/2467]	Loss: 0.105107
     Train Epoch: [1160/2467]	Loss: 0.133097
     Train Epoch: [1180/2467]	Loss: 0.055493
          Train Epoch: [1180/2467]	Loss: 0.247296Train Epoch: [1180/2467]	Loss: 0.137216

     Train Epoch: [1180/2467]	Loss: 0.152721
     Train Epoch: [1200/2467]	Loss: 0.293638
         Train Epoch: [1200/2467]	Loss: 0.119306
 Train Epoch: [1200/2467]	Loss: 0.091671
     Train Epoch: [1200/2467]	Loss: 0.239442
          Train Epoch: [1220/2467]	Loss: 0.239629    Train Epoch: [1220/2467]	Loss: 0.218150

     Train Epoch: [1220/2467]	Loss: 0.093051
 Train Epoch: [1220/2467]	Loss: 0.258591
     Train Epoch: [1240/2467]	Loss: 0.272215    
 Train Epoch: [1240/2467]	Loss: 0.285422
     Train Epoch: [1240/2467]	Loss: 0.196446
     Train Epoch: [1240/2467]	Loss: 0.141407
          Train Epoch: [1260/2467]	Loss: 0.065886
Train Epoch: [1260/2467]	Loss: 0.170978
     Train Epoch: [1260/2467]	Loss: 0.052468
     Train Epoch: [1260/2467]	Loss: 0.394646
         Train Epoch: [1280/2467]	Loss: 0.073651     
Train Epoch: [1280/2467]	Loss: 0.531335
     Train Epoch: [1280/2467]	Loss: 0.228641
 Train Epoch: [1280/2467]	Loss: 0.405485
             Train Epoch: [1300/2467]	Loss: 0.087958
 Train Epoch: [1300/2467]	Loss: 0.074453
 Train Epoch: [1300/2467]	Loss: 0.312286
     Train Epoch: [1300/2467]	Loss: 0.272403
             Train Epoch: [1320/2467]	Loss: 0.052785
     Train Epoch: [1320/2467]	Loss: 0.136188
 Train Epoch: [1320/2467]	Loss: 0.181177
 Train Epoch: [1320/2467]	Loss: 0.208538
         Train Epoch: [1340/2467]	Loss: 0.413768
 Train Epoch: [1340/2467]	Loss: 0.249397
         Train Epoch: [1340/2467]	Loss: 0.095034
 Train Epoch: [1340/2467]	Loss: 0.155862
         Train Epoch: [1360/2467]	Loss: 0.121601    
     Train Epoch: [1360/2467]	Loss: 0.248917 
Train Epoch: [1360/2467]	Loss: 0.181624
 Train Epoch: [1360/2467]	Loss: 0.062220
             Train Epoch: [1380/2467]	Loss: 0.334847
  Train Epoch: [1380/2467]	Loss: 0.223696Train Epoch: [1380/2467]	Loss: 0.318266

     Train Epoch: [1380/2467]	Loss: 0.203356
             Train Epoch: [1400/2467]	Loss: 0.137596
  Train Epoch: [1400/2467]	Loss: 0.152733Train Epoch: [1400/2467]	Loss: 0.164981
    
 Train Epoch: [1400/2467]	Loss: 0.096686
         Train Epoch: [1420/2467]	Loss: 0.389266
     Train Epoch: [1420/2467]	Loss: 0.081993
     Train Epoch: [1420/2467]	Loss: 0.139058
 Train Epoch: [1420/2467]	Loss: 0.122036
         Train Epoch: [1440/2467]	Loss: 0.019379
 Train Epoch: [1440/2467]	Loss: 0.126763
         Train Epoch: [1440/2467]	Loss: 0.072267
 Train Epoch: [1440/2467]	Loss: 0.040826
             Train Epoch: [1460/2467]	Loss: 0.196585 
Train Epoch: [1460/2467]	Loss: 0.131502
 Train Epoch: [1460/2467]	Loss: 0.041386
     Train Epoch: [1460/2467]	Loss: 0.193292
         Train Epoch: [1480/2467]	Loss: 0.257046 
Train Epoch: [1480/2467]	Loss: 0.195842
         Train Epoch: [1480/2467]	Loss: 0.013359
 Train Epoch: [1480/2467]	Loss: 0.028572
          Train Epoch: [1500/2467]	Loss: 0.014921Train Epoch: [1500/2467]	Loss: 0.087062

     Train Epoch: [1500/2467]	Loss: 0.133976
     Train Epoch: [1500/2467]	Loss: 0.121396
         Train Epoch: [1520/2467]	Loss: 0.226524
 Train Epoch: [1520/2467]	Loss: 0.191353
         Train Epoch: [1520/2467]	Loss: 0.146460
 Train Epoch: [1520/2467]	Loss: 0.285888
             Train Epoch: [1540/2467]	Loss: 0.099272 
 Train Epoch: [1540/2467]	Loss: 0.294404
Train Epoch: [1540/2467]	Loss: 0.047847
     Train Epoch: [1540/2467]	Loss: 0.197501
             Train Epoch: [1560/2467]	Loss: 0.276458
 Train Epoch: [1560/2467]	Loss: 0.378002 
Train Epoch: [1560/2467]	Loss: 0.151572
     Train Epoch: [1560/2467]	Loss: 0.452586
         Train Epoch: [1580/2467]	Loss: 0.256183
     Train Epoch: [1580/2467]	Loss: 0.217314
 Train Epoch: [1580/2467]	Loss: 0.112187
     Train Epoch: [1580/2467]	Loss: 0.133215
         Train Epoch: [1600/2467]	Loss: 0.254911
 Train Epoch: [1600/2467]	Loss: 0.212124
         Train Epoch: [1600/2467]	Loss: 0.149404
 Train Epoch: [1600/2467]	Loss: 0.059501
          Train Epoch: [1620/2467]	Loss: 0.092598Train Epoch: [1620/2467]	Loss: 0.175643

     Train Epoch: [1620/2467]	Loss: 0.245128
     Train Epoch: [1620/2467]	Loss: 0.033575
          Train Epoch: [1640/2467]	Loss: 0.222791Train Epoch: [1640/2467]	Loss: 0.144871
    
 Train Epoch: [1640/2467]	Loss: 0.081163
     Train Epoch: [1640/2467]	Loss: 0.223072
         Train Epoch: [1660/2467]	Loss: 0.198582 
    Train Epoch: [1660/2467]	Loss: 0.130922
     Train Epoch: [1660/2467]	Loss: 0.231992
 Train Epoch: [1660/2467]	Loss: 0.101826
     Train Epoch: [1680/2467]	Loss: 0.166974
          Train Epoch: [1680/2467]	Loss: 0.056172Train Epoch: [1680/2467]	Loss: 0.197550

     Train Epoch: [1680/2467]	Loss: 0.162892
         Train Epoch: [1700/2467]	Loss: 0.188236
     Train Epoch: [1700/2467]	Loss: 0.270742
     Train Epoch: [1700/2467]	Loss: 0.087135
 Train Epoch: [1700/2467]	Loss: 0.190000
          Train Epoch: [1720/2467]	Loss: 0.153386Train Epoch: [1720/2467]	Loss: 0.046839

     Train Epoch: [1720/2467]	Loss: 0.250364    
 Train Epoch: [1720/2467]	Loss: 0.155168
     Train Epoch: [1740/2467]	Loss: 0.025516
          Train Epoch: [1740/2467]	Loss: 0.084507
Train Epoch: [1740/2467]	Loss: 0.370771
     Train Epoch: [1740/2467]	Loss: 0.199902
     Train Epoch: [1760/2467]	Loss: 0.135014
     Train Epoch: [1760/2467]	Loss: 0.052536
         Train Epoch: [1760/2467]	Loss: 0.092556
 Train Epoch: [1760/2467]	Loss: 0.143515
         Train Epoch: [1780/2467]	Loss: 0.172363
 Train Epoch: [1780/2467]	Loss: 0.090673
     Train Epoch: [1780/2467]	Loss: 0.258814
     Train Epoch: [1780/2467]	Loss: 0.262370
     Train Epoch: [1800/2467]	Loss: 0.333421
          Train Epoch: [1800/2467]	Loss: 0.240806Train Epoch: [1800/2467]	Loss: 0.181092

     Train Epoch: [1800/2467]	Loss: 0.186577
         Train Epoch: [1820/2467]	Loss: 0.130558
 Train Epoch: [1820/2467]	Loss: 0.136228
     Train Epoch: [1820/2467]	Loss: 0.319207
     Train Epoch: [1820/2467]	Loss: 0.127843
         Train Epoch: [1840/2467]	Loss: 0.270979     
Train Epoch: [1840/2467]	Loss: 0.135202
 Train Epoch: [1840/2467]	Loss: 0.242825
     Train Epoch: [1840/2467]	Loss: 0.046823
         Train Epoch: [1860/2467]	Loss: 0.202285
     Train Epoch: [1860/2467]	Loss: 0.151033
     Train Epoch: [1860/2467]	Loss: 0.100259
 Train Epoch: [1860/2467]	Loss: 0.125619
         Train Epoch: [1880/2467]	Loss: 0.047625
 Train Epoch: [1880/2467]	Loss: 0.024335
     Train Epoch: [1880/2467]	Loss: 0.100697
     Train Epoch: [1880/2467]	Loss: 0.050759
             Train Epoch: [1900/2467]	Loss: 0.115539 
Train Epoch: [1900/2467]	Loss: 0.114508
     Train Epoch: [1900/2467]	Loss: 0.291425
 Train Epoch: [1900/2467]	Loss: 0.272643
     Train Epoch: [1920/2467]	Loss: 0.110156    
 Train Epoch: [1920/2467]	Loss: 0.214415
     Train Epoch: [1920/2467]	Loss: 0.176021
     Train Epoch: [1920/2467]	Loss: 0.016396
     Train Epoch: [1940/2467]	Loss: 0.173337
             Train Epoch: [1940/2467]	Loss: 0.110621 
 Train Epoch: [1940/2467]	Loss: 0.071151
Train Epoch: [1940/2467]	Loss: 0.366697
         Train Epoch: [1960/2467]	Loss: 0.154597
 Train Epoch: [1960/2467]	Loss: 0.181458
     Train Epoch: [1960/2467]	Loss: 0.104287
     Train Epoch: [1960/2467]	Loss: 0.090039
         Train Epoch: [1980/2467]	Loss: 0.246980
         Train Epoch: [1980/2467]	Loss: 0.244820
  Train Epoch: [1980/2467]	Loss: 0.251904Train Epoch: [1980/2467]	Loss: 0.103807

             Train Epoch: [2000/2467]	Loss: 0.175735
 Train Epoch: [2000/2467]	Loss: 0.035288
 Train Epoch: [2000/2467]	Loss: 0.229536    
 Train Epoch: [2000/2467]	Loss: 0.054902
         Train Epoch: [2020/2467]	Loss: 0.148460    
 Train Epoch: [2020/2467]	Loss: 0.177144
     Train Epoch: [2020/2467]	Loss: 0.211620
 Train Epoch: [2020/2467]	Loss: 0.080913
         Train Epoch: [2040/2467]	Loss: 0.115276
     Train Epoch: [2040/2467]	Loss: 0.368925    
 Train Epoch: [2040/2467]	Loss: 0.085395
 Train Epoch: [2040/2467]	Loss: 0.180559
         Train Epoch: [2060/2467]	Loss: 0.230597
     Train Epoch: [2060/2467]	Loss: 0.180436
 Train Epoch: [2060/2467]	Loss: 0.074904
     Train Epoch: [2060/2467]	Loss: 0.188618
          Train Epoch: [2080/2467]	Loss: 0.053719
Train Epoch: [2080/2467]	Loss: 0.072767
     Train Epoch: [2080/2467]	Loss: 0.260594
     Train Epoch: [2080/2467]	Loss: 0.031377
         Train Epoch: [2100/2467]	Loss: 0.493036
 Train Epoch: [2100/2467]	Loss: 0.098138    
     Train Epoch: [2100/2467]	Loss: 0.118214
 Train Epoch: [2100/2467]	Loss: 0.076113
     Train Epoch: [2120/2467]	Loss: 0.043664
          Train Epoch: [2120/2467]	Loss: 0.215868
Train Epoch: [2120/2467]	Loss: 0.090605
     Train Epoch: [2120/2467]	Loss: 0.167998
     Train Epoch: [2140/2467]	Loss: 0.188663
         Train Epoch: [2140/2467]	Loss: 0.487097
 Train Epoch: [2140/2467]	Loss: 0.255165
     Train Epoch: [2140/2467]	Loss: 0.327269
             Train Epoch: [2160/2467]	Loss: 0.093663
 Train Epoch: [2160/2467]	Loss: 0.272219
     Train Epoch: [2160/2467]	Loss: 0.049838
 Train Epoch: [2160/2467]	Loss: 0.177978
     Train Epoch: [2180/2467]	Loss: 0.146925    
     Train Epoch: [2180/2467]	Loss: 0.270145
 Train Epoch: [2180/2467]	Loss: 0.165279
     Train Epoch: [2180/2467]	Loss: 0.066820
     Train Epoch: [2200/2467]	Loss: 0.164704
             Train Epoch: [2200/2467]	Loss: 0.287784 
Train Epoch: [2200/2467]	Loss: 0.040012
 Train Epoch: [2200/2467]	Loss: 0.083923
          Train Epoch: [2220/2467]	Loss: 0.198732
Train Epoch: [2220/2467]	Loss: 0.064154
         Train Epoch: [2220/2467]	Loss: 0.095132 
Train Epoch: [2220/2467]	Loss: 0.124029
          Train Epoch: [2240/2467]	Loss: 0.130690
Train Epoch: [2240/2467]	Loss: 0.023250
     Train Epoch: [2240/2467]	Loss: 0.037144
     Train Epoch: [2240/2467]	Loss: 0.069987
     Train Epoch: [2260/2467]	Loss: 0.218625
         Train Epoch: [2260/2467]	Loss: 0.022745 
Train Epoch: [2260/2467]	Loss: 0.346865
     Train Epoch: [2260/2467]	Loss: 0.204425
     Train Epoch: [2280/2467]	Loss: 0.133080
             Train Epoch: [2280/2467]	Loss: 0.111520
 Train Epoch: [2280/2467]	Loss: 0.084850
 Train Epoch: [2280/2467]	Loss: 0.144230
         Train Epoch: [2300/2467]	Loss: 0.046730
     Train Epoch: [2300/2467]	Loss: 0.244908
 Train Epoch: [2300/2467]	Loss: 0.079116
     Train Epoch: [2300/2467]	Loss: 0.098618
         Train Epoch: [2320/2467]	Loss: 0.248902
     Train Epoch: [2320/2467]	Loss: 0.020857
 Train Epoch: [2320/2467]	Loss: 0.141801
     Train Epoch: [2320/2467]	Loss: 0.155094
     Train Epoch: [2340/2467]	Loss: 0.139191    
 Train Epoch: [2340/2467]	Loss: 0.179768
     Train Epoch: [2340/2467]	Loss: 0.450554
     Train Epoch: [2340/2467]	Loss: 0.146703
         Train Epoch: [2360/2467]	Loss: 0.148278
 Train Epoch: [2360/2467]	Loss: 0.357799
     Train Epoch: [2360/2467]	Loss: 0.139339
     Train Epoch: [2360/2467]	Loss: 0.054876
          Train Epoch: [2380/2467]	Loss: 0.439006
Train Epoch: [2380/2467]	Loss: 0.102117
     Train Epoch: [2380/2467]	Loss: 0.169710
     Train Epoch: [2380/2467]	Loss: 0.219644
              Train Epoch: [2400/2467]	Loss: 0.101472Train Epoch: [2400/2467]	Loss: 0.185691 

Train Epoch: [2400/2467]	Loss: 0.166763
     Train Epoch: [2400/2467]	Loss: 0.093623
         Train Epoch: [2420/2467]	Loss: 0.103594    
 Train Epoch: [2420/2467]	Loss: 0.122076
 Train Epoch: [2420/2467]	Loss: 0.220943
     Train Epoch: [2420/2467]	Loss: 0.334605
     Train Epoch: [2440/2467]	Loss: 0.141711    
 Train Epoch: [2440/2467]	Loss: 0.258011
     Train Epoch: [2440/2467]	Loss: 0.112137
     Train Epoch: [2440/2467]	Loss: 0.021961
         Train Epoch: [2460/2467]	Loss: 0.087002
         Train Epoch: [2460/2467]	Loss: 0.236217
  Train Epoch: [2460/2467]	Loss: 0.137345Train Epoch: [2460/2467]	Loss: 0.120463

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 26 epoch =====
     2025-05-11.09-30-59
     ===== running 26 epoch =====
     2025-05-11.09-30-59
after set grad
after prog
start loop
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 26 epoch =====
     2025-05-11.09-30-59
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 26 epoch =====
     2025-05-11.09-31-00
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
          Train Epoch: [0/2467]	Loss: 0.230394Train Epoch: [0/2467]	Loss: 0.297797

         Train Epoch: [0/2467]	Loss: 0.172987
 Train Epoch: [0/2467]	Loss: 0.089466
              Train Epoch: [20/2467]	Loss: 0.118129    
Train Epoch: [20/2467]	Loss: 0.069067
 Train Epoch: [20/2467]	Loss: 0.175928 
Train Epoch: [20/2467]	Loss: 0.308544
     Train Epoch: [40/2467]	Loss: 0.117580
     Train Epoch: [40/2467]	Loss: 0.539909
     Train Epoch: [40/2467]	Loss: 0.129964
     Train Epoch: [40/2467]	Loss: 0.100152
     Train Epoch: [60/2467]	Loss: 0.164322
         Train Epoch: [60/2467]	Loss: 0.042924
 Train Epoch: [60/2467]	Loss: 0.093923
     Train Epoch: [60/2467]	Loss: 0.249508
     Train Epoch: [80/2467]	Loss: 0.285598    
          Train Epoch: [80/2467]	Loss: 0.085370Train Epoch: [80/2467]	Loss: 0.100008

 Train Epoch: [80/2467]	Loss: 0.120041
             Train Epoch: [100/2467]	Loss: 0.165940 
Train Epoch: [100/2467]	Loss: 0.187073
 Train Epoch: [100/2467]	Loss: 0.293108
     Train Epoch: [100/2467]	Loss: 0.078641
     Train Epoch: [120/2467]	Loss: 0.062386
         Train Epoch: [120/2467]	Loss: 0.081978    
  Train Epoch: [120/2467]	Loss: 0.233822Train Epoch: [120/2467]	Loss: 0.406236

         Train Epoch: [140/2467]	Loss: 0.256024     
Train Epoch: [140/2467]	Loss: 0.037028
 Train Epoch: [140/2467]	Loss: 0.158398
     Train Epoch: [140/2467]	Loss: 0.098042
          Train Epoch: [160/2467]	Loss: 0.145524    Train Epoch: [160/2467]	Loss: 0.225408

      Train Epoch: [160/2467]	Loss: 0.142903
Train Epoch: [160/2467]	Loss: 0.149960
              Train Epoch: [180/2467]	Loss: 0.192884Train Epoch: [180/2467]	Loss: 0.160053

 Train Epoch: [180/2467]	Loss: 0.086345
     Train Epoch: [180/2467]	Loss: 0.112003
          Train Epoch: [200/2467]	Loss: 0.068220Train Epoch: [200/2467]	Loss: 0.114030

         Train Epoch: [200/2467]	Loss: 0.067867
 Train Epoch: [200/2467]	Loss: 0.101415
     Train Epoch: [220/2467]	Loss: 0.064291    
     Train Epoch: [220/2467]	Loss: 0.190397
     Train Epoch: [220/2467]	Loss: 0.143269
 Train Epoch: [220/2467]	Loss: 0.213532
     Train Epoch: [240/2467]	Loss: 0.191315
     Train Epoch: [240/2467]	Loss: 0.228145
     Train Epoch: [240/2467]	Loss: 0.036340
     Train Epoch: [240/2467]	Loss: 0.021150
     Train Epoch: [260/2467]	Loss: 0.037299    
 Train Epoch: [260/2467]	Loss: 0.050741
     Train Epoch: [260/2467]	Loss: 0.055591
     Train Epoch: [260/2467]	Loss: 0.114655
     Train Epoch: [280/2467]	Loss: 0.132331
          Train Epoch: [280/2467]	Loss: 0.101338
Train Epoch: [280/2467]	Loss: 0.222414
     Train Epoch: [280/2467]	Loss: 0.131458
     Train Epoch: [300/2467]	Loss: 0.344963
         Train Epoch: [300/2467]	Loss: 0.036235
 Train Epoch: [300/2467]	Loss: 0.088857
     Train Epoch: [300/2467]	Loss: 0.163758
         Train Epoch: [320/2467]	Loss: 0.151663 
Train Epoch: [320/2467]	Loss: 0.167654    
     Train Epoch: [320/2467]	Loss: 0.294873
 Train Epoch: [320/2467]	Loss: 0.398460
     Train Epoch: [340/2467]	Loss: 0.015568
     Train Epoch: [340/2467]	Loss: 0.048934
     Train Epoch: [340/2467]	Loss: 0.092034
     Train Epoch: [340/2467]	Loss: 0.187972
         Train Epoch: [360/2467]	Loss: 0.065082
     Train Epoch: [360/2467]	Loss: 0.211300 
Train Epoch: [360/2467]	Loss: 0.054004
     Train Epoch: [360/2467]	Loss: 0.173159
              Train Epoch: [380/2467]	Loss: 0.592444Train Epoch: [380/2467]	Loss: 0.086474

     Train Epoch: [380/2467]	Loss: 0.210795
 Train Epoch: [380/2467]	Loss: 0.086351
         Train Epoch: [400/2467]	Loss: 0.092176
 Train Epoch: [400/2467]	Loss: 0.052560
     Train Epoch: [400/2467]	Loss: 0.157635
     Train Epoch: [400/2467]	Loss: 0.343933
     Train Epoch: [420/2467]	Loss: 0.218152
         Train Epoch: [420/2467]	Loss: 0.255478
     Train Epoch: [420/2467]	Loss: 0.133089
 Train Epoch: [420/2467]	Loss: 0.129346
     Train Epoch: [440/2467]	Loss: 0.354761
     Train Epoch: [440/2467]	Loss: 0.086826    
 Train Epoch: [440/2467]	Loss: 0.050019
     Train Epoch: [440/2467]	Loss: 0.140175
     Train Epoch: [460/2467]	Loss: 0.105267
         Train Epoch: [460/2467]	Loss: 0.158196
 Train Epoch: [460/2467]	Loss: 0.132324
     Train Epoch: [460/2467]	Loss: 0.239577
         Train Epoch: [480/2467]	Loss: 0.274447
 Train Epoch: [480/2467]	Loss: 0.049828
          Train Epoch: [480/2467]	Loss: 0.150181Train Epoch: [480/2467]	Loss: 0.328779

         Train Epoch: [500/2467]	Loss: 0.089055    
  Train Epoch: [500/2467]	Loss: 0.147650Train Epoch: [500/2467]	Loss: 0.104249

     Train Epoch: [500/2467]	Loss: 0.226005
         Train Epoch: [520/2467]	Loss: 0.154675
     Train Epoch: [520/2467]	Loss: 0.250075
 Train Epoch: [520/2467]	Loss: 0.082382
     Train Epoch: [520/2467]	Loss: 0.126356
     Train Epoch: [540/2467]	Loss: 0.161463
     Train Epoch: [540/2467]	Loss: 0.292492
         Train Epoch: [540/2467]	Loss: 0.059852 
Train Epoch: [540/2467]	Loss: 0.108276
          Train Epoch: [560/2467]	Loss: 0.159022    Train Epoch: [560/2467]	Loss: 0.164781

 Train Epoch: [560/2467]	Loss: 0.210347
     Train Epoch: [560/2467]	Loss: 0.156676
         Train Epoch: [580/2467]	Loss: 0.410261
     Train Epoch: [580/2467]	Loss: 0.177830
     Train Epoch: [580/2467]	Loss: 0.055795
 Train Epoch: [580/2467]	Loss: 0.138851
     Train Epoch: [600/2467]	Loss: 0.177009
             Train Epoch: [600/2467]	Loss: 0.122458
  Train Epoch: [600/2467]	Loss: 0.226544Train Epoch: [600/2467]	Loss: 0.130507

          Train Epoch: [620/2467]	Loss: 0.161773Train Epoch: [620/2467]	Loss: 0.149799

          Train Epoch: [620/2467]	Loss: 0.028843
Train Epoch: [620/2467]	Loss: 0.231757
         Train Epoch: [640/2467]	Loss: 0.091792 
Train Epoch: [640/2467]	Loss: 0.038197    
     Train Epoch: [640/2467]	Loss: 0.180477
 Train Epoch: [640/2467]	Loss: 0.207420
         Train Epoch: [660/2467]	Loss: 0.376935
     Train Epoch: [660/2467]	Loss: 0.036281 
Train Epoch: [660/2467]	Loss: 0.060391
     Train Epoch: [660/2467]	Loss: 0.189719
         Train Epoch: [680/2467]	Loss: 0.145984 
Train Epoch: [680/2467]	Loss: 0.070787
     Train Epoch: [680/2467]	Loss: 0.070407
     Train Epoch: [680/2467]	Loss: 0.047415
     Train Epoch: [700/2467]	Loss: 0.065505
             Train Epoch: [700/2467]	Loss: 0.365713
  Train Epoch: [700/2467]	Loss: 0.375469Train Epoch: [700/2467]	Loss: 0.079647

              Train Epoch: [720/2467]	Loss: 0.198103Train Epoch: [720/2467]	Loss: 0.148691

 Train Epoch: [720/2467]	Loss: 0.078308
     Train Epoch: [720/2467]	Loss: 0.031107
          Train Epoch: [740/2467]	Loss: 0.381261Train Epoch: [740/2467]	Loss: 0.081117

     Train Epoch: [740/2467]	Loss: 0.068190
     Train Epoch: [740/2467]	Loss: 0.079380
     Train Epoch: [760/2467]	Loss: 0.445808
         Train Epoch: [760/2467]	Loss: 0.083859 
Train Epoch: [760/2467]	Loss: 0.256389
     Train Epoch: [760/2467]	Loss: 0.307204
         Train Epoch: [780/2467]	Loss: 0.483241 
Train Epoch: [780/2467]	Loss: 0.332170    
      Train Epoch: [780/2467]	Loss: 0.305495Train Epoch: [780/2467]	Loss: 0.123889

         Train Epoch: [800/2467]	Loss: 0.200059
 Train Epoch: [800/2467]	Loss: 0.203424
     Train Epoch: [800/2467]	Loss: 0.130211
     Train Epoch: [800/2467]	Loss: 0.170976
     Train Epoch: [820/2467]	Loss: 0.222913
         Train Epoch: [820/2467]	Loss: 0.426075
 Train Epoch: [820/2467]	Loss: 0.044992
     Train Epoch: [820/2467]	Loss: 0.041861
          Train Epoch: [840/2467]	Loss: 0.130026    
Train Epoch: [840/2467]	Loss: 0.322042
 Train Epoch: [840/2467]	Loss: 0.074113
     Train Epoch: [840/2467]	Loss: 0.114665
     Train Epoch: [860/2467]	Loss: 0.070785    
 Train Epoch: [860/2467]	Loss: 0.221148
          Train Epoch: [860/2467]	Loss: 0.177374Train Epoch: [860/2467]	Loss: 0.068511

     Train Epoch: [880/2467]	Loss: 0.224607
             Train Epoch: [880/2467]	Loss: 0.135208
  Train Epoch: [880/2467]	Loss: 0.139565
Train Epoch: [880/2467]	Loss: 0.228056
             Train Epoch: [900/2467]	Loss: 0.188422
  Train Epoch: [900/2467]	Loss: 0.095073Train Epoch: [900/2467]	Loss: 0.177981

     Train Epoch: [900/2467]	Loss: 0.196511
         Train Epoch: [920/2467]	Loss: 0.291434
 Train Epoch: [920/2467]	Loss: 0.047589
     Train Epoch: [920/2467]	Loss: 0.499504
     Train Epoch: [920/2467]	Loss: 0.247987
     Train Epoch: [940/2467]	Loss: 0.039335
             Train Epoch: [940/2467]	Loss: 0.173343  
Train Epoch: [940/2467]	Loss: 0.084248Train Epoch: [940/2467]	Loss: 0.052458

         Train Epoch: [960/2467]	Loss: 0.058952    
      Train Epoch: [960/2467]	Loss: 0.084844
Train Epoch: [960/2467]	Loss: 0.277594
 Train Epoch: [960/2467]	Loss: 0.078697
         Train Epoch: [980/2467]	Loss: 0.147423
     Train Epoch: [980/2467]	Loss: 0.209703
 Train Epoch: [980/2467]	Loss: 0.127470
     Train Epoch: [980/2467]	Loss: 0.076861
          Train Epoch: [1000/2467]	Loss: 0.334335Train Epoch: [1000/2467]	Loss: 0.219809    

 Train Epoch: [1000/2467]	Loss: 0.035473
     Train Epoch: [1000/2467]	Loss: 0.044164
          Train Epoch: [1020/2467]	Loss: 0.101906Train Epoch: [1020/2467]	Loss: 0.141520

         Train Epoch: [1020/2467]	Loss: 0.370630
 Train Epoch: [1020/2467]	Loss: 0.090453
             Train Epoch: [1040/2467]	Loss: 0.231929
  Train Epoch: [1040/2467]	Loss: 0.105676Train Epoch: [1040/2467]	Loss: 0.148542

     Train Epoch: [1040/2467]	Loss: 0.102622
          Train Epoch: [1060/2467]	Loss: 0.449427Train Epoch: [1060/2467]	Loss: 0.243878

         Train Epoch: [1060/2467]	Loss: 0.100530
 Train Epoch: [1060/2467]	Loss: 0.118443
          Train Epoch: [1080/2467]	Loss: 0.137531Train Epoch: [1080/2467]	Loss: 0.126358

     Train Epoch: [1080/2467]	Loss: 0.216613
     Train Epoch: [1080/2467]	Loss: 0.228664
         Train Epoch: [1100/2467]	Loss: 0.236626
 Train Epoch: [1100/2467]	Loss: 0.233356
     Train Epoch: [1100/2467]	Loss: 0.264525
     Train Epoch: [1100/2467]	Loss: 0.246039
              Train Epoch: [1120/2467]	Loss: 0.151649Train Epoch: [1120/2467]	Loss: 0.120458

 Train Epoch: [1120/2467]	Loss: 0.139432
     Train Epoch: [1120/2467]	Loss: 0.226618
     Train Epoch: [1140/2467]	Loss: 0.042885
          Train Epoch: [1140/2467]	Loss: 0.274440Train Epoch: [1140/2467]	Loss: 0.386604

     Train Epoch: [1140/2467]	Loss: 0.075473
         Train Epoch: [1160/2467]	Loss: 0.223388    
 Train Epoch: [1160/2467]	Loss: 0.071457 
Train Epoch: [1160/2467]	Loss: 0.112928
     Train Epoch: [1160/2467]	Loss: 0.120749
     Train Epoch: [1180/2467]	Loss: 0.058669
     Train Epoch: [1180/2467]	Loss: 0.198168
         Train Epoch: [1180/2467]	Loss: 0.147475
 Train Epoch: [1180/2467]	Loss: 0.125184
     Train Epoch: [1200/2467]	Loss: 0.281911
     Train Epoch: [1200/2467]	Loss: 0.113899
     Train Epoch: [1200/2467]	Loss: 0.245993    
 Train Epoch: [1200/2467]	Loss: 0.074110
         Train Epoch: [1220/2467]	Loss: 0.233839
     Train Epoch: [1220/2467]	Loss: 0.200187
     Train Epoch: [1220/2467]	Loss: 0.099082
 Train Epoch: [1220/2467]	Loss: 0.272443
     Train Epoch: [1240/2467]	Loss: 0.275215
     Train Epoch: [1240/2467]	Loss: 0.133970    
 Train Epoch: [1240/2467]	Loss: 0.309589
     Train Epoch: [1240/2467]	Loss: 0.254923
              Train Epoch: [1260/2467]	Loss: 0.121050Train Epoch: [1260/2467]	Loss: 0.054199

 Train Epoch: [1260/2467]	Loss: 0.068060
     Train Epoch: [1260/2467]	Loss: 0.393537
         Train Epoch: [1280/2467]	Loss: 0.066316 
Train Epoch: [1280/2467]	Loss: 0.216503
          Train Epoch: [1280/2467]	Loss: 0.432870
Train Epoch: [1280/2467]	Loss: 0.202747
         Train Epoch: [1300/2467]	Loss: 0.059122    
 Train Epoch: [1300/2467]	Loss: 0.085534
 Train Epoch: [1300/2467]	Loss: 0.314975    
 Train Epoch: [1300/2467]	Loss: 0.203220
          Train Epoch: [1320/2467]	Loss: 0.048711
Train Epoch: [1320/2467]	Loss: 0.128044
     Train Epoch: [1320/2467]	Loss: 0.223670
     Train Epoch: [1320/2467]	Loss: 0.251093
     Train Epoch: [1340/2467]	Loss: 0.304867    
 Train Epoch: [1340/2467]	Loss: 0.100175
     Train Epoch: [1340/2467]	Loss: 0.149027
     Train Epoch: [1340/2467]	Loss: 0.212142
              Train Epoch: [1360/2467]	Loss: 0.193826Train Epoch: [1360/2467]	Loss: 0.150199

 Train Epoch: [1360/2467]	Loss: 0.080112
     Train Epoch: [1360/2467]	Loss: 0.052560
             Train Epoch: [1380/2467]	Loss: 0.276315
 Train Epoch: [1380/2467]	Loss: 0.182461
 Train Epoch: [1380/2467]	Loss: 0.225455
     Train Epoch: [1380/2467]	Loss: 0.247473
     Train Epoch: [1400/2467]	Loss: 0.107863
         Train Epoch: [1400/2467]	Loss: 0.155764
 Train Epoch: [1400/2467]	Loss: 0.115256
     Train Epoch: [1400/2467]	Loss: 0.166804
             Train Epoch: [1420/2467]	Loss: 0.140865 
Train Epoch: [1420/2467]	Loss: 0.362165
 Train Epoch: [1420/2467]	Loss: 0.084773
     Train Epoch: [1420/2467]	Loss: 0.147371
     Train Epoch: [1440/2467]	Loss: 0.027312
     Train Epoch: [1440/2467]	Loss: 0.049137
     Train Epoch: [1440/2467]	Loss: 0.050038
     Train Epoch: [1440/2467]	Loss: 0.130858
             Train Epoch: [1460/2467]	Loss: 0.111266 
Train Epoch: [1460/2467]	Loss: 0.105578
 Train Epoch: [1460/2467]	Loss: 0.043625
     Train Epoch: [1460/2467]	Loss: 0.240351
     Train Epoch: [1480/2467]	Loss: 0.180698
              Train Epoch: [1480/2467]	Loss: 0.036895Train Epoch: [1480/2467]	Loss: 0.261464

 Train Epoch: [1480/2467]	Loss: 0.013901
          Train Epoch: [1500/2467]	Loss: 0.141783Train Epoch: [1500/2467]	Loss: 0.084803

     Train Epoch: [1500/2467]	Loss: 0.018035
     Train Epoch: [1500/2467]	Loss: 0.106658
         Train Epoch: [1520/2467]	Loss: 0.163424
 Train Epoch: [1520/2467]	Loss: 0.160855
     Train Epoch: [1520/2467]	Loss: 0.252077
     Train Epoch: [1520/2467]	Loss: 0.222111
             Train Epoch: [1540/2467]	Loss: 0.057560
  Train Epoch: [1540/2467]	Loss: 0.099997Train Epoch: [1540/2467]	Loss: 0.273183

     Train Epoch: [1540/2467]	Loss: 0.127621
         Train Epoch: [1560/2467]	Loss: 0.268767
 Train Epoch: [1560/2467]	Loss: 0.383309
     Train Epoch: [1560/2467]	Loss: 0.090745
     Train Epoch: [1560/2467]	Loss: 0.437159
         Train Epoch: [1580/2467]	Loss: 0.121322
 Train Epoch: [1580/2467]	Loss: 0.320242
          Train Epoch: [1580/2467]	Loss: 0.233905Train Epoch: [1580/2467]	Loss: 0.133320

          Train Epoch: [1600/2467]	Loss: 0.277583    Train Epoch: [1600/2467]	Loss: 0.050276

 Train Epoch: [1600/2467]	Loss: 0.261431    
 Train Epoch: [1600/2467]	Loss: 0.154258
     Train Epoch: [1620/2467]	Loss: 0.090603
         Train Epoch: [1620/2467]	Loss: 0.199987
 Train Epoch: [1620/2467]	Loss: 0.048956
     Train Epoch: [1620/2467]	Loss: 0.253909
          Train Epoch: [1640/2467]	Loss: 0.226717Train Epoch: [1640/2467]	Loss: 0.219520

         Train Epoch: [1640/2467]	Loss: 0.160736
 Train Epoch: [1640/2467]	Loss: 0.085297
     Train Epoch: [1660/2467]	Loss: 0.219266
          Train Epoch: [1660/2467]	Loss: 0.170951
Train Epoch: [1660/2467]	Loss: 0.224805    
 Train Epoch: [1660/2467]	Loss: 0.104612
     Train Epoch: [1680/2467]	Loss: 0.142086
         Train Epoch: [1680/2467]	Loss: 0.055436
 Train Epoch: [1680/2467]	Loss: 0.168134
     Train Epoch: [1680/2467]	Loss: 0.253838
     Train Epoch: [1700/2467]	Loss: 0.291053
          Train Epoch: [1700/2467]	Loss: 0.076663Train Epoch: [1700/2467]	Loss: 0.214352

     Train Epoch: [1700/2467]	Loss: 0.129613
         Train Epoch: [1720/2467]	Loss: 0.062563
 Train Epoch: [1720/2467]	Loss: 0.186210    
 Train Epoch: [1720/2467]	Loss: 0.153240
     Train Epoch: [1720/2467]	Loss: 0.241001
     Train Epoch: [1740/2467]	Loss: 0.024123
         Train Epoch: [1740/2467]	Loss: 0.064491
 Train Epoch: [1740/2467]	Loss: 0.343154
     Train Epoch: [1740/2467]	Loss: 0.192077
     Train Epoch: [1760/2467]	Loss: 0.141890
          Train Epoch: [1760/2467]	Loss: 0.095788Train Epoch: [1760/2467]	Loss: 0.062710
    
 Train Epoch: [1760/2467]	Loss: 0.139084
             Train Epoch: [1780/2467]	Loss: 0.109161 
Train Epoch: [1780/2467]	Loss: 0.324001    
 Train Epoch: [1780/2467]	Loss: 0.278753 
Train Epoch: [1780/2467]	Loss: 0.263104
     Train Epoch: [1800/2467]	Loss: 0.240411
         Train Epoch: [1800/2467]	Loss: 0.211059 
Train Epoch: [1800/2467]	Loss: 0.164610
     Train Epoch: [1800/2467]	Loss: 0.279896
     Train Epoch: [1820/2467]	Loss: 0.159834
         Train Epoch: [1820/2467]	Loss: 0.132361
 Train Epoch: [1820/2467]	Loss: 0.307036
     Train Epoch: [1820/2467]	Loss: 0.111254
             Train Epoch: [1840/2467]	Loss: 0.115696
 Train Epoch: [1840/2467]	Loss: 0.291062
     Train Epoch: [1840/2467]	Loss: 0.274640 
Train Epoch: [1840/2467]	Loss: 0.051180
          Train Epoch: [1860/2467]	Loss: 0.137778Train Epoch: [1860/2467]	Loss: 0.157245

     Train Epoch: [1860/2467]	Loss: 0.157645
     Train Epoch: [1860/2467]	Loss: 0.109536
         Train Epoch: [1880/2467]	Loss: 0.049803
 Train Epoch: [1880/2467]	Loss: 0.024719    
     Train Epoch: [1880/2467]	Loss: 0.109446
 Train Epoch: [1880/2467]	Loss: 0.063505
     Train Epoch: [1900/2467]	Loss: 0.309962    
      Train Epoch: [1900/2467]	Loss: 0.300159
Train Epoch: [1900/2467]	Loss: 0.100762    
 Train Epoch: [1900/2467]	Loss: 0.140358
         Train Epoch: [1920/2467]	Loss: 0.110573
     Train Epoch: [1920/2467]	Loss: 0.203033
 Train Epoch: [1920/2467]	Loss: 0.023439
     Train Epoch: [1920/2467]	Loss: 0.214431
     Train Epoch: [1940/2467]	Loss: 0.188445
     Train Epoch: [1940/2467]	Loss: 0.355495    
 Train Epoch: [1940/2467]	Loss: 0.064941
     Train Epoch: [1940/2467]	Loss: 0.142339
         Train Epoch: [1960/2467]	Loss: 0.177583
     Train Epoch: [1960/2467]	Loss: 0.172288
     Train Epoch: [1960/2467]	Loss: 0.098635
 Train Epoch: [1960/2467]	Loss: 0.085609
     Train Epoch: [1980/2467]	Loss: 0.249335
     Train Epoch: [1980/2467]	Loss: 0.255726
          Train Epoch: [1980/2467]	Loss: 0.103290Train Epoch: [1980/2467]	Loss: 0.193540

         Train Epoch: [2000/2467]	Loss: 0.142995
     Train Epoch: [2000/2467]	Loss: 0.023784
 Train Epoch: [2000/2467]	Loss: 0.206647
     Train Epoch: [2000/2467]	Loss: 0.057602
     Train Epoch: [2020/2467]	Loss: 0.147307
     Train Epoch: [2020/2467]	Loss: 0.066435
     Train Epoch: [2020/2467]	Loss: 0.189042
     Train Epoch: [2020/2467]	Loss: 0.160493
         Train Epoch: [2040/2467]	Loss: 0.088400
      Train Epoch: [2040/2467]	Loss: 0.173815Train Epoch: [2040/2467]	Loss: 0.367330

     Train Epoch: [2040/2467]	Loss: 0.088335
         Train Epoch: [2060/2467]	Loss: 0.079889
      Train Epoch: [2060/2467]	Loss: 0.201987Train Epoch: [2060/2467]	Loss: 0.139989

     Train Epoch: [2060/2467]	Loss: 0.174214
     Train Epoch: [2080/2467]	Loss: 0.049329
          Train Epoch: [2080/2467]	Loss: 0.242363
Train Epoch: [2080/2467]	Loss: 0.042773
     Train Epoch: [2080/2467]	Loss: 0.080503
         Train Epoch: [2100/2467]	Loss: 0.461645
 Train Epoch: [2100/2467]	Loss: 0.092784    
     Train Epoch: [2100/2467]	Loss: 0.144446
 Train Epoch: [2100/2467]	Loss: 0.108136
             Train Epoch: [2120/2467]	Loss: 0.048955
  Train Epoch: [2120/2467]	Loss: 0.099543    Train Epoch: [2120/2467]	Loss: 0.224027

 Train Epoch: [2120/2467]	Loss: 0.153266
     Train Epoch: [2140/2467]	Loss: 0.182519
     Train Epoch: [2140/2467]	Loss: 0.529547
     Train Epoch: [2140/2467]	Loss: 0.257874
     Train Epoch: [2140/2467]	Loss: 0.308304
             Train Epoch: [2160/2467]	Loss: 0.096016
 Train Epoch: [2160/2467]	Loss: 0.042297 
Train Epoch: [2160/2467]	Loss: 0.316016
     Train Epoch: [2160/2467]	Loss: 0.152581
              Train Epoch: [2180/2467]	Loss: 0.285121Train Epoch: [2180/2467]	Loss: 0.123105

 Train Epoch: [2180/2467]	Loss: 0.168744
     Train Epoch: [2180/2467]	Loss: 0.072577
             Train Epoch: [2200/2467]	Loss: 0.122406
  Train Epoch: [2200/2467]	Loss: 0.130247
Train Epoch: [2200/2467]	Loss: 0.050510
     Train Epoch: [2200/2467]	Loss: 0.088680
     Train Epoch: [2220/2467]	Loss: 0.068927    
 Train Epoch: [2220/2467]	Loss: 0.184523
     Train Epoch: [2220/2467]	Loss: 0.104851
     Train Epoch: [2220/2467]	Loss: 0.050134
         Train Epoch: [2240/2467]	Loss: 0.072611
 Train Epoch: [2240/2467]	Loss: 0.021211
         Train Epoch: [2240/2467]	Loss: 0.037438 
Train Epoch: [2240/2467]	Loss: 0.116184
     Train Epoch: [2260/2467]	Loss: 0.318270
          Train Epoch: [2260/2467]	Loss: 0.023460Train Epoch: [2260/2467]	Loss: 0.196767

     Train Epoch: [2260/2467]	Loss: 0.204759
     Train Epoch: [2280/2467]	Loss: 0.097806
         Train Epoch: [2280/2467]	Loss: 0.133396
 Train Epoch: [2280/2467]	Loss: 0.102911
     Train Epoch: [2280/2467]	Loss: 0.096544
              Train Epoch: [2300/2467]	Loss: 0.263337 Train Epoch: [2300/2467]	Loss: 0.051266
Train Epoch: [2300/2467]	Loss: 0.081363

     Train Epoch: [2300/2467]	Loss: 0.099816
         Train Epoch: [2320/2467]	Loss: 0.158963
 Train Epoch: [2320/2467]	Loss: 0.158923    
 Train Epoch: [2320/2467]	Loss: 0.017773
     Train Epoch: [2320/2467]	Loss: 0.187223
     Train Epoch: [2340/2467]	Loss: 0.136152
         Train Epoch: [2340/2467]	Loss: 0.203928
 Train Epoch: [2340/2467]	Loss: 0.498919
     Train Epoch: [2340/2467]	Loss: 0.119903
     Train Epoch: [2360/2467]	Loss: 0.064379
         Train Epoch: [2360/2467]	Loss: 0.335283
 Train Epoch: [2360/2467]	Loss: 0.130482
     Train Epoch: [2360/2467]	Loss: 0.154984
     Train Epoch: [2380/2467]	Loss: 0.258513
         Train Epoch: [2380/2467]	Loss: 0.139482
 Train Epoch: [2380/2467]	Loss: 0.412883
     Train Epoch: [2380/2467]	Loss: 0.151468
         Train Epoch: [2400/2467]	Loss: 0.161570
     Train Epoch: [2400/2467]	Loss: 0.097881
 Train Epoch: [2400/2467]	Loss: 0.140492
     Train Epoch: [2400/2467]	Loss: 0.111272
     Train Epoch: [2420/2467]	Loss: 0.262429
          Train Epoch: [2420/2467]	Loss: 0.108955
Train Epoch: [2420/2467]	Loss: 0.049073
     Train Epoch: [2420/2467]	Loss: 0.313785
     Train Epoch: [2440/2467]	Loss: 0.138751    
 Train Epoch: [2440/2467]	Loss: 0.245736
     Train Epoch: [2440/2467]	Loss: 0.033293
     Train Epoch: [2440/2467]	Loss: 0.134111
                 Train Epoch: [2460/2467]	Loss: 0.130096  
Train Epoch: [2460/2467]	Loss: 0.081237Train Epoch: [2460/2467]	Loss: 0.121026

 Train Epoch: [2460/2467]	Loss: 0.304302
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 27 epoch =====
     2025-05-11.09-52-49
after set grad
after prog
start loop
     ===== running 27 epoch =====
     2025-05-11.09-52-49
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
     ===== running 27 epoch =====
     2025-05-11.09-52-49
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 27 epoch =====
     2025-05-11.09-52-49
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.102139
         Train Epoch: [0/2467]	Loss: 0.257722 
Train Epoch: [0/2467]	Loss: 0.170827
     Train Epoch: [0/2467]	Loss: 0.327270
     Train Epoch: [20/2467]	Loss: 0.076015
     Train Epoch: [20/2467]	Loss: 0.339324
     Train Epoch: [20/2467]	Loss: 0.179031
     Train Epoch: [20/2467]	Loss: 0.130549
     Train Epoch: [40/2467]	Loss: 0.116091
     Train Epoch: [40/2467]	Loss: 0.145618
          Train Epoch: [40/2467]	Loss: 0.469523
Train Epoch: [40/2467]	Loss: 0.134892
         Train Epoch: [60/2467]	Loss: 0.161768    
 Train Epoch: [60/2467]	Loss: 0.057842
     Train Epoch: [60/2467]	Loss: 0.198094
 Train Epoch: [60/2467]	Loss: 0.129462
          Train Epoch: [80/2467]	Loss: 0.070683    Train Epoch: [80/2467]	Loss: 0.280286

     Train Epoch: [80/2467]	Loss: 0.106819
 Train Epoch: [80/2467]	Loss: 0.111175
             Train Epoch: [100/2467]	Loss: 0.174217
      Train Epoch: [100/2467]	Loss: 0.116200Train Epoch: [100/2467]	Loss: 0.073258

 Train Epoch: [100/2467]	Loss: 0.216530
     Train Epoch: [120/2467]	Loss: 0.067230
         Train Epoch: [120/2467]	Loss: 0.083052
 Train Epoch: [120/2467]	Loss: 0.373214    
 Train Epoch: [120/2467]	Loss: 0.233769
          Train Epoch: [140/2467]	Loss: 0.252726
Train Epoch: [140/2467]	Loss: 0.039560
     Train Epoch: [140/2467]	Loss: 0.053996
     Train Epoch: [140/2467]	Loss: 0.111918
         Train Epoch: [160/2467]	Loss: 0.135782
         Train Epoch: [160/2467]	Loss: 0.204261
 Train Epoch: [160/2467]	Loss: 0.100791 
Train Epoch: [160/2467]	Loss: 0.135149
             Train Epoch: [180/2467]	Loss: 0.194674
 Train Epoch: [180/2467]	Loss: 0.107199 
Train Epoch: [180/2467]	Loss: 0.099670
     Train Epoch: [180/2467]	Loss: 0.119652
             Train Epoch: [200/2467]	Loss: 0.070110
 Train Epoch: [200/2467]	Loss: 0.114348
 Train Epoch: [200/2467]	Loss: 0.074911
     Train Epoch: [200/2467]	Loss: 0.097025
     Train Epoch: [220/2467]	Loss: 0.066618    
 Train Epoch: [220/2467]	Loss: 0.156502
     Train Epoch: [220/2467]	Loss: 0.214469
     Train Epoch: [220/2467]	Loss: 0.141883
             Train Epoch: [240/2467]	Loss: 0.170193
 Train Epoch: [240/2467]	Loss: 0.247038
 Train Epoch: [240/2467]	Loss: 0.020587
     Train Epoch: [240/2467]	Loss: 0.035528
         Train Epoch: [260/2467]	Loss: 0.039193
 Train Epoch: [260/2467]	Loss: 0.050117
     Train Epoch: [260/2467]	Loss: 0.062368
     Train Epoch: [260/2467]	Loss: 0.106596
     Train Epoch: [280/2467]	Loss: 0.123117
         Train Epoch: [280/2467]	Loss: 0.098932
 Train Epoch: [280/2467]	Loss: 0.263021
     Train Epoch: [280/2467]	Loss: 0.112023
              Train Epoch: [300/2467]	Loss: 0.345607Train Epoch: [300/2467]	Loss: 0.163747

     Train Epoch: [300/2467]	Loss: 0.033893
 Train Epoch: [300/2467]	Loss: 0.071145
          Train Epoch: [320/2467]	Loss: 0.171310Train Epoch: [320/2467]	Loss: 0.159579
    
     Train Epoch: [320/2467]	Loss: 0.276705
 Train Epoch: [320/2467]	Loss: 0.416056
         Train Epoch: [340/2467]	Loss: 0.086403
     Train Epoch: [340/2467]	Loss: 0.018760
     Train Epoch: [340/2467]	Loss: 0.055554
 Train Epoch: [340/2467]	Loss: 0.202745
     Train Epoch: [360/2467]	Loss: 0.206911    
     Train Epoch: [360/2467]	Loss: 0.068140 
Train Epoch: [360/2467]	Loss: 0.070646
     Train Epoch: [360/2467]	Loss: 0.171884
              Train Epoch: [380/2467]	Loss: 0.185357
Train Epoch: [380/2467]	Loss: 0.353037
 Train Epoch: [380/2467]	Loss: 0.089115
     Train Epoch: [380/2467]	Loss: 0.128557
          Train Epoch: [400/2467]	Loss: 0.032356Train Epoch: [400/2467]	Loss: 0.072044

     Train Epoch: [400/2467]	Loss: 0.161338
     Train Epoch: [400/2467]	Loss: 0.165042
     Train Epoch: [420/2467]	Loss: 0.217617
     Train Epoch: [420/2467]	Loss: 0.273631    
 Train Epoch: [420/2467]	Loss: 0.130615
     Train Epoch: [420/2467]	Loss: 0.113388
     Train Epoch: [440/2467]	Loss: 0.142968
          Train Epoch: [440/2467]	Loss: 0.372308Train Epoch: [440/2467]	Loss: 0.050469

     Train Epoch: [440/2467]	Loss: 0.075686
          Train Epoch: [460/2467]	Loss: 0.084452Train Epoch: [460/2467]	Loss: 0.161737

     Train Epoch: [460/2467]	Loss: 0.130962
     Train Epoch: [460/2467]	Loss: 0.238166
         Train Epoch: [480/2467]	Loss: 0.308146
 Train Epoch: [480/2467]	Loss: 0.120527
     Train Epoch: [480/2467]	Loss: 0.051756
     Train Epoch: [480/2467]	Loss: 0.318738
     Train Epoch: [500/2467]	Loss: 0.073834
     Train Epoch: [500/2467]	Loss: 0.110020
     Train Epoch: [500/2467]	Loss: 0.093092
     Train Epoch: [500/2467]	Loss: 0.210758
         Train Epoch: [520/2467]	Loss: 0.180281
 Train Epoch: [520/2467]	Loss: 0.272671
     Train Epoch: [520/2467]	Loss: 0.128357
     Train Epoch: [520/2467]	Loss: 0.077628
         Train Epoch: [540/2467]	Loss: 0.148409
 Train Epoch: [540/2467]	Loss: 0.060309
     Train Epoch: [540/2467]	Loss: 0.250241
     Train Epoch: [540/2467]	Loss: 0.098046
         Train Epoch: [560/2467]	Loss: 0.128002
     Train Epoch: [560/2467]	Loss: 0.171200    
 Train Epoch: [560/2467]	Loss: 0.184392 
Train Epoch: [560/2467]	Loss: 0.135598
     Train Epoch: [580/2467]	Loss: 0.263778
         Train Epoch: [580/2467]	Loss: 0.137750
 Train Epoch: [580/2467]	Loss: 0.131378
     Train Epoch: [580/2467]	Loss: 0.064773
          Train Epoch: [600/2467]	Loss: 0.181691
Train Epoch: [600/2467]	Loss: 0.145745
     Train Epoch: [600/2467]	Loss: 0.131823
     Train Epoch: [600/2467]	Loss: 0.186440
     Train Epoch: [620/2467]	Loss: 0.134257
         Train Epoch: [620/2467]	Loss: 0.027435
 Train Epoch: [620/2467]	Loss: 0.122665
     Train Epoch: [620/2467]	Loss: 0.178505
         Train Epoch: [640/2467]	Loss: 0.050600
 Train Epoch: [640/2467]	Loss: 0.101005
         Train Epoch: [640/2467]	Loss: 0.147603
 Train Epoch: [640/2467]	Loss: 0.175421
     Train Epoch: [660/2467]	Loss: 0.414071
     Train Epoch: [660/2467]	Loss: 0.175242
     Train Epoch: [660/2467]	Loss: 0.042953
     Train Epoch: [660/2467]	Loss: 0.024667
     Train Epoch: [680/2467]	Loss: 0.115516
          Train Epoch: [680/2467]	Loss: 0.034335Train Epoch: [680/2467]	Loss: 0.072631

     Train Epoch: [680/2467]	Loss: 0.107419
     Train Epoch: [700/2467]	Loss: 0.062330
         Train Epoch: [700/2467]	Loss: 0.348845 
Train Epoch: [700/2467]	Loss: 0.337077
     Train Epoch: [700/2467]	Loss: 0.036301
     Train Epoch: [720/2467]	Loss: 0.017064
         Train Epoch: [720/2467]	Loss: 0.075519
 Train Epoch: [720/2467]	Loss: 0.118152
     Train Epoch: [720/2467]	Loss: 0.201586
     Train Epoch: [740/2467]	Loss: 0.061361
             Train Epoch: [740/2467]	Loss: 0.070711
 Train Epoch: [740/2467]	Loss: 0.130597 
Train Epoch: [740/2467]	Loss: 0.296582
     Train Epoch: [760/2467]	Loss: 0.406217    
     Train Epoch: [760/2467]	Loss: 0.055764
     Train Epoch: [760/2467]	Loss: 0.287191
 Train Epoch: [760/2467]	Loss: 0.279548
     Train Epoch: [780/2467]	Loss: 0.373067
     Train Epoch: [780/2467]	Loss: 0.327941
     Train Epoch: [780/2467]	Loss: 0.132695
     Train Epoch: [780/2467]	Loss: 0.296435
         Train Epoch: [800/2467]	Loss: 0.156342    
  Train Epoch: [800/2467]	Loss: 0.167835Train Epoch: [800/2467]	Loss: 0.182195

     Train Epoch: [800/2467]	Loss: 0.107883
              Train Epoch: [820/2467]	Loss: 0.205263Train Epoch: [820/2467]	Loss: 0.040099

 Train Epoch: [820/2467]	Loss: 0.447354
     Train Epoch: [820/2467]	Loss: 0.057264
     Train Epoch: [840/2467]	Loss: 0.105290
     Train Epoch: [840/2467]	Loss: 0.129382
         Train Epoch: [840/2467]	Loss: 0.339475
 Train Epoch: [840/2467]	Loss: 0.059643
              Train Epoch: [860/2467]	Loss: 0.227477Train Epoch: [860/2467]	Loss: 0.128680

 Train Epoch: [860/2467]	Loss: 0.179514
     Train Epoch: [860/2467]	Loss: 0.075683
         Train Epoch: [880/2467]	Loss: 0.149619
 Train Epoch: [880/2467]	Loss: 0.241542
     Train Epoch: [880/2467]	Loss: 0.135874
     Train Epoch: [880/2467]	Loss: 0.235922
          Train Epoch: [900/2467]	Loss: 0.185396Train Epoch: [900/2467]	Loss: 0.087737

     Train Epoch: [900/2467]	Loss: 0.179679
     Train Epoch: [900/2467]	Loss: 0.172806
         Train Epoch: [920/2467]	Loss: 0.275951
 Train Epoch: [920/2467]	Loss: 0.282925
     Train Epoch: [920/2467]	Loss: 0.331477
     Train Epoch: [920/2467]	Loss: 0.052894
     Train Epoch: [940/2467]	Loss: 0.086265
         Train Epoch: [940/2467]	Loss: 0.049754
 Train Epoch: [940/2467]	Loss: 0.183911
     Train Epoch: [940/2467]	Loss: 0.039132
         Train Epoch: [960/2467]	Loss: 0.077374
 Train Epoch: [960/2467]	Loss: 0.068297
         Train Epoch: [960/2467]	Loss: 0.282090
 Train Epoch: [960/2467]	Loss: 0.079053
     Train Epoch: [980/2467]	Loss: 0.102516
     Train Epoch: [980/2467]	Loss: 0.063900    
 Train Epoch: [980/2467]	Loss: 0.233653
     Train Epoch: [980/2467]	Loss: 0.111815
             Train Epoch: [1000/2467]	Loss: 0.207870  
Train Epoch: [1000/2467]	Loss: 0.055568Train Epoch: [1000/2467]	Loss: 0.263231

     Train Epoch: [1000/2467]	Loss: 0.034697
              Train Epoch: [1020/2467]	Loss: 0.049488Train Epoch: [1020/2467]	Loss: 0.196147

 Train Epoch: [1020/2467]	Loss: 0.358545
     Train Epoch: [1020/2467]	Loss: 0.074420
     Train Epoch: [1040/2467]	Loss: 0.241120
         Train Epoch: [1040/2467]	Loss: 0.094039
     Train Epoch: [1040/2467]	Loss: 0.109151
 Train Epoch: [1040/2467]	Loss: 0.173920
          Train Epoch: [1060/2467]	Loss: 0.182610
Train Epoch: [1060/2467]	Loss: 0.408287
         Train Epoch: [1060/2467]	Loss: 0.091333
 Train Epoch: [1060/2467]	Loss: 0.108824
     Train Epoch: [1080/2467]	Loss: 0.130926
     Train Epoch: [1080/2467]	Loss: 0.214481
         Train Epoch: [1080/2467]	Loss: 0.122060
 Train Epoch: [1080/2467]	Loss: 0.202737
         Train Epoch: [1100/2467]	Loss: 0.233801
 Train Epoch: [1100/2467]	Loss: 0.200031
         Train Epoch: [1100/2467]	Loss: 0.240206
 Train Epoch: [1100/2467]	Loss: 0.252473
     Train Epoch: [1120/2467]	Loss: 0.150416
         Train Epoch: [1120/2467]	Loss: 0.137647
 Train Epoch: [1120/2467]	Loss: 0.131675
     Train Epoch: [1120/2467]	Loss: 0.280193
     Train Epoch: [1140/2467]	Loss: 0.255747
               Train Epoch: [1140/2467]	Loss: 0.057739Train Epoch: [1140/2467]	Loss: 0.084691Train Epoch: [1140/2467]	Loss: 0.380270


     Train Epoch: [1160/2467]	Loss: 0.229897
         Train Epoch: [1160/2467]	Loss: 0.087092    
 Train Epoch: [1160/2467]	Loss: 0.175605
 Train Epoch: [1160/2467]	Loss: 0.116835
     Train Epoch: [1180/2467]	Loss: 0.062105
     Train Epoch: [1180/2467]	Loss: 0.134912
     Train Epoch: [1180/2467]	Loss: 0.137554
     Train Epoch: [1180/2467]	Loss: 0.177462
         Train Epoch: [1200/2467]	Loss: 0.302224
     Train Epoch: [1200/2467]	Loss: 0.074482
 Train Epoch: [1200/2467]	Loss: 0.118442
     Train Epoch: [1200/2467]	Loss: 0.215877
         Train Epoch: [1220/2467]	Loss: 0.240723
 Train Epoch: [1220/2467]	Loss: 0.197015
     Train Epoch: [1220/2467]	Loss: 0.083935
     Train Epoch: [1220/2467]	Loss: 0.265947
         Train Epoch: [1240/2467]	Loss: 0.280361
 Train Epoch: [1240/2467]	Loss: 0.119822
     Train Epoch: [1240/2467]	Loss: 0.275177
     Train Epoch: [1240/2467]	Loss: 0.220023
              Train Epoch: [1260/2467]	Loss: 0.126114Train Epoch: [1260/2467]	Loss: 0.063114

 Train Epoch: [1260/2467]	Loss: 0.085135
     Train Epoch: [1260/2467]	Loss: 0.408517
             Train Epoch: [1280/2467]	Loss: 0.209570
 Train Epoch: [1280/2467]	Loss: 0.074542    
 Train Epoch: [1280/2467]	Loss: 0.414037
 Train Epoch: [1280/2467]	Loss: 0.175088
     Train Epoch: [1300/2467]	Loss: 0.048445
     Train Epoch: [1300/2467]	Loss: 0.286801
         Train Epoch: [1300/2467]	Loss: 0.190434
 Train Epoch: [1300/2467]	Loss: 0.078217
         Train Epoch: [1320/2467]	Loss: 0.045691
     Train Epoch: [1320/2467]	Loss: 0.128907
 Train Epoch: [1320/2467]	Loss: 0.184760
     Train Epoch: [1320/2467]	Loss: 0.191634
             Train Epoch: [1340/2467]	Loss: 0.242005 
 Train Epoch: [1340/2467]	Loss: 0.119521Train Epoch: [1340/2467]	Loss: 0.304114

     Train Epoch: [1340/2467]	Loss: 0.152146
         Train Epoch: [1360/2467]	Loss: 0.108666
 Train Epoch: [1360/2467]	Loss: 0.072352
     Train Epoch: [1360/2467]	Loss: 0.112859
     Train Epoch: [1360/2467]	Loss: 0.045672
     Train Epoch: [1380/2467]	Loss: 0.248903
         Train Epoch: [1380/2467]	Loss: 0.312464 
Train Epoch: [1380/2467]	Loss: 0.222838
     Train Epoch: [1380/2467]	Loss: 0.185680
                 Train Epoch: [1400/2467]	Loss: 0.090191
  Train Epoch: [1400/2467]	Loss: 0.155242 Train Epoch: [1400/2467]	Loss: 0.094178
Train Epoch: [1400/2467]	Loss: 0.175162

              Train Epoch: [1420/2467]	Loss: 0.087036Train Epoch: [1420/2467]	Loss: 0.365800

 Train Epoch: [1420/2467]	Loss: 0.143237
     Train Epoch: [1420/2467]	Loss: 0.127812
     Train Epoch: [1440/2467]	Loss: 0.043071
         Train Epoch: [1440/2467]	Loss: 0.040133
 Train Epoch: [1440/2467]	Loss: 0.099643
     Train Epoch: [1440/2467]	Loss: 0.029140
              Train Epoch: [1460/2467]	Loss: 0.041216 
Train Epoch: [1460/2467]	Loss: 0.097391Train Epoch: [1460/2467]	Loss: 0.111537

     Train Epoch: [1460/2467]	Loss: 0.227531
     Train Epoch: [1480/2467]	Loss: 0.181668
         Train Epoch: [1480/2467]	Loss: 0.228061
     Train Epoch: [1480/2467]	Loss: 0.014041
 Train Epoch: [1480/2467]	Loss: 0.023271
     Train Epoch: [1500/2467]	Loss: 0.138504    
      Train Epoch: [1500/2467]	Loss: 0.089346Train Epoch: [1500/2467]	Loss: 0.014938

     Train Epoch: [1500/2467]	Loss: 0.108788
     Train Epoch: [1520/2467]	Loss: 0.146764    
 Train Epoch: [1520/2467]	Loss: 0.166985
          Train Epoch: [1520/2467]	Loss: 0.269519
Train Epoch: [1520/2467]	Loss: 0.259479
     Train Epoch: [1540/2467]	Loss: 0.043818
     Train Epoch: [1540/2467]	Loss: 0.270131
     Train Epoch: [1540/2467]	Loss: 0.094988
     Train Epoch: [1540/2467]	Loss: 0.127586
         Train Epoch: [1560/2467]	Loss: 0.248173    
 Train Epoch: [1560/2467]	Loss: 0.126760
 Train Epoch: [1560/2467]	Loss: 0.324837
     Train Epoch: [1560/2467]	Loss: 0.488695
     Train Epoch: [1580/2467]	Loss: 0.343394
     Train Epoch: [1580/2467]	Loss: 0.206885
     Train Epoch: [1580/2467]	Loss: 0.173659
     Train Epoch: [1580/2467]	Loss: 0.120368
          Train Epoch: [1600/2467]	Loss: 0.273414Train Epoch: [1600/2467]	Loss: 0.057888

         Train Epoch: [1600/2467]	Loss: 0.327592
 Train Epoch: [1600/2467]	Loss: 0.131090
     Train Epoch: [1620/2467]	Loss: 0.104268
     Train Epoch: [1620/2467]	Loss: 0.177075
     Train Epoch: [1620/2467]	Loss: 0.239679
     Train Epoch: [1620/2467]	Loss: 0.040536
     Train Epoch: [1640/2467]	Loss: 0.214455
          Train Epoch: [1640/2467]	Loss: 0.221886Train Epoch: [1640/2467]	Loss: 0.140792

     Train Epoch: [1640/2467]	Loss: 0.103989
     Train Epoch: [1660/2467]	Loss: 0.195694
         Train Epoch: [1660/2467]	Loss: 0.247648
 Train Epoch: [1660/2467]	Loss: 0.129563
     Train Epoch: [1660/2467]	Loss: 0.060018
     Train Epoch: [1680/2467]	Loss: 0.152237
     Train Epoch: [1680/2467]	Loss: 0.189112
     Train Epoch: [1680/2467]	Loss: 0.052204
     Train Epoch: [1680/2467]	Loss: 0.158546
                 Train Epoch: [1700/2467]	Loss: 0.093515
  Train Epoch: [1700/2467]	Loss: 0.327554 
Train Epoch: [1700/2467]	Loss: 0.141073Train Epoch: [1700/2467]	Loss: 0.143606

         Train Epoch: [1720/2467]	Loss: 0.052282
     Train Epoch: [1720/2467]	Loss: 0.112720
 Train Epoch: [1720/2467]	Loss: 0.262617
     Train Epoch: [1720/2467]	Loss: 0.184430
     Train Epoch: [1740/2467]	Loss: 0.023335
         Train Epoch: [1740/2467]	Loss: 0.324385
 Train Epoch: [1740/2467]	Loss: 0.051744
     Train Epoch: [1740/2467]	Loss: 0.185270
             Train Epoch: [1760/2467]	Loss: 0.056820
 Train Epoch: [1760/2467]	Loss: 0.099254 
Train Epoch: [1760/2467]	Loss: 0.121891
     Train Epoch: [1760/2467]	Loss: 0.125350
     Train Epoch: [1780/2467]	Loss: 0.228140
     Train Epoch: [1780/2467]	Loss: 0.073337
         Train Epoch: [1780/2467]	Loss: 0.272710
 Train Epoch: [1780/2467]	Loss: 0.226858
             Train Epoch: [1800/2467]	Loss: 0.237112    
  Train Epoch: [1800/2467]	Loss: 0.171906Train Epoch: [1800/2467]	Loss: 0.176720

 Train Epoch: [1800/2467]	Loss: 0.280912
     Train Epoch: [1820/2467]	Loss: 0.145984    
 Train Epoch: [1820/2467]	Loss: 0.203678
     Train Epoch: [1820/2467]	Loss: 0.294742
     Train Epoch: [1820/2467]	Loss: 0.151519
         Train Epoch: [1840/2467]	Loss: 0.267328
 Train Epoch: [1840/2467]	Loss: 0.111794
     Train Epoch: [1840/2467]	Loss: 0.299868
     Train Epoch: [1840/2467]	Loss: 0.055840
         Train Epoch: [1860/2467]	Loss: 0.162603     
Train Epoch: [1860/2467]	Loss: 0.125753
 Train Epoch: [1860/2467]	Loss: 0.117585
     Train Epoch: [1860/2467]	Loss: 0.098618
          Train Epoch: [1880/2467]	Loss: 0.025694
Train Epoch: [1880/2467]	Loss: 0.046380
     Train Epoch: [1880/2467]	Loss: 0.100347
     Train Epoch: [1880/2467]	Loss: 0.057042
          Train Epoch: [1900/2467]	Loss: 0.099903Train Epoch: [1900/2467]	Loss: 0.281194

     Train Epoch: [1900/2467]	Loss: 0.272474
     Train Epoch: [1900/2467]	Loss: 0.121296
             Train Epoch: [1920/2467]	Loss: 0.016193
  Train Epoch: [1920/2467]	Loss: 0.212322Train Epoch: [1920/2467]	Loss: 0.117467

     Train Epoch: [1920/2467]	Loss: 0.164488
         Train Epoch: [1940/2467]	Loss: 0.392799    
 Train Epoch: [1940/2467]	Loss: 0.065311 
Train Epoch: [1940/2467]	Loss: 0.115787
     Train Epoch: [1940/2467]	Loss: 0.150502
         Train Epoch: [1960/2467]	Loss: 0.132651 
Train Epoch: [1960/2467]	Loss: 0.199707
     Train Epoch: [1960/2467]	Loss: 0.086006
     Train Epoch: [1960/2467]	Loss: 0.094920
     Train Epoch: [1980/2467]	Loss: 0.231383
         Train Epoch: [1980/2467]	Loss: 0.229581 
Train Epoch: [1980/2467]	Loss: 0.206678
     Train Epoch: [1980/2467]	Loss: 0.100909
         Train Epoch: [2000/2467]	Loss: 0.176148 
Train Epoch: [2000/2467]	Loss: 0.054035
         Train Epoch: [2000/2467]	Loss: 0.217983
 Train Epoch: [2000/2467]	Loss: 0.020416
             Train Epoch: [2020/2467]	Loss: 0.142578
 Train Epoch: [2020/2467]	Loss: 0.131845
 Train Epoch: [2020/2467]	Loss: 0.158609
     Train Epoch: [2020/2467]	Loss: 0.053218
         Train Epoch: [2040/2467]	Loss: 0.370165
 Train Epoch: [2040/2467]	Loss: 0.098683
         Train Epoch: [2040/2467]	Loss: 0.097161
 Train Epoch: [2040/2467]	Loss: 0.193716
     Train Epoch: [2060/2467]	Loss: 0.232588
         Train Epoch: [2060/2467]	Loss: 0.093290
 Train Epoch: [2060/2467]	Loss: 0.106836
     Train Epoch: [2060/2467]	Loss: 0.196138
     Train Epoch: [2080/2467]	Loss: 0.079651
     Train Epoch: [2080/2467]	Loss: 0.030626
     Train Epoch: [2080/2467]	Loss: 0.221294
     Train Epoch: [2080/2467]	Loss: 0.042683
         Train Epoch: [2100/2467]	Loss: 0.091235
 Train Epoch: [2100/2467]	Loss: 0.446009
     Train Epoch: [2100/2467]	Loss: 0.069806
     Train Epoch: [2100/2467]	Loss: 0.119739
     Train Epoch: [2120/2467]	Loss: 0.039608
         Train Epoch: [2120/2467]	Loss: 0.209830
 Train Epoch: [2120/2467]	Loss: 0.172512
     Train Epoch: [2120/2467]	Loss: 0.085569
         Train Epoch: [2140/2467]	Loss: 0.179623
     Train Epoch: [2140/2467]	Loss: 0.279653
 Train Epoch: [2140/2467]	Loss: 0.494711    
 Train Epoch: [2140/2467]	Loss: 0.341566
              Train Epoch: [2160/2467]	Loss: 0.133957Train Epoch: [2160/2467]	Loss: 0.039214

 Train Epoch: [2160/2467]	Loss: 0.309523    
 Train Epoch: [2160/2467]	Loss: 0.123645
     Train Epoch: [2180/2467]	Loss: 0.119302
     Train Epoch: [2180/2467]	Loss: 0.066662    
 Train Epoch: [2180/2467]	Loss: 0.166258
     Train Epoch: [2180/2467]	Loss: 0.277468
     Train Epoch: [2200/2467]	Loss: 0.041704
             Train Epoch: [2200/2467]	Loss: 0.113222
  Train Epoch: [2200/2467]	Loss: 0.154284
Train Epoch: [2200/2467]	Loss: 0.103849
     Train Epoch: [2220/2467]	Loss: 0.088144
         Train Epoch: [2220/2467]	Loss: 0.177245
     Train Epoch: [2220/2467]	Loss: 0.126353
 Train Epoch: [2220/2467]	Loss: 0.054053
         Train Epoch: [2240/2467]	Loss: 0.028714
 Train Epoch: [2240/2467]	Loss: 0.080546    
 Train Epoch: [2240/2467]	Loss: 0.110182    
 Train Epoch: [2240/2467]	Loss: 0.037080
     Train Epoch: [2260/2467]	Loss: 0.302498
         Train Epoch: [2260/2467]	Loss: 0.027296
 Train Epoch: [2260/2467]	Loss: 0.193076
     Train Epoch: [2260/2467]	Loss: 0.207767
     Train Epoch: [2280/2467]	Loss: 0.183930
              Train Epoch: [2280/2467]	Loss: 0.065992Train Epoch: [2280/2467]	Loss: 0.145110
 
Train Epoch: [2280/2467]	Loss: 0.170859
     Train Epoch: [2300/2467]	Loss: 0.049889
         Train Epoch: [2300/2467]	Loss: 0.061971
 Train Epoch: [2300/2467]	Loss: 0.245516
     Train Epoch: [2300/2467]	Loss: 0.092905
     Train Epoch: [2320/2467]	Loss: 0.175745
             Train Epoch: [2320/2467]	Loss: 0.019215
  Train Epoch: [2320/2467]	Loss: 0.160151Train Epoch: [2320/2467]	Loss: 0.145999

         Train Epoch: [2340/2467]	Loss: 0.132332
 Train Epoch: [2340/2467]	Loss: 0.177357
     Train Epoch: [2340/2467]	Loss: 0.104582
     Train Epoch: [2340/2467]	Loss: 0.476407
     Train Epoch: [2360/2467]	Loss: 0.067489
         Train Epoch: [2360/2467]	Loss: 0.339331
 Train Epoch: [2360/2467]	Loss: 0.161573
     Train Epoch: [2360/2467]	Loss: 0.157332
         Train Epoch: [2380/2467]	Loss: 0.416657 
Train Epoch: [2380/2467]	Loss: 0.205959
     Train Epoch: [2380/2467]	Loss: 0.189264
     Train Epoch: [2380/2467]	Loss: 0.081905
     Train Epoch: [2400/2467]	Loss: 0.184344
     Train Epoch: [2400/2467]	Loss: 0.095833
     Train Epoch: [2400/2467]	Loss: 0.092146
     Train Epoch: [2400/2467]	Loss: 0.176833
     Train Epoch: [2420/2467]	Loss: 0.071003    
 Train Epoch: [2420/2467]	Loss: 0.114826
     Train Epoch: [2420/2467]	Loss: 0.230417
     Train Epoch: [2420/2467]	Loss: 0.317613
         Train Epoch: [2440/2467]	Loss: 0.017151 
Train Epoch: [2440/2467]	Loss: 0.139033
     Train Epoch: [2440/2467]	Loss: 0.250908
     Train Epoch: [2440/2467]	Loss: 0.105213
     Train Epoch: [2460/2467]	Loss: 0.141231
         Train Epoch: [2460/2467]	Loss: 0.083000 
Train Epoch: [2460/2467]	Loss: 0.142835
     Train Epoch: [2460/2467]	Loss: 0.241632
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 28 epoch =====
     2025-05-11.10-14-38
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 28 epoch =====
     2025-05-11.10-14-38
     ===== running 28 epoch =====
     2025-05-11.10-14-39
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 28 epoch =====
     2025-05-11.10-14-39
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.299556
         Train Epoch: [0/2467]	Loss: 0.160492
 Train Epoch: [0/2467]	Loss: 0.095994
     Train Epoch: [0/2467]	Loss: 0.271391
          Train Epoch: [20/2467]	Loss: 0.067285
Train Epoch: [20/2467]	Loss: 0.108097    
     Train Epoch: [20/2467]	Loss: 0.330027
 Train Epoch: [20/2467]	Loss: 0.142189
         Train Epoch: [40/2467]	Loss: 0.111031
     Train Epoch: [40/2467]	Loss: 0.142143
 Train Epoch: [40/2467]	Loss: 0.391328
     Train Epoch: [40/2467]	Loss: 0.123444
             Train Epoch: [60/2467]	Loss: 0.250441
  Train Epoch: [60/2467]	Loss: 0.171443
Train Epoch: [60/2467]	Loss: 0.033126    
 Train Epoch: [60/2467]	Loss: 0.119328
             Train Epoch: [80/2467]	Loss: 0.078008    
  Train Epoch: [80/2467]	Loss: 0.264805Train Epoch: [80/2467]	Loss: 0.119407

 Train Epoch: [80/2467]	Loss: 0.098500
         Train Epoch: [100/2467]	Loss: 0.074547
 Train Epoch: [100/2467]	Loss: 0.173525    
     Train Epoch: [100/2467]	Loss: 0.119301
 Train Epoch: [100/2467]	Loss: 0.244711
     Train Epoch: [120/2467]	Loss: 0.055703
         Train Epoch: [120/2467]	Loss: 0.327296
 Train Epoch: [120/2467]	Loss: 0.193565
     Train Epoch: [120/2467]	Loss: 0.083335
          Train Epoch: [140/2467]	Loss: 0.193447Train Epoch: [140/2467]	Loss: 0.034223
    
 Train Epoch: [140/2467]	Loss: 0.112443
     Train Epoch: [140/2467]	Loss: 0.058973
         Train Epoch: [160/2467]	Loss: 0.109756    
 Train Epoch: [160/2467]	Loss: 0.209155 
Train Epoch: [160/2467]	Loss: 0.115523
     Train Epoch: [160/2467]	Loss: 0.119161
                  Train Epoch: [180/2467]	Loss: 0.236732Train Epoch: [180/2467]	Loss: 0.106180 

 Train Epoch: [180/2467]	Loss: 0.149409
Train Epoch: [180/2467]	Loss: 0.137950
         Train Epoch: [200/2467]	Loss: 0.058234
 Train Epoch: [200/2467]	Loss: 0.115107
     Train Epoch: [200/2467]	Loss: 0.094335
     Train Epoch: [200/2467]	Loss: 0.078480
     Train Epoch: [220/2467]	Loss: 0.052322    
 Train Epoch: [220/2467]	Loss: 0.172846
     Train Epoch: [220/2467]	Loss: 0.131353
     Train Epoch: [220/2467]	Loss: 0.216530
         Train Epoch: [240/2467]	Loss: 0.154933
 Train Epoch: [240/2467]	Loss: 0.255960
     Train Epoch: [240/2467]	Loss: 0.037575
     Train Epoch: [240/2467]	Loss: 0.018888
     Train Epoch: [260/2467]	Loss: 0.041085    
      Train Epoch: [260/2467]	Loss: 0.046075Train Epoch: [260/2467]	Loss: 0.059045

     Train Epoch: [260/2467]	Loss: 0.083768
         Train Epoch: [280/2467]	Loss: 0.104288
 Train Epoch: [280/2467]	Loss: 0.179872
         Train Epoch: [280/2467]	Loss: 0.114407
 Train Epoch: [280/2467]	Loss: 0.100825
     Train Epoch: [300/2467]	Loss: 0.349896
         Train Epoch: [300/2467]	Loss: 0.033367
     Train Epoch: [300/2467]	Loss: 0.082438
 Train Epoch: [300/2467]	Loss: 0.176868
             Train Epoch: [320/2467]	Loss: 0.169719 
Train Epoch: [320/2467]	Loss: 0.160726
 Train Epoch: [320/2467]	Loss: 0.243793    
 Train Epoch: [320/2467]	Loss: 0.435862
         Train Epoch: [340/2467]	Loss: 0.016098
     Train Epoch: [340/2467]	Loss: 0.051432
 Train Epoch: [340/2467]	Loss: 0.064921
     Train Epoch: [340/2467]	Loss: 0.187508
          Train Epoch: [360/2467]	Loss: 0.061155Train Epoch: [360/2467]	Loss: 0.225043

         Train Epoch: [360/2467]	Loss: 0.137505 
Train Epoch: [360/2467]	Loss: 0.173996
     Train Epoch: [380/2467]	Loss: 0.317909
             Train Epoch: [380/2467]	Loss: 0.074983
  Train Epoch: [380/2467]	Loss: 0.144060Train Epoch: [380/2467]	Loss: 0.209415

          Train Epoch: [400/2467]	Loss: 0.047978Train Epoch: [400/2467]	Loss: 0.083984

         Train Epoch: [400/2467]	Loss: 0.134905
 Train Epoch: [400/2467]	Loss: 0.175752
         Train Epoch: [420/2467]	Loss: 0.113194
 Train Epoch: [420/2467]	Loss: 0.135657
     Train Epoch: [420/2467]	Loss: 0.258251
     Train Epoch: [420/2467]	Loss: 0.212121
              Train Epoch: [440/2467]	Loss: 0.153353Train Epoch: [440/2467]	Loss: 0.347824

 Train Epoch: [440/2467]	Loss: 0.072684
     Train Epoch: [440/2467]	Loss: 0.050485
         Train Epoch: [460/2467]	Loss: 0.111430     
Train Epoch: [460/2467]	Loss: 0.249744
     Train Epoch: [460/2467]	Loss: 0.160768
 Train Epoch: [460/2467]	Loss: 0.114765
     Train Epoch: [480/2467]	Loss: 0.246714    
     Train Epoch: [480/2467]	Loss: 0.037553
 Train Epoch: [480/2467]	Loss: 0.139984
     Train Epoch: [480/2467]	Loss: 0.327324
         Train Epoch: [500/2467]	Loss: 0.101567
     Train Epoch: [500/2467]	Loss: 0.137680
 Train Epoch: [500/2467]	Loss: 0.115903
     Train Epoch: [500/2467]	Loss: 0.203286
         Train Epoch: [520/2467]	Loss: 0.143867
 Train Epoch: [520/2467]	Loss: 0.248170
     Train Epoch: [520/2467]	Loss: 0.086812
     Train Epoch: [520/2467]	Loss: 0.158696
     Train Epoch: [540/2467]	Loss: 0.149024
             Train Epoch: [540/2467]	Loss: 0.251203
 Train Epoch: [540/2467]	Loss: 0.060088
 Train Epoch: [540/2467]	Loss: 0.086565
              Train Epoch: [560/2467]	Loss: 0.150402Train Epoch: [560/2467]	Loss: 0.188225

     Train Epoch: [560/2467]	Loss: 0.138434
 Train Epoch: [560/2467]	Loss: 0.174429
         Train Epoch: [580/2467]	Loss: 0.062091
 Train Epoch: [580/2467]	Loss: 0.163196
     Train Epoch: [580/2467]	Loss: 0.143033
     Train Epoch: [580/2467]	Loss: 0.311578
     Train Epoch: [600/2467]	Loss: 0.241407
     Train Epoch: [600/2467]	Loss: 0.110896
     Train Epoch: [600/2467]	Loss: 0.189741
     Train Epoch: [600/2467]	Loss: 0.155822
         Train Epoch: [620/2467]	Loss: 0.139388    
  Train Epoch: [620/2467]	Loss: 0.129147
Train Epoch: [620/2467]	Loss: 0.025996
     Train Epoch: [620/2467]	Loss: 0.191222
         Train Epoch: [640/2467]	Loss: 0.034493
      Train Epoch: [640/2467]	Loss: 0.129518
Train Epoch: [640/2467]	Loss: 0.191400
     Train Epoch: [640/2467]	Loss: 0.106720
         Train Epoch: [660/2467]	Loss: 0.352366 
Train Epoch: [660/2467]	Loss: 0.025708
         Train Epoch: [660/2467]	Loss: 0.041266 
Train Epoch: [660/2467]	Loss: 0.149744
     Train Epoch: [680/2467]	Loss: 0.046264
     Train Epoch: [680/2467]	Loss: 0.032916
     Train Epoch: [680/2467]	Loss: 0.069646
     Train Epoch: [680/2467]	Loss: 0.118376
         Train Epoch: [700/2467]	Loss: 0.036050
     Train Epoch: [700/2467]	Loss: 0.295945
 Train Epoch: [700/2467]	Loss: 0.298996
     Train Epoch: [700/2467]	Loss: 0.095240
          Train Epoch: [720/2467]	Loss: 0.145950Train Epoch: [720/2467]	Loss: 0.018434

     Train Epoch: [720/2467]	Loss: 0.211308
     Train Epoch: [720/2467]	Loss: 0.175090
         Train Epoch: [740/2467]	Loss: 0.077586
 Train Epoch: [740/2467]	Loss: 0.084361
         Train Epoch: [740/2467]	Loss: 0.319680 
Train Epoch: [740/2467]	Loss: 0.051764
         Train Epoch: [760/2467]	Loss: 0.269700
 Train Epoch: [760/2467]	Loss: 0.049149
     Train Epoch: [760/2467]	Loss: 0.400875
     Train Epoch: [760/2467]	Loss: 0.303348
     Train Epoch: [780/2467]	Loss: 0.328837        
  Train Epoch: [780/2467]	Loss: 0.291584Train Epoch: [780/2467]	Loss: 0.106654

     Train Epoch: [780/2467]	Loss: 0.371885
     Train Epoch: [800/2467]	Loss: 0.212719    
      Train Epoch: [800/2467]	Loss: 0.155758Train Epoch: [800/2467]	Loss: 0.175083

     Train Epoch: [800/2467]	Loss: 0.087973
          Train Epoch: [820/2467]	Loss: 0.203066    Train Epoch: [820/2467]	Loss: 0.037156

     Train Epoch: [820/2467]	Loss: 0.412359
 Train Epoch: [820/2467]	Loss: 0.070824
              Train Epoch: [840/2467]	Loss: 0.293701Train Epoch: [840/2467]	Loss: 0.058951

 Train Epoch: [840/2467]	Loss: 0.144611
     Train Epoch: [840/2467]	Loss: 0.128367
         Train Epoch: [860/2467]	Loss: 0.056872
     Train Epoch: [860/2467]	Loss: 0.219961
 Train Epoch: [860/2467]	Loss: 0.206862
     Train Epoch: [860/2467]	Loss: 0.082836
          Train Epoch: [880/2467]	Loss: 0.213778Train Epoch: [880/2467]	Loss: 0.143198

     Train Epoch: [880/2467]	Loss: 0.171013
     Train Epoch: [880/2467]	Loss: 0.117848
         Train Epoch: [900/2467]	Loss: 0.228180 
Train Epoch: [900/2467]	Loss: 0.210618
     Train Epoch: [900/2467]	Loss: 0.067416
     Train Epoch: [900/2467]	Loss: 0.192048
          Train Epoch: [920/2467]	Loss: 0.280189
Train Epoch: [920/2467]	Loss: 0.254613
     Train Epoch: [920/2467]	Loss: 0.091666
     Train Epoch: [920/2467]	Loss: 0.247160
     Train Epoch: [940/2467]	Loss: 0.092677
         Train Epoch: [940/2467]	Loss: 0.044147
 Train Epoch: [940/2467]	Loss: 0.158508
     Train Epoch: [940/2467]	Loss: 0.029422
             Train Epoch: [960/2467]	Loss: 0.056760 
Train Epoch: [960/2467]	Loss: 0.276584
     Train Epoch: [960/2467]	Loss: 0.075439
 Train Epoch: [960/2467]	Loss: 0.071926
         Train Epoch: [980/2467]	Loss: 0.106931
     Train Epoch: [980/2467]	Loss: 0.196278
 Train Epoch: [980/2467]	Loss: 0.067276    
 Train Epoch: [980/2467]	Loss: 0.146672
             Train Epoch: [1000/2467]	Loss: 0.278342
      Train Epoch: [1000/2467]	Loss: 0.034497Train Epoch: [1000/2467]	Loss: 0.219650

 Train Epoch: [1000/2467]	Loss: 0.039578
              Train Epoch: [1020/2467]	Loss: 0.055497Train Epoch: [1020/2467]	Loss: 0.071441

 Train Epoch: [1020/2467]	Loss: 0.377702
     Train Epoch: [1020/2467]	Loss: 0.168441
     Train Epoch: [1040/2467]	Loss: 0.232549
     Train Epoch: [1040/2467]	Loss: 0.131704
     Train Epoch: [1040/2467]	Loss: 0.103208
     Train Epoch: [1040/2467]	Loss: 0.103267
     Train Epoch: [1060/2467]	Loss: 0.182870
         Train Epoch: [1060/2467]	Loss: 0.401868
 Train Epoch: [1060/2467]	Loss: 0.106745
     Train Epoch: [1060/2467]	Loss: 0.085163
         Train Epoch: [1080/2467]	Loss: 0.150009    
 Train Epoch: [1080/2467]	Loss: 0.114740
 Train Epoch: [1080/2467]	Loss: 0.241207
     Train Epoch: [1080/2467]	Loss: 0.238751
         Train Epoch: [1100/2467]	Loss: 0.228031
     Train Epoch: [1100/2467]	Loss: 0.187362    
  Train Epoch: [1100/2467]	Loss: 0.208414
Train Epoch: [1100/2467]	Loss: 0.277939
         Train Epoch: [1120/2467]	Loss: 0.111814
     Train Epoch: [1120/2467]	Loss: 0.252372
 Train Epoch: [1120/2467]	Loss: 0.118362    
 Train Epoch: [1120/2467]	Loss: 0.151587
         Train Epoch: [1140/2467]	Loss: 0.050805 
Train Epoch: [1140/2467]	Loss: 0.256764    
 Train Epoch: [1140/2467]	Loss: 0.066234
     Train Epoch: [1140/2467]	Loss: 0.385429
     Train Epoch: [1160/2467]	Loss: 0.227446    
     Train Epoch: [1160/2467]	Loss: 0.062988
 Train Epoch: [1160/2467]	Loss: 0.127984
     Train Epoch: [1160/2467]	Loss: 0.127827
     Train Epoch: [1180/2467]	Loss: 0.067735
     Train Epoch: [1180/2467]	Loss: 0.175770    
     Train Epoch: [1180/2467]	Loss: 0.130896
 Train Epoch: [1180/2467]	Loss: 0.125697
     Train Epoch: [1200/2467]	Loss: 0.271822    
 Train Epoch: [1200/2467]	Loss: 0.108901
     Train Epoch: [1200/2467]	Loss: 0.077561
     Train Epoch: [1200/2467]	Loss: 0.210120
     Train Epoch: [1220/2467]	Loss: 0.252296
         Train Epoch: [1220/2467]	Loss: 0.085233
 Train Epoch: [1220/2467]	Loss: 0.246133
     Train Epoch: [1220/2467]	Loss: 0.196878
     Train Epoch: [1240/2467]	Loss: 0.275241
         Train Epoch: [1240/2467]	Loss: 0.145386 
    Train Epoch: [1240/2467]	Loss: 0.167630
 Train Epoch: [1240/2467]	Loss: 0.248272
             Train Epoch: [1260/2467]	Loss: 0.054658 
Train Epoch: [1260/2467]	Loss: 0.122992
 Train Epoch: [1260/2467]	Loss: 0.057620
     Train Epoch: [1260/2467]	Loss: 0.377627
              Train Epoch: [1280/2467]	Loss: 0.070898Train Epoch: [1280/2467]	Loss: 0.182778

 Train Epoch: [1280/2467]	Loss: 0.184982
     Train Epoch: [1280/2467]	Loss: 0.450946
          Train Epoch: [1300/2467]	Loss: 0.039684Train Epoch: [1300/2467]	Loss: 0.064170
    
 Train Epoch: [1300/2467]	Loss: 0.292317
     Train Epoch: [1300/2467]	Loss: 0.208067
          Train Epoch: [1320/2467]	Loss: 0.046916Train Epoch: [1320/2467]	Loss: 0.124458

     Train Epoch: [1320/2467]	Loss: 0.184740
     Train Epoch: [1320/2467]	Loss: 0.152897
     Train Epoch: [1340/2467]	Loss: 0.288201
         Train Epoch: [1340/2467]	Loss: 0.158065
 Train Epoch: [1340/2467]	Loss: 0.098462
     Train Epoch: [1340/2467]	Loss: 0.203371
     Train Epoch: [1360/2467]	Loss: 0.107758
         Train Epoch: [1360/2467]	Loss: 0.117030
 Train Epoch: [1360/2467]	Loss: 0.057999
     Train Epoch: [1360/2467]	Loss: 0.098536
     Train Epoch: [1380/2467]	Loss: 0.284795
         Train Epoch: [1380/2467]	Loss: 0.220716
     Train Epoch: [1380/2467]	Loss: 0.262672
 Train Epoch: [1380/2467]	Loss: 0.181599
     Train Epoch: [1400/2467]	Loss: 0.087866
         Train Epoch: [1400/2467]	Loss: 0.087039
 Train Epoch: [1400/2467]	Loss: 0.149782
     Train Epoch: [1400/2467]	Loss: 0.219506
         Train Epoch: [1420/2467]	Loss: 0.134297    
  Train Epoch: [1420/2467]	Loss: 0.386173Train Epoch: [1420/2467]	Loss: 0.077705

     Train Epoch: [1420/2467]	Loss: 0.147297
     Train Epoch: [1440/2467]	Loss: 0.050384
              Train Epoch: [1440/2467]	Loss: 0.120447Train Epoch: [1440/2467]	Loss: 0.048844

 Train Epoch: [1440/2467]	Loss: 0.039790
     Train Epoch: [1460/2467]	Loss: 0.113964
     Train Epoch: [1460/2467]	Loss: 0.106184
     Train Epoch: [1460/2467]	Loss: 0.043122
     Train Epoch: [1460/2467]	Loss: 0.228403
     Train Epoch: [1480/2467]	Loss: 0.219796
         Train Epoch: [1480/2467]	Loss: 0.199018 
Train Epoch: [1480/2467]	Loss: 0.025053
     Train Epoch: [1480/2467]	Loss: 0.011868
     Train Epoch: [1500/2467]	Loss: 0.112073
             Train Epoch: [1500/2467]	Loss: 0.080081  
Train Epoch: [1500/2467]	Loss: 0.135268Train Epoch: [1500/2467]	Loss: 0.015618

     Train Epoch: [1520/2467]	Loss: 0.252037
         Train Epoch: [1520/2467]	Loss: 0.145485 
Train Epoch: [1520/2467]	Loss: 0.243382    
 Train Epoch: [1520/2467]	Loss: 0.124275
         Train Epoch: [1540/2467]	Loss: 0.035628
 Train Epoch: [1540/2467]	Loss: 0.265279
     Train Epoch: [1540/2467]	Loss: 0.188719
     Train Epoch: [1540/2467]	Loss: 0.086908
     Train Epoch: [1560/2467]	Loss: 0.248456
         Train Epoch: [1560/2467]	Loss: 0.065013
 Train Epoch: [1560/2467]	Loss: 0.327139
     Train Epoch: [1560/2467]	Loss: 0.443393
         Train Epoch: [1580/2467]	Loss: 0.261774
     Train Epoch: [1580/2467]	Loss: 0.090169
 Train Epoch: [1580/2467]	Loss: 0.193495
     Train Epoch: [1580/2467]	Loss: 0.112856
              Train Epoch: [1600/2467]	Loss: 0.258731Train Epoch: [1600/2467]	Loss: 0.074831

 Train Epoch: [1600/2467]	Loss: 0.216786
     Train Epoch: [1600/2467]	Loss: 0.128355
     Train Epoch: [1620/2467]	Loss: 0.103892
     Train Epoch: [1620/2467]	Loss: 0.164456
     Train Epoch: [1620/2467]	Loss: 0.242862
     Train Epoch: [1620/2467]	Loss: 0.039533
             Train Epoch: [1640/2467]	Loss: 0.231368
  Train Epoch: [1640/2467]	Loss: 0.199801Train Epoch: [1640/2467]	Loss: 0.137157

     Train Epoch: [1640/2467]	Loss: 0.163686
     Train Epoch: [1660/2467]	Loss: 0.173253
             Train Epoch: [1660/2467]	Loss: 0.199045
 Train Epoch: [1660/2467]	Loss: 0.064973
 Train Epoch: [1660/2467]	Loss: 0.124547
     Train Epoch: [1680/2467]	Loss: 0.134108
         Train Epoch: [1680/2467]	Loss: 0.043016
 Train Epoch: [1680/2467]	Loss: 0.310318    
 Train Epoch: [1680/2467]	Loss: 0.129565
     Train Epoch: [1700/2467]	Loss: 0.308186
          Train Epoch: [1700/2467]	Loss: 0.105215Train Epoch: [1700/2467]	Loss: 0.094571

     Train Epoch: [1700/2467]	Loss: 0.167660
         Train Epoch: [1720/2467]	Loss: 0.121310
 Train Epoch: [1720/2467]	Loss: 0.047730    
     Train Epoch: [1720/2467]	Loss: 0.217355
 Train Epoch: [1720/2467]	Loss: 0.149054
     Train Epoch: [1740/2467]	Loss: 0.021404
         Train Epoch: [1740/2467]	Loss: 0.316555
 Train Epoch: [1740/2467]	Loss: 0.100065
     Train Epoch: [1740/2467]	Loss: 0.189235
     Train Epoch: [1760/2467]	Loss: 0.128701
         Train Epoch: [1760/2467]	Loss: 0.095636
     Train Epoch: [1760/2467]	Loss: 0.050837
 Train Epoch: [1760/2467]	Loss: 0.112138
             Train Epoch: [1780/2467]	Loss: 0.146314
 Train Epoch: [1780/2467]	Loss: 0.199620
 Train Epoch: [1780/2467]	Loss: 0.278597
     Train Epoch: [1780/2467]	Loss: 0.082913
         Train Epoch: [1800/2467]	Loss: 0.279917
 Train Epoch: [1800/2467]	Loss: 0.232800    
     Train Epoch: [1800/2467]	Loss: 0.194452 
Train Epoch: [1800/2467]	Loss: 0.128828
     Train Epoch: [1820/2467]	Loss: 0.107528
         Train Epoch: [1820/2467]	Loss: 0.128187 
Train Epoch: [1820/2467]	Loss: 0.306917
     Train Epoch: [1820/2467]	Loss: 0.104553
             Train Epoch: [1840/2467]	Loss: 0.269962
  Train Epoch: [1840/2467]	Loss: 0.279776
Train Epoch: [1840/2467]	Loss: 0.110105
     Train Epoch: [1840/2467]	Loss: 0.043827
     Train Epoch: [1860/2467]	Loss: 0.149777
     Train Epoch: [1860/2467]	Loss: 0.132863
          Train Epoch: [1860/2467]	Loss: 0.145100Train Epoch: [1860/2467]	Loss: 0.127155

          Train Epoch: [1880/2467]	Loss: 0.024980Train Epoch: [1880/2467]	Loss: 0.040157

     Train Epoch: [1880/2467]	Loss: 0.111592
     Train Epoch: [1880/2467]	Loss: 0.056007
         Train Epoch: [1900/2467]	Loss: 0.266232
     Train Epoch: [1900/2467]	Loss: 0.100801
 Train Epoch: [1900/2467]	Loss: 0.124392
     Train Epoch: [1900/2467]	Loss: 0.255756
     Train Epoch: [1920/2467]	Loss: 0.116114
     Train Epoch: [1920/2467]	Loss: 0.217412
     Train Epoch: [1920/2467]	Loss: 0.152695
     Train Epoch: [1920/2467]	Loss: 0.018616
     Train Epoch: [1940/2467]	Loss: 0.154000    
     Train Epoch: [1940/2467]	Loss: 0.350420
     Train Epoch: [1940/2467]	Loss: 0.119131
 Train Epoch: [1940/2467]	Loss: 0.063242
     Train Epoch: [1960/2467]	Loss: 0.169382    
     Train Epoch: [1960/2467]	Loss: 0.105294
 Train Epoch: [1960/2467]	Loss: 0.088384
     Train Epoch: [1960/2467]	Loss: 0.129503
         Train Epoch: [1980/2467]	Loss: 0.201988
 Train Epoch: [1980/2467]	Loss: 0.252699
          Train Epoch: [1980/2467]	Loss: 0.104572
Train Epoch: [1980/2467]	Loss: 0.184892
          Train Epoch: [2000/2467]	Loss: 0.054901    Train Epoch: [2000/2467]	Loss: 0.130130

 Train Epoch: [2000/2467]	Loss: 0.021439
     Train Epoch: [2000/2467]	Loss: 0.207337
     Train Epoch: [2020/2467]	Loss: 0.136265
     Train Epoch: [2020/2467]	Loss: 0.111692
     Train Epoch: [2020/2467]	Loss: 0.054713
     Train Epoch: [2020/2467]	Loss: 0.154297
          Train Epoch: [2040/2467]	Loss: 0.076456Train Epoch: [2040/2467]	Loss: 0.366750

     Train Epoch: [2040/2467]	Loss: 0.148385
     Train Epoch: [2040/2467]	Loss: 0.084624
     Train Epoch: [2060/2467]	Loss: 0.194946
         Train Epoch: [2060/2467]	Loss: 0.064597
 Train Epoch: [2060/2467]	Loss: 0.111275
     Train Epoch: [2060/2467]	Loss: 0.172841
         Train Epoch: [2080/2467]	Loss: 0.049802
 Train Epoch: [2080/2467]	Loss: 0.062859
          Train Epoch: [2080/2467]	Loss: 0.024794Train Epoch: [2080/2467]	Loss: 0.227997

     Train Epoch: [2100/2467]	Loss: 0.097506
     Train Epoch: [2100/2467]	Loss: 0.132143
     Train Epoch: [2100/2467]	Loss: 0.444471
     Train Epoch: [2100/2467]	Loss: 0.088610
         Train Epoch: [2120/2467]	Loss: 0.044156
 Train Epoch: [2120/2467]	Loss: 0.203109
          Train Epoch: [2120/2467]	Loss: 0.197177Train Epoch: [2120/2467]	Loss: 0.091592

         Train Epoch: [2140/2467]	Loss: 0.191304
 Train Epoch: [2140/2467]	Loss: 0.488789
          Train Epoch: [2140/2467]	Loss: 0.230587Train Epoch: [2140/2467]	Loss: 0.322804

     Train Epoch: [2160/2467]	Loss: 0.099595
     Train Epoch: [2160/2467]	Loss: 0.306849
     Train Epoch: [2160/2467]	Loss: 0.141845
     Train Epoch: [2160/2467]	Loss: 0.043275
         Train Epoch: [2180/2467]	Loss: 0.155924
     Train Epoch: [2180/2467]	Loss: 0.268791
 Train Epoch: [2180/2467]	Loss: 0.172301
     Train Epoch: [2180/2467]	Loss: 0.090015
         Train Epoch: [2200/2467]	Loss: 0.147952
 Train Epoch: [2200/2467]	Loss: 0.208648
     Train Epoch: [2200/2467]	Loss: 0.118304
     Train Epoch: [2200/2467]	Loss: 0.025366
         Train Epoch: [2220/2467]	Loss: 0.086752
         Train Epoch: [2220/2467]	Loss: 0.136636
 Train Epoch: [2220/2467]	Loss: 0.085627
 Train Epoch: [2220/2467]	Loss: 0.116366
     Train Epoch: [2240/2467]	Loss: 0.026684
              Train Epoch: [2240/2467]	Loss: 0.070487Train Epoch: [2240/2467]	Loss: 0.116849
 
Train Epoch: [2240/2467]	Loss: 0.034104
         Train Epoch: [2260/2467]	Loss: 0.201983
 Train Epoch: [2260/2467]	Loss: 0.188651    
 Train Epoch: [2260/2467]	Loss: 0.359167
     Train Epoch: [2260/2467]	Loss: 0.021294
     Train Epoch: [2280/2467]	Loss: 0.088859
         Train Epoch: [2280/2467]	Loss: 0.089532    
 Train Epoch: [2280/2467]	Loss: 0.127579 
Train Epoch: [2280/2467]	Loss: 0.139435
     Train Epoch: [2300/2467]	Loss: 0.043438
         Train Epoch: [2300/2467]	Loss: 0.259086 
Train Epoch: [2300/2467]	Loss: 0.063838
     Train Epoch: [2300/2467]	Loss: 0.084846
     Train Epoch: [2320/2467]	Loss: 0.030540
     Train Epoch: [2320/2467]	Loss: 0.183726
     Train Epoch: [2320/2467]	Loss: 0.166347
     Train Epoch: [2320/2467]	Loss: 0.257341
     Train Epoch: [2340/2467]	Loss: 0.150568
         Train Epoch: [2340/2467]	Loss: 0.130374    
 Train Epoch: [2340/2467]	Loss: 0.315290
 Train Epoch: [2340/2467]	Loss: 0.466974
     Train Epoch: [2360/2467]	Loss: 0.143201
     Train Epoch: [2360/2467]	Loss: 0.175714
     Train Epoch: [2360/2467]	Loss: 0.055294
     Train Epoch: [2360/2467]	Loss: 0.381234
             Train Epoch: [2380/2467]	Loss: 0.197941
  Train Epoch: [2380/2467]	Loss: 0.396420Train Epoch: [2380/2467]	Loss: 0.114578

     Train Epoch: [2380/2467]	Loss: 0.163109
     Train Epoch: [2400/2467]	Loss: 0.174540
     Train Epoch: [2400/2467]	Loss: 0.092876
     Train Epoch: [2400/2467]	Loss: 0.134129
     Train Epoch: [2400/2467]	Loss: 0.122398
          Train Epoch: [2420/2467]	Loss: 0.257525Train Epoch: [2420/2467]	Loss: 0.028961

         Train Epoch: [2420/2467]	Loss: 0.305579
 Train Epoch: [2420/2467]	Loss: 0.110195
     Train Epoch: [2440/2467]	Loss: 0.146775
     Train Epoch: [2440/2467]	Loss: 0.016202
     Train Epoch: [2440/2467]	Loss: 0.234068
     Train Epoch: [2440/2467]	Loss: 0.135171
     Train Epoch: [2460/2467]	Loss: 0.149583
     Train Epoch: [2460/2467]	Loss: 0.077394
     Train Epoch: [2460/2467]	Loss: 0.235343
     Train Epoch: [2460/2467]	Loss: 0.124021
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 29 epoch =====
     2025-05-11.10-36-27
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 29 epoch =====
     2025-05-11.10-36-27
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 29 epoch =====
     2025-05-11.10-36-27
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 29 epoch =====
     2025-05-11.10-36-28
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
          Train Epoch: [0/2467]	Loss: 0.285150    Train Epoch: [0/2467]	Loss: 0.287295

     Train Epoch: [0/2467]	Loss: 0.094692
 Train Epoch: [0/2467]	Loss: 0.167208
     Train Epoch: [20/2467]	Loss: 0.064842
         Train Epoch: [20/2467]	Loss: 0.307054
 Train Epoch: [20/2467]	Loss: 0.127670
     Train Epoch: [20/2467]	Loss: 0.113095
         Train Epoch: [40/2467]	Loss: 0.135578
 Train Epoch: [40/2467]	Loss: 0.378673
     Train Epoch: [40/2467]	Loss: 0.135634
     Train Epoch: [40/2467]	Loss: 0.106689
         Train Epoch: [60/2467]	Loss: 0.158410
 Train Epoch: [60/2467]	Loss: 0.183941
     Train Epoch: [60/2467]	Loss: 0.073455
     Train Epoch: [60/2467]	Loss: 0.034299
     Train Epoch: [80/2467]	Loss: 0.125415
          Train Epoch: [80/2467]	Loss: 0.135163Train Epoch: [80/2467]	Loss: 0.145597

     Train Epoch: [80/2467]	Loss: 0.343962
         Train Epoch: [100/2467]	Loss: 0.196731    
 Train Epoch: [100/2467]	Loss: 0.201335 
Train Epoch: [100/2467]	Loss: 0.150606
     Train Epoch: [100/2467]	Loss: 0.084900
     Train Epoch: [120/2467]	Loss: 0.052988
         Train Epoch: [120/2467]	Loss: 0.340577
     Train Epoch: [120/2467]	Loss: 0.078972 
Train Epoch: [120/2467]	Loss: 0.241344
          Train Epoch: [140/2467]	Loss: 0.046689
Train Epoch: [140/2467]	Loss: 0.191059
         Train Epoch: [140/2467]	Loss: 0.111712
 Train Epoch: [140/2467]	Loss: 0.065222
         Train Epoch: [160/2467]	Loss: 0.205949
         Train Epoch: [160/2467]	Loss: 0.134376
  Train Epoch: [160/2467]	Loss: 0.136031
Train Epoch: [160/2467]	Loss: 0.083351
         Train Epoch: [180/2467]	Loss: 0.124425
 Train Epoch: [180/2467]	Loss: 0.180529    
     Train Epoch: [180/2467]	Loss: 0.102343
 Train Epoch: [180/2467]	Loss: 0.122876
     Train Epoch: [200/2467]	Loss: 0.119510
             Train Epoch: [200/2467]	Loss: 0.067697
  Train Epoch: [200/2467]	Loss: 0.065795
Train Epoch: [200/2467]	Loss: 0.064905
         Train Epoch: [220/2467]	Loss: 0.067633
 Train Epoch: [220/2467]	Loss: 0.138985
         Train Epoch: [220/2467]	Loss: 0.141730
 Train Epoch: [220/2467]	Loss: 0.219548
     Train Epoch: [240/2467]	Loss: 0.194017
     Train Epoch: [240/2467]	Loss: 0.221443
     Train Epoch: [240/2467]	Loss: 0.035200
     Train Epoch: [240/2467]	Loss: 0.020430
              Train Epoch: [260/2467]	Loss: 0.028103Train Epoch: [260/2467]	Loss: 0.048576

 Train Epoch: [260/2467]	Loss: 0.054631
     Train Epoch: [260/2467]	Loss: 0.074680
     Train Epoch: [280/2467]	Loss: 0.392096
     Train Epoch: [280/2467]	Loss: 0.183546
     Train Epoch: [280/2467]	Loss: 0.093855
     Train Epoch: [280/2467]	Loss: 0.143630
             Train Epoch: [300/2467]	Loss: 0.151111
 Train Epoch: [300/2467]	Loss: 0.332754 
    Train Epoch: [300/2467]	Loss: 0.036977
 Train Epoch: [300/2467]	Loss: 0.091086
          Train Epoch: [320/2467]	Loss: 0.151218
Train Epoch: [320/2467]	Loss: 0.178677
     Train Epoch: [320/2467]	Loss: 0.235362
     Train Epoch: [320/2467]	Loss: 0.392463
             Train Epoch: [340/2467]	Loss: 0.016469
  Train Epoch: [340/2467]	Loss: 0.098667
Train Epoch: [340/2467]	Loss: 0.039342
     Train Epoch: [340/2467]	Loss: 0.212594
     Train Epoch: [360/2467]	Loss: 0.136556
         Train Epoch: [360/2467]	Loss: 0.074811
 Train Epoch: [360/2467]	Loss: 0.172075    
 Train Epoch: [360/2467]	Loss: 0.217239
             Train Epoch: [380/2467]	Loss: 0.281723
 Train Epoch: [380/2467]	Loss: 0.196134 
Train Epoch: [380/2467]	Loss: 0.101632
     Train Epoch: [380/2467]	Loss: 0.123277
         Train Epoch: [400/2467]	Loss: 0.052478
 Train Epoch: [400/2467]	Loss: 0.070955
         Train Epoch: [400/2467]	Loss: 0.129631
 Train Epoch: [400/2467]	Loss: 0.150988
     Train Epoch: [420/2467]	Loss: 0.216184
          Train Epoch: [420/2467]	Loss: 0.270189
Train Epoch: [420/2467]	Loss: 0.119319
     Train Epoch: [420/2467]	Loss: 0.121347
         Train Epoch: [440/2467]	Loss: 0.321231
     Train Epoch: [440/2467]	Loss: 0.117220 
Train Epoch: [440/2467]	Loss: 0.070022
     Train Epoch: [440/2467]	Loss: 0.057879
     Train Epoch: [460/2467]	Loss: 0.095683
     Train Epoch: [460/2467]	Loss: 0.134598
     Train Epoch: [460/2467]	Loss: 0.132818
     Train Epoch: [460/2467]	Loss: 0.228662
     Train Epoch: [480/2467]	Loss: 0.268535    
      Train Epoch: [480/2467]	Loss: 0.025964Train Epoch: [480/2467]	Loss: 0.149356

     Train Epoch: [480/2467]	Loss: 0.351264
          Train Epoch: [500/2467]	Loss: 0.134574Train Epoch: [500/2467]	Loss: 0.108121

          Train Epoch: [500/2467]	Loss: 0.219645Train Epoch: [500/2467]	Loss: 0.104918

             Train Epoch: [520/2467]	Loss: 0.126397
 Train Epoch: [520/2467]	Loss: 0.252193
 Train Epoch: [520/2467]	Loss: 0.128239
     Train Epoch: [520/2467]	Loss: 0.071750
     Train Epoch: [540/2467]	Loss: 0.058930    
 Train Epoch: [540/2467]	Loss: 0.262866
     Train Epoch: [540/2467]	Loss: 0.173430
     Train Epoch: [540/2467]	Loss: 0.102220
          Train Epoch: [560/2467]	Loss: 0.169164    Train Epoch: [560/2467]	Loss: 0.134851

     Train Epoch: [560/2467]	Loss: 0.150076
 Train Epoch: [560/2467]	Loss: 0.164652
                  Train Epoch: [580/2467]	Loss: 0.175419Train Epoch: [580/2467]	Loss: 0.252328

 Train Epoch: [580/2467]	Loss: 0.050274
 Train Epoch: [580/2467]	Loss: 0.142602
         Train Epoch: [600/2467]	Loss: 0.135003
 Train Epoch: [600/2467]	Loss: 0.124436
     Train Epoch: [600/2467]	Loss: 0.202327
     Train Epoch: [600/2467]	Loss: 0.177162
         Train Epoch: [620/2467]	Loss: 0.143561    
 Train Epoch: [620/2467]	Loss: 0.118357
 Train Epoch: [620/2467]	Loss: 0.029407
     Train Epoch: [620/2467]	Loss: 0.267720
     Train Epoch: [640/2467]	Loss: 0.041732    
 Train Epoch: [640/2467]	Loss: 0.161351
     Train Epoch: [640/2467]	Loss: 0.118402
     Train Epoch: [640/2467]	Loss: 0.158629
     Train Epoch: [660/2467]	Loss: 0.024546
         Train Epoch: [660/2467]	Loss: 0.048361 
Train Epoch: [660/2467]	Loss: 0.363827
     Train Epoch: [660/2467]	Loss: 0.152481
         Train Epoch: [680/2467]	Loss: 0.098188
     Train Epoch: [680/2467]	Loss: 0.034742 
Train Epoch: [680/2467]	Loss: 0.076303
     Train Epoch: [680/2467]	Loss: 0.114219
                 Train Epoch: [700/2467]	Loss: 0.033855 
 Train Epoch: [700/2467]	Loss: 0.348941Train Epoch: [700/2467]	Loss: 0.377647

 Train Epoch: [700/2467]	Loss: 0.065611
              Train Epoch: [720/2467]	Loss: 0.114322Train Epoch: [720/2467]	Loss: 0.036342

 Train Epoch: [720/2467]	Loss: 0.173181
     Train Epoch: [720/2467]	Loss: 0.077715
     Train Epoch: [740/2467]	Loss: 0.300627
     Train Epoch: [740/2467]	Loss: 0.074038
     Train Epoch: [740/2467]	Loss: 0.049549
     Train Epoch: [740/2467]	Loss: 0.084001
         Train Epoch: [760/2467]	Loss: 0.443133
 Train Epoch: [760/2467]	Loss: 0.253616
     Train Epoch: [760/2467]	Loss: 0.260044
     Train Epoch: [760/2467]	Loss: 0.052642
          Train Epoch: [780/2467]	Loss: 0.283288    Train Epoch: [780/2467]	Loss: 0.300397

 Train Epoch: [780/2467]	Loss: 0.118543
     Train Epoch: [780/2467]	Loss: 0.358354
     Train Epoch: [800/2467]	Loss: 0.185804
              Train Epoch: [800/2467]	Loss: 0.176738 
Train Epoch: [800/2467]	Loss: 0.121334
Train Epoch: [800/2467]	Loss: 0.147658
     Train Epoch: [820/2467]	Loss: 0.046876
          Train Epoch: [820/2467]	Loss: 0.224073Train Epoch: [820/2467]	Loss: 0.425409
    
 Train Epoch: [820/2467]	Loss: 0.060264
         Train Epoch: [840/2467]	Loss: 0.331127 
Train Epoch: [840/2467]	Loss: 0.129739
         Train Epoch: [840/2467]	Loss: 0.064427 
Train Epoch: [840/2467]	Loss: 0.100921
     Train Epoch: [860/2467]	Loss: 0.224415
             Train Epoch: [860/2467]	Loss: 0.185331
 Train Epoch: [860/2467]	Loss: 0.045190
 Train Epoch: [860/2467]	Loss: 0.069826
     Train Epoch: [880/2467]	Loss: 0.206729
          Train Epoch: [880/2467]	Loss: 0.276269Train Epoch: [880/2467]	Loss: 0.136521

     Train Epoch: [880/2467]	Loss: 0.125482
         Train Epoch: [900/2467]	Loss: 0.166719
     Train Epoch: [900/2467]	Loss: 0.085368    
 Train Epoch: [900/2467]	Loss: 0.173512 
Train Epoch: [900/2467]	Loss: 0.182309
         Train Epoch: [920/2467]	Loss: 0.277284
     Train Epoch: [920/2467]	Loss: 0.261333 
Train Epoch: [920/2467]	Loss: 0.251373
     Train Epoch: [920/2467]	Loss: 0.038026
     Train Epoch: [940/2467]	Loss: 0.045930
         Train Epoch: [940/2467]	Loss: 0.044498
 Train Epoch: [940/2467]	Loss: 0.114009
     Train Epoch: [940/2467]	Loss: 0.155352
         Train Epoch: [960/2467]	Loss: 0.454277    
 Train Epoch: [960/2467]	Loss: 0.311438 
Train Epoch: [960/2467]	Loss: 0.080925
     Train Epoch: [960/2467]	Loss: 0.084922
         Train Epoch: [980/2467]	Loss: 0.085838    
 Train Epoch: [980/2467]	Loss: 0.068375 
Train Epoch: [980/2467]	Loss: 0.210348
     Train Epoch: [980/2467]	Loss: 0.135806
                   Train Epoch: [1000/2467]	Loss: 0.043800Train Epoch: [1000/2467]	Loss: 0.211797
Train Epoch: [1000/2467]	Loss: 0.294121

 Train Epoch: [1000/2467]	Loss: 0.035453
         Train Epoch: [1020/2467]	Loss: 0.048724 
Train Epoch: [1020/2467]	Loss: 0.143765    
     Train Epoch: [1020/2467]	Loss: 0.393062
 Train Epoch: [1020/2467]	Loss: 0.078269
             Train Epoch: [1040/2467]	Loss: 0.127544 
Train Epoch: [1040/2467]	Loss: 0.216455 
Train Epoch: [1040/2467]	Loss: 0.104462
     Train Epoch: [1040/2467]	Loss: 0.096048
          Train Epoch: [1060/2467]	Loss: 0.377211
Train Epoch: [1060/2467]	Loss: 0.174185
         Train Epoch: [1060/2467]	Loss: 0.108917
 Train Epoch: [1060/2467]	Loss: 0.108913
         Train Epoch: [1080/2467]	Loss: 0.137593
     Train Epoch: [1080/2467]	Loss: 0.126213
 Train Epoch: [1080/2467]	Loss: 0.223538
     Train Epoch: [1080/2467]	Loss: 0.274000
         Train Epoch: [1100/2467]	Loss: 0.223037
     Train Epoch: [1100/2467]	Loss: 0.217181
 Train Epoch: [1100/2467]	Loss: 0.264709
     Train Epoch: [1100/2467]	Loss: 0.219851
         Train Epoch: [1120/2467]	Loss: 0.113617
 Train Epoch: [1120/2467]	Loss: 0.272484
     Train Epoch: [1120/2467]	Loss: 0.122407
     Train Epoch: [1120/2467]	Loss: 0.148023
         Train Epoch: [1140/2467]	Loss: 0.048567
 Train Epoch: [1140/2467]	Loss: 0.269260
     Train Epoch: [1140/2467]	Loss: 0.089273
     Train Epoch: [1140/2467]	Loss: 0.371818
     Train Epoch: [1160/2467]	Loss: 0.211869
     Train Epoch: [1160/2467]	Loss: 0.070611
     Train Epoch: [1160/2467]	Loss: 0.119062
     Train Epoch: [1160/2467]	Loss: 0.121894
     Train Epoch: [1180/2467]	Loss: 0.055893
         Train Epoch: [1180/2467]	Loss: 0.159245
 Train Epoch: [1180/2467]	Loss: 0.118855
     Train Epoch: [1180/2467]	Loss: 0.121324
     Train Epoch: [1200/2467]	Loss: 0.283121    
 Train Epoch: [1200/2467]	Loss: 0.112876
          Train Epoch: [1200/2467]	Loss: 0.188055
Train Epoch: [1200/2467]	Loss: 0.075824
         Train Epoch: [1220/2467]	Loss: 0.210519
 Train Epoch: [1220/2467]	Loss: 0.233031
     Train Epoch: [1220/2467]	Loss: 0.079002
     Train Epoch: [1220/2467]	Loss: 0.219285
         Train Epoch: [1240/2467]	Loss: 0.311053     
Train Epoch: [1240/2467]	Loss: 0.162209
 Train Epoch: [1240/2467]	Loss: 0.263867
     Train Epoch: [1240/2467]	Loss: 0.266527
         Train Epoch: [1260/2467]	Loss: 0.138040
 Train Epoch: [1260/2467]	Loss: 0.062373
     Train Epoch: [1260/2467]	Loss: 0.373573
     Train Epoch: [1260/2467]	Loss: 0.053763
     Train Epoch: [1280/2467]	Loss: 0.059284
     Train Epoch: [1280/2467]	Loss: 0.194791
     Train Epoch: [1280/2467]	Loss: 0.454524
     Train Epoch: [1280/2467]	Loss: 0.146254
     Train Epoch: [1300/2467]	Loss: 0.035213
          Train Epoch: [1300/2467]	Loss: 0.372431
Train Epoch: [1300/2467]	Loss: 0.193803
     Train Epoch: [1300/2467]	Loss: 0.083326
         Train Epoch: [1320/2467]	Loss: 0.118267 
    Train Epoch: [1320/2467]	Loss: 0.044510
 Train Epoch: [1320/2467]	Loss: 0.136401    
 Train Epoch: [1320/2467]	Loss: 0.258907
          Train Epoch: [1340/2467]	Loss: 0.302535    
Train Epoch: [1340/2467]	Loss: 0.251235
 Train Epoch: [1340/2467]	Loss: 0.099236    
 Train Epoch: [1340/2467]	Loss: 0.146427
     Train Epoch: [1360/2467]	Loss: 0.124167
         Train Epoch: [1360/2467]	Loss: 0.052897
 Train Epoch: [1360/2467]	Loss: 0.109648
     Train Epoch: [1360/2467]	Loss: 0.069727
         Train Epoch: [1380/2467]	Loss: 0.267453
     Train Epoch: [1380/2467]	Loss: 0.203254    
 Train Epoch: [1380/2467]	Loss: 0.252991 
Train Epoch: [1380/2467]	Loss: 0.158626
         Train Epoch: [1400/2467]	Loss: 0.106018
 Train Epoch: [1400/2467]	Loss: 0.149928
         Train Epoch: [1400/2467]	Loss: 0.147556
 Train Epoch: [1400/2467]	Loss: 0.105330
         Train Epoch: [1420/2467]	Loss: 0.075979 
    Train Epoch: [1420/2467]	Loss: 0.341029
 Train Epoch: [1420/2467]	Loss: 0.141998
     Train Epoch: [1420/2467]	Loss: 0.127775
     Train Epoch: [1440/2467]	Loss: 0.015338
     Train Epoch: [1440/2467]	Loss: 0.103048
          Train Epoch: [1440/2467]	Loss: 0.052437Train Epoch: [1440/2467]	Loss: 0.047227

     Train Epoch: [1460/2467]	Loss: 0.115021
     Train Epoch: [1460/2467]	Loss: 0.102376
     Train Epoch: [1460/2467]	Loss: 0.155332
     Train Epoch: [1460/2467]	Loss: 0.039554
     Train Epoch: [1480/2467]	Loss: 0.189155
             Train Epoch: [1480/2467]	Loss: 0.010490
 Train Epoch: [1480/2467]	Loss: 0.250028
 Train Epoch: [1480/2467]	Loss: 0.022425
     Train Epoch: [1500/2467]	Loss: 0.083732
         Train Epoch: [1500/2467]	Loss: 0.134096
 Train Epoch: [1500/2467]	Loss: 0.075343
     Train Epoch: [1500/2467]	Loss: 0.019122
         Train Epoch: [1520/2467]	Loss: 0.161179
 Train Epoch: [1520/2467]	Loss: 0.124020
     Train Epoch: [1520/2467]	Loss: 0.272641
     Train Epoch: [1520/2467]	Loss: 0.202609
         Train Epoch: [1540/2467]	Loss: 0.023715
 Train Epoch: [1540/2467]	Loss: 0.084960
          Train Epoch: [1540/2467]	Loss: 0.260057Train Epoch: [1540/2467]	Loss: 0.185528

         Train Epoch: [1560/2467]	Loss: 0.226291
     Train Epoch: [1560/2467]	Loss: 0.060454
 Train Epoch: [1560/2467]	Loss: 0.304765
     Train Epoch: [1560/2467]	Loss: 0.447906
         Train Epoch: [1580/2467]	Loss: 0.253024 
Train Epoch: [1580/2467]	Loss: 0.072672    
 Train Epoch: [1580/2467]	Loss: 0.195472
     Train Epoch: [1580/2467]	Loss: 0.100933
     Train Epoch: [1600/2467]	Loss: 0.258002
          Train Epoch: [1600/2467]	Loss: 0.114328
Train Epoch: [1600/2467]	Loss: 0.220424
     Train Epoch: [1600/2467]	Loss: 0.025131
     Train Epoch: [1620/2467]	Loss: 0.095994
         Train Epoch: [1620/2467]	Loss: 0.170955
 Train Epoch: [1620/2467]	Loss: 0.037334    
 Train Epoch: [1620/2467]	Loss: 0.246241
     Train Epoch: [1640/2467]	Loss: 0.225781
         Train Epoch: [1640/2467]	Loss: 0.123240
 Train Epoch: [1640/2467]	Loss: 0.071466
     Train Epoch: [1640/2467]	Loss: 0.202248
         Train Epoch: [1660/2467]	Loss: 0.171947
      Train Epoch: [1660/2467]	Loss: 0.121117Train Epoch: [1660/2467]	Loss: 0.199792

     Train Epoch: [1660/2467]	Loss: 0.064613
     Train Epoch: [1680/2467]	Loss: 0.120662
         Train Epoch: [1680/2467]	Loss: 0.049684
 Train Epoch: [1680/2467]	Loss: 0.152472
     Train Epoch: [1680/2467]	Loss: 0.158941
         Train Epoch: [1700/2467]	Loss: 0.268041
 Train Epoch: [1700/2467]	Loss: 0.139230
     Train Epoch: [1700/2467]	Loss: 0.091824
     Train Epoch: [1700/2467]	Loss: 0.135166
     Train Epoch: [1720/2467]	Loss: 0.048528
     Train Epoch: [1720/2467]	Loss: 0.184363
     Train Epoch: [1720/2467]	Loss: 0.127620
     Train Epoch: [1720/2467]	Loss: 0.265076
     Train Epoch: [1740/2467]	Loss: 0.021695
     Train Epoch: [1740/2467]	Loss: 0.408743
     Train Epoch: [1740/2467]	Loss: 0.189527
     Train Epoch: [1740/2467]	Loss: 0.045506
     Train Epoch: [1760/2467]	Loss: 0.245250
     Train Epoch: [1760/2467]	Loss: 0.084220
     Train Epoch: [1760/2467]	Loss: 0.111032
     Train Epoch: [1760/2467]	Loss: 0.050466
     Train Epoch: [1780/2467]	Loss: 0.151008
         Train Epoch: [1780/2467]	Loss: 0.091648    
  Train Epoch: [1780/2467]	Loss: 0.205682Train Epoch: [1780/2467]	Loss: 0.230963

         Train Epoch: [1800/2467]	Loss: 0.263447
     Train Epoch: [1800/2467]	Loss: 0.243456
     Train Epoch: [1800/2467]	Loss: 0.170958
 Train Epoch: [1800/2467]	Loss: 0.128387
         Train Epoch: [1820/2467]	Loss: 0.143246
 Train Epoch: [1820/2467]	Loss: 0.132669
     Train Epoch: [1820/2467]	Loss: 0.099379
     Train Epoch: [1820/2467]	Loss: 0.271577
     Train Epoch: [1840/2467]	Loss: 0.297048
              Train Epoch: [1840/2467]	Loss: 0.129033Train Epoch: [1840/2467]	Loss: 0.036813

 Train Epoch: [1840/2467]	Loss: 0.228624
     Train Epoch: [1860/2467]	Loss: 0.159344
     Train Epoch: [1860/2467]	Loss: 0.121670
     Train Epoch: [1860/2467]	Loss: 0.132865
     Train Epoch: [1860/2467]	Loss: 0.098560
         Train Epoch: [1880/2467]	Loss: 0.045835    
 Train Epoch: [1880/2467]	Loss: 0.021981
 Train Epoch: [1880/2467]	Loss: 0.094650
     Train Epoch: [1880/2467]	Loss: 0.054944
              Train Epoch: [1900/2467]	Loss: 0.312609Train Epoch: [1900/2467]	Loss: 0.105096

 Train Epoch: [1900/2467]	Loss: 0.133022
     Train Epoch: [1900/2467]	Loss: 0.265454
         Train Epoch: [1920/2467]	Loss: 0.104032
 Train Epoch: [1920/2467]	Loss: 0.017367
         Train Epoch: [1920/2467]	Loss: 0.154317
 Train Epoch: [1920/2467]	Loss: 0.206762
     Train Epoch: [1940/2467]	Loss: 0.141403    
     Train Epoch: [1940/2467]	Loss: 0.333016
     Train Epoch: [1940/2467]	Loss: 0.110234
 Train Epoch: [1940/2467]	Loss: 0.066607
         Train Epoch: [1960/2467]	Loss: 0.123920
 Train Epoch: [1960/2467]	Loss: 0.173128    
     Train Epoch: [1960/2467]	Loss: 0.102081
 Train Epoch: [1960/2467]	Loss: 0.083242
     Train Epoch: [1980/2467]	Loss: 0.219753
         Train Epoch: [1980/2467]	Loss: 0.178166
     Train Epoch: [1980/2467]	Loss: 0.265676
 Train Epoch: [1980/2467]	Loss: 0.106533
         Train Epoch: [2000/2467]	Loss: 0.129450
     Train Epoch: [2000/2467]	Loss: 0.048746
 Train Epoch: [2000/2467]	Loss: 0.020061
     Train Epoch: [2000/2467]	Loss: 0.176227
         Train Epoch: [2020/2467]	Loss: 0.138524
     Train Epoch: [2020/2467]	Loss: 0.157133
 Train Epoch: [2020/2467]	Loss: 0.128920
     Train Epoch: [2020/2467]	Loss: 0.095514
         Train Epoch: [2040/2467]	Loss: 0.379157
 Train Epoch: [2040/2467]	Loss: 0.092668    
     Train Epoch: [2040/2467]	Loss: 0.157464
 Train Epoch: [2040/2467]	Loss: 0.069553
         Train Epoch: [2060/2467]	Loss: 0.212932
     Train Epoch: [2060/2467]	Loss: 0.163174
 Train Epoch: [2060/2467]	Loss: 0.186128
     Train Epoch: [2060/2467]	Loss: 0.148726
     Train Epoch: [2080/2467]	Loss: 0.073189
         Train Epoch: [2080/2467]	Loss: 0.035371
 Train Epoch: [2080/2467]	Loss: 0.265407
     Train Epoch: [2080/2467]	Loss: 0.068434
         Train Epoch: [2100/2467]	Loss: 0.088485
 Train Epoch: [2100/2467]	Loss: 0.459890
     Train Epoch: [2100/2467]	Loss: 0.117768
     Train Epoch: [2100/2467]	Loss: 0.101571
     Train Epoch: [2120/2467]	Loss: 0.043446
          Train Epoch: [2120/2467]	Loss: 0.106277Train Epoch: [2120/2467]	Loss: 0.212741

     Train Epoch: [2120/2467]	Loss: 0.206260
     Train Epoch: [2140/2467]	Loss: 0.518536
         Train Epoch: [2140/2467]	Loss: 0.312065
     Train Epoch: [2140/2467]	Loss: 0.245583
 Train Epoch: [2140/2467]	Loss: 0.191974
     Train Epoch: [2160/2467]	Loss: 0.057997
     Train Epoch: [2160/2467]	Loss: 0.141853
     Train Epoch: [2160/2467]	Loss: 0.104391
     Train Epoch: [2160/2467]	Loss: 0.288030
             Train Epoch: [2180/2467]	Loss: 0.270827
      Train Epoch: [2180/2467]	Loss: 0.156666 Train Epoch: [2180/2467]	Loss: 0.075786
Train Epoch: [2180/2467]	Loss: 0.154934

          Train Epoch: [2200/2467]	Loss: 0.129675    Train Epoch: [2200/2467]	Loss: 0.030739

 Train Epoch: [2200/2467]	Loss: 0.148964
     Train Epoch: [2200/2467]	Loss: 0.096783
     Train Epoch: [2220/2467]	Loss: 0.089501    
     Train Epoch: [2220/2467]	Loss: 0.138283
 Train Epoch: [2220/2467]	Loss: 0.062831
     Train Epoch: [2220/2467]	Loss: 0.107026
          Train Epoch: [2240/2467]	Loss: 0.025972Train Epoch: [2240/2467]	Loss: 0.077322

     Train Epoch: [2240/2467]	Loss: 0.108495    
 Train Epoch: [2240/2467]	Loss: 0.051112
     Train Epoch: [2260/2467]	Loss: 0.339864
     Train Epoch: [2260/2467]	Loss: 0.202346
     Train Epoch: [2260/2467]	Loss: 0.088844
     Train Epoch: [2260/2467]	Loss: 0.212009
             Train Epoch: [2280/2467]	Loss: 0.068637  
Train Epoch: [2280/2467]	Loss: 0.099960Train Epoch: [2280/2467]	Loss: 0.142575

     Train Epoch: [2280/2467]	Loss: 0.084163
     Train Epoch: [2300/2467]	Loss: 0.057652    
          Train Epoch: [2300/2467]	Loss: 0.045000Train Epoch: [2300/2467]	Loss: 0.242312

 Train Epoch: [2300/2467]	Loss: 0.086855
         Train Epoch: [2320/2467]	Loss: 0.160753
          Train Epoch: [2320/2467]	Loss: 0.021801Train Epoch: [2320/2467]	Loss: 0.144658

 Train Epoch: [2320/2467]	Loss: 0.146227
         Train Epoch: [2340/2467]	Loss: 0.139290
     Train Epoch: [2340/2467]	Loss: 0.229191
     Train Epoch: [2340/2467]	Loss: 0.471401 
Train Epoch: [2340/2467]	Loss: 0.116893
         Train Epoch: [2360/2467]	Loss: 0.051159
 Train Epoch: [2360/2467]	Loss: 0.334456
     Train Epoch: [2360/2467]	Loss: 0.142861
     Train Epoch: [2360/2467]	Loss: 0.149878
     Train Epoch: [2380/2467]	Loss: 0.213354
     Train Epoch: [2380/2467]	Loss: 0.146788
     Train Epoch: [2380/2467]	Loss: 0.400466
     Train Epoch: [2380/2467]	Loss: 0.126807
     Train Epoch: [2400/2467]	Loss: 0.163487
     Train Epoch: [2400/2467]	Loss: 0.096635
     Train Epoch: [2400/2467]	Loss: 0.075151
     Train Epoch: [2400/2467]	Loss: 0.115800
     Train Epoch: [2420/2467]	Loss: 0.232338
         Train Epoch: [2420/2467]	Loss: 0.030180
 Train Epoch: [2420/2467]	Loss: 0.091865
     Train Epoch: [2420/2467]	Loss: 0.388413
             Train Epoch: [2440/2467]	Loss: 0.127409
  Train Epoch: [2440/2467]	Loss: 0.280348
Train Epoch: [2440/2467]	Loss: 0.024153
     Train Epoch: [2440/2467]	Loss: 0.119000
         Train Epoch: [2460/2467]	Loss: 0.099956
 Train Epoch: [2460/2467]	Loss: 0.124174
         Train Epoch: [2460/2467]	Loss: 0.090095
 Train Epoch: [2460/2467]	Loss: 0.286923
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 30 epoch =====
     2025-05-11.10-58-17
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 30 epoch =====
     2025-05-11.10-58-17
     ===== running 30 epoch =====
     2025-05-11.10-58-17
after set grad
after prog
start loop
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 30 epoch =====
     2025-05-11.10-58-18
after set grad
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
             Train Epoch: [0/2467]	Loss: 0.176943
  Train Epoch: [0/2467]	Loss: 0.102076
Train Epoch: [0/2467]	Loss: 0.222462
     Train Epoch: [0/2467]	Loss: 0.269235
     Train Epoch: [20/2467]	Loss: 0.043953    
     Train Epoch: [20/2467]	Loss: 0.318536
 Train Epoch: [20/2467]	Loss: 0.131497
     Train Epoch: [20/2467]	Loss: 0.100004
     Train Epoch: [40/2467]	Loss: 0.099457
         Train Epoch: [40/2467]	Loss: 0.129429
 Train Epoch: [40/2467]	Loss: 0.363965    
 Train Epoch: [40/2467]	Loss: 0.106094
     Train Epoch: [60/2467]	Loss: 0.254994
         Train Epoch: [60/2467]	Loss: 0.027895
 Train Epoch: [60/2467]	Loss: 0.072609
     Train Epoch: [60/2467]	Loss: 0.152017
         Train Epoch: [80/2467]	Loss: 0.263860
     Train Epoch: [80/2467]	Loss: 0.073982
 Train Epoch: [80/2467]	Loss: 0.112932
     Train Epoch: [80/2467]	Loss: 0.114077
         Train Epoch: [100/2467]	Loss: 0.155491
 Train Epoch: [100/2467]	Loss: 0.202998
     Train Epoch: [100/2467]	Loss: 0.087178
     Train Epoch: [100/2467]	Loss: 0.079531
     Train Epoch: [120/2467]	Loss: 0.057901
     Train Epoch: [120/2467]	Loss: 0.345010
     Train Epoch: [120/2467]	Loss: 0.214028
     Train Epoch: [120/2467]	Loss: 0.077069
     Train Epoch: [140/2467]	Loss: 0.172208
         Train Epoch: [140/2467]	Loss: 0.144088
 Train Epoch: [140/2467]	Loss: 0.028217
     Train Epoch: [140/2467]	Loss: 0.058315
         Train Epoch: [160/2467]	Loss: 0.237576
 Train Epoch: [160/2467]	Loss: 0.104277
          Train Epoch: [160/2467]	Loss: 0.095738
Train Epoch: [160/2467]	Loss: 0.119826
     Train Epoch: [180/2467]	Loss: 0.176455
          Train Epoch: [180/2467]	Loss: 0.082269Train Epoch: [180/2467]	Loss: 0.127430

     Train Epoch: [180/2467]	Loss: 0.134452
     Train Epoch: [200/2467]	Loss: 0.115166
              Train Epoch: [200/2467]	Loss: 0.048029Train Epoch: [200/2467]	Loss: 0.052160 

Train Epoch: [200/2467]	Loss: 0.065659
             Train Epoch: [220/2467]	Loss: 0.129164 
Train Epoch: [220/2467]	Loss: 0.053425
 Train Epoch: [220/2467]	Loss: 0.207890
     Train Epoch: [220/2467]	Loss: 0.153346
         Train Epoch: [240/2467]	Loss: 0.166213
     Train Epoch: [240/2467]	Loss: 0.018115
 Train Epoch: [240/2467]	Loss: 0.222886
     Train Epoch: [240/2467]	Loss: 0.040220
     Train Epoch: [260/2467]	Loss: 0.030372
         Train Epoch: [260/2467]	Loss: 0.049471
     Train Epoch: [260/2467]	Loss: 0.053409
 Train Epoch: [260/2467]	Loss: 0.074870
             Train Epoch: [280/2467]	Loss: 0.245251
  Train Epoch: [280/2467]	Loss: 0.093131Train Epoch: [280/2467]	Loss: 0.207803

     Train Epoch: [280/2467]	Loss: 0.142245
          Train Epoch: [300/2467]	Loss: 0.169430Train Epoch: [300/2467]	Loss: 0.314896

     Train Epoch: [300/2467]	Loss: 0.032452    
 Train Epoch: [300/2467]	Loss: 0.068580
          Train Epoch: [320/2467]	Loss: 0.170831
Train Epoch: [320/2467]	Loss: 0.152177    
     Train Epoch: [320/2467]	Loss: 0.231226
 Train Epoch: [320/2467]	Loss: 0.388238
                    Train Epoch: [340/2467]	Loss: 0.014115
Train Epoch: [340/2467]	Loss: 0.045142
Train Epoch: [340/2467]	Loss: 0.069977
Train Epoch: [340/2467]	Loss: 0.165280
         Train Epoch: [360/2467]	Loss: 0.193128
 Train Epoch: [360/2467]	Loss: 0.067376
          Train Epoch: [360/2467]	Loss: 0.142769
Train Epoch: [360/2467]	Loss: 0.057188
     Train Epoch: [380/2467]	Loss: 0.362148    
     Train Epoch: [380/2467]	Loss: 0.077237
 Train Epoch: [380/2467]	Loss: 0.111838
     Train Epoch: [380/2467]	Loss: 0.207473
         Train Epoch: [400/2467]	Loss: 0.047219 
Train Epoch: [400/2467]	Loss: 0.148643
         Train Epoch: [400/2467]	Loss: 0.156297
 Train Epoch: [400/2467]	Loss: 0.097000
         Train Epoch: [420/2467]	Loss: 0.097185
 Train Epoch: [420/2467]	Loss: 0.281981
     Train Epoch: [420/2467]	Loss: 0.122909
     Train Epoch: [420/2467]	Loss: 0.210640
     Train Epoch: [440/2467]	Loss: 0.332761
         Train Epoch: [440/2467]	Loss: 0.083117
     Train Epoch: [440/2467]	Loss: 0.049611
 Train Epoch: [440/2467]	Loss: 0.133493
         Train Epoch: [460/2467]	Loss: 0.116789
 Train Epoch: [460/2467]	Loss: 0.096380
     Train Epoch: [460/2467]	Loss: 0.146094
     Train Epoch: [460/2467]	Loss: 0.224691
     Train Epoch: [480/2467]	Loss: 0.277727
         Train Epoch: [480/2467]	Loss: 0.112971 
Train Epoch: [480/2467]	Loss: 0.146520
     Train Epoch: [480/2467]	Loss: 0.341352
         Train Epoch: [500/2467]	Loss: 0.087084
 Train Epoch: [500/2467]	Loss: 0.112633
     Train Epoch: [500/2467]	Loss: 0.200516
     Train Epoch: [500/2467]	Loss: 0.080792
     Train Epoch: [520/2467]	Loss: 0.121965
         Train Epoch: [520/2467]	Loss: 0.254198
 Train Epoch: [520/2467]	Loss: 0.171937
     Train Epoch: [520/2467]	Loss: 0.092801
         Train Epoch: [540/2467]	Loss: 0.058898
     Train Epoch: [540/2467]	Loss: 0.156425
     Train Epoch: [540/2467]	Loss: 0.240834
 Train Epoch: [540/2467]	Loss: 0.111697
     Train Epoch: [560/2467]	Loss: 0.119324
         Train Epoch: [560/2467]	Loss: 0.158530
     Train Epoch: [560/2467]	Loss: 0.135975
 Train Epoch: [560/2467]	Loss: 0.124871
     Train Epoch: [580/2467]	Loss: 0.250205
         Train Epoch: [580/2467]	Loss: 0.127575
 Train Epoch: [580/2467]	Loss: 0.134897
     Train Epoch: [580/2467]	Loss: 0.056317
     Train Epoch: [600/2467]	Loss: 0.218512
     Train Epoch: [600/2467]	Loss: 0.115422
     Train Epoch: [600/2467]	Loss: 0.218261
     Train Epoch: [600/2467]	Loss: 0.163422
         Train Epoch: [620/2467]	Loss: 0.148674
 Train Epoch: [620/2467]	Loss: 0.027998
     Train Epoch: [620/2467]	Loss: 0.195215
     Train Epoch: [620/2467]	Loss: 0.155934
          Train Epoch: [640/2467]	Loss: 0.039790Train Epoch: [640/2467]	Loss: 0.174719
    
 Train Epoch: [640/2467]	Loss: 0.161478    
 Train Epoch: [640/2467]	Loss: 0.146642
         Train Epoch: [660/2467]	Loss: 0.341679
     Train Epoch: [660/2467]	Loss: 0.043285    
 Train Epoch: [660/2467]	Loss: 0.023196 
Train Epoch: [660/2467]	Loss: 0.171782
     Train Epoch: [680/2467]	Loss: 0.070802
     Train Epoch: [680/2467]	Loss: 0.033267
     Train Epoch: [680/2467]	Loss: 0.050231
     Train Epoch: [680/2467]	Loss: 0.118967
         Train Epoch: [700/2467]	Loss: 0.075717         
Train Epoch: [700/2467]	Loss: 0.039817
  Train Epoch: [700/2467]	Loss: 0.297642
Train Epoch: [700/2467]	Loss: 0.316663
     Train Epoch: [720/2467]	Loss: 0.116464
         Train Epoch: [720/2467]	Loss: 0.181005
 Train Epoch: [720/2467]	Loss: 0.072095
     Train Epoch: [720/2467]	Loss: 0.017145
         Train Epoch: [740/2467]	Loss: 0.053278
 Train Epoch: [740/2467]	Loss: 0.339648    
 Train Epoch: [740/2467]	Loss: 0.087689
     Train Epoch: [740/2467]	Loss: 0.075058
     Train Epoch: [760/2467]	Loss: 0.349868
     Train Epoch: [760/2467]	Loss: 0.258168
     Train Epoch: [760/2467]	Loss: 0.286178
     Train Epoch: [760/2467]	Loss: 0.052437
     Train Epoch: [780/2467]	Loss: 0.356608
         Train Epoch: [780/2467]	Loss: 0.292972
     Train Epoch: [780/2467]	Loss: 0.378547
 Train Epoch: [780/2467]	Loss: 0.125207
     Train Epoch: [800/2467]	Loss: 0.145044
     Train Epoch: [800/2467]	Loss: 0.174206
     Train Epoch: [800/2467]	Loss: 0.093047
     Train Epoch: [800/2467]	Loss: 0.150964
         Train Epoch: [820/2467]	Loss: 0.323296    
 Train Epoch: [820/2467]	Loss: 0.038141 
Train Epoch: [820/2467]	Loss: 0.399526
     Train Epoch: [820/2467]	Loss: 0.042944
         Train Epoch: [840/2467]	Loss: 0.114233
     Train Epoch: [840/2467]	Loss: 0.289052
 Train Epoch: [840/2467]	Loss: 0.054845    
 Train Epoch: [840/2467]	Loss: 0.098278
             Train Epoch: [860/2467]	Loss: 0.048388 
 Train Epoch: [860/2467]	Loss: 0.184404Train Epoch: [860/2467]	Loss: 0.226651

     Train Epoch: [860/2467]	Loss: 0.078207
             Train Epoch: [880/2467]	Loss: 0.200441 Train Epoch: [880/2467]	Loss: 0.219841

 Train Epoch: [880/2467]	Loss: 0.123253
     Train Epoch: [880/2467]	Loss: 0.145858
     Train Epoch: [900/2467]	Loss: 0.198313
          Train Epoch: [900/2467]	Loss: 0.087549Train Epoch: [900/2467]	Loss: 0.192763

     Train Epoch: [900/2467]	Loss: 0.177299
     Train Epoch: [920/2467]	Loss: 0.270828    
 Train Epoch: [920/2467]	Loss: 0.337236
         Train Epoch: [920/2467]	Loss: 0.243183 
Train Epoch: [920/2467]	Loss: 0.041851
         Train Epoch: [940/2467]	Loss: 0.080255
     Train Epoch: [940/2467]	Loss: 0.037687
 Train Epoch: [940/2467]	Loss: 0.163510
     Train Epoch: [940/2467]	Loss: 0.044285
     Train Epoch: [960/2467]	Loss: 0.066464    
 Train Epoch: [960/2467]	Loss: 0.101382    
 Train Epoch: [960/2467]	Loss: 0.091267
     Train Epoch: [960/2467]	Loss: 0.256600
         Train Epoch: [980/2467]	Loss: 0.086717
 Train Epoch: [980/2467]	Loss: 0.051115
         Train Epoch: [980/2467]	Loss: 0.111178
 Train Epoch: [980/2467]	Loss: 0.201590
         Train Epoch: [1000/2467]	Loss: 0.042669
 Train Epoch: [1000/2467]	Loss: 0.218321
         Train Epoch: [1000/2467]	Loss: 0.192620
 Train Epoch: [1000/2467]	Loss: 0.036504
     Train Epoch: [1020/2467]	Loss: 0.144719
         Train Epoch: [1020/2467]	Loss: 0.054298
 Train Epoch: [1020/2467]	Loss: 0.364915
     Train Epoch: [1020/2467]	Loss: 0.066551
     Train Epoch: [1040/2467]	Loss: 0.216737
     Train Epoch: [1040/2467]	Loss: 0.152604
     Train Epoch: [1040/2467]	Loss: 0.098070
     Train Epoch: [1040/2467]	Loss: 0.106336
     Train Epoch: [1060/2467]	Loss: 0.164194
              Train Epoch: [1060/2467]	Loss: 0.094204Train Epoch: [1060/2467]	Loss: 0.371126

 Train Epoch: [1060/2467]	Loss: 0.097707
     Train Epoch: [1080/2467]	Loss: 0.103650
         Train Epoch: [1080/2467]	Loss: 0.171018
 Train Epoch: [1080/2467]	Loss: 0.244172
     Train Epoch: [1080/2467]	Loss: 0.228161
     Train Epoch: [1100/2467]	Loss: 0.212556
     Train Epoch: [1100/2467]	Loss: 0.234591
          Train Epoch: [1100/2467]	Loss: 0.207425Train Epoch: [1100/2467]	Loss: 0.262979

     Train Epoch: [1120/2467]	Loss: 0.094155
     Train Epoch: [1120/2467]	Loss: 0.136608
     Train Epoch: [1120/2467]	Loss: 0.253396
     Train Epoch: [1120/2467]	Loss: 0.124081
         Train Epoch: [1140/2467]	Loss: 0.266832 
Train Epoch: [1140/2467]	Loss: 0.067922
     Train Epoch: [1140/2467]	Loss: 0.371276
     Train Epoch: [1140/2467]	Loss: 0.048356
         Train Epoch: [1160/2467]	Loss: 0.229815
     Train Epoch: [1160/2467]	Loss: 0.060904
 Train Epoch: [1160/2467]	Loss: 0.107035
     Train Epoch: [1160/2467]	Loss: 0.121541
     Train Epoch: [1180/2467]	Loss: 0.051537
     Train Epoch: [1180/2467]	Loss: 0.129124
     Train Epoch: [1180/2467]	Loss: 0.098588
     Train Epoch: [1180/2467]	Loss: 0.158676
         Train Epoch: [1200/2467]	Loss: 0.286432
 Train Epoch: [1200/2467]	Loss: 0.057594
     Train Epoch: [1200/2467]	Loss: 0.111607
     Train Epoch: [1200/2467]	Loss: 0.225422
          Train Epoch: [1220/2467]	Loss: 0.236622
Train Epoch: [1220/2467]	Loss: 0.195332    
 Train Epoch: [1220/2467]	Loss: 0.089290
     Train Epoch: [1220/2467]	Loss: 0.221916
         Train Epoch: [1240/2467]	Loss: 0.146592 
Train Epoch: [1240/2467]	Loss: 0.180468    
 Train Epoch: [1240/2467]	Loss: 0.258335
     Train Epoch: [1240/2467]	Loss: 0.254614
              Train Epoch: [1260/2467]	Loss: 0.067901Train Epoch: [1260/2467]	Loss: 0.117894

 Train Epoch: [1260/2467]	Loss: 0.054934
     Train Epoch: [1260/2467]	Loss: 0.374373
         Train Epoch: [1280/2467]	Loss: 0.058189
 Train Epoch: [1280/2467]	Loss: 0.180611
     Train Epoch: [1280/2467]	Loss: 0.400910
     Train Epoch: [1280/2467]	Loss: 0.233061
         Train Epoch: [1300/2467]	Loss: 0.060761    
 Train Epoch: [1300/2467]	Loss: 0.093333
 Train Epoch: [1300/2467]	Loss: 0.274136
     Train Epoch: [1300/2467]	Loss: 0.247911
     Train Epoch: [1320/2467]	Loss: 0.044036
             Train Epoch: [1320/2467]	Loss: 0.240001 
Train Epoch: [1320/2467]	Loss: 0.130922
 Train Epoch: [1320/2467]	Loss: 0.200999
         Train Epoch: [1340/2467]	Loss: 0.297270
     Train Epoch: [1340/2467]	Loss: 0.223252
     Train Epoch: [1340/2467]	Loss: 0.162139
 Train Epoch: [1340/2467]	Loss: 0.184743
             Train Epoch: [1360/2467]	Loss: 0.084788
  Train Epoch: [1360/2467]	Loss: 0.134190Train Epoch: [1360/2467]	Loss: 0.125794

     Train Epoch: [1360/2467]	Loss: 0.072866
         Train Epoch: [1380/2467]	Loss: 0.257099
 Train Epoch: [1380/2467]	Loss: 0.208653    
     Train Epoch: [1380/2467]	Loss: 0.265927
 Train Epoch: [1380/2467]	Loss: 0.204479
             Train Epoch: [1400/2467]	Loss: 0.091748
      Train Epoch: [1400/2467]	Loss: 0.165781
Train Epoch: [1400/2467]	Loss: 0.168685
 Train Epoch: [1400/2467]	Loss: 0.086452
         Train Epoch: [1420/2467]	Loss: 0.121987
     Train Epoch: [1420/2467]	Loss: 0.342364
 Train Epoch: [1420/2467]	Loss: 0.076571
     Train Epoch: [1420/2467]	Loss: 0.148373
         Train Epoch: [1440/2467]	Loss: 0.093605    
     Train Epoch: [1440/2467]	Loss: 0.018221
 Train Epoch: [1440/2467]	Loss: 0.095835
 Train Epoch: [1440/2467]	Loss: 0.040841
     Train Epoch: [1460/2467]	Loss: 0.107922    
 Train Epoch: [1460/2467]	Loss: 0.042125
         Train Epoch: [1460/2467]	Loss: 0.250099
 Train Epoch: [1460/2467]	Loss: 0.184372
             Train Epoch: [1480/2467]	Loss: 0.028442
     Train Epoch: [1480/2467]	Loss: 0.228506 
Train Epoch: [1480/2467]	Loss: 0.173533
 Train Epoch: [1480/2467]	Loss: 0.012369
     Train Epoch: [1500/2467]	Loss: 0.132272
         Train Epoch: [1500/2467]	Loss: 0.069130
     Train Epoch: [1500/2467]	Loss: 0.096772
 Train Epoch: [1500/2467]	Loss: 0.014850
     Train Epoch: [1520/2467]	Loss: 0.200387
     Train Epoch: [1520/2467]	Loss: 0.241857
         Train Epoch: [1520/2467]	Loss: 0.234832
 Train Epoch: [1520/2467]	Loss: 0.136880
         Train Epoch: [1540/2467]	Loss: 0.035012
 Train Epoch: [1540/2467]	Loss: 0.267921
     Train Epoch: [1540/2467]	Loss: 0.132455
     Train Epoch: [1540/2467]	Loss: 0.083998
     Train Epoch: [1560/2467]	Loss: 0.243889
         Train Epoch: [1560/2467]	Loss: 0.313785 
Train Epoch: [1560/2467]	Loss: 0.065542
     Train Epoch: [1560/2467]	Loss: 0.421616
         Train Epoch: [1580/2467]	Loss: 0.264298
     Train Epoch: [1580/2467]	Loss: 0.110348
 Train Epoch: [1580/2467]	Loss: 0.198123
     Train Epoch: [1580/2467]	Loss: 0.084607
          Train Epoch: [1600/2467]	Loss: 0.217127Train Epoch: [1600/2467]	Loss: 0.029365
    
     Train Epoch: [1600/2467]	Loss: 0.193807
 Train Epoch: [1600/2467]	Loss: 0.091564
     Train Epoch: [1620/2467]	Loss: 0.090499
     Train Epoch: [1620/2467]	Loss: 0.168492
     Train Epoch: [1620/2467]	Loss: 0.225073
     Train Epoch: [1620/2467]	Loss: 0.048319
         Train Epoch: [1640/2467]	Loss: 0.210343
     Train Epoch: [1640/2467]	Loss: 0.190943    
  Train Epoch: [1640/2467]	Loss: 0.054075
Train Epoch: [1640/2467]	Loss: 0.129430
              Train Epoch: [1660/2467]	Loss: 0.198594
Train Epoch: [1660/2467]	Loss: 0.055348
     Train Epoch: [1660/2467]	Loss: 0.168183
 Train Epoch: [1660/2467]	Loss: 0.150449
     Train Epoch: [1680/2467]	Loss: 0.100606    
 Train Epoch: [1680/2467]	Loss: 0.046462
          Train Epoch: [1680/2467]	Loss: 0.225664Train Epoch: [1680/2467]	Loss: 0.150983

         Train Epoch: [1700/2467]	Loss: 0.121976     
Train Epoch: [1700/2467]	Loss: 0.272494    
  Train Epoch: [1700/2467]	Loss: 0.073976Train Epoch: [1700/2467]	Loss: 0.124601

     Train Epoch: [1720/2467]	Loss: 0.054608
         Train Epoch: [1720/2467]	Loss: 0.198108
 Train Epoch: [1720/2467]	Loss: 0.104869
     Train Epoch: [1720/2467]	Loss: 0.157590
     Train Epoch: [1740/2467]	Loss: 0.020892
              Train Epoch: [1740/2467]	Loss: 0.270857
Train Epoch: [1740/2467]	Loss: 0.056496 
Train Epoch: [1740/2467]	Loss: 0.201639
     Train Epoch: [1760/2467]	Loss: 0.149835
         Train Epoch: [1760/2467]	Loss: 0.049675
 Train Epoch: [1760/2467]	Loss: 0.084077
     Train Epoch: [1760/2467]	Loss: 0.108556
     Train Epoch: [1780/2467]	Loss: 0.125471
          Train Epoch: [1780/2467]	Loss: 0.084160Train Epoch: [1780/2467]	Loss: 0.255487

     Train Epoch: [1780/2467]	Loss: 0.194792
     Train Epoch: [1800/2467]	Loss: 0.249250
             Train Epoch: [1800/2467]	Loss: 0.152182
  Train Epoch: [1800/2467]	Loss: 0.233536Train Epoch: [1800/2467]	Loss: 0.140649

     Train Epoch: [1820/2467]	Loss: 0.160358
     Train Epoch: [1820/2467]	Loss: 0.290335
         Train Epoch: [1820/2467]	Loss: 0.104324
 Train Epoch: [1820/2467]	Loss: 0.144401
             Train Epoch: [1840/2467]	Loss: 0.122593
 Train Epoch: [1840/2467]	Loss: 0.262078     
Train Epoch: [1840/2467]	Loss: 0.196322
 Train Epoch: [1840/2467]	Loss: 0.045394
     Train Epoch: [1860/2467]	Loss: 0.147533
         Train Epoch: [1860/2467]	Loss: 0.134536
 Train Epoch: [1860/2467]	Loss: 0.100957
     Train Epoch: [1860/2467]	Loss: 0.133150
         Train Epoch: [1880/2467]	Loss: 0.042550 
Train Epoch: [1880/2467]	Loss: 0.030176    
     Train Epoch: [1880/2467]	Loss: 0.094296
 Train Epoch: [1880/2467]	Loss: 0.079321
             Train Epoch: [1900/2467]	Loss: 0.274180
     Train Epoch: [1900/2467]	Loss: 0.100543
  Train Epoch: [1900/2467]	Loss: 0.109508Train Epoch: [1900/2467]	Loss: 0.246674

         Train Epoch: [1920/2467]	Loss: 0.202531
 Train Epoch: [1920/2467]	Loss: 0.198066
     Train Epoch: [1920/2467]	Loss: 0.143366
     Train Epoch: [1920/2467]	Loss: 0.022393
     Train Epoch: [1940/2467]	Loss: 0.132212
         Train Epoch: [1940/2467]	Loss: 0.107471 
Train Epoch: [1940/2467]	Loss: 0.336421    
 Train Epoch: [1940/2467]	Loss: 0.059497
         Train Epoch: [1960/2467]	Loss: 0.110682
 Train Epoch: [1960/2467]	Loss: 0.160171    
      Train Epoch: [1960/2467]	Loss: 0.089823Train Epoch: [1960/2467]	Loss: 0.092825

     Train Epoch: [1980/2467]	Loss: 0.227520
     Train Epoch: [1980/2467]	Loss: 0.186595    
      Train Epoch: [1980/2467]	Loss: 0.106004Train Epoch: [1980/2467]	Loss: 0.226371

     Train Epoch: [2000/2467]	Loss: 0.136932
         Train Epoch: [2000/2467]	Loss: 0.047537
 Train Epoch: [2000/2467]	Loss: 0.014583
     Train Epoch: [2000/2467]	Loss: 0.197306
     Train Epoch: [2020/2467]	Loss: 0.134298
     Train Epoch: [2020/2467]	Loss: 0.123548
     Train Epoch: [2020/2467]	Loss: 0.063602
     Train Epoch: [2020/2467]	Loss: 0.155959
             Train Epoch: [2040/2467]	Loss: 0.366734
  Train Epoch: [2040/2467]	Loss: 0.077820Train Epoch: [2040/2467]	Loss: 0.160938

     Train Epoch: [2040/2467]	Loss: 0.070180
         Train Epoch: [2060/2467]	Loss: 0.204564
 Train Epoch: [2060/2467]	Loss: 0.191136
          Train Epoch: [2060/2467]	Loss: 0.075249
Train Epoch: [2060/2467]	Loss: 0.092671
         Train Epoch: [2080/2467]	Loss: 0.033038
 Train Epoch: [2080/2467]	Loss: 0.062634        
 Train Epoch: [2080/2467]	Loss: 0.213588 
Train Epoch: [2080/2467]	Loss: 0.025044
          Train Epoch: [2100/2467]	Loss: 0.428354Train Epoch: [2100/2467]	Loss: 0.087335

         Train Epoch: [2100/2467]	Loss: 0.102796
 Train Epoch: [2100/2467]	Loss: 0.056565
         Train Epoch: [2120/2467]	Loss: 0.202437
 Train Epoch: [2120/2467]	Loss: 0.087394
     Train Epoch: [2120/2467]	Loss: 0.158361
     Train Epoch: [2120/2467]	Loss: 0.050109
     Train Epoch: [2140/2467]	Loss: 0.178597
         Train Epoch: [2140/2467]	Loss: 0.294321
 Train Epoch: [2140/2467]	Loss: 0.454110
     Train Epoch: [2140/2467]	Loss: 0.229727
         Train Epoch: [2160/2467]	Loss: 0.036591
 Train Epoch: [2160/2467]	Loss: 0.083922
         Train Epoch: [2160/2467]	Loss: 0.299306
 Train Epoch: [2160/2467]	Loss: 0.158062
          Train Epoch: [2180/2467]	Loss: 0.138805
Train Epoch: [2180/2467]	Loss: 0.063309
     Train Epoch: [2180/2467]	Loss: 0.269528
     Train Epoch: [2180/2467]	Loss: 0.142365
     Train Epoch: [2200/2467]	Loss: 0.101903
              Train Epoch: [2200/2467]	Loss: 0.081992
Train Epoch: [2200/2467]	Loss: 0.125526 
Train Epoch: [2200/2467]	Loss: 0.032934
              Train Epoch: [2220/2467]	Loss: 0.057297
Train Epoch: [2220/2467]	Loss: 0.095583 
Train Epoch: [2220/2467]	Loss: 0.153724
     Train Epoch: [2220/2467]	Loss: 0.098662
         Train Epoch: [2240/2467]	Loss: 0.024015
     Train Epoch: [2240/2467]	Loss: 0.068646
 Train Epoch: [2240/2467]	Loss: 0.108033
     Train Epoch: [2240/2467]	Loss: 0.041014
     Train Epoch: [2260/2467]	Loss: 0.297971
     Train Epoch: [2260/2467]	Loss: 0.025140
     Train Epoch: [2260/2467]	Loss: 0.168429
     Train Epoch: [2260/2467]	Loss: 0.182845
     Train Epoch: [2280/2467]	Loss: 0.089527
         Train Epoch: [2280/2467]	Loss: 0.082648
     Train Epoch: [2280/2467]	Loss: 0.088969 
Train Epoch: [2280/2467]	Loss: 0.128148
         Train Epoch: [2300/2467]	Loss: 0.046861
     Train Epoch: [2300/2467]	Loss: 0.057234
 Train Epoch: [2300/2467]	Loss: 0.243083
     Train Epoch: [2300/2467]	Loss: 0.087899
     Train Epoch: [2320/2467]	Loss: 0.164396
         Train Epoch: [2320/2467]	Loss: 0.015193    
 Train Epoch: [2320/2467]	Loss: 0.168340
 Train Epoch: [2320/2467]	Loss: 0.137278
             Train Epoch: [2340/2467]	Loss: 0.108865
  Train Epoch: [2340/2467]	Loss: 0.126314
Train Epoch: [2340/2467]	Loss: 0.237879
     Train Epoch: [2340/2467]	Loss: 0.463278
         Train Epoch: [2360/2467]	Loss: 0.047822
 Train Epoch: [2360/2467]	Loss: 0.336303
     Train Epoch: [2360/2467]	Loss: 0.141712
     Train Epoch: [2360/2467]	Loss: 0.138185
             Train Epoch: [2380/2467]	Loss: 0.183801
  Train Epoch: [2380/2467]	Loss: 0.395718Train Epoch: [2380/2467]	Loss: 0.069808

     Train Epoch: [2380/2467]	Loss: 0.178590
     Train Epoch: [2400/2467]	Loss: 0.153867
         Train Epoch: [2400/2467]	Loss: 0.101394
 Train Epoch: [2400/2467]	Loss: 0.143585
     Train Epoch: [2400/2467]	Loss: 0.087677
         Train Epoch: [2420/2467]	Loss: 0.031466
     Train Epoch: [2420/2467]	Loss: 0.137646
 Train Epoch: [2420/2467]	Loss: 0.258224
     Train Epoch: [2420/2467]	Loss: 0.212852
         Train Epoch: [2440/2467]	Loss: 0.175121     
Train Epoch: [2440/2467]	Loss: 0.022246
 Train Epoch: [2440/2467]	Loss: 0.252053
     Train Epoch: [2440/2467]	Loss: 0.104098
         Train Epoch: [2460/2467]	Loss: 0.132565 
    Train Epoch: [2460/2467]	Loss: 0.080877    
  Train Epoch: [2460/2467]	Loss: 0.107797
Train Epoch: [2460/2467]	Loss: 0.224810
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 31 epoch =====
     2025-05-11.11-20-07
after set grad
after prog
start loop
     ===== running 31 epoch =====
     2025-05-11.11-20-07
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 31 epoch =====
     2025-05-11.11-20-07
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 31 epoch =====
     2025-05-11.11-20-07
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.166712
         Train Epoch: [0/2467]	Loss: 0.088641 
Train Epoch: [0/2467]	Loss: 0.220326
     Train Epoch: [0/2467]	Loss: 0.256355
     Train Epoch: [20/2467]	Loss: 0.081452
     Train Epoch: [20/2467]	Loss: 0.316440
     Train Epoch: [20/2467]	Loss: 0.148904
     Train Epoch: [20/2467]	Loss: 0.091305
         Train Epoch: [40/2467]	Loss: 0.133107
     Train Epoch: [40/2467]	Loss: 0.149045
 Train Epoch: [40/2467]	Loss: 0.357314
     Train Epoch: [40/2467]	Loss: 0.116112
         Train Epoch: [60/2467]	Loss: 0.145850
 Train Epoch: [60/2467]	Loss: 0.030039
     Train Epoch: [60/2467]	Loss: 0.093896
     Train Epoch: [60/2467]	Loss: 0.158858
     Train Epoch: [80/2467]	Loss: 0.252141
          Train Epoch: [80/2467]	Loss: 0.100018Train Epoch: [80/2467]	Loss: 0.111906

     Train Epoch: [80/2467]	Loss: 0.076286
         Train Epoch: [100/2467]	Loss: 0.101580
     Train Epoch: [100/2467]	Loss: 0.157886
     Train Epoch: [100/2467]	Loss: 0.097362
 Train Epoch: [100/2467]	Loss: 0.168885
         Train Epoch: [120/2467]	Loss: 0.374703
 Train Epoch: [120/2467]	Loss: 0.082290    
 Train Epoch: [120/2467]	Loss: 0.212465
     Train Epoch: [120/2467]	Loss: 0.047517
         Train Epoch: [140/2467]	Loss: 0.168435
 Train Epoch: [140/2467]	Loss: 0.032971
     Train Epoch: [140/2467]	Loss: 0.054938
     Train Epoch: [140/2467]	Loss: 0.120189
     Train Epoch: [160/2467]	Loss: 0.186497
          Train Epoch: [160/2467]	Loss: 0.114155Train Epoch: [160/2467]	Loss: 0.089670

     Train Epoch: [160/2467]	Loss: 0.129798
     Train Epoch: [180/2467]	Loss: 0.117522
          Train Epoch: [180/2467]	Loss: 0.168547
Train Epoch: [180/2467]	Loss: 0.100529
     Train Epoch: [180/2467]	Loss: 0.064850
          Train Epoch: [200/2467]	Loss: 0.070528Train Epoch: [200/2467]	Loss: 0.072988

     Train Epoch: [200/2467]	Loss: 0.055599
     Train Epoch: [200/2467]	Loss: 0.116893
         Train Epoch: [220/2467]	Loss: 0.143318    
 Train Epoch: [220/2467]	Loss: 0.109332 
Train Epoch: [220/2467]	Loss: 0.132681
     Train Epoch: [220/2467]	Loss: 0.212187
         Train Epoch: [240/2467]	Loss: 0.018894
 Train Epoch: [240/2467]	Loss: 0.211796
     Train Epoch: [240/2467]	Loss: 0.040950    
 Train Epoch: [240/2467]	Loss: 0.155135
         Train Epoch: [260/2467]	Loss: 0.049067
 Train Epoch: [260/2467]	Loss: 0.033766    
      Train Epoch: [260/2467]	Loss: 0.069512Train Epoch: [260/2467]	Loss: 0.070992

         Train Epoch: [280/2467]	Loss: 0.137887 
Train Epoch: [280/2467]	Loss: 0.176782
     Train Epoch: [280/2467]	Loss: 0.088429
     Train Epoch: [280/2467]	Loss: 0.100722
     Train Epoch: [300/2467]	Loss: 0.310125    
 Train Epoch: [300/2467]	Loss: 0.033283
     Train Epoch: [300/2467]	Loss: 0.070135
     Train Epoch: [300/2467]	Loss: 0.177260
     Train Epoch: [320/2467]	Loss: 0.151650
     Train Epoch: [320/2467]	Loss: 0.394663    
 Train Epoch: [320/2467]	Loss: 0.240116
     Train Epoch: [320/2467]	Loss: 0.159029
     Train Epoch: [340/2467]	Loss: 0.016763
         Train Epoch: [340/2467]	Loss: 0.054603
     Train Epoch: [340/2467]	Loss: 0.060410
 Train Epoch: [340/2467]	Loss: 0.174070
     Train Epoch: [360/2467]	Loss: 0.055881
         Train Epoch: [360/2467]	Loss: 0.208587 
Train Epoch: [360/2467]	Loss: 0.049350
     Train Epoch: [360/2467]	Loss: 0.156515
              Train Epoch: [380/2467]	Loss: 0.082891
Train Epoch: [380/2467]	Loss: 0.325041
 Train Epoch: [380/2467]	Loss: 0.089719
     Train Epoch: [380/2467]	Loss: 0.175369
     Train Epoch: [400/2467]	Loss: 0.062328
         Train Epoch: [400/2467]	Loss: 0.152705
 Train Epoch: [400/2467]	Loss: 0.177959
     Train Epoch: [400/2467]	Loss: 0.075559
     Train Epoch: [420/2467]	Loss: 0.232800
         Train Epoch: [420/2467]	Loss: 0.272750
 Train Epoch: [420/2467]	Loss: 0.133003
     Train Epoch: [420/2467]	Loss: 0.102092
     Train Epoch: [440/2467]	Loss: 0.344081
         Train Epoch: [440/2467]	Loss: 0.076149 
Train Epoch: [440/2467]	Loss: 0.138355
     Train Epoch: [440/2467]	Loss: 0.049130
         Train Epoch: [460/2467]	Loss: 0.216914    
     Train Epoch: [460/2467]	Loss: 0.149466 
Train Epoch: [460/2467]	Loss: 0.080474
 Train Epoch: [460/2467]	Loss: 0.108767
     Train Epoch: [480/2467]	Loss: 0.282657    
     Train Epoch: [480/2467]	Loss: 0.043225
 Train Epoch: [480/2467]	Loss: 0.139278
     Train Epoch: [480/2467]	Loss: 0.324137
          Train Epoch: [500/2467]	Loss: 0.341103Train Epoch: [500/2467]	Loss: 0.085236

         Train Epoch: [500/2467]	Loss: 0.168808
 Train Epoch: [500/2467]	Loss: 0.111697
             Train Epoch: [520/2467]	Loss: 0.112764
 Train Epoch: [520/2467]	Loss: 0.253243 
Train Epoch: [520/2467]	Loss: 0.104344
     Train Epoch: [520/2467]	Loss: 0.074476
     Train Epoch: [540/2467]	Loss: 0.158342
         Train Epoch: [540/2467]	Loss: 0.214197
     Train Epoch: [540/2467]	Loss: 0.060028
 Train Epoch: [540/2467]	Loss: 0.077610
         Train Epoch: [560/2467]	Loss: 0.117573 
    Train Epoch: [560/2467]	Loss: 0.129424
     Train Epoch: [560/2467]	Loss: 0.265162
 Train Epoch: [560/2467]	Loss: 0.130168
         Train Epoch: [580/2467]	Loss: 0.236392
 Train Epoch: [580/2467]	Loss: 0.158439    
 Train Epoch: [580/2467]	Loss: 0.054204
     Train Epoch: [580/2467]	Loss: 0.150825
             Train Epoch: [600/2467]	Loss: 0.151068
 Train Epoch: [600/2467]	Loss: 0.107564    
 Train Epoch: [600/2467]	Loss: 0.123486
 Train Epoch: [600/2467]	Loss: 0.194808
         Train Epoch: [620/2467]	Loss: 0.138050
     Train Epoch: [620/2467]	Loss: 0.124273
 Train Epoch: [620/2467]	Loss: 0.024922    
 Train Epoch: [620/2467]	Loss: 0.159874
     Train Epoch: [640/2467]	Loss: 0.035667
          Train Epoch: [640/2467]	Loss: 0.168713Train Epoch: [640/2467]	Loss: 0.138800

     Train Epoch: [640/2467]	Loss: 0.110509
     Train Epoch: [660/2467]	Loss: 0.372867
     Train Epoch: [660/2467]	Loss: 0.035635
         Train Epoch: [660/2467]	Loss: 0.024645
 Train Epoch: [660/2467]	Loss: 0.139997
     Train Epoch: [680/2467]	Loss: 0.263628
     Train Epoch: [680/2467]	Loss: 0.040444
     Train Epoch: [680/2467]	Loss: 0.039581
     Train Epoch: [680/2467]	Loss: 0.106234
         Train Epoch: [700/2467]	Loss: 0.038414
 Train Epoch: [700/2467]	Loss: 0.065721
         Train Epoch: [700/2467]	Loss: 0.249614
 Train Epoch: [700/2467]	Loss: 0.321672
          Train Epoch: [720/2467]	Loss: 0.119301
Train Epoch: [720/2467]	Loss: 0.014335
     Train Epoch: [720/2467]	Loss: 0.177660
     Train Epoch: [720/2467]	Loss: 0.073454
                 Train Epoch: [740/2467]	Loss: 0.292258
 Train Epoch: [740/2467]	Loss: 0.071525
 Train Epoch: [740/2467]	Loss: 0.112004 
Train Epoch: [740/2467]	Loss: 0.085120
         Train Epoch: [760/2467]	Loss: 0.349561
     Train Epoch: [760/2467]	Loss: 0.047038
 Train Epoch: [760/2467]	Loss: 0.219986
     Train Epoch: [760/2467]	Loss: 0.313910
         Train Epoch: [780/2467]	Loss: 0.304679
 Train Epoch: [780/2467]	Loss: 0.279287
     Train Epoch: [780/2467]	Loss: 0.367578
     Train Epoch: [780/2467]	Loss: 0.106255
     Train Epoch: [800/2467]	Loss: 0.177898    
      Train Epoch: [800/2467]	Loss: 0.180668Train Epoch: [800/2467]	Loss: 0.149412

     Train Epoch: [800/2467]	Loss: 0.104817
     Train Epoch: [820/2467]	Loss: 0.371801
          Train Epoch: [820/2467]	Loss: 0.398791Train Epoch: [820/2467]	Loss: 0.051355

     Train Epoch: [820/2467]	Loss: 0.057632
          Train Epoch: [840/2467]	Loss: 0.115847
Train Epoch: [840/2467]	Loss: 0.134573
          Train Epoch: [840/2467]	Loss: 0.308835Train Epoch: [840/2467]	Loss: 0.057335

              Train Epoch: [860/2467]	Loss: 0.043524Train Epoch: [860/2467]	Loss: 0.280840

     Train Epoch: [860/2467]	Loss: 0.191773
 Train Epoch: [860/2467]	Loss: 0.057647
         Train Epoch: [880/2467]	Loss: 0.229704 
Train Epoch: [880/2467]	Loss: 0.136982    
 Train Epoch: [880/2467]	Loss: 0.120177
     Train Epoch: [880/2467]	Loss: 0.202028
     Train Epoch: [900/2467]	Loss: 0.171223
     Train Epoch: [900/2467]	Loss: 0.080609
     Train Epoch: [900/2467]	Loss: 0.181249
     Train Epoch: [900/2467]	Loss: 0.162853
         Train Epoch: [920/2467]	Loss: 0.301014
 Train Epoch: [920/2467]	Loss: 0.247589
          Train Epoch: [920/2467]	Loss: 0.038912Train Epoch: [920/2467]	Loss: 0.229848

             Train Epoch: [940/2467]	Loss: 0.049332
  Train Epoch: [940/2467]	Loss: 0.099973Train Epoch: [940/2467]	Loss: 0.173923
    
 Train Epoch: [940/2467]	Loss: 0.041376
              Train Epoch: [960/2467]	Loss: 0.253218
Train Epoch: [960/2467]	Loss: 0.057012
 Train Epoch: [960/2467]	Loss: 0.074335
     Train Epoch: [960/2467]	Loss: 0.087524
     Train Epoch: [980/2467]	Loss: 0.084830
          Train Epoch: [980/2467]	Loss: 0.042234Train Epoch: [980/2467]	Loss: 0.231271
    
 Train Epoch: [980/2467]	Loss: 0.119686
     Train Epoch: [1000/2467]	Loss: 0.040072
         Train Epoch: [1000/2467]	Loss: 0.247371
     Train Epoch: [1000/2467]	Loss: 0.232372
 Train Epoch: [1000/2467]	Loss: 0.029002
         Train Epoch: [1020/2467]	Loss: 0.043668
 Train Epoch: [1020/2467]	Loss: 0.136692
     Train Epoch: [1020/2467]	Loss: 0.342381
     Train Epoch: [1020/2467]	Loss: 0.067524
     Train Epoch: [1040/2467]	Loss: 0.299060
         Train Epoch: [1040/2467]	Loss: 0.149532
 Train Epoch: [1040/2467]	Loss: 0.116734
     Train Epoch: [1040/2467]	Loss: 0.114395
         Train Epoch: [1060/2467]	Loss: 0.385301
     Train Epoch: [1060/2467]	Loss: 0.156723
 Train Epoch: [1060/2467]	Loss: 0.109770
     Train Epoch: [1060/2467]	Loss: 0.094664
     Train Epoch: [1080/2467]	Loss: 0.120805
         Train Epoch: [1080/2467]	Loss: 0.197634
 Train Epoch: [1080/2467]	Loss: 0.256338
     Train Epoch: [1080/2467]	Loss: 0.109522
     Train Epoch: [1100/2467]	Loss: 0.177628
     Train Epoch: [1100/2467]	Loss: 0.260601
     Train Epoch: [1100/2467]	Loss: 0.265486
     Train Epoch: [1100/2467]	Loss: 0.195442
             Train Epoch: [1120/2467]	Loss: 0.114967
 Train Epoch: [1120/2467]	Loss: 0.196682 
Train Epoch: [1120/2467]	Loss: 0.122308
     Train Epoch: [1120/2467]	Loss: 0.127449
     Train Epoch: [1140/2467]	Loss: 0.058259
         Train Epoch: [1140/2467]	Loss: 0.288923
     Train Epoch: [1140/2467]	Loss: 0.363692
 Train Epoch: [1140/2467]	Loss: 0.070814
          Train Epoch: [1160/2467]	Loss: 0.092746
Train Epoch: [1160/2467]	Loss: 0.063255
         Train Epoch: [1160/2467]	Loss: 0.215038
 Train Epoch: [1160/2467]	Loss: 0.120790
     Train Epoch: [1180/2467]	Loss: 0.053489
     Train Epoch: [1180/2467]	Loss: 0.183765
         Train Epoch: [1180/2467]	Loss: 0.123551
 Train Epoch: [1180/2467]	Loss: 0.097479
     Train Epoch: [1200/2467]	Loss: 0.280835
         Train Epoch: [1200/2467]	Loss: 0.119138
 Train Epoch: [1200/2467]	Loss: 0.246692
     Train Epoch: [1200/2467]	Loss: 0.073726
          Train Epoch: [1220/2467]	Loss: 0.233892Train Epoch: [1220/2467]	Loss: 0.196856

         Train Epoch: [1220/2467]	Loss: 0.078312
 Train Epoch: [1220/2467]	Loss: 0.221039
         Train Epoch: [1240/2467]	Loss: 0.247493
 Train Epoch: [1240/2467]	Loss: 0.115575
         Train Epoch: [1240/2467]	Loss: 0.252224
 Train Epoch: [1240/2467]	Loss: 0.129566
              Train Epoch: [1260/2467]	Loss: 0.143357Train Epoch: [1260/2467]	Loss: 0.055464
 
Train Epoch: [1260/2467]	Loss: 0.056593
     Train Epoch: [1260/2467]	Loss: 0.381588
         Train Epoch: [1280/2467]	Loss: 0.053032
     Train Epoch: [1280/2467]	Loss: 0.190128    
 Train Epoch: [1280/2467]	Loss: 0.487560 
Train Epoch: [1280/2467]	Loss: 0.454486
         Train Epoch: [1300/2467]	Loss: 0.068399
         Train Epoch: [1300/2467]	Loss: 0.266186
  Train Epoch: [1300/2467]	Loss: 0.263005
Train Epoch: [1300/2467]	Loss: 0.048688
         Train Epoch: [1320/2467]	Loss: 0.043472
     Train Epoch: [1320/2467]	Loss: 0.105305
     Train Epoch: [1320/2467]	Loss: 0.235912
 Train Epoch: [1320/2467]	Loss: 0.164752
          Train Epoch: [1340/2467]	Loss: 0.096066Train Epoch: [1340/2467]	Loss: 0.300506

     Train Epoch: [1340/2467]	Loss: 0.147784
     Train Epoch: [1340/2467]	Loss: 0.204147
               Train Epoch: [1360/2467]	Loss: 0.216898Train Epoch: [1360/2467]	Loss: 0.079418Train Epoch: [1360/2467]	Loss: 0.108993


     Train Epoch: [1360/2467]	Loss: 0.047604
     Train Epoch: [1380/2467]	Loss: 0.300120
     Train Epoch: [1380/2467]	Loss: 0.204532    
 Train Epoch: [1380/2467]	Loss: 0.367508
     Train Epoch: [1380/2467]	Loss: 0.270031
     Train Epoch: [1400/2467]	Loss: 0.094995
     Train Epoch: [1400/2467]	Loss: 0.177916
          Train Epoch: [1400/2467]	Loss: 0.164464
Train Epoch: [1400/2467]	Loss: 0.088124
         Train Epoch: [1420/2467]	Loss: 0.134975 
    Train Epoch: [1420/2467]	Loss: 0.313510
 Train Epoch: [1420/2467]	Loss: 0.076483    
 Train Epoch: [1420/2467]	Loss: 0.137779
         Train Epoch: [1440/2467]	Loss: 0.112300
     Train Epoch: [1440/2467]	Loss: 0.043009
 Train Epoch: [1440/2467]	Loss: 0.045300
     Train Epoch: [1440/2467]	Loss: 0.013709
         Train Epoch: [1460/2467]	Loss: 0.091134
     Train Epoch: [1460/2467]	Loss: 0.046128
 Train Epoch: [1460/2467]	Loss: 0.097706
     Train Epoch: [1460/2467]	Loss: 0.267016
         Train Epoch: [1480/2467]	Loss: 0.200988 
Train Epoch: [1480/2467]	Loss: 0.217291
         Train Epoch: [1480/2467]	Loss: 0.148525 
Train Epoch: [1480/2467]	Loss: 0.022266
     Train Epoch: [1500/2467]	Loss: 0.098598    
 Train Epoch: [1500/2467]	Loss: 0.133177
         Train Epoch: [1500/2467]	Loss: 0.087479 
Train Epoch: [1500/2467]	Loss: 0.015426
          Train Epoch: [1520/2467]	Loss: 0.129952Train Epoch: [1520/2467]	Loss: 0.206060

     Train Epoch: [1520/2467]	Loss: 0.128880
     Train Epoch: [1520/2467]	Loss: 0.248629
         Train Epoch: [1540/2467]	Loss: 0.053117
     Train Epoch: [1540/2467]	Loss: 0.080985
     Train Epoch: [1540/2467]	Loss: 0.261793
 Train Epoch: [1540/2467]	Loss: 0.101762
             Train Epoch: [1560/2467]	Loss: 0.258854 
Train Epoch: [1560/2467]	Loss: 0.060111
 Train Epoch: [1560/2467]	Loss: 0.320459
     Train Epoch: [1560/2467]	Loss: 0.498774
          Train Epoch: [1580/2467]	Loss: 0.079763Train Epoch: [1580/2467]	Loss: 0.249219

         Train Epoch: [1580/2467]	Loss: 0.202153 
Train Epoch: [1580/2467]	Loss: 0.091542
         Train Epoch: [1600/2467]	Loss: 0.032774
 Train Epoch: [1600/2467]	Loss: 0.223366
     Train Epoch: [1600/2467]	Loss: 0.220982
     Train Epoch: [1600/2467]	Loss: 0.083894
     Train Epoch: [1620/2467]	Loss: 0.084819
          Train Epoch: [1620/2467]	Loss: 0.159605Train Epoch: [1620/2467]	Loss: 0.029633

     Train Epoch: [1620/2467]	Loss: 0.235843
     Train Epoch: [1640/2467]	Loss: 0.182381
     Train Epoch: [1640/2467]	Loss: 0.133769
         Train Epoch: [1640/2467]	Loss: 0.177008
 Train Epoch: [1640/2467]	Loss: 0.077754
     Train Epoch: [1660/2467]	Loss: 0.178381
     Train Epoch: [1660/2467]	Loss: 0.179579
     Train Epoch: [1660/2467]	Loss: 0.111501
     Train Epoch: [1660/2467]	Loss: 0.073305
          Train Epoch: [1680/2467]	Loss: 0.111942Train Epoch: [1680/2467]	Loss: 0.167545

     Train Epoch: [1680/2467]	Loss: 0.047902
     Train Epoch: [1680/2467]	Loss: 0.174277
     Train Epoch: [1700/2467]	Loss: 0.297693
         Train Epoch: [1700/2467]	Loss: 0.124356 
Train Epoch: [1700/2467]	Loss: 0.087029
     Train Epoch: [1700/2467]	Loss: 0.163728
         Train Epoch: [1720/2467]	Loss: 0.065066
     Train Epoch: [1720/2467]	Loss: 0.228920
     Train Epoch: [1720/2467]	Loss: 0.105022
 Train Epoch: [1720/2467]	Loss: 0.141950
          Train Epoch: [1740/2467]	Loss: 0.024226Train Epoch: [1740/2467]	Loss: 0.032399

          Train Epoch: [1740/2467]	Loss: 0.281263Train Epoch: [1740/2467]	Loss: 0.184999

     Train Epoch: [1760/2467]	Loss: 0.165595
         Train Epoch: [1760/2467]	Loss: 0.091506     
Train Epoch: [1760/2467]	Loss: 0.077157
 Train Epoch: [1760/2467]	Loss: 0.125100
     Train Epoch: [1780/2467]	Loss: 0.179354
          Train Epoch: [1780/2467]	Loss: 0.096447Train Epoch: [1780/2467]	Loss: 0.272332

     Train Epoch: [1780/2467]	Loss: 0.226820
              Train Epoch: [1800/2467]	Loss: 0.253556
Train Epoch: [1800/2467]	Loss: 0.227630
 Train Epoch: [1800/2467]	Loss: 0.196560
     Train Epoch: [1800/2467]	Loss: 0.185253
             Train Epoch: [1820/2467]	Loss: 0.137425
  Train Epoch: [1820/2467]	Loss: 0.349430Train Epoch: [1820/2467]	Loss: 0.146339

     Train Epoch: [1820/2467]	Loss: 0.158660
          Train Epoch: [1840/2467]	Loss: 0.288749    Train Epoch: [1840/2467]	Loss: 0.145469

     Train Epoch: [1840/2467]	Loss: 0.219994 
Train Epoch: [1840/2467]	Loss: 0.049133
         Train Epoch: [1860/2467]	Loss: 0.166319
 Train Epoch: [1860/2467]	Loss: 0.127625
     Train Epoch: [1860/2467]	Loss: 0.135312
     Train Epoch: [1860/2467]	Loss: 0.095185
          Train Epoch: [1880/2467]	Loss: 0.041014Train Epoch: [1880/2467]	Loss: 0.098500

     Train Epoch: [1880/2467]	Loss: 0.066443
     Train Epoch: [1880/2467]	Loss: 0.025314
         Train Epoch: [1900/2467]	Loss: 0.093069 
Train Epoch: [1900/2467]	Loss: 0.268592
     Train Epoch: [1900/2467]	Loss: 0.241029
     Train Epoch: [1900/2467]	Loss: 0.114215
         Train Epoch: [1920/2467]	Loss: 0.106261
 Train Epoch: [1920/2467]	Loss: 0.015740
     Train Epoch: [1920/2467]	Loss: 0.182374
     Train Epoch: [1920/2467]	Loss: 0.199487
          Train Epoch: [1940/2467]	Loss: 0.110428Train Epoch: [1940/2467]	Loss: 0.305203
    
 Train Epoch: [1940/2467]	Loss: 0.071141
     Train Epoch: [1940/2467]	Loss: 0.156072
     Train Epoch: [1960/2467]	Loss: 0.173575    
     Train Epoch: [1960/2467]	Loss: 0.098587 
Train Epoch: [1960/2467]	Loss: 0.077791
     Train Epoch: [1960/2467]	Loss: 0.120190
     Train Epoch: [1980/2467]	Loss: 0.282421
     Train Epoch: [1980/2467]	Loss: 0.235686
     Train Epoch: [1980/2467]	Loss: 0.109027
     Train Epoch: [1980/2467]	Loss: 0.192226
     Train Epoch: [2000/2467]	Loss: 0.045792
         Train Epoch: [2000/2467]	Loss: 0.024857
 Train Epoch: [2000/2467]	Loss: 0.184901
     Train Epoch: [2000/2467]	Loss: 0.127077
         Train Epoch: [2020/2467]	Loss: 0.144995
 Train Epoch: [2020/2467]	Loss: 0.128774
     Train Epoch: [2020/2467]	Loss: 0.053818
     Train Epoch: [2020/2467]	Loss: 0.146068
          Train Epoch: [2040/2467]	Loss: 0.370088Train Epoch: [2040/2467]	Loss: 0.085753

         Train Epoch: [2040/2467]	Loss: 0.157978 
Train Epoch: [2040/2467]	Loss: 0.097681
     Train Epoch: [2060/2467]	Loss: 0.208759
         Train Epoch: [2060/2467]	Loss: 0.059628
 Train Epoch: [2060/2467]	Loss: 0.119000
     Train Epoch: [2060/2467]	Loss: 0.158037
         Train Epoch: [2080/2467]	Loss: 0.056169
     Train Epoch: [2080/2467]	Loss: 0.081997
     Train Epoch: [2080/2467]	Loss: 0.208936
 Train Epoch: [2080/2467]	Loss: 0.029388
          Train Epoch: [2100/2467]	Loss: 0.096322Train Epoch: [2100/2467]	Loss: 0.455125

          Train Epoch: [2100/2467]	Loss: 0.120631
Train Epoch: [2100/2467]	Loss: 0.094671
              Train Epoch: [2120/2467]	Loss: 0.041073Train Epoch: [2120/2467]	Loss: 0.204658

 Train Epoch: [2120/2467]	Loss: 0.096098    
 Train Epoch: [2120/2467]	Loss: 0.148407
     Train Epoch: [2140/2467]	Loss: 0.182887
     Train Epoch: [2140/2467]	Loss: 0.450592
          Train Epoch: [2140/2467]	Loss: 0.240671Train Epoch: [2140/2467]	Loss: 0.328017

          Train Epoch: [2160/2467]	Loss: 0.091828Train Epoch: [2160/2467]	Loss: 0.056269

     Train Epoch: [2160/2467]	Loss: 0.275536
     Train Epoch: [2160/2467]	Loss: 0.099669
     Train Epoch: [2180/2467]	Loss: 0.131801
         Train Epoch: [2180/2467]	Loss: 0.154569 
Train Epoch: [2180/2467]	Loss: 0.258679
     Train Epoch: [2180/2467]	Loss: 0.064668
     Train Epoch: [2200/2467]	Loss: 0.127196
              Train Epoch: [2200/2467]	Loss: 0.141611Train Epoch: [2200/2467]	Loss: 0.033382

 Train Epoch: [2200/2467]	Loss: 0.097374
          Train Epoch: [2220/2467]	Loss: 0.068868Train Epoch: [2220/2467]	Loss: 0.076118
    
 Train Epoch: [2220/2467]	Loss: 0.143856
     Train Epoch: [2220/2467]	Loss: 0.109435
          Train Epoch: [2240/2467]	Loss: 0.028948Train Epoch: [2240/2467]	Loss: 0.087299

     Train Epoch: [2240/2467]	Loss: 0.033663
     Train Epoch: [2240/2467]	Loss: 0.065948
          Train Epoch: [2260/2467]	Loss: 0.186547Train Epoch: [2260/2467]	Loss: 0.341615

     Train Epoch: [2260/2467]	Loss: 0.029307
     Train Epoch: [2260/2467]	Loss: 0.198466
     Train Epoch: [2280/2467]	Loss: 0.091604
              Train Epoch: [2280/2467]	Loss: 0.137958Train Epoch: [2280/2467]	Loss: 0.070642

 Train Epoch: [2280/2467]	Loss: 0.137389
         Train Epoch: [2300/2467]	Loss: 0.048851
     Train Epoch: [2300/2467]	Loss: 0.062240
 Train Epoch: [2300/2467]	Loss: 0.266644
     Train Epoch: [2300/2467]	Loss: 0.091601
         Train Epoch: [2320/2467]	Loss: 0.125525
 Train Epoch: [2320/2467]	Loss: 0.145944
         Train Epoch: [2320/2467]	Loss: 0.135377
 Train Epoch: [2320/2467]	Loss: 0.020143
         Train Epoch: [2340/2467]	Loss: 0.107376 
Train Epoch: [2340/2467]	Loss: 0.125903
         Train Epoch: [2340/2467]	Loss: 0.221871 
Train Epoch: [2340/2467]	Loss: 0.439441
     Train Epoch: [2360/2467]	Loss: 0.076115
     Train Epoch: [2360/2467]	Loss: 0.144009
     Train Epoch: [2360/2467]	Loss: 0.136955
     Train Epoch: [2360/2467]	Loss: 0.332367
     Train Epoch: [2380/2467]	Loss: 0.219847
     Train Epoch: [2380/2467]	Loss: 0.401845
     Train Epoch: [2380/2467]	Loss: 0.180399
     Train Epoch: [2380/2467]	Loss: 0.077098
         Train Epoch: [2400/2467]	Loss: 0.160703
     Train Epoch: [2400/2467]	Loss: 0.082036
 Train Epoch: [2400/2467]	Loss: 0.142422
     Train Epoch: [2400/2467]	Loss: 0.065539
             Train Epoch: [2420/2467]	Loss: 0.108761 
Train Epoch: [2420/2467]	Loss: 0.202005 
Train Epoch: [2420/2467]	Loss: 0.298613
     Train Epoch: [2420/2467]	Loss: 0.027427
         Train Epoch: [2440/2467]	Loss: 0.163297
 Train Epoch: [2440/2467]	Loss: 0.223134
     Train Epoch: [2440/2467]	Loss: 0.107049
     Train Epoch: [2440/2467]	Loss: 0.020893
     Train Epoch: [2460/2467]	Loss: 0.097934    
 Train Epoch: [2460/2467]	Loss: 0.069021
     Train Epoch: [2460/2467]	Loss: 0.253838
     Train Epoch: [2460/2467]	Loss: 0.137794
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 32 epoch =====
     2025-05-11.11-41-55
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
    huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
 ===== running 32 epoch =====
     2025-05-11.11-41-55
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 32 epoch =====
     2025-05-11.11-41-56
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 32 epoch =====
     2025-05-11.11-41-56
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.165803
         Train Epoch: [0/2467]	Loss: 0.311717
     Train Epoch: [0/2467]	Loss: 0.072399
 Train Epoch: [0/2467]	Loss: 0.218325
          Train Epoch: [20/2467]	Loss: 0.057717Train Epoch: [20/2467]	Loss: 0.097747    

 Train Epoch: [20/2467]	Loss: 0.278876
     Train Epoch: [20/2467]	Loss: 0.105646
         Train Epoch: [40/2467]	Loss: 0.336190
     Train Epoch: [40/2467]	Loss: 0.110545
 Train Epoch: [40/2467]	Loss: 0.129321
     Train Epoch: [40/2467]	Loss: 0.130000
     Train Epoch: [60/2467]	Loss: 0.197771
     Train Epoch: [60/2467]	Loss: 0.029633
          Train Epoch: [60/2467]	Loss: 0.165939Train Epoch: [60/2467]	Loss: 0.079536

     Train Epoch: [80/2467]	Loss: 0.234293
             Train Epoch: [80/2467]	Loss: 0.062992
  Train Epoch: [80/2467]	Loss: 0.114325
Train Epoch: [80/2467]	Loss: 0.092896
         Train Epoch: [100/2467]	Loss: 0.093865 
Train Epoch: [100/2467]	Loss: 0.054104
     Train Epoch: [100/2467]	Loss: 0.159548
     Train Epoch: [100/2467]	Loss: 0.218692
         Train Epoch: [120/2467]	Loss: 0.041793    
 Train Epoch: [120/2467]	Loss: 0.205500
 Train Epoch: [120/2467]	Loss: 0.079984
     Train Epoch: [120/2467]	Loss: 0.309122
         Train Epoch: [140/2467]	Loss: 0.111034
     Train Epoch: [140/2467]	Loss: 0.031186 
Train Epoch: [140/2467]	Loss: 0.053432    
 Train Epoch: [140/2467]	Loss: 0.207507
         Train Epoch: [160/2467]	Loss: 0.095781
 Train Epoch: [160/2467]	Loss: 0.117998
         Train Epoch: [160/2467]	Loss: 0.122415
 Train Epoch: [160/2467]	Loss: 0.221392
     Train Epoch: [180/2467]	Loss: 0.109480
         Train Epoch: [180/2467]	Loss: 0.164293
 Train Epoch: [180/2467]	Loss: 0.082499
     Train Epoch: [180/2467]	Loss: 0.086327
         Train Epoch: [200/2467]	Loss: 0.107070
 Train Epoch: [200/2467]	Loss: 0.040956
         Train Epoch: [200/2467]	Loss: 0.052895
 Train Epoch: [200/2467]	Loss: 0.072049
     Train Epoch: [220/2467]	Loss: 0.114315
         Train Epoch: [220/2467]	Loss: 0.048105
 Train Epoch: [220/2467]	Loss: 0.147271
     Train Epoch: [220/2467]	Loss: 0.193143
          Train Epoch: [240/2467]	Loss: 0.205357Train Epoch: [240/2467]	Loss: 0.016567

         Train Epoch: [240/2467]	Loss: 0.152230 
Train Epoch: [240/2467]	Loss: 0.038031
         Train Epoch: [260/2467]	Loss: 0.045738
 Train Epoch: [260/2467]	Loss: 0.054677
     Train Epoch: [260/2467]	Loss: 0.050914
     Train Epoch: [260/2467]	Loss: 0.030466
          Train Epoch: [280/2467]	Loss: 0.085800
Train Epoch: [280/2467]	Loss: 0.105749
     Train Epoch: [280/2467]	Loss: 0.101731
     Train Epoch: [280/2467]	Loss: 0.169912
         Train Epoch: [300/2467]	Loss: 0.144036
 Train Epoch: [300/2467]	Loss: 0.032842
         Train Epoch: [300/2467]	Loss: 0.074707
 Train Epoch: [300/2467]	Loss: 0.306259
     Train Epoch: [320/2467]	Loss: 0.267524
         Train Epoch: [320/2467]	Loss: 0.162110
 Train Epoch: [320/2467]	Loss: 0.381279
     Train Epoch: [320/2467]	Loss: 0.149323
         Train Epoch: [340/2467]	Loss: 0.076064 
Train Epoch: [340/2467]	Loss: 0.045304
         Train Epoch: [340/2467]	Loss: 0.013340
 Train Epoch: [340/2467]	Loss: 0.185300
     Train Epoch: [360/2467]	Loss: 0.205222
         Train Epoch: [360/2467]	Loss: 0.069364
 Train Epoch: [360/2467]	Loss: 0.082388
     Train Epoch: [360/2467]	Loss: 0.145513
     Train Epoch: [380/2467]	Loss: 0.166471
         Train Epoch: [380/2467]	Loss: 0.277441
 Train Epoch: [380/2467]	Loss: 0.172825
     Train Epoch: [380/2467]	Loss: 0.066408
     Train Epoch: [400/2467]	Loss: 0.065568
     Train Epoch: [400/2467]	Loss: 0.046993
          Train Epoch: [400/2467]	Loss: 0.277947Train Epoch: [400/2467]	Loss: 0.172577

     Train Epoch: [420/2467]	Loss: 0.218274
     Train Epoch: [420/2467]	Loss: 0.247283    
     Train Epoch: [420/2467]	Loss: 0.112965
 Train Epoch: [420/2467]	Loss: 0.101316
     Train Epoch: [440/2467]	Loss: 0.140266
     Train Epoch: [440/2467]	Loss: 0.311114
     Train Epoch: [440/2467]	Loss: 0.074831
     Train Epoch: [440/2467]	Loss: 0.048126
         Train Epoch: [460/2467]	Loss: 0.219548
 Train Epoch: [460/2467]	Loss: 0.148932    
 Train Epoch: [460/2467]	Loss: 0.092346
     Train Epoch: [460/2467]	Loss: 0.126616
          Train Epoch: [480/2467]	Loss: 0.025609Train Epoch: [480/2467]	Loss: 0.136359

         Train Epoch: [480/2467]	Loss: 0.299174
 Train Epoch: [480/2467]	Loss: 0.305509
             Train Epoch: [500/2467]	Loss: 0.106781 
Train Epoch: [500/2467]	Loss: 0.048143
 Train Epoch: [500/2467]	Loss: 0.088390
     Train Epoch: [500/2467]	Loss: 0.269230
     Train Epoch: [520/2467]	Loss: 0.103288    
     Train Epoch: [520/2467]	Loss: 0.230546
 Train Epoch: [520/2467]	Loss: 0.161132    
 Train Epoch: [520/2467]	Loss: 0.060490
         Train Epoch: [540/2467]	Loss: 0.228707
      Train Epoch: [540/2467]	Loss: 0.057197
Train Epoch: [540/2467]	Loss: 0.086974
     Train Epoch: [540/2467]	Loss: 0.143286
         Train Epoch: [560/2467]	Loss: 0.136207 
Train Epoch: [560/2467]	Loss: 0.173595
     Train Epoch: [560/2467]	Loss: 0.131582
     Train Epoch: [560/2467]	Loss: 0.118387
         Train Epoch: [580/2467]	Loss: 0.245780
 Train Epoch: [580/2467]	Loss: 0.130152
          Train Epoch: [580/2467]	Loss: 0.109924Train Epoch: [580/2467]	Loss: 0.057523

          Train Epoch: [600/2467]	Loss: 0.111552Train Epoch: [600/2467]	Loss: 0.171619

     Train Epoch: [600/2467]	Loss: 0.162597
     Train Epoch: [600/2467]	Loss: 0.128109
         Train Epoch: [620/2467]	Loss: 0.140156
 Train Epoch: [620/2467]	Loss: 0.029948
          Train Epoch: [620/2467]	Loss: 0.163159Train Epoch: [620/2467]	Loss: 0.244267

             Train Epoch: [640/2467]	Loss: 0.121685
  Train Epoch: [640/2467]	Loss: 0.173476Train Epoch: [640/2467]	Loss: 0.036237

     Train Epoch: [640/2467]	Loss: 0.148225
     Train Epoch: [660/2467]	Loss: 0.026415
     Train Epoch: [660/2467]	Loss: 0.035728
         Train Epoch: [660/2467]	Loss: 0.143927 
Train Epoch: [660/2467]	Loss: 0.359690
         Train Epoch: [680/2467]	Loss: 0.113313
 Train Epoch: [680/2467]	Loss: 0.039251
          Train Epoch: [680/2467]	Loss: 0.081794Train Epoch: [680/2467]	Loss: 0.039155

         Train Epoch: [700/2467]	Loss: 0.046947
 Train Epoch: [700/2467]	Loss: 0.274526
         Train Epoch: [700/2467]	Loss: 0.069654
 Train Epoch: [700/2467]	Loss: 0.278516
          Train Epoch: [720/2467]	Loss: 0.025505Train Epoch: [720/2467]	Loss: 0.175196

     Train Epoch: [720/2467]	Loss: 0.074927
     Train Epoch: [720/2467]	Loss: 0.107331
         Train Epoch: [740/2467]	Loss: 0.045383
 Train Epoch: [740/2467]	Loss: 0.070038
     Train Epoch: [740/2467]	Loss: 0.077437
     Train Epoch: [740/2467]	Loss: 0.317163
         Train Epoch: [760/2467]	Loss: 0.228825
     Train Epoch: [760/2467]	Loss: 0.366373
 Train Epoch: [760/2467]	Loss: 0.049400
     Train Epoch: [760/2467]	Loss: 0.257732
     Train Epoch: [780/2467]	Loss: 0.390876
         Train Epoch: [780/2467]	Loss: 0.272188
 Train Epoch: [780/2467]	Loss: 0.305675
     Train Epoch: [780/2467]	Loss: 0.100055
     Train Epoch: [800/2467]	Loss: 0.161711
         Train Epoch: [800/2467]	Loss: 0.156843
     Train Epoch: [800/2467]	Loss: 0.113677
 Train Epoch: [800/2467]	Loss: 0.125328
         Train Epoch: [820/2467]	Loss: 0.038337
     Train Epoch: [820/2467]	Loss: 0.381470
     Train Epoch: [820/2467]	Loss: 0.202316
 Train Epoch: [820/2467]	Loss: 0.050580
          Train Epoch: [840/2467]	Loss: 0.085415Train Epoch: [840/2467]	Loss: 0.307639

     Train Epoch: [840/2467]	Loss: 0.108808
     Train Epoch: [840/2467]	Loss: 0.050099
     Train Epoch: [860/2467]	Loss: 0.228306
     Train Epoch: [860/2467]	Loss: 0.139288
     Train Epoch: [860/2467]	Loss: 0.056341    
 Train Epoch: [860/2467]	Loss: 0.050359
     Train Epoch: [880/2467]	Loss: 0.130858
          Train Epoch: [880/2467]	Loss: 0.142694
Train Epoch: [880/2467]	Loss: 0.201813
     Train Epoch: [880/2467]	Loss: 0.218334
     Train Epoch: [900/2467]	Loss: 0.174735
     Train Epoch: [900/2467]	Loss: 0.147793
     Train Epoch: [900/2467]	Loss: 0.087190
     Train Epoch: [900/2467]	Loss: 0.172494
         Train Epoch: [920/2467]	Loss: 0.219078
 Train Epoch: [920/2467]	Loss: 0.289763
     Train Epoch: [920/2467]	Loss: 0.216768
     Train Epoch: [920/2467]	Loss: 0.038829
     Train Epoch: [940/2467]	Loss: 0.047022
          Train Epoch: [940/2467]	Loss: 0.170291Train Epoch: [940/2467]	Loss: 0.034142

     Train Epoch: [940/2467]	Loss: 0.079747
         Train Epoch: [960/2467]	Loss: 0.253880
 Train Epoch: [960/2467]	Loss: 0.077803
         Train Epoch: [960/2467]	Loss: 0.065605 
Train Epoch: [960/2467]	Loss: 0.056363
         Train Epoch: [980/2467]	Loss: 0.233087
 Train Epoch: [980/2467]	Loss: 0.048526
          Train Epoch: [980/2467]	Loss: 0.119971
Train Epoch: [980/2467]	Loss: 0.112159
     Train Epoch: [1000/2467]	Loss: 0.035278
         Train Epoch: [1000/2467]	Loss: 0.028880
     Train Epoch: [1000/2467]	Loss: 0.231415
 Train Epoch: [1000/2467]	Loss: 0.189491
         Train Epoch: [1020/2467]	Loss: 0.126095
 Train Epoch: [1020/2467]	Loss: 0.357664
     Train Epoch: [1020/2467]	Loss: 0.022678
     Train Epoch: [1020/2467]	Loss: 0.068865
         Train Epoch: [1040/2467]	Loss: 0.143519
     Train Epoch: [1040/2467]	Loss: 0.281443
 Train Epoch: [1040/2467]	Loss: 0.078320
     Train Epoch: [1040/2467]	Loss: 0.105065
     Train Epoch: [1060/2467]	Loss: 0.143361
     Train Epoch: [1060/2467]	Loss: 0.084853
         Train Epoch: [1060/2467]	Loss: 0.093171
 Train Epoch: [1060/2467]	Loss: 0.359186
         Train Epoch: [1080/2467]	Loss: 0.103841
 Train Epoch: [1080/2467]	Loss: 0.213861
     Train Epoch: [1080/2467]	Loss: 0.206801
     Train Epoch: [1080/2467]	Loss: 0.144843
         Train Epoch: [1100/2467]	Loss: 0.200381
 Train Epoch: [1100/2467]	Loss: 0.251680
          Train Epoch: [1100/2467]	Loss: 0.195478Train Epoch: [1100/2467]	Loss: 0.243107

         Train Epoch: [1120/2467]	Loss: 0.223703
     Train Epoch: [1120/2467]	Loss: 0.115574
 Train Epoch: [1120/2467]	Loss: 0.109076    
 Train Epoch: [1120/2467]	Loss: 0.113479
             Train Epoch: [1140/2467]	Loss: 0.045861
      Train Epoch: [1140/2467]	Loss: 0.351413Train Epoch: [1140/2467]	Loss: 0.080456

 Train Epoch: [1140/2467]	Loss: 0.254622
     Train Epoch: [1160/2467]	Loss: 0.073924
         Train Epoch: [1160/2467]	Loss: 0.110434
 Train Epoch: [1160/2467]	Loss: 0.215574
     Train Epoch: [1160/2467]	Loss: 0.092445
         Train Epoch: [1180/2467]	Loss: 0.088676
     Train Epoch: [1180/2467]	Loss: 0.065479
 Train Epoch: [1180/2467]	Loss: 0.121622
     Train Epoch: [1180/2467]	Loss: 0.118389
     Train Epoch: [1200/2467]	Loss: 0.067686
         Train Epoch: [1200/2467]	Loss: 0.103097
     Train Epoch: [1200/2467]	Loss: 0.286329 
Train Epoch: [1200/2467]	Loss: 0.192349
         Train Epoch: [1220/2467]	Loss: 0.157951
 Train Epoch: [1220/2467]	Loss: 0.074135    
     Train Epoch: [1220/2467]	Loss: 0.218817
 Train Epoch: [1220/2467]	Loss: 0.216646
     Train Epoch: [1240/2467]	Loss: 0.220141
         Train Epoch: [1240/2467]	Loss: 0.136786
 Train Epoch: [1240/2467]	Loss: 0.170695
     Train Epoch: [1240/2467]	Loss: 0.260916
     Train Epoch: [1260/2467]	Loss: 0.056459
         Train Epoch: [1260/2467]	Loss: 0.106093 
Train Epoch: [1260/2467]	Loss: 0.094031
     Train Epoch: [1260/2467]	Loss: 0.365382
             Train Epoch: [1280/2467]	Loss: 0.172944 
Train Epoch: [1280/2467]	Loss: 0.160879
 Train Epoch: [1280/2467]	Loss: 0.100585
     Train Epoch: [1280/2467]	Loss: 0.451993
     Train Epoch: [1300/2467]	Loss: 0.262476
             Train Epoch: [1300/2467]	Loss: 0.037948
  Train Epoch: [1300/2467]	Loss: 0.140337Train Epoch: [1300/2467]	Loss: 0.090160

     Train Epoch: [1320/2467]	Loss: 0.042927
         Train Epoch: [1320/2467]	Loss: 0.107384
 Train Epoch: [1320/2467]	Loss: 0.137812
     Train Epoch: [1320/2467]	Loss: 0.269432
         Train Epoch: [1340/2467]	Loss: 0.100297
     Train Epoch: [1340/2467]	Loss: 0.189160
 Train Epoch: [1340/2467]	Loss: 0.349821
     Train Epoch: [1340/2467]	Loss: 0.140246
         Train Epoch: [1360/2467]	Loss: 0.097366
     Train Epoch: [1360/2467]	Loss: 0.105857
     Train Epoch: [1360/2467]	Loss: 0.056702
 Train Epoch: [1360/2467]	Loss: 0.068340
     Train Epoch: [1380/2467]	Loss: 0.189519
     Train Epoch: [1380/2467]	Loss: 0.258421
     Train Epoch: [1380/2467]	Loss: 0.192422
     Train Epoch: [1380/2467]	Loss: 0.311535
     Train Epoch: [1400/2467]	Loss: 0.089254
     Train Epoch: [1400/2467]	Loss: 0.102979    
 Train Epoch: [1400/2467]	Loss: 0.158008
     Train Epoch: [1400/2467]	Loss: 0.152808
         Train Epoch: [1420/2467]	Loss: 0.364380
 Train Epoch: [1420/2467]	Loss: 0.077591
         Train Epoch: [1420/2467]	Loss: 0.174330
 Train Epoch: [1420/2467]	Loss: 0.120395
     Train Epoch: [1440/2467]	Loss: 0.020234
         Train Epoch: [1440/2467]	Loss: 0.042344
 Train Epoch: [1440/2467]	Loss: 0.039190
     Train Epoch: [1440/2467]	Loss: 0.100215
              Train Epoch: [1460/2467]	Loss: 0.037885Train Epoch: [1460/2467]	Loss: 0.110718

     Train Epoch: [1460/2467]	Loss: 0.101163 
Train Epoch: [1460/2467]	Loss: 0.204538
     Train Epoch: [1480/2467]	Loss: 0.026677
          Train Epoch: [1480/2467]	Loss: 0.011775
Train Epoch: [1480/2467]	Loss: 0.176442
     Train Epoch: [1480/2467]	Loss: 0.205644
     Train Epoch: [1500/2467]	Loss: 0.114084
     Train Epoch: [1500/2467]	Loss: 0.019607
         Train Epoch: [1500/2467]	Loss: 0.122529
 Train Epoch: [1500/2467]	Loss: 0.064049
          Train Epoch: [1520/2467]	Loss: 0.202074Train Epoch: [1520/2467]	Loss: 0.110364

          Train Epoch: [1520/2467]	Loss: 0.156351Train Epoch: [1520/2467]	Loss: 0.225746

     Train Epoch: [1540/2467]	Loss: 0.071032
         Train Epoch: [1540/2467]	Loss: 0.257979 
Train Epoch: [1540/2467]	Loss: 0.056834
     Train Epoch: [1540/2467]	Loss: 0.108897
     Train Epoch: [1560/2467]	Loss: 0.323349
         Train Epoch: [1560/2467]	Loss: 0.406058
 Train Epoch: [1560/2467]	Loss: 0.078073
     Train Epoch: [1560/2467]	Loss: 0.231302
         Train Epoch: [1580/2467]	Loss: 0.078502
 Train Epoch: [1580/2467]	Loss: 0.189559
         Train Epoch: [1580/2467]	Loss: 0.224672
 Train Epoch: [1580/2467]	Loss: 0.123231
          Train Epoch: [1600/2467]	Loss: 0.072167Train Epoch: [1600/2467]	Loss: 0.203614

     Train Epoch: [1600/2467]	Loss: 0.099267
     Train Epoch: [1600/2467]	Loss: 0.214417
         Train Epoch: [1620/2467]	Loss: 0.032253 
Train Epoch: [1620/2467]	Loss: 0.163029
         Train Epoch: [1620/2467]	Loss: 0.223421
 Train Epoch: [1620/2467]	Loss: 0.091989
             Train Epoch: [1640/2467]	Loss: 0.129785 
    Train Epoch: [1640/2467]	Loss: 0.169159
 Train Epoch: [1640/2467]	Loss: 0.201073 
Train Epoch: [1640/2467]	Loss: 0.096941
     Train Epoch: [1660/2467]	Loss: 0.194432
             Train Epoch: [1660/2467]	Loss: 0.051607
 Train Epoch: [1660/2467]	Loss: 0.159833
 Train Epoch: [1660/2467]	Loss: 0.122986
          Train Epoch: [1680/2467]	Loss: 0.139440
Train Epoch: [1680/2467]	Loss: 0.041241
     Train Epoch: [1680/2467]	Loss: 0.188907
     Train Epoch: [1680/2467]	Loss: 0.136360
     Train Epoch: [1700/2467]	Loss: 0.143549
         Train Epoch: [1700/2467]	Loss: 0.089345
 Train Epoch: [1700/2467]	Loss: 0.287184
     Train Epoch: [1700/2467]	Loss: 0.116751
          Train Epoch: [1720/2467]	Loss: 0.107907Train Epoch: [1720/2467]	Loss: 0.205471

     Train Epoch: [1720/2467]	Loss: 0.051580
     Train Epoch: [1720/2467]	Loss: 0.137345
     Train Epoch: [1740/2467]	Loss: 0.029023
          Train Epoch: [1740/2467]	Loss: 0.276435Train Epoch: [1740/2467]	Loss: 0.019398

     Train Epoch: [1740/2467]	Loss: 0.176486
          Train Epoch: [1760/2467]	Loss: 0.107242Train Epoch: [1760/2467]	Loss: 0.143160

         Train Epoch: [1760/2467]	Loss: 0.083694
 Train Epoch: [1760/2467]	Loss: 0.052690
     Train Epoch: [1780/2467]	Loss: 0.217597
     Train Epoch: [1780/2467]	Loss: 0.224270
     Train Epoch: [1780/2467]	Loss: 0.075362
     Train Epoch: [1780/2467]	Loss: 0.132964
         Train Epoch: [1800/2467]	Loss: 0.093688
 Train Epoch: [1800/2467]	Loss: 0.264092
     Train Epoch: [1800/2467]	Loss: 0.118216
     Train Epoch: [1800/2467]	Loss: 0.215945
     Train Epoch: [1820/2467]	Loss: 0.162504    
 Train Epoch: [1820/2467]	Loss: 0.298907
         Train Epoch: [1820/2467]	Loss: 0.118706
 Train Epoch: [1820/2467]	Loss: 0.114680
     Train Epoch: [1840/2467]	Loss: 0.125320    
 Train Epoch: [1840/2467]	Loss: 0.171413
     Train Epoch: [1840/2467]	Loss: 0.042888
     Train Epoch: [1840/2467]	Loss: 0.254031
     Train Epoch: [1860/2467]	Loss: 0.125541
         Train Epoch: [1860/2467]	Loss: 0.141669
 Train Epoch: [1860/2467]	Loss: 0.093969
     Train Epoch: [1860/2467]	Loss: 0.153629
          Train Epoch: [1880/2467]	Loss: 0.079176Train Epoch: [1880/2467]	Loss: 0.021479

         Train Epoch: [1880/2467]	Loss: 0.041086
 Train Epoch: [1880/2467]	Loss: 0.053207
     Train Epoch: [1900/2467]	Loss: 0.090940
     Train Epoch: [1900/2467]	Loss: 0.101790
     Train Epoch: [1900/2467]	Loss: 0.227335
     Train Epoch: [1900/2467]	Loss: 0.255317
     Train Epoch: [1920/2467]	Loss: 0.015805
         Train Epoch: [1920/2467]	Loss: 0.181891
 Train Epoch: [1920/2467]	Loss: 0.099431
     Train Epoch: [1920/2467]	Loss: 0.206800
     Train Epoch: [1940/2467]	Loss: 0.061864
          Train Epoch: [1940/2467]	Loss: 0.121765Train Epoch: [1940/2467]	Loss: 0.099040

     Train Epoch: [1940/2467]	Loss: 0.356748
     Train Epoch: [1960/2467]	Loss: 0.110473
     Train Epoch: [1960/2467]	Loss: 0.092822
     Train Epoch: [1960/2467]	Loss: 0.081200
     Train Epoch: [1960/2467]	Loss: 0.158648
     Train Epoch: [1980/2467]	Loss: 0.098999
     Train Epoch: [1980/2467]	Loss: 0.178247
     Train Epoch: [1980/2467]	Loss: 0.224824
     Train Epoch: [1980/2467]	Loss: 0.189812
         Train Epoch: [2000/2467]	Loss: 0.018609 
Train Epoch: [2000/2467]	Loss: 0.130882
     Train Epoch: [2000/2467]	Loss: 0.165531    
 Train Epoch: [2000/2467]	Loss: 0.048120
     Train Epoch: [2020/2467]	Loss: 0.139977
         Train Epoch: [2020/2467]	Loss: 0.121891
     Train Epoch: [2020/2467]	Loss: 0.152427
 Train Epoch: [2020/2467]	Loss: 0.060635
         Train Epoch: [2040/2467]	Loss: 0.066796
 Train Epoch: [2040/2467]	Loss: 0.141672
         Train Epoch: [2040/2467]	Loss: 0.072918
 Train Epoch: [2040/2467]	Loss: 0.359630
         Train Epoch: [2060/2467]	Loss: 0.171716
 Train Epoch: [2060/2467]	Loss: 0.066367
         Train Epoch: [2060/2467]	Loss: 0.118632
 Train Epoch: [2060/2467]	Loss: 0.208956
     Train Epoch: [2080/2467]	Loss: 0.038898    
 Train Epoch: [2080/2467]	Loss: 0.232620    
     Train Epoch: [2080/2467]	Loss: 0.066160
 Train Epoch: [2080/2467]	Loss: 0.023290
         Train Epoch: [2100/2467]	Loss: 0.085197 
Train Epoch: [2100/2467]	Loss: 0.458146    
     Train Epoch: [2100/2467]	Loss: 0.062543 
Train Epoch: [2100/2467]	Loss: 0.083541
     Train Epoch: [2120/2467]	Loss: 0.139203
         Train Epoch: [2120/2467]	Loss: 0.038113
 Train Epoch: [2120/2467]	Loss: 0.088013
     Train Epoch: [2120/2467]	Loss: 0.211092
     Train Epoch: [2140/2467]	Loss: 0.431280
         Train Epoch: [2140/2467]	Loss: 0.236720
 Train Epoch: [2140/2467]	Loss: 0.314515
     Train Epoch: [2140/2467]	Loss: 0.171094
         Train Epoch: [2160/2467]	Loss: 0.048267 
Train Epoch: [2160/2467]	Loss: 0.282437
         Train Epoch: [2160/2467]	Loss: 0.205683 
Train Epoch: [2160/2467]	Loss: 0.082774
         Train Epoch: [2180/2467]	Loss: 0.070935
     Train Epoch: [2180/2467]	Loss: 0.243229
 Train Epoch: [2180/2467]	Loss: 0.130620
     Train Epoch: [2180/2467]	Loss: 0.156092
         Train Epoch: [2200/2467]	Loss: 0.125576
 Train Epoch: [2200/2467]	Loss: 0.027541
         Train Epoch: [2200/2467]	Loss: 0.105844 
Train Epoch: [2200/2467]	Loss: 0.083793
     Train Epoch: [2220/2467]	Loss: 0.069980
         Train Epoch: [2220/2467]	Loss: 0.079547
 Train Epoch: [2220/2467]	Loss: 0.133886
     Train Epoch: [2220/2467]	Loss: 0.104690
          Train Epoch: [2240/2467]	Loss: 0.087493Train Epoch: [2240/2467]	Loss: 0.090270

         Train Epoch: [2240/2467]	Loss: 0.036752 
Train Epoch: [2240/2467]	Loss: 0.027515
     Train Epoch: [2260/2467]	Loss: 0.221559
     Train Epoch: [2260/2467]	Loss: 0.203210
     Train Epoch: [2260/2467]	Loss: 0.291233
     Train Epoch: [2260/2467]	Loss: 0.018990
     Train Epoch: [2280/2467]	Loss: 0.162813
         Train Epoch: [2280/2467]	Loss: 0.056162 
Train Epoch: [2280/2467]	Loss: 0.091196
     Train Epoch: [2280/2467]	Loss: 0.111094
              Train Epoch: [2300/2467]	Loss: 0.077555
Train Epoch: [2300/2467]	Loss: 0.230935 
Train Epoch: [2300/2467]	Loss: 0.084399
     Train Epoch: [2300/2467]	Loss: 0.050154
         Train Epoch: [2320/2467]	Loss: 0.016964
 Train Epoch: [2320/2467]	Loss: 0.135040
     Train Epoch: [2320/2467]	Loss: 0.147484
     Train Epoch: [2320/2467]	Loss: 0.169641
     Train Epoch: [2340/2467]	Loss: 0.129374
         Train Epoch: [2340/2467]	Loss: 0.214978
 Train Epoch: [2340/2467]	Loss: 0.139993
     Train Epoch: [2340/2467]	Loss: 0.439537
         Train Epoch: [2360/2467]	Loss: 0.333567
     Train Epoch: [2360/2467]	Loss: 0.199761
 Train Epoch: [2360/2467]	Loss: 0.125794
     Train Epoch: [2360/2467]	Loss: 0.149818
         Train Epoch: [2380/2467]	Loss: 0.290971
 Train Epoch: [2380/2467]	Loss: 0.361029
     Train Epoch: [2380/2467]	Loss: 0.135602
     Train Epoch: [2380/2467]	Loss: 0.064015
         Train Epoch: [2400/2467]	Loss: 0.088912
 Train Epoch: [2400/2467]	Loss: 0.114530    
     Train Epoch: [2400/2467]	Loss: 0.168970 
Train Epoch: [2400/2467]	Loss: 0.079998
          Train Epoch: [2420/2467]	Loss: 0.105626Train Epoch: [2420/2467]	Loss: 0.022250

          Train Epoch: [2420/2467]	Loss: 0.208558
Train Epoch: [2420/2467]	Loss: 0.282750
     Train Epoch: [2440/2467]	Loss: 0.013237
             Train Epoch: [2440/2467]	Loss: 0.201871
 Train Epoch: [2440/2467]	Loss: 0.136332
 Train Epoch: [2440/2467]	Loss: 0.103457
         Train Epoch: [2460/2467]	Loss: 0.074526 
Train Epoch: [2460/2467]	Loss: 0.127289
     Train Epoch: [2460/2467]	Loss: 0.096940
     Train Epoch: [2460/2467]	Loss: 0.244549
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 33 epoch =====
     2025-05-11.12-03-48
after set grad
after prog
start loop
     ===== running 33 epoch =====
     2025-05-11.12-03-48
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 33 epoch =====
     2025-05-11.12-03-48
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 33 epoch =====
     2025-05-11.12-03-48
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
             Train Epoch: [0/2467]	Loss: 0.162342 
 Train Epoch: [0/2467]	Loss: 0.279611Train Epoch: [0/2467]	Loss: 0.067060

     Train Epoch: [0/2467]	Loss: 0.233276
         Train Epoch: [20/2467]	Loss: 0.056595    
 Train Epoch: [20/2467]	Loss: 0.298453
 Train Epoch: [20/2467]	Loss: 0.097954    
 Train Epoch: [20/2467]	Loss: 0.101417
         Train Epoch: [40/2467]	Loss: 0.115407
 Train Epoch: [40/2467]	Loss: 0.133179
          Train Epoch: [40/2467]	Loss: 0.376587Train Epoch: [40/2467]	Loss: 0.132417

     Train Epoch: [60/2467]	Loss: 0.028393
         Train Epoch: [60/2467]	Loss: 0.088872
 Train Epoch: [60/2467]	Loss: 0.121329
     Train Epoch: [60/2467]	Loss: 0.216992
          Train Epoch: [80/2467]	Loss: 0.091202Train Epoch: [80/2467]	Loss: 0.246556

     Train Epoch: [80/2467]	Loss: 0.094222
     Train Epoch: [80/2467]	Loss: 0.052368
         Train Epoch: [100/2467]	Loss: 0.076762
     Train Epoch: [100/2467]	Loss: 0.153913
 Train Epoch: [100/2467]	Loss: 0.080684
     Train Epoch: [100/2467]	Loss: 0.180715
     Train Epoch: [120/2467]	Loss: 0.042629
         Train Epoch: [120/2467]	Loss: 0.317642
 Train Epoch: [120/2467]	Loss: 0.184201
     Train Epoch: [120/2467]	Loss: 0.084998
         Train Epoch: [140/2467]	Loss: 0.031233
     Train Epoch: [140/2467]	Loss: 0.097991
 Train Epoch: [140/2467]	Loss: 0.162166
     Train Epoch: [140/2467]	Loss: 0.055305
     Train Epoch: [160/2467]	Loss: 0.081104
         Train Epoch: [160/2467]	Loss: 0.114203
 Train Epoch: [160/2467]	Loss: 0.208483
     Train Epoch: [160/2467]	Loss: 0.107262
         Train Epoch: [180/2467]	Loss: 0.179741
 Train Epoch: [180/2467]	Loss: 0.079318
         Train Epoch: [180/2467]	Loss: 0.101576
 Train Epoch: [180/2467]	Loss: 0.121936
         Train Epoch: [200/2467]	Loss: 0.102614
 Train Epoch: [200/2467]	Loss: 0.054398
     Train Epoch: [200/2467]	Loss: 0.060384
     Train Epoch: [200/2467]	Loss: 0.050707
         Train Epoch: [220/2467]	Loss: 0.055101
     Train Epoch: [220/2467]	Loss: 0.141580
 Train Epoch: [220/2467]	Loss: 0.203165
     Train Epoch: [220/2467]	Loss: 0.128748
     Train Epoch: [240/2467]	Loss: 0.016654
          Train Epoch: [240/2467]	Loss: 0.216594Train Epoch: [240/2467]	Loss: 0.036552

     Train Epoch: [240/2467]	Loss: 0.155325
     Train Epoch: [260/2467]	Loss: 0.044516    
 Train Epoch: [260/2467]	Loss: 0.072834
     Train Epoch: [260/2467]	Loss: 0.055488
     Train Epoch: [260/2467]	Loss: 0.056317
     Train Epoch: [280/2467]	Loss: 0.105572
         Train Epoch: [280/2467]	Loss: 0.153300 
Train Epoch: [280/2467]	Loss: 0.135532
     Train Epoch: [280/2467]	Loss: 0.083780
         Train Epoch: [300/2467]	Loss: 0.133037 
Train Epoch: [300/2467]	Loss: 0.036064
         Train Epoch: [300/2467]	Loss: 0.066079
 Train Epoch: [300/2467]	Loss: 0.330977
     Train Epoch: [320/2467]	Loss: 0.181577
         Train Epoch: [320/2467]	Loss: 0.381600
 Train Epoch: [320/2467]	Loss: 0.217986
     Train Epoch: [320/2467]	Loss: 0.169584
         Train Epoch: [340/2467]	Loss: 0.060051
 Train Epoch: [340/2467]	Loss: 0.041552
         Train Epoch: [340/2467]	Loss: 0.016192
 Train Epoch: [340/2467]	Loss: 0.158418
     Train Epoch: [360/2467]	Loss: 0.059800
          Train Epoch: [360/2467]	Loss: 0.131907Train Epoch: [360/2467]	Loss: 0.059945

     Train Epoch: [360/2467]	Loss: 0.188065
     Train Epoch: [380/2467]	Loss: 0.076272    
          Train Epoch: [380/2467]	Loss: 0.170652Train Epoch: [380/2467]	Loss: 0.085481

 Train Epoch: [380/2467]	Loss: 0.295948
     Train Epoch: [400/2467]	Loss: 0.061444
         Train Epoch: [400/2467]	Loss: 0.152350
     Train Epoch: [400/2467]	Loss: 0.045583
 Train Epoch: [400/2467]	Loss: 0.217136
     Train Epoch: [420/2467]	Loss: 0.256567
     Train Epoch: [420/2467]	Loss: 0.117670
     Train Epoch: [420/2467]	Loss: 0.082736
     Train Epoch: [420/2467]	Loss: 0.203355
     Train Epoch: [440/2467]	Loss: 0.130565
     Train Epoch: [440/2467]	Loss: 0.317638
         Train Epoch: [440/2467]	Loss: 0.069498
 Train Epoch: [440/2467]	Loss: 0.046668
     Train Epoch: [460/2467]	Loss: 0.138932
         Train Epoch: [460/2467]	Loss: 0.122596 
Train Epoch: [460/2467]	Loss: 0.073315
     Train Epoch: [460/2467]	Loss: 0.216068
          Train Epoch: [480/2467]	Loss: 0.318591Train Epoch: [480/2467]	Loss: 0.251104

     Train Epoch: [480/2467]	Loss: 0.025988
     Train Epoch: [480/2467]	Loss: 0.162496
     Train Epoch: [500/2467]	Loss: 0.091118
          Train Epoch: [500/2467]	Loss: 0.095918Train Epoch: [500/2467]	Loss: 0.092278

     Train Epoch: [500/2467]	Loss: 0.281799
     Train Epoch: [520/2467]	Loss: 0.101854
             Train Epoch: [520/2467]	Loss: 0.428499
  Train Epoch: [520/2467]	Loss: 0.075749Train Epoch: [520/2467]	Loss: 0.113684

     Train Epoch: [540/2467]	Loss: 0.147226
     Train Epoch: [540/2467]	Loss: 0.228318
     Train Epoch: [540/2467]	Loss: 0.048554
     Train Epoch: [540/2467]	Loss: 0.087906
     Train Epoch: [560/2467]	Loss: 0.131942
         Train Epoch: [560/2467]	Loss: 0.154812
 Train Epoch: [560/2467]	Loss: 0.124381
     Train Epoch: [560/2467]	Loss: 0.159847
     Train Epoch: [580/2467]	Loss: 0.236139
             Train Epoch: [580/2467]	Loss: 0.118505
  Train Epoch: [580/2467]	Loss: 0.058827Train Epoch: [580/2467]	Loss: 0.131117

     Train Epoch: [600/2467]	Loss: 0.112484    
 Train Epoch: [600/2467]	Loss: 0.166143
     Train Epoch: [600/2467]	Loss: 0.113209
     Train Epoch: [600/2467]	Loss: 0.164248
         Train Epoch: [620/2467]	Loss: 0.099269
 Train Epoch: [620/2467]	Loss: 0.029411
          Train Epoch: [620/2467]	Loss: 0.188000Train Epoch: [620/2467]	Loss: 0.127483

         Train Epoch: [640/2467]	Loss: 0.155065
     Train Epoch: [640/2467]	Loss: 0.054861
 Train Epoch: [640/2467]	Loss: 0.164838
     Train Epoch: [640/2467]	Loss: 0.085603
     Train Epoch: [660/2467]	Loss: 0.021846
               Train Epoch: [660/2467]	Loss: 0.134813Train Epoch: [660/2467]	Loss: 0.045427
Train Epoch: [660/2467]	Loss: 0.343635

     Train Epoch: [680/2467]	Loss: 0.110374
             Train Epoch: [680/2467]	Loss: 0.044376
  Train Epoch: [680/2467]	Loss: 0.064630
Train Epoch: [680/2467]	Loss: 0.029013
         Train Epoch: [700/2467]	Loss: 0.058812
 Train Epoch: [700/2467]	Loss: 0.282478
         Train Epoch: [700/2467]	Loss: 0.067158
 Train Epoch: [700/2467]	Loss: 0.301014
     Train Epoch: [720/2467]	Loss: 0.014758    
         Train Epoch: [720/2467]	Loss: 0.139328
 Train Epoch: [720/2467]	Loss: 0.067001
 Train Epoch: [720/2467]	Loss: 0.082618
     Train Epoch: [740/2467]	Loss: 0.046358
         Train Epoch: [740/2467]	Loss: 0.068994
 Train Epoch: [740/2467]	Loss: 0.299125
     Train Epoch: [740/2467]	Loss: 0.075746
         Train Epoch: [760/2467]	Loss: 0.081279
 Train Epoch: [760/2467]	Loss: 0.269502
          Train Epoch: [760/2467]	Loss: 0.245290Train Epoch: [760/2467]	Loss: 0.345667

     Train Epoch: [780/2467]	Loss: 0.340694
     Train Epoch: [780/2467]	Loss: 0.297043    
 Train Epoch: [780/2467]	Loss: 0.275845
     Train Epoch: [780/2467]	Loss: 0.123805
         Train Epoch: [800/2467]	Loss: 0.177814
 Train Epoch: [800/2467]	Loss: 0.170795
          Train Epoch: [800/2467]	Loss: 0.156514
Train Epoch: [800/2467]	Loss: 0.092551
         Train Epoch: [820/2467]	Loss: 0.035558
 Train Epoch: [820/2467]	Loss: 0.374364
         Train Epoch: [820/2467]	Loss: 0.046313
 Train Epoch: [820/2467]	Loss: 0.170955
     Train Epoch: [840/2467]	Loss: 0.310171
         Train Epoch: [840/2467]	Loss: 0.082489
     Train Epoch: [840/2467]	Loss: 0.100769
 Train Epoch: [840/2467]	Loss: 0.045391
     Train Epoch: [860/2467]	Loss: 0.215379
     Train Epoch: [860/2467]	Loss: 0.111663
     Train Epoch: [860/2467]	Loss: 0.044837
     Train Epoch: [860/2467]	Loss: 0.055755
         Train Epoch: [880/2467]	Loss: 0.124787
 Train Epoch: [880/2467]	Loss: 0.117472
          Train Epoch: [880/2467]	Loss: 0.196214
Train Epoch: [880/2467]	Loss: 0.195845
     Train Epoch: [900/2467]	Loss: 0.167363
         Train Epoch: [900/2467]	Loss: 0.069226
     Train Epoch: [900/2467]	Loss: 0.147121
 Train Epoch: [900/2467]	Loss: 0.195070
     Train Epoch: [920/2467]	Loss: 0.235946
         Train Epoch: [920/2467]	Loss: 0.209673
 Train Epoch: [920/2467]	Loss: 0.045322
     Train Epoch: [920/2467]	Loss: 0.231008
          Train Epoch: [940/2467]	Loss: 0.025970
Train Epoch: [940/2467]	Loss: 0.040420
     Train Epoch: [940/2467]	Loss: 0.187196
     Train Epoch: [940/2467]	Loss: 0.062937
         Train Epoch: [960/2467]	Loss: 0.239343
 Train Epoch: [960/2467]	Loss: 0.072575
     Train Epoch: [960/2467]	Loss: 0.067295
     Train Epoch: [960/2467]	Loss: 0.059741
         Train Epoch: [980/2467]	Loss: 0.203431
 Train Epoch: [980/2467]	Loss: 0.061762
         Train Epoch: [980/2467]	Loss: 0.099869 
Train Epoch: [980/2467]	Loss: 0.108029
          Train Epoch: [1000/2467]	Loss: 0.028692Train Epoch: [1000/2467]	Loss: 0.223301

     Train Epoch: [1000/2467]	Loss: 0.206047
     Train Epoch: [1000/2467]	Loss: 0.037239
         Train Epoch: [1020/2467]	Loss: 0.147697
 Train Epoch: [1020/2467]	Loss: 0.352038
     Train Epoch: [1020/2467]	Loss: 0.085439
     Train Epoch: [1020/2467]	Loss: 0.072145
     Train Epoch: [1040/2467]	Loss: 0.102118
     Train Epoch: [1040/2467]	Loss: 0.153918
         Train Epoch: [1040/2467]	Loss: 0.234728 
Train Epoch: [1040/2467]	Loss: 0.128374
     Train Epoch: [1060/2467]	Loss: 0.153771
         Train Epoch: [1060/2467]	Loss: 0.101017
 Train Epoch: [1060/2467]	Loss: 0.082048
     Train Epoch: [1060/2467]	Loss: 0.391193
         Train Epoch: [1080/2467]	Loss: 0.094837
 Train Epoch: [1080/2467]	Loss: 0.177244
         Train Epoch: [1080/2467]	Loss: 0.153214
 Train Epoch: [1080/2467]	Loss: 0.273467
              Train Epoch: [1100/2467]	Loss: 0.198965Train Epoch: [1100/2467]	Loss: 0.269456

 Train Epoch: [1100/2467]	Loss: 0.198601
     Train Epoch: [1100/2467]	Loss: 0.240330
     Train Epoch: [1120/2467]	Loss: 0.240137
         Train Epoch: [1120/2467]	Loss: 0.114635
 Train Epoch: [1120/2467]	Loss: 0.091955
     Train Epoch: [1120/2467]	Loss: 0.143053
     Train Epoch: [1140/2467]	Loss: 0.334949
         Train Epoch: [1140/2467]	Loss: 0.248203
 Train Epoch: [1140/2467]	Loss: 0.068316
     Train Epoch: [1140/2467]	Loss: 0.051017
     Train Epoch: [1160/2467]	Loss: 0.063481
     Train Epoch: [1160/2467]	Loss: 0.125556
     Train Epoch: [1160/2467]	Loss: 0.215187
     Train Epoch: [1160/2467]	Loss: 0.109160
         Train Epoch: [1180/2467]	Loss: 0.101312
     Train Epoch: [1180/2467]	Loss: 0.130524
 Train Epoch: [1180/2467]	Loss: 0.148619
     Train Epoch: [1180/2467]	Loss: 0.045833
         Train Epoch: [1200/2467]	Loss: 0.066645
 Train Epoch: [1200/2467]	Loss: 0.112776
     Train Epoch: [1200/2467]	Loss: 0.224971
     Train Epoch: [1200/2467]	Loss: 0.265377
         Train Epoch: [1220/2467]	Loss: 0.154324
 Train Epoch: [1220/2467]	Loss: 0.092372
         Train Epoch: [1220/2467]	Loss: 0.226311
 Train Epoch: [1220/2467]	Loss: 0.216903
              Train Epoch: [1240/2467]	Loss: 0.238952Train Epoch: [1240/2467]	Loss: 0.119813

 Train Epoch: [1240/2467]	Loss: 0.119427
     Train Epoch: [1240/2467]	Loss: 0.255490
     Train Epoch: [1260/2467]	Loss: 0.057218
         Train Epoch: [1260/2467]	Loss: 0.047714
 Train Epoch: [1260/2467]	Loss: 0.395932    
 Train Epoch: [1260/2467]	Loss: 0.091828
     Train Epoch: [1280/2467]	Loss: 0.128255    
     Train Epoch: [1280/2467]	Loss: 0.191372    
 Train Epoch: [1280/2467]	Loss: 0.067550
 Train Epoch: [1280/2467]	Loss: 0.427136
     Train Epoch: [1300/2467]	Loss: 0.266612
     Train Epoch: [1300/2467]	Loss: 0.052271    
 Train Epoch: [1300/2467]	Loss: 0.165047
     Train Epoch: [1300/2467]	Loss: 0.084435
         Train Epoch: [1320/2467]	Loss: 0.132590
 Train Epoch: [1320/2467]	Loss: 0.095568
     Train Epoch: [1320/2467]	Loss: 0.223243
     Train Epoch: [1320/2467]	Loss: 0.041592
                  Train Epoch: [1340/2467]	Loss: 0.115033 Train Epoch: [1340/2467]	Loss: 0.126842

Train Epoch: [1340/2467]	Loss: 0.258435 
Train Epoch: [1340/2467]	Loss: 0.190960
     Train Epoch: [1360/2467]	Loss: 0.059797
     Train Epoch: [1360/2467]	Loss: 0.108840
     Train Epoch: [1360/2467]	Loss: 0.048261
     Train Epoch: [1360/2467]	Loss: 0.098925
              Train Epoch: [1380/2467]	Loss: 0.237572Train Epoch: [1380/2467]	Loss: 0.153817

 Train Epoch: [1380/2467]	Loss: 0.265587
     Train Epoch: [1380/2467]	Loss: 0.219453
         Train Epoch: [1400/2467]	Loss: 0.134198
     Train Epoch: [1400/2467]	Loss: 0.083366
 Train Epoch: [1400/2467]	Loss: 0.122007
     Train Epoch: [1400/2467]	Loss: 0.086996
         Train Epoch: [1420/2467]	Loss: 0.072623 
Train Epoch: [1420/2467]	Loss: 0.338445
     Train Epoch: [1420/2467]	Loss: 0.139118
     Train Epoch: [1420/2467]	Loss: 0.120220
     Train Epoch: [1440/2467]	Loss: 0.021371    
 Train Epoch: [1440/2467]	Loss: 0.055730
     Train Epoch: [1440/2467]	Loss: 0.042318
     Train Epoch: [1440/2467]	Loss: 1.020570
     Train Epoch: [1460/2467]	Loss: 0.042716
     Train Epoch: [1460/2467]	Loss: 0.103837
     Train Epoch: [1460/2467]	Loss: 0.157939
     Train Epoch: [1460/2467]	Loss: 0.090887
     Train Epoch: [1480/2467]	Loss: 0.020394
          Train Epoch: [1480/2467]	Loss: 0.185309Train Epoch: [1480/2467]	Loss: 0.013735

     Train Epoch: [1480/2467]	Loss: 0.259101
             Train Epoch: [1500/2467]	Loss: 0.015466
     Train Epoch: [1500/2467]	Loss: 0.131701 
Train Epoch: [1500/2467]	Loss: 0.103723
 Train Epoch: [1500/2467]	Loss: 0.063584
          Train Epoch: [1520/2467]	Loss: 0.123044Train Epoch: [1520/2467]	Loss: 0.183472          Train Epoch: [1520/2467]	Loss: 0.139789Train Epoch: [1520/2467]	Loss: 0.209973



     Train Epoch: [1540/2467]	Loss: 0.087120
         Train Epoch: [1540/2467]	Loss: 0.264177
     Train Epoch: [1540/2467]	Loss: 0.031738
 Train Epoch: [1540/2467]	Loss: 0.333218
     Train Epoch: [1560/2467]	Loss: 0.415139
         Train Epoch: [1560/2467]	Loss: 0.528977
 Train Epoch: [1560/2467]	Loss: 0.092073
     Train Epoch: [1560/2467]	Loss: 0.255197
         Train Epoch: [1580/2467]	Loss: 0.077562
 Train Epoch: [1580/2467]	Loss: 0.204325
          Train Epoch: [1580/2467]	Loss: 0.237716Train Epoch: [1580/2467]	Loss: 0.088997

         Train Epoch: [1600/2467]	Loss: 0.202017
         Train Epoch: [1600/2467]	Loss: 0.045285
  Train Epoch: [1600/2467]	Loss: 0.222723Train Epoch: [1600/2467]	Loss: 0.108201

     Train Epoch: [1620/2467]	Loss: 0.146627
     Train Epoch: [1620/2467]	Loss: 0.224099
     Train Epoch: [1620/2467]	Loss: 0.086595
     Train Epoch: [1620/2467]	Loss: 0.052679
         Train Epoch: [1640/2467]	Loss: 0.180458
     Train Epoch: [1640/2467]	Loss: 0.125549
 Train Epoch: [1640/2467]	Loss: 0.199143
     Train Epoch: [1640/2467]	Loss: 0.068987
     Train Epoch: [1660/2467]	Loss: 0.123327    
 Train Epoch: [1660/2467]	Loss: 0.171439
         Train Epoch: [1660/2467]	Loss: 0.050046
 Train Epoch: [1660/2467]	Loss: 0.168470
     Train Epoch: [1680/2467]	Loss: 0.126054
              Train Epoch: [1680/2467]	Loss: 0.189555Train Epoch: [1680/2467]	Loss: 0.038671

 Train Epoch: [1680/2467]	Loss: 0.150302
         Train Epoch: [1700/2467]	Loss: 0.070772
 Train Epoch: [1700/2467]	Loss: 0.281114
     Train Epoch: [1700/2467]	Loss: 0.170104
     Train Epoch: [1700/2467]	Loss: 0.103690
         Train Epoch: [1720/2467]	Loss: 0.185124
     Train Epoch: [1720/2467]	Loss: 0.223949
     Train Epoch: [1720/2467]	Loss: 0.044505
 Train Epoch: [1720/2467]	Loss: 0.137297
     Train Epoch: [1740/2467]	Loss: 0.037186
         Train Epoch: [1740/2467]	Loss: 0.318030
 Train Epoch: [1740/2467]	Loss: 0.018610
     Train Epoch: [1740/2467]	Loss: 0.173075
     Train Epoch: [1760/2467]	Loss: 0.115363    
     Train Epoch: [1760/2467]	Loss: 0.046965 
Train Epoch: [1760/2467]	Loss: 0.103764
     Train Epoch: [1760/2467]	Loss: 0.072774
         Train Epoch: [1780/2467]	Loss: 0.118853
 Train Epoch: [1780/2467]	Loss: 0.188660
     Train Epoch: [1780/2467]	Loss: 0.075287
     Train Epoch: [1780/2467]	Loss: 0.200183
     Train Epoch: [1800/2467]	Loss: 0.082178
         Train Epoch: [1800/2467]	Loss: 0.128929
 Train Epoch: [1800/2467]	Loss: 0.268125
     Train Epoch: [1800/2467]	Loss: 0.271237
     Train Epoch: [1820/2467]	Loss: 0.169818
         Train Epoch: [1820/2467]	Loss: 0.110445 
Train Epoch: [1820/2467]	Loss: 0.099419
     Train Epoch: [1820/2467]	Loss: 0.269489
     Train Epoch: [1840/2467]	Loss: 0.186201
         Train Epoch: [1840/2467]	Loss: 0.265549 
Train Epoch: [1840/2467]	Loss: 0.040545
     Train Epoch: [1840/2467]	Loss: 0.128941
         Train Epoch: [1860/2467]	Loss: 0.109878
     Train Epoch: [1860/2467]	Loss: 0.127289
     Train Epoch: [1860/2467]	Loss: 0.089316
 Train Epoch: [1860/2467]	Loss: 0.147569
         Train Epoch: [1880/2467]	Loss: 0.091191
 Train Epoch: [1880/2467]	Loss: 0.025659
     Train Epoch: [1880/2467]	Loss: 0.056372
     Train Epoch: [1880/2467]	Loss: 0.040338
         Train Epoch: [1900/2467]	Loss: 0.111666 
Train Epoch: [1900/2467]	Loss: 0.248805
     Train Epoch: [1900/2467]	Loss: 0.240599
     Train Epoch: [1900/2467]	Loss: 0.106888
     Train Epoch: [1920/2467]	Loss: 0.013250
          Train Epoch: [1920/2467]	Loss: 0.174363Train Epoch: [1920/2467]	Loss: 0.103531

     Train Epoch: [1920/2467]	Loss: 0.156150
     Train Epoch: [1940/2467]	Loss: 0.056396
     Train Epoch: [1940/2467]	Loss: 0.117236
     Train Epoch: [1940/2467]	Loss: 0.311168
     Train Epoch: [1940/2467]	Loss: 0.137989
     Train Epoch: [1960/2467]	Loss: 0.103010
         Train Epoch: [1960/2467]	Loss: 0.105639
 Train Epoch: [1960/2467]	Loss: 0.073967
     Train Epoch: [1960/2467]	Loss: 0.160877
         Train Epoch: [1980/2467]	Loss: 0.091544 
Train Epoch: [1980/2467]	Loss: 0.205480
     Train Epoch: [1980/2467]	Loss: 0.196166
     Train Epoch: [1980/2467]	Loss: 0.221061
         Train Epoch: [2000/2467]	Loss: 0.121056
 Train Epoch: [2000/2467]	Loss: 0.014542
     Train Epoch: [2000/2467]	Loss: 0.046319
     Train Epoch: [2000/2467]	Loss: 0.168626
     Train Epoch: [2020/2467]	Loss: 0.143388
     Train Epoch: [2020/2467]	Loss: 0.169870
     Train Epoch: [2020/2467]	Loss: 0.048164
     Train Epoch: [2020/2467]	Loss: 0.118528
          Train Epoch: [2040/2467]	Loss: 0.143083Train Epoch: [2040/2467]	Loss: 0.069670

         Train Epoch: [2040/2467]	Loss: 0.076323
 Train Epoch: [2040/2467]	Loss: 0.352815
     Train Epoch: [2060/2467]	Loss: 0.053695
          Train Epoch: [2060/2467]	Loss: 0.195242Train Epoch: [2060/2467]	Loss: 0.084846

     Train Epoch: [2060/2467]	Loss: 0.172963
     Train Epoch: [2080/2467]	Loss: 0.212210
     Train Epoch: [2080/2467]	Loss: 0.034109
     Train Epoch: [2080/2467]	Loss: 0.059945
     Train Epoch: [2080/2467]	Loss: 0.040064
         Train Epoch: [2100/2467]	Loss: 0.403258
 Train Epoch: [2100/2467]	Loss: 0.090604
         Train Epoch: [2100/2467]	Loss: 0.083287
 Train Epoch: [2100/2467]	Loss: 0.049800
     Train Epoch: [2120/2467]	Loss: 0.039131
             Train Epoch: [2120/2467]	Loss: 0.167444
  Train Epoch: [2120/2467]	Loss: 0.147715Train Epoch: [2120/2467]	Loss: 0.082107

         Train Epoch: [2140/2467]	Loss: 0.466148
     Train Epoch: [2140/2467]	Loss: 0.232586
 Train Epoch: [2140/2467]	Loss: 0.297638
     Train Epoch: [2140/2467]	Loss: 0.171290
          Train Epoch: [2160/2467]	Loss: 0.050897Train Epoch: [2160/2467]	Loss: 0.281139

          Train Epoch: [2160/2467]	Loss: 0.100402
Train Epoch: [2160/2467]	Loss: 0.073618
     Train Epoch: [2180/2467]	Loss: 0.242352
         Train Epoch: [2180/2467]	Loss: 0.148981
 Train Epoch: [2180/2467]	Loss: 0.133527
     Train Epoch: [2180/2467]	Loss: 0.061908
          Train Epoch: [2200/2467]	Loss: 0.111825
Train Epoch: [2200/2467]	Loss: 0.030519    
 Train Epoch: [2200/2467]	Loss: 0.089813    
 Train Epoch: [2200/2467]	Loss: 0.079781
     Train Epoch: [2220/2467]	Loss: 0.074764
         Train Epoch: [2220/2467]	Loss: 0.128760
 Train Epoch: [2220/2467]	Loss: 0.084413
     Train Epoch: [2220/2467]	Loss: 0.102165
     Train Epoch: [2240/2467]	Loss: 0.090499
         Train Epoch: [2240/2467]	Loss: 0.022341
 Train Epoch: [2240/2467]	Loss: 0.037902
     Train Epoch: [2240/2467]	Loss: 0.074707
     Train Epoch: [2260/2467]	Loss: 0.156771
          Train Epoch: [2260/2467]	Loss: 0.186001Train Epoch: [2260/2467]	Loss: 0.033753

     Train Epoch: [2260/2467]	Loss: 0.246026
              Train Epoch: [2280/2467]	Loss: 0.106883Train Epoch: [2280/2467]	Loss: 0.136468

 Train Epoch: [2280/2467]	Loss: 0.065321
     Train Epoch: [2280/2467]	Loss: 0.096525
          Train Epoch: [2300/2467]	Loss: 0.055626Train Epoch: [2300/2467]	Loss: 0.274887

     Train Epoch: [2300/2467]	Loss: 0.041353    
 Train Epoch: [2300/2467]	Loss: 0.081450
     Train Epoch: [2320/2467]	Loss: 0.149390
          Train Epoch: [2320/2467]	Loss: 0.124917Train Epoch: [2320/2467]	Loss: 0.017881

     Train Epoch: [2320/2467]	Loss: 0.139547
     Train Epoch: [2340/2467]	Loss: 0.099579
     Train Epoch: [2340/2467]	Loss: 0.193831
     Train Epoch: [2340/2467]	Loss: 0.435699
     Train Epoch: [2340/2467]	Loss: 0.172831
             Train Epoch: [2360/2467]	Loss: 0.135129
      Train Epoch: [2360/2467]	Loss: 0.050447
Train Epoch: [2360/2467]	Loss: 0.320219
 Train Epoch: [2360/2467]	Loss: 0.127818
     Train Epoch: [2380/2467]	Loss: 0.068676
     Train Epoch: [2380/2467]	Loss: 0.366353
     Train Epoch: [2380/2467]	Loss: 0.152056
     Train Epoch: [2380/2467]	Loss: 0.193371
             Train Epoch: [2400/2467]	Loss: 0.188164
 Train Epoch: [2400/2467]	Loss: 0.090687
 Train Epoch: [2400/2467]	Loss: 0.148198    
 Train Epoch: [2400/2467]	Loss: 0.098179
         Train Epoch: [2420/2467]	Loss: 0.089149
 Train Epoch: [2420/2467]	Loss: 0.029211
     Train Epoch: [2420/2467]	Loss: 0.189152
     Train Epoch: [2420/2467]	Loss: 0.260726
     Train Epoch: [2440/2467]	Loss: 0.014080
     Train Epoch: [2440/2467]	Loss: 0.230451
          Train Epoch: [2440/2467]	Loss: 0.138284
Train Epoch: [2440/2467]	Loss: 0.094828
     Train Epoch: [2460/2467]	Loss: 0.115486
         Train Epoch: [2460/2467]	Loss: 0.070013
     Train Epoch: [2460/2467]	Loss: 0.086724
 Train Epoch: [2460/2467]	Loss: 0.228974
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 34 epoch =====
     2025-05-11.12-25-39
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 34 epoch =====
     2025-05-11.12-25-40
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 34 epoch =====
     2025-05-11.12-25-40
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
     ===== running 34 epoch =====
     2025-05-11.12-25-40
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
         Train Epoch: [0/2467]	Loss: 0.203859
     Train Epoch: [0/2467]	Loss: 0.065887
 Train Epoch: [0/2467]	Loss: 0.271535
     Train Epoch: [0/2467]	Loss: 0.163051
     Train Epoch: [20/2467]	Loss: 0.116468
         Train Epoch: [20/2467]	Loss: 0.286389    
 Train Epoch: [20/2467]	Loss: 0.050553
 Train Epoch: [20/2467]	Loss: 0.107564
              Train Epoch: [40/2467]	Loss: 0.382223 
Train Epoch: [40/2467]	Loss: 0.086341    
Train Epoch: [40/2467]	Loss: 0.107456
 Train Epoch: [40/2467]	Loss: 0.112827
     Train Epoch: [60/2467]	Loss: 0.183432
         Train Epoch: [60/2467]	Loss: 0.024073
     Train Epoch: [60/2467]	Loss: 0.115999
 Train Epoch: [60/2467]	Loss: 0.069249
     Train Epoch: [80/2467]	Loss: 0.252279
         Train Epoch: [80/2467]	Loss: 0.094932
     Train Epoch: [80/2467]	Loss: 0.054098
 Train Epoch: [80/2467]	Loss: 0.105379
     Train Epoch: [100/2467]	Loss: 0.087030
          Train Epoch: [100/2467]	Loss: 0.073513Train Epoch: [100/2467]	Loss: 0.154793

     Train Epoch: [100/2467]	Loss: 0.183559
     Train Epoch: [120/2467]	Loss: 0.042064
     Train Epoch: [120/2467]	Loss: 0.316604
     Train Epoch: [120/2467]	Loss: 0.185747
     Train Epoch: [120/2467]	Loss: 0.074132
         Train Epoch: [140/2467]	Loss: 0.022936
 Train Epoch: [140/2467]	Loss: 0.104793
         Train Epoch: [140/2467]	Loss: 0.149138
 Train Epoch: [140/2467]	Loss: 0.049278
     Train Epoch: [160/2467]	Loss: 0.072325
         Train Epoch: [160/2467]	Loss: 0.111872
 Train Epoch: [160/2467]	Loss: 0.220262
     Train Epoch: [160/2467]	Loss: 0.117018
     Train Epoch: [180/2467]	Loss: 0.103301
     Train Epoch: [180/2467]	Loss: 0.100943
     Train Epoch: [180/2467]	Loss: 0.093568
     Train Epoch: [180/2467]	Loss: 0.180589
         Train Epoch: [200/2467]	Loss: 0.107417
 Train Epoch: [200/2467]	Loss: 0.052449
     Train Epoch: [200/2467]	Loss: 0.058828
     Train Epoch: [200/2467]	Loss: 0.046974
         Train Epoch: [220/2467]	Loss: 0.053582
 Train Epoch: [220/2467]	Loss: 0.149273
     Train Epoch: [220/2467]	Loss: 0.201887
     Train Epoch: [220/2467]	Loss: 0.097991
         Train Epoch: [240/2467]	Loss: 0.017800
 Train Epoch: [240/2467]	Loss: 0.195069
     Train Epoch: [240/2467]	Loss: 0.037295    
 Train Epoch: [240/2467]	Loss: 0.141217
     Train Epoch: [260/2467]	Loss: 0.051336
     Train Epoch: [260/2467]	Loss: 0.053557    
     Train Epoch: [260/2467]	Loss: 0.031156
 Train Epoch: [260/2467]	Loss: 0.057648
         Train Epoch: [280/2467]	Loss: 0.072049
 Train Epoch: [280/2467]	Loss: 0.208418    
     Train Epoch: [280/2467]	Loss: 0.116113
 Train Epoch: [280/2467]	Loss: 0.077488
          Train Epoch: [300/2467]	Loss: 0.032081
Train Epoch: [300/2467]	Loss: 0.167041
         Train Epoch: [300/2467]	Loss: 0.066775 
Train Epoch: [300/2467]	Loss: 0.310224
     Train Epoch: [320/2467]	Loss: 0.138754
     Train Epoch: [320/2467]	Loss: 0.253412
     Train Epoch: [320/2467]	Loss: 0.159219
     Train Epoch: [320/2467]	Loss: 0.367690
     Train Epoch: [340/2467]	Loss: 0.043186
     Train Epoch: [340/2467]	Loss: 0.171527
     Train Epoch: [340/2467]	Loss: 0.065562
     Train Epoch: [340/2467]	Loss: 0.014442
         Train Epoch: [360/2467]	Loss: 0.062485
 Train Epoch: [360/2467]	Loss: 0.052010
     Train Epoch: [360/2467]	Loss: 0.137235
     Train Epoch: [360/2467]	Loss: 0.192828
     Train Epoch: [380/2467]	Loss: 0.062732
     Train Epoch: [380/2467]	Loss: 0.296136
         Train Epoch: [380/2467]	Loss: 0.169470
 Train Epoch: [380/2467]	Loss: 0.125818
     Train Epoch: [400/2467]	Loss: 0.058719
     Train Epoch: [400/2467]	Loss: 0.025719
         Train Epoch: [400/2467]	Loss: 0.154435
 Train Epoch: [400/2467]	Loss: 0.089527
     Train Epoch: [420/2467]	Loss: 0.200949
         Train Epoch: [420/2467]	Loss: 0.102559 
Train Epoch: [420/2467]	Loss: 0.268252
     Train Epoch: [420/2467]	Loss: 0.079980
     Train Epoch: [440/2467]	Loss: 0.123982
     Train Epoch: [440/2467]	Loss: 0.334474    
 Train Epoch: [440/2467]	Loss: 0.076687
     Train Epoch: [440/2467]	Loss: 0.050085
     Train Epoch: [460/2467]	Loss: 0.207106
             Train Epoch: [460/2467]	Loss: 0.147832
 Train Epoch: [460/2467]	Loss: 0.076107
 Train Epoch: [460/2467]	Loss: 0.119278
         Train Epoch: [480/2467]	Loss: 0.034444
 Train Epoch: [480/2467]	Loss: 0.141296
     Train Epoch: [480/2467]	Loss: 0.311957    
 Train Epoch: [480/2467]	Loss: 0.258022
     Train Epoch: [500/2467]	Loss: 0.033500
     Train Epoch: [500/2467]	Loss: 0.088105
     Train Epoch: [500/2467]	Loss: 0.074770
     Train Epoch: [500/2467]	Loss: 0.183861
         Train Epoch: [520/2467]	Loss: 0.297613
 Train Epoch: [520/2467]	Loss: 0.094230
         Train Epoch: [520/2467]	Loss: 0.081143 
Train Epoch: [520/2467]	Loss: 0.060248
         Train Epoch: [540/2467]	Loss: 0.144502 
Train Epoch: [540/2467]	Loss: 0.224449
     Train Epoch: [540/2467]	Loss: 0.081470
     Train Epoch: [540/2467]	Loss: 0.053446
     Train Epoch: [560/2467]	Loss: 0.145266
         Train Epoch: [560/2467]	Loss: 0.158424
 Train Epoch: [560/2467]	Loss: 0.126019
     Train Epoch: [560/2467]	Loss: 0.119911
         Train Epoch: [580/2467]	Loss: 0.256563 
Train Epoch: [580/2467]	Loss: 0.118156
     Train Epoch: [580/2467]	Loss: 0.122308
     Train Epoch: [580/2467]	Loss: 0.051637
         Train Epoch: [600/2467]	Loss: 0.169449
 Train Epoch: [600/2467]	Loss: 0.109128
         Train Epoch: [600/2467]	Loss: 0.157080
 Train Epoch: [600/2467]	Loss: 0.142993
         Train Epoch: [620/2467]	Loss: 0.123465
 Train Epoch: [620/2467]	Loss: 0.025567
         Train Epoch: [620/2467]	Loss: 0.123229
 Train Epoch: [620/2467]	Loss: 0.161303
         Train Epoch: [640/2467]	Loss: 0.102854 
Train Epoch: [640/2467]	Loss: 0.170590
         Train Epoch: [640/2467]	Loss: 0.102038 
Train Epoch: [640/2467]	Loss: 0.034974
     Train Epoch: [660/2467]	Loss: 0.036809
          Train Epoch: [660/2467]	Loss: 0.330994Train Epoch: [660/2467]	Loss: 0.136824

     Train Epoch: [660/2467]	Loss: 0.020880
     Train Epoch: [680/2467]	Loss: 0.103037
         Train Epoch: [680/2467]	Loss: 0.038839
 Train Epoch: [680/2467]	Loss: 0.069136
     Train Epoch: [680/2467]	Loss: 0.029458
         Train Epoch: [700/2467]	Loss: 0.038169
 Train Epoch: [700/2467]	Loss: 0.298952
          Train Epoch: [700/2467]	Loss: 0.069832Train Epoch: [700/2467]	Loss: 0.254929

     Train Epoch: [720/2467]	Loss: 0.019423
         Train Epoch: [720/2467]	Loss: 0.061820 
Train Epoch: [720/2467]	Loss: 0.158722
     Train Epoch: [720/2467]	Loss: 0.107948
          Train Epoch: [740/2467]	Loss: 0.075555Train Epoch: [740/2467]	Loss: 0.078215

     Train Epoch: [740/2467]	Loss: 0.299442
     Train Epoch: [740/2467]	Loss: 0.039839
     Train Epoch: [760/2467]	Loss: 0.043318
     Train Epoch: [760/2467]	Loss: 0.230795
          Train Epoch: [760/2467]	Loss: 0.353646Train Epoch: [760/2467]	Loss: 0.236308

     Train Epoch: [780/2467]	Loss: 0.304147
         Train Epoch: [780/2467]	Loss: 0.273896
     Train Epoch: [780/2467]	Loss: 0.282524
 Train Epoch: [780/2467]	Loss: 0.159136
         Train Epoch: [800/2467]	Loss: 0.146107
     Train Epoch: [800/2467]	Loss: 0.178840
     Train Epoch: [800/2467]	Loss: 0.114634
 Train Epoch: [800/2467]	Loss: 0.080640
     Train Epoch: [820/2467]	Loss: 0.040836
             Train Epoch: [820/2467]	Loss: 0.466403
  Train Epoch: [820/2467]	Loss: 0.162412Train Epoch: [820/2467]	Loss: 0.056506

     Train Epoch: [840/2467]	Loss: 0.294362
     Train Epoch: [840/2467]	Loss: 0.076598
         Train Epoch: [840/2467]	Loss: 0.050385
 Train Epoch: [840/2467]	Loss: 0.110661
     Train Epoch: [860/2467]	Loss: 0.147121
     Train Epoch: [860/2467]	Loss: 0.046918
     Train Epoch: [860/2467]	Loss: 0.196528
     Train Epoch: [860/2467]	Loss: 0.029923
     Train Epoch: [880/2467]	Loss: 0.128355
         Train Epoch: [880/2467]	Loss: 0.149248
 Train Epoch: [880/2467]	Loss: 0.197316
     Train Epoch: [880/2467]	Loss: 0.174765
          Train Epoch: [900/2467]	Loss: 0.162006Train Epoch: [900/2467]	Loss: 0.067605

     Train Epoch: [900/2467]	Loss: 0.171368
     Train Epoch: [900/2467]	Loss: 0.170328
     Train Epoch: [920/2467]	Loss: 0.225070
         Train Epoch: [920/2467]	Loss: 0.044430
 Train Epoch: [920/2467]	Loss: 0.224456
     Train Epoch: [920/2467]	Loss: 0.234209
          Train Epoch: [940/2467]	Loss: 0.045654Train Epoch: [940/2467]	Loss: 0.036082

         Train Epoch: [940/2467]	Loss: 0.156403
 Train Epoch: [940/2467]	Loss: 0.065967
         Train Epoch: [960/2467]	Loss: 0.061572
 Train Epoch: [960/2467]	Loss: 0.243725    
     Train Epoch: [960/2467]	Loss: 0.062749
 Train Epoch: [960/2467]	Loss: 0.057907
     Train Epoch: [980/2467]	Loss: 0.054089
          Train Epoch: [980/2467]	Loss: 0.103831
Train Epoch: [980/2467]	Loss: 0.102627
     Train Epoch: [980/2467]	Loss: 0.190509
     Train Epoch: [1000/2467]	Loss: 0.029244
          Train Epoch: [1000/2467]	Loss: 0.037930Train Epoch: [1000/2467]	Loss: 0.190272

     Train Epoch: [1000/2467]	Loss: 0.225055
          Train Epoch: [1020/2467]	Loss: 0.332476
Train Epoch: [1020/2467]	Loss: 0.126925
         Train Epoch: [1020/2467]	Loss: 0.033301
 Train Epoch: [1020/2467]	Loss: 0.073229
     Train Epoch: [1040/2467]	Loss: 0.098522
     Train Epoch: [1040/2467]	Loss: 0.148849
     Train Epoch: [1040/2467]	Loss: 0.083442
     Train Epoch: [1040/2467]	Loss: 0.249924
     Train Epoch: [1060/2467]	Loss: 0.141790    
 Train Epoch: [1060/2467]	Loss: 0.086746
         Train Epoch: [1060/2467]	Loss: 0.366625
 Train Epoch: [1060/2467]	Loss: 0.094081
     Train Epoch: [1080/2467]	Loss: 0.155088
         Train Epoch: [1080/2467]	Loss: 0.145794
 Train Epoch: [1080/2467]	Loss: 0.214030
     Train Epoch: [1080/2467]	Loss: 0.100112
         Train Epoch: [1100/2467]	Loss: 0.257460
 Train Epoch: [1100/2467]	Loss: 0.206097
         Train Epoch: [1100/2467]	Loss: 0.220445
 Train Epoch: [1100/2467]	Loss: 0.237794
     Train Epoch: [1120/2467]	Loss: 0.182874
         Train Epoch: [1120/2467]	Loss: 0.108637
 Train Epoch: [1120/2467]	Loss: 0.109116
     Train Epoch: [1120/2467]	Loss: 0.130331
     Train Epoch: [1140/2467]	Loss: 0.383520
         Train Epoch: [1140/2467]	Loss: 0.044057 
Train Epoch: [1140/2467]	Loss: 0.089222
     Train Epoch: [1140/2467]	Loss: 0.239027
     Train Epoch: [1160/2467]	Loss: 0.071412
          Train Epoch: [1160/2467]	Loss: 0.239620Train Epoch: [1160/2467]	Loss: 0.111850

     Train Epoch: [1160/2467]	Loss: 0.094760
     Train Epoch: [1180/2467]	Loss: 0.121465
         Train Epoch: [1180/2467]	Loss: 0.128615
 Train Epoch: [1180/2467]	Loss: 0.131797
     Train Epoch: [1180/2467]	Loss: 0.041976
     Train Epoch: [1200/2467]	Loss: 0.103841
         Train Epoch: [1200/2467]	Loss: 0.287279
 Train Epoch: [1200/2467]	Loss: 0.096875    
 Train Epoch: [1200/2467]	Loss: 0.231917
         Train Epoch: [1220/2467]	Loss: 0.146211
 Train Epoch: [1220/2467]	Loss: 0.093511
         Train Epoch: [1220/2467]	Loss: 0.234734
 Train Epoch: [1220/2467]	Loss: 0.217368
     Train Epoch: [1240/2467]	Loss: 0.251190
         Train Epoch: [1240/2467]	Loss: 0.265665
 Train Epoch: [1240/2467]	Loss: 0.133952
     Train Epoch: [1240/2467]	Loss: 0.102418
         Train Epoch: [1260/2467]	Loss: 0.054310
 Train Epoch: [1260/2467]	Loss: 0.053142
         Train Epoch: [1260/2467]	Loss: 0.108493
 Train Epoch: [1260/2467]	Loss: 0.386365
         Train Epoch: [1280/2467]	Loss: 0.143994
 Train Epoch: [1280/2467]	Loss: 0.176189
         Train Epoch: [1280/2467]	Loss: 0.057362 
Train Epoch: [1280/2467]	Loss: 0.387588
         Train Epoch: [1300/2467]	Loss: 0.068548
 Train Epoch: [1300/2467]	Loss: 0.276678
         Train Epoch: [1300/2467]	Loss: 0.034526
 Train Epoch: [1300/2467]	Loss: 0.153335
     Train Epoch: [1320/2467]	Loss: 0.039687
         Train Epoch: [1320/2467]	Loss: 0.112317
 Train Epoch: [1320/2467]	Loss: 0.127758
     Train Epoch: [1320/2467]	Loss: 0.135102
     Train Epoch: [1340/2467]	Loss: 0.093302
         Train Epoch: [1340/2467]	Loss: 0.136215
 Train Epoch: [1340/2467]	Loss: 0.280141
     Train Epoch: [1340/2467]	Loss: 0.189530
         Train Epoch: [1360/2467]	Loss: 0.065649
 Train Epoch: [1360/2467]	Loss: 0.108502
     Train Epoch: [1360/2467]	Loss: 0.037501
     Train Epoch: [1360/2467]	Loss: 0.100681
     Train Epoch: [1380/2467]	Loss: 0.158965    
      Train Epoch: [1380/2467]	Loss: 0.245435Train Epoch: [1380/2467]	Loss: 0.235323

     Train Epoch: [1380/2467]	Loss: 0.208309
     Train Epoch: [1400/2467]	Loss: 0.071174
     Train Epoch: [1400/2467]	Loss: 0.145126
          Train Epoch: [1400/2467]	Loss: 0.090845Train Epoch: [1400/2467]	Loss: 0.114014

          Train Epoch: [1420/2467]	Loss: 0.314806
Train Epoch: [1420/2467]	Loss: 0.077792
     Train Epoch: [1420/2467]	Loss: 0.139691
     Train Epoch: [1420/2467]	Loss: 0.145903
     Train Epoch: [1440/2467]	Loss: 0.027512
         Train Epoch: [1440/2467]	Loss: 0.201914 
Train Epoch: [1440/2467]	Loss: 0.046428
     Train Epoch: [1440/2467]	Loss: 0.042897
     Train Epoch: [1460/2467]	Loss: 0.043305
         Train Epoch: [1460/2467]	Loss: 0.078206
     Train Epoch: [1460/2467]	Loss: 0.086645
 Train Epoch: [1460/2467]	Loss: 0.232397
     Train Epoch: [1480/2467]	Loss: 0.021204
         Train Epoch: [1480/2467]	Loss: 0.010526
 Train Epoch: [1480/2467]	Loss: 0.201939
     Train Epoch: [1480/2467]	Loss: 0.166365
             Train Epoch: [1500/2467]	Loss: 0.135598
     Train Epoch: [1500/2467]	Loss: 0.066454 
Train Epoch: [1500/2467]	Loss: 0.012824
 Train Epoch: [1500/2467]	Loss: 0.075975
     Train Epoch: [1520/2467]	Loss: 0.092500
     Train Epoch: [1520/2467]	Loss: 0.207069
     Train Epoch: [1520/2467]	Loss: 0.173548
     Train Epoch: [1520/2467]	Loss: 0.134068
             Train Epoch: [1540/2467]	Loss: 0.142191
      Train Epoch: [1540/2467]	Loss: 0.271070Train Epoch: [1540/2467]	Loss: 0.032983

 Train Epoch: [1540/2467]	Loss: 0.129290
     Train Epoch: [1560/2467]	Loss: 0.309746
         Train Epoch: [1560/2467]	Loss: 0.453130 
Train Epoch: [1560/2467]	Loss: 0.227188
     Train Epoch: [1560/2467]	Loss: 0.083298
         Train Epoch: [1580/2467]	Loss: 0.193689
     Train Epoch: [1580/2467]	Loss: 0.090606
     Train Epoch: [1580/2467]	Loss: 0.230972
 Train Epoch: [1580/2467]	Loss: 0.090154
     Train Epoch: [1600/2467]	Loss: 0.217987
         Train Epoch: [1600/2467]	Loss: 0.241948
 Train Epoch: [1600/2467]	Loss: 0.078609
     Train Epoch: [1600/2467]	Loss: 0.033607
         Train Epoch: [1620/2467]	Loss: 0.152682
 Train Epoch: [1620/2467]	Loss: 0.040558
     Train Epoch: [1620/2467]	Loss: 0.226473
     Train Epoch: [1620/2467]	Loss: 0.086933
         Train Epoch: [1640/2467]	Loss: 0.177176
 Train Epoch: [1640/2467]	Loss: 0.117384
     Train Epoch: [1640/2467]	Loss: 0.187342
     Train Epoch: [1640/2467]	Loss: 0.061553
          Train Epoch: [1660/2467]	Loss: 0.124436Train Epoch: [1660/2467]	Loss: 0.162893

         Train Epoch: [1660/2467]	Loss: 0.061307
 Train Epoch: [1660/2467]	Loss: 0.163264
         Train Epoch: [1680/2467]	Loss: 0.046282
 Train Epoch: [1680/2467]	Loss: 0.111868
     Train Epoch: [1680/2467]	Loss: 0.182307
     Train Epoch: [1680/2467]	Loss: 0.134214
     Train Epoch: [1700/2467]	Loss: 0.082486
     Train Epoch: [1700/2467]	Loss: 0.065398    
     Train Epoch: [1700/2467]	Loss: 0.267699
 Train Epoch: [1700/2467]	Loss: 0.119884
     Train Epoch: [1720/2467]	Loss: 0.113033
              Train Epoch: [1720/2467]	Loss: 0.214474Train Epoch: [1720/2467]	Loss: 0.038345
 
Train Epoch: [1720/2467]	Loss: 0.149850
     Train Epoch: [1740/2467]	Loss: 0.050330
             Train Epoch: [1740/2467]	Loss: 0.021305
  Train Epoch: [1740/2467]	Loss: 0.170493
Train Epoch: [1740/2467]	Loss: 0.264169
     Train Epoch: [1760/2467]	Loss: 0.053428    
 Train Epoch: [1760/2467]	Loss: 0.065816
     Train Epoch: [1760/2467]	Loss: 0.098779
     Train Epoch: [1760/2467]	Loss: 0.114174
         Train Epoch: [1780/2467]	Loss: 0.172619
 Train Epoch: [1780/2467]	Loss: 0.210678
     Train Epoch: [1780/2467]	Loss: 0.069168
     Train Epoch: [1780/2467]	Loss: 0.119337
     Train Epoch: [1800/2467]	Loss: 0.157621
     Train Epoch: [1800/2467]	Loss: 0.164119
     Train Epoch: [1800/2467]	Loss: 0.243667
     Train Epoch: [1800/2467]	Loss: 0.272742
     Train Epoch: [1820/2467]	Loss: 0.263372
              Train Epoch: [1820/2467]	Loss: 0.081143
Train Epoch: [1820/2467]	Loss: 0.138234
 Train Epoch: [1820/2467]	Loss: 0.126816
     Train Epoch: [1840/2467]	Loss: 0.092758
         Train Epoch: [1840/2467]	Loss: 0.180770
     Train Epoch: [1840/2467]	Loss: 0.252867
 Train Epoch: [1840/2467]	Loss: 0.039534
         Train Epoch: [1860/2467]	Loss: 0.126599
 Train Epoch: [1860/2467]	Loss: 0.126522    
 Train Epoch: [1860/2467]	Loss: 0.113719
     Train Epoch: [1860/2467]	Loss: 0.134627
     Train Epoch: [1880/2467]	Loss: 0.025263    
 Train Epoch: [1880/2467]	Loss: 0.074297
          Train Epoch: [1880/2467]	Loss: 0.055849Train Epoch: [1880/2467]	Loss: 0.040899

     Train Epoch: [1900/2467]	Loss: 0.093205
         Train Epoch: [1900/2467]	Loss: 0.097322
     Train Epoch: [1900/2467]	Loss: 0.243690
 Train Epoch: [1900/2467]	Loss: 0.276245
                    Train Epoch: [1920/2467]	Loss: 0.116283
Train Epoch: [1920/2467]	Loss: 0.023470
Train Epoch: [1920/2467]	Loss: 0.175205
Train Epoch: [1920/2467]	Loss: 0.140641
     Train Epoch: [1940/2467]	Loss: 0.070982
         Train Epoch: [1940/2467]	Loss: 0.100797
 Train Epoch: [1940/2467]	Loss: 0.127351    
 Train Epoch: [1940/2467]	Loss: 0.336252
     Train Epoch: [1960/2467]	Loss: 0.107744
         Train Epoch: [1960/2467]	Loss: 0.075373
 Train Epoch: [1960/2467]	Loss: 0.099490
     Train Epoch: [1960/2467]	Loss: 0.197900
          Train Epoch: [1980/2467]	Loss: 0.106031
Train Epoch: [1980/2467]	Loss: 0.171436
     Train Epoch: [1980/2467]	Loss: 0.208630
     Train Epoch: [1980/2467]	Loss: 0.220302
     Train Epoch: [2000/2467]	Loss: 0.163295    
 Train Epoch: [2000/2467]	Loss: 0.014560    
 Train Epoch: [2000/2467]	Loss: 0.132275
     Train Epoch: [2000/2467]	Loss: 0.049772
     Train Epoch: [2020/2467]	Loss: 0.116489    
      Train Epoch: [2020/2467]	Loss: 0.048414Train Epoch: [2020/2467]	Loss: 0.138327

     Train Epoch: [2020/2467]	Loss: 0.141704
     Train Epoch: [2040/2467]	Loss: 0.064333
              Train Epoch: [2040/2467]	Loss: 0.120839Train Epoch: [2040/2467]	Loss: 0.071373

 Train Epoch: [2040/2467]	Loss: 0.383383
         Train Epoch: [2060/2467]	Loss: 0.159293
     Train Epoch: [2060/2467]	Loss: 0.062802
 Train Epoch: [2060/2467]	Loss: 0.083859
     Train Epoch: [2060/2467]	Loss: 0.192293
         Train Epoch: [2080/2467]	Loss: 0.033632
          Train Epoch: [2080/2467]	Loss: 0.038198 Train Epoch: [2080/2467]	Loss: 0.208433

Train Epoch: [2080/2467]	Loss: 0.068378
     Train Epoch: [2100/2467]	Loss: 0.443654
     Train Epoch: [2100/2467]	Loss: 0.087654
     Train Epoch: [2100/2467]	Loss: 0.051957
     Train Epoch: [2100/2467]	Loss: 0.102286
         Train Epoch: [2120/2467]	Loss: 0.040183
 Train Epoch: [2120/2467]	Loss: 0.134321
     Train Epoch: [2120/2467]	Loss: 0.098898
     Train Epoch: [2120/2467]	Loss: 0.175712
         Train Epoch: [2140/2467]	Loss: 0.274033
     Train Epoch: [2140/2467]	Loss: 0.453360
 Train Epoch: [2140/2467]	Loss: 0.289497
     Train Epoch: [2140/2467]	Loss: 0.167411
     Train Epoch: [2160/2467]	Loss: 0.048477
         Train Epoch: [2160/2467]	Loss: 0.320387
     Train Epoch: [2160/2467]	Loss: 0.102472
 Train Epoch: [2160/2467]	Loss: 0.054185
     Train Epoch: [2180/2467]	Loss: 0.065526    
     Train Epoch: [2180/2467]	Loss: 0.231239
     Train Epoch: [2180/2467]	Loss: 0.134705
 Train Epoch: [2180/2467]	Loss: 0.147016
     Train Epoch: [2200/2467]	Loss: 0.120015
             Train Epoch: [2200/2467]	Loss: 0.065059
  Train Epoch: [2200/2467]	Loss: 0.029130Train Epoch: [2200/2467]	Loss: 0.092234

     Train Epoch: [2220/2467]	Loss: 0.055541
     Train Epoch: [2220/2467]	Loss: 0.098611
     Train Epoch: [2220/2467]	Loss: 0.105503
     Train Epoch: [2220/2467]	Loss: 0.073088
         Train Epoch: [2240/2467]	Loss: 0.070965
 Train Epoch: [2240/2467]	Loss: 0.084303
          Train Epoch: [2240/2467]	Loss: 0.023136Train Epoch: [2240/2467]	Loss: 0.040502

     Train Epoch: [2260/2467]	Loss: 0.173399
          Train Epoch: [2260/2467]	Loss: 0.184139Train Epoch: [2260/2467]	Loss: 0.020710

     Train Epoch: [2260/2467]	Loss: 0.246161
     Train Epoch: [2280/2467]	Loss: 0.111370
          Train Epoch: [2280/2467]	Loss: 0.072189Train Epoch: [2280/2467]	Loss: 0.088171

     Train Epoch: [2280/2467]	Loss: 0.100424
     Train Epoch: [2300/2467]	Loss: 0.232480
          Train Epoch: [2300/2467]	Loss: 0.089632Train Epoch: [2300/2467]	Loss: 0.051394

     Train Epoch: [2300/2467]	Loss: 0.041019
         Train Epoch: [2320/2467]	Loss: 0.134762
     Train Epoch: [2320/2467]	Loss: 0.015746
 Train Epoch: [2320/2467]	Loss: 0.158282
     Train Epoch: [2320/2467]	Loss: 0.185673
             Train Epoch: [2340/2467]	Loss: 0.195626     
 Train Epoch: [2340/2467]	Loss: 0.107618Train Epoch: [2340/2467]	Loss: 0.130033

 Train Epoch: [2340/2467]	Loss: 0.436751
     Train Epoch: [2360/2467]	Loss: 0.130453
     Train Epoch: [2360/2467]	Loss: 0.332665
     Train Epoch: [2360/2467]	Loss: 0.132942
     Train Epoch: [2360/2467]	Loss: 0.048758
     Train Epoch: [2380/2467]	Loss: 0.060925
     Train Epoch: [2380/2467]	Loss: 0.170330    
     Train Epoch: [2380/2467]	Loss: 0.377177
 Train Epoch: [2380/2467]	Loss: 0.153740
     Train Epoch: [2400/2467]	Loss: 0.122667    
     Train Epoch: [2400/2467]	Loss: 0.152061 
Train Epoch: [2400/2467]	Loss: 0.084788
     Train Epoch: [2400/2467]	Loss: 0.085068
     Train Epoch: [2420/2467]	Loss: 0.021861
              Train Epoch: [2420/2467]	Loss: 0.112275Train Epoch: [2420/2467]	Loss: 0.189857

 Train Epoch: [2420/2467]	Loss: 0.241507
     Train Epoch: [2440/2467]	Loss: 0.013408
          Train Epoch: [2440/2467]	Loss: 0.195732
Train Epoch: [2440/2467]	Loss: 0.124721    
 Train Epoch: [2440/2467]	Loss: 0.098101
         Train Epoch: [2460/2467]	Loss: 0.062318
     Train Epoch: [2460/2467]	Loss: 0.096807
     Train Epoch: [2460/2467]	Loss: 0.224466
 Train Epoch: [2460/2467]	Loss: 0.123301
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 35 epoch =====
     2025-05-11.12-47-31
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 35 epoch =====
     2025-05-11.12-47-31
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
     ===== running 35 epoch =====
     2025-05-11.12-47-31
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
     ===== running 35 epoch =====
     2025-05-11.12-47-31
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
          Train Epoch: [0/2467]	Loss: 0.245433Train Epoch: [0/2467]	Loss: 0.070465

     Train Epoch: [0/2467]	Loss: 0.169973
     Train Epoch: [0/2467]	Loss: 0.217214
         Train Epoch: [20/2467]	Loss: 0.300180
     Train Epoch: [20/2467]	Loss: 0.079794 
Train Epoch: [20/2467]	Loss: 0.099963
     Train Epoch: [20/2467]	Loss: 0.061302
     Train Epoch: [40/2467]	Loss: 0.088126
         Train Epoch: [40/2467]	Loss: 0.095136
 Train Epoch: [40/2467]	Loss: 0.486808
     Train Epoch: [40/2467]	Loss: 0.102474
     Train Epoch: [60/2467]	Loss: 0.037687
          Train Epoch: [60/2467]	Loss: 0.128972
Train Epoch: [60/2467]	Loss: 0.074101
     Train Epoch: [60/2467]	Loss: 0.174386
         Train Epoch: [80/2467]	Loss: 0.322380
 Train Epoch: [80/2467]	Loss: 0.097008
     Train Epoch: [80/2467]	Loss: 0.049702
     Train Epoch: [80/2467]	Loss: 0.112468
     Train Epoch: [100/2467]	Loss: 0.091784
          Train Epoch: [100/2467]	Loss: 0.080289Train Epoch: [100/2467]	Loss: 0.179620

     Train Epoch: [100/2467]	Loss: 0.184170
     Train Epoch: [120/2467]	Loss: 0.045560
     Train Epoch: [120/2467]	Loss: 0.325210    
 Train Epoch: [120/2467]	Loss: 0.195982
     Train Epoch: [120/2467]	Loss: 0.076590
     Train Epoch: [140/2467]	Loss: 0.028783
         Train Epoch: [140/2467]	Loss: 0.093719
     Train Epoch: [140/2467]	Loss: 0.209233
 Train Epoch: [140/2467]	Loss: 0.051571
     Train Epoch: [160/2467]	Loss: 0.082644
         Train Epoch: [160/2467]	Loss: 0.124501
     Train Epoch: [160/2467]	Loss: 0.196260
 Train Epoch: [160/2467]	Loss: 0.114185
         Train Epoch: [180/2467]	Loss: 0.065534
 Train Epoch: [180/2467]	Loss: 0.154543
     Train Epoch: [180/2467]	Loss: 0.095073    
 Train Epoch: [180/2467]	Loss: 0.124148
     Train Epoch: [200/2467]	Loss: 0.047281    
     Train Epoch: [200/2467]	Loss: 0.060396
 Train Epoch: [200/2467]	Loss: 0.045370
     Train Epoch: [200/2467]	Loss: 0.116273
         Train Epoch: [220/2467]	Loss: 0.052071 
Train Epoch: [220/2467]	Loss: 0.148661
     Train Epoch: [220/2467]	Loss: 0.202259
     Train Epoch: [220/2467]	Loss: 0.097398
     Train Epoch: [240/2467]	Loss: 0.019029
             Train Epoch: [240/2467]	Loss: 0.151658  
Train Epoch: [240/2467]	Loss: 0.038897Train Epoch: [240/2467]	Loss: 0.183399

     Train Epoch: [260/2467]	Loss: 0.050169
         Train Epoch: [260/2467]	Loss: 0.051728
     Train Epoch: [260/2467]	Loss: 0.027554
 Train Epoch: [260/2467]	Loss: 0.040684
         Train Epoch: [280/2467]	Loss: 0.078582
 Train Epoch: [280/2467]	Loss: 0.173242
     Train Epoch: [280/2467]	Loss: 0.152365
     Train Epoch: [280/2467]	Loss: 0.069102
         Train Epoch: [300/2467]	Loss: 0.034116 
Train Epoch: [300/2467]	Loss: 0.150886
     Train Epoch: [300/2467]	Loss: 0.058258
     Train Epoch: [300/2467]	Loss: 0.313905
     Train Epoch: [320/2467]	Loss: 0.155460
         Train Epoch: [320/2467]	Loss: 0.149784
 Train Epoch: [320/2467]	Loss: 0.377778
     Train Epoch: [320/2467]	Loss: 0.206054
             Train Epoch: [340/2467]	Loss: 0.046774
 Train Epoch: [340/2467]	Loss: 0.211250 
Train Epoch: [340/2467]	Loss: 0.014132
     Train Epoch: [340/2467]	Loss: 0.066265
     Train Epoch: [360/2467]	Loss: 0.182101
         Train Epoch: [360/2467]	Loss: 0.049528
 Train Epoch: [360/2467]	Loss: 0.056334
     Train Epoch: [360/2467]	Loss: 0.145587
         Train Epoch: [380/2467]	Loss: 0.068574 
Train Epoch: [380/2467]	Loss: 0.295271    
     Train Epoch: [380/2467]	Loss: 0.175533 
Train Epoch: [380/2467]	Loss: 0.073699
     Train Epoch: [400/2467]	Loss: 0.062439
         Train Epoch: [400/2467]	Loss: 0.156239
 Train Epoch: [400/2467]	Loss: 0.112782
     Train Epoch: [400/2467]	Loss: 0.030684
     Train Epoch: [420/2467]	Loss: 0.214990
     Train Epoch: [420/2467]	Loss: 0.270858
         Train Epoch: [420/2467]	Loss: 0.103715
 Train Epoch: [420/2467]	Loss: 0.075160
     Train Epoch: [440/2467]	Loss: 0.148465
     Train Epoch: [440/2467]	Loss: 0.297301
         Train Epoch: [440/2467]	Loss: 0.066036
 Train Epoch: [440/2467]	Loss: 0.043909
         Train Epoch: [460/2467]	Loss: 0.139000
 Train Epoch: [460/2467]	Loss: 0.228405
         Train Epoch: [460/2467]	Loss: 0.113442
 Train Epoch: [460/2467]	Loss: 0.080356
         Train Epoch: [480/2467]	Loss: 0.125974
 Train Epoch: [480/2467]	Loss: 0.072831
         Train Epoch: [480/2467]	Loss: 0.308503 
Train Epoch: [480/2467]	Loss: 0.227563
     Train Epoch: [500/2467]	Loss: 0.039038
          Train Epoch: [500/2467]	Loss: 0.068387Train Epoch: [500/2467]	Loss: 0.087158

     Train Epoch: [500/2467]	Loss: 0.147315
     Train Epoch: [520/2467]	Loss: 0.083466    
     Train Epoch: [520/2467]	Loss: 0.245224
     Train Epoch: [520/2467]	Loss: 0.105399
 Train Epoch: [520/2467]	Loss: 0.058868
     Train Epoch: [540/2467]	Loss: 0.233976
         Train Epoch: [540/2467]	Loss: 0.051591 
Train Epoch: [540/2467]	Loss: 0.075182
     Train Epoch: [540/2467]	Loss: 0.138101
     Train Epoch: [560/2467]	Loss: 0.121563
         Train Epoch: [560/2467]	Loss: 0.140379
     Train Epoch: [560/2467]	Loss: 0.120578
 Train Epoch: [560/2467]	Loss: 0.118605
             Train Epoch: [580/2467]	Loss: 0.051113 
Train Epoch: [580/2467]	Loss: 0.119192
 Train Epoch: [580/2467]	Loss: 0.116724
     Train Epoch: [580/2467]	Loss: 0.238909
         Train Epoch: [600/2467]	Loss: 0.177655
 Train Epoch: [600/2467]	Loss: 0.154930    
 Train Epoch: [600/2467]	Loss: 0.153618
     Train Epoch: [600/2467]	Loss: 0.108461
         Train Epoch: [620/2467]	Loss: 0.108394
     Train Epoch: [620/2467]	Loss: 0.027455
     Train Epoch: [620/2467]	Loss: 0.163142
 Train Epoch: [620/2467]	Loss: 0.125490
         Train Epoch: [640/2467]	Loss: 0.151751
 Train Epoch: [640/2467]	Loss: 0.033736        
 Train Epoch: [640/2467]	Loss: 0.096244
 Train Epoch: [640/2467]	Loss: 0.076915
         Train Epoch: [660/2467]	Loss: 0.021098
 Train Epoch: [660/2467]	Loss: 0.039810
         Train Epoch: [660/2467]	Loss: 0.327471
 Train Epoch: [660/2467]	Loss: 0.118244
         Train Epoch: [680/2467]	Loss: 0.110678
         Train Epoch: [680/2467]	Loss: 0.062155
  Train Epoch: [680/2467]	Loss: 0.028625Train Epoch: [680/2467]	Loss: 0.044351

             Train Epoch: [700/2467]	Loss: 0.034294
 Train Epoch: [700/2467]	Loss: 0.274457
 Train Epoch: [700/2467]	Loss: 0.269307
     Train Epoch: [700/2467]	Loss: 0.061016
          Train Epoch: [720/2467]	Loss: 0.022574Train Epoch: [720/2467]	Loss: 0.207977

          Train Epoch: [720/2467]	Loss: 0.060120Train Epoch: [720/2467]	Loss: 0.093889

     Train Epoch: [740/2467]	Loss: 0.058907
         Train Epoch: [740/2467]	Loss: 0.065467
     Train Epoch: [740/2467]	Loss: 0.281672
 Train Epoch: [740/2467]	Loss: 0.109691
         Train Epoch: [760/2467]	Loss: 0.225993
 Train Epoch: [760/2467]	Loss: 0.036750
          Train Epoch: [760/2467]	Loss: 0.348402Train Epoch: [760/2467]	Loss: 0.269164

     Train Epoch: [780/2467]	Loss: 0.334029
     Train Epoch: [780/2467]	Loss: 0.280893
     Train Epoch: [780/2467]	Loss: 0.101063
     Train Epoch: [780/2467]	Loss: 0.268153
             Train Epoch: [800/2467]	Loss: 0.165781
 Train Epoch: [800/2467]	Loss: 0.110196
 Train Epoch: [800/2467]	Loss: 0.145252
     Train Epoch: [800/2467]	Loss: 0.095510
     Train Epoch: [820/2467]	Loss: 0.363093
          Train Epoch: [820/2467]	Loss: 0.164775Train Epoch: [820/2467]	Loss: 0.048992

     Train Epoch: [820/2467]	Loss: 0.034066
         Train Epoch: [840/2467]	Loss: 0.338316
 Train Epoch: [840/2467]	Loss: 0.077098
         Train Epoch: [840/2467]	Loss: 0.068122 
Train Epoch: [840/2467]	Loss: 0.111670
         Train Epoch: [860/2467]	Loss: 0.121238
     Train Epoch: [860/2467]	Loss: 0.044329
 Train Epoch: [860/2467]	Loss: 0.028499
     Train Epoch: [860/2467]	Loss: 0.206725
         Train Epoch: [880/2467]	Loss: 0.126239
 Train Epoch: [880/2467]	Loss: 0.112802
     Train Epoch: [880/2467]	Loss: 0.166320
     Train Epoch: [880/2467]	Loss: 0.293161
     Train Epoch: [900/2467]	Loss: 0.156183
     Train Epoch: [900/2467]	Loss: 0.060664    
 Train Epoch: [900/2467]	Loss: 0.154082
     Train Epoch: [900/2467]	Loss: 0.156154
         Train Epoch: [920/2467]	Loss: 0.205934
 Train Epoch: [920/2467]	Loss: 0.230862
         Train Epoch: [920/2467]	Loss: 0.131893
 Train Epoch: [920/2467]	Loss: 0.296345
         Train Epoch: [940/2467]	Loss: 0.039409
 Train Epoch: [940/2467]	Loss: 0.031433
     Train Epoch: [940/2467]	Loss: 0.162365
     Train Epoch: [940/2467]	Loss: 0.075195
     Train Epoch: [960/2467]	Loss: 0.237010
             Train Epoch: [960/2467]	Loss: 0.066349
 Train Epoch: [960/2467]	Loss: 0.067778
 Train Epoch: [960/2467]	Loss: 0.054346
         Train Epoch: [980/2467]	Loss: 0.108823
     Train Epoch: [980/2467]	Loss: 0.108715
     Train Epoch: [980/2467]	Loss: 0.210639
 Train Epoch: [980/2467]	Loss: 0.056066
              Train Epoch: [1000/2467]	Loss: 0.032613Train Epoch: [1000/2467]	Loss: 0.035516

 Train Epoch: [1000/2467]	Loss: 0.183176
     Train Epoch: [1000/2467]	Loss: 0.222644
     Train Epoch: [1020/2467]	Loss: 0.357214
         Train Epoch: [1020/2467]	Loss: 0.066986
     Train Epoch: [1020/2467]	Loss: 0.025975
 Train Epoch: [1020/2467]	Loss: 0.123502
     Train Epoch: [1040/2467]	Loss: 0.101172
         Train Epoch: [1040/2467]	Loss: 0.222681
     Train Epoch: [1040/2467]	Loss: 0.082311
 Train Epoch: [1040/2467]	Loss: 0.153910
              Train Epoch: [1060/2467]	Loss: 0.341878Train Epoch: [1060/2467]	Loss: 0.125279

 Train Epoch: [1060/2467]	Loss: 0.071437
     Train Epoch: [1060/2467]	Loss: 0.088473
              Train Epoch: [1080/2467]	Loss: 0.090602Train Epoch: [1080/2467]	Loss: 0.141951

 Train Epoch: [1080/2467]	Loss: 0.124034
     Train Epoch: [1080/2467]	Loss: 0.201208
             Train Epoch: [1100/2467]	Loss: 0.174670
      Train Epoch: [1100/2467]	Loss: 0.243692Train Epoch: [1100/2467]	Loss: 0.163617

 Train Epoch: [1100/2467]	Loss: 0.228172
     Train Epoch: [1120/2467]	Loss: 0.202609
         Train Epoch: [1120/2467]	Loss: 0.096025
 Train Epoch: [1120/2467]	Loss: 0.096304    
 Train Epoch: [1120/2467]	Loss: 0.119313
             Train Epoch: [1140/2467]	Loss: 0.043557
  Train Epoch: [1140/2467]	Loss: 0.315006Train Epoch: [1140/2467]	Loss: 0.239227
    
 Train Epoch: [1140/2467]	Loss: 0.073079
             Train Epoch: [1160/2467]	Loss: 0.067340     
Train Epoch: [1160/2467]	Loss: 0.104454
  Train Epoch: [1160/2467]	Loss: 0.096142Train Epoch: [1160/2467]	Loss: 0.206914

         Train Epoch: [1180/2467]	Loss: 0.050770    
 Train Epoch: [1180/2467]	Loss: 0.105193
 Train Epoch: [1180/2467]	Loss: 0.121119    
 Train Epoch: [1180/2467]	Loss: 0.088696
     Train Epoch: [1200/2467]	Loss: 0.060578
          Train Epoch: [1200/2467]	Loss: 0.089650Train Epoch: [1200/2467]	Loss: 0.288970

     Train Epoch: [1200/2467]	Loss: 0.193312
         Train Epoch: [1220/2467]	Loss: 0.159373
 Train Epoch: [1220/2467]	Loss: 0.089519
         Train Epoch: [1220/2467]	Loss: 0.202810
 Train Epoch: [1220/2467]	Loss: 0.227993
     Train Epoch: [1240/2467]	Loss: 0.230401
     Train Epoch: [1240/2467]	Loss: 0.138301
     Train Epoch: [1240/2467]	Loss: 0.105759
     Train Epoch: [1240/2467]	Loss: 0.250070
     Train Epoch: [1260/2467]	Loss: 0.057395
         Train Epoch: [1260/2467]	Loss: 0.057442
 Train Epoch: [1260/2467]	Loss: 0.084750
     Train Epoch: [1260/2467]	Loss: 0.372345
         Train Epoch: [1280/2467]	Loss: 0.150903
 Train Epoch: [1280/2467]	Loss: 0.168259
     Train Epoch: [1280/2467]	Loss: 0.403226
     Train Epoch: [1280/2467]	Loss: 0.075974
     Train Epoch: [1300/2467]	Loss: 0.062241
     Train Epoch: [1300/2467]	Loss: 0.037333
     Train Epoch: [1300/2467]	Loss: 0.160056
     Train Epoch: [1300/2467]	Loss: 0.293477
     Train Epoch: [1320/2467]	Loss: 0.045412
     Train Epoch: [1320/2467]	Loss: 0.106190    
 Train Epoch: [1320/2467]	Loss: 0.127555
     Train Epoch: [1320/2467]	Loss: 0.201376
         Train Epoch: [1340/2467]	Loss: 0.179207
     Train Epoch: [1340/2467]	Loss: 0.099531
 Train Epoch: [1340/2467]	Loss: 0.127772
     Train Epoch: [1340/2467]	Loss: 0.253586
     Train Epoch: [1360/2467]	Loss: 0.108158
     Train Epoch: [1360/2467]	Loss: 0.048601    
 Train Epoch: [1360/2467]	Loss: 0.041319
     Train Epoch: [1360/2467]	Loss: 0.097289
          Train Epoch: [1380/2467]	Loss: 0.140004
Train Epoch: [1380/2467]	Loss: 0.255123
     Train Epoch: [1380/2467]	Loss: 0.209256
     Train Epoch: [1380/2467]	Loss: 0.228137
     Train Epoch: [1400/2467]	Loss: 0.074318
         Train Epoch: [1400/2467]	Loss: 0.083179 
Train Epoch: [1400/2467]	Loss: 0.143430
     Train Epoch: [1400/2467]	Loss: 0.108555
         Train Epoch: [1420/2467]	Loss: 0.322045
     Train Epoch: [1420/2467]	Loss: 0.069452
     Train Epoch: [1420/2467]	Loss: 0.123742
 Train Epoch: [1420/2467]	Loss: 0.122516
     Train Epoch: [1440/2467]	Loss: 0.014149
     Train Epoch: [1440/2467]	Loss: 0.134273
     Train Epoch: [1440/2467]	Loss: 0.047248
     Train Epoch: [1440/2467]	Loss: 0.040871
     Train Epoch: [1460/2467]	Loss: 0.086209    
 Train Epoch: [1460/2467]	Loss: 0.094794
     Train Epoch: [1460/2467]	Loss: 0.138726
     Train Epoch: [1460/2467]	Loss: 0.036851
     Train Epoch: [1480/2467]	Loss: 0.022729
     Train Epoch: [1480/2467]	Loss: 0.011742
          Train Epoch: [1480/2467]	Loss: 0.206818Train Epoch: [1480/2467]	Loss: 0.187533

     Train Epoch: [1500/2467]	Loss: 0.080217
             Train Epoch: [1500/2467]	Loss: 0.140899
  Train Epoch: [1500/2467]	Loss: 0.059054Train Epoch: [1500/2467]	Loss: 0.015653

              Train Epoch: [1520/2467]	Loss: 0.230872Train Epoch: [1520/2467]	Loss: 0.085041

 Train Epoch: [1520/2467]	Loss: 0.226511
     Train Epoch: [1520/2467]	Loss: 0.130886
         Train Epoch: [1540/2467]	Loss: 0.261225    
 Train Epoch: [1540/2467]	Loss: 0.035413
     Train Epoch: [1540/2467]	Loss: 0.074426
 Train Epoch: [1540/2467]	Loss: 0.092922
     Train Epoch: [1560/2467]	Loss: 0.069790
     Train Epoch: [1560/2467]	Loss: 0.326248
     Train Epoch: [1560/2467]	Loss: 0.424304
     Train Epoch: [1560/2467]	Loss: 0.235029
         Train Epoch: [1580/2467]	Loss: 0.103140
 Train Epoch: [1580/2467]	Loss: 0.184229
     Train Epoch: [1580/2467]	Loss: 0.261045
     Train Epoch: [1580/2467]	Loss: 0.084992
          Train Epoch: [1600/2467]	Loss: 0.035735
Train Epoch: [1600/2467]	Loss: 0.229470
         Train Epoch: [1600/2467]	Loss: 0.225317
 Train Epoch: [1600/2467]	Loss: 0.075323
         Train Epoch: [1620/2467]	Loss: 0.036408
 Train Epoch: [1620/2467]	Loss: 0.142945
         Train Epoch: [1620/2467]	Loss: 0.082352
 Train Epoch: [1620/2467]	Loss: 0.233010
         Train Epoch: [1640/2467]	Loss: 0.134289     
Train Epoch: [1640/2467]	Loss: 0.168490
     Train Epoch: [1640/2467]	Loss: 0.172275
 Train Epoch: [1640/2467]	Loss: 0.062559
     Train Epoch: [1660/2467]	Loss: 0.145376
             Train Epoch: [1660/2467]	Loss: 0.151625
  Train Epoch: [1660/2467]	Loss: 0.050134Train Epoch: [1660/2467]	Loss: 0.124853

     Train Epoch: [1680/2467]	Loss: 0.115462
         Train Epoch: [1680/2467]	Loss: 0.112107
 Train Epoch: [1680/2467]	Loss: 0.040629
     Train Epoch: [1680/2467]	Loss: 0.156767
         Train Epoch: [1700/2467]	Loss: 0.077663
 Train Epoch: [1700/2467]	Loss: 0.066810
         Train Epoch: [1700/2467]	Loss: 0.268026
 Train Epoch: [1700/2467]	Loss: 0.093121
     Train Epoch: [1720/2467]	Loss: 0.095192
         Train Epoch: [1720/2467]	Loss: 0.208285
      Train Epoch: [1720/2467]	Loss: 0.134544Train Epoch: [1720/2467]	Loss: 0.050199

         Train Epoch: [1740/2467]	Loss: 0.264097
 Train Epoch: [1740/2467]	Loss: 0.019236
     Train Epoch: [1740/2467]	Loss: 0.180039
     Train Epoch: [1740/2467]	Loss: 0.024683
     Train Epoch: [1760/2467]	Loss: 0.123084
     Train Epoch: [1760/2467]	Loss: 0.049761
     Train Epoch: [1760/2467]	Loss: 0.087120
     Train Epoch: [1760/2467]	Loss: 0.115350
         Train Epoch: [1780/2467]	Loss: 0.118113
 Train Epoch: [1780/2467]	Loss: 0.160146
         Train Epoch: [1780/2467]	Loss: 0.082828
 Train Epoch: [1780/2467]	Loss: 0.240239
         Train Epoch: [1800/2467]	Loss: 0.217494
     Train Epoch: [1800/2467]	Loss: 0.091724
 Train Epoch: [1800/2467]	Loss: 0.218607
     Train Epoch: [1800/2467]	Loss: 0.125357
         Train Epoch: [1820/2467]	Loss: 0.127360 
Train Epoch: [1820/2467]	Loss: 0.264819
     Train Epoch: [1820/2467]	Loss: 0.111912
     Train Epoch: [1820/2467]	Loss: 0.113967
     Train Epoch: [1840/2467]	Loss: 0.100212
     Train Epoch: [1840/2467]	Loss: 0.199065
         Train Epoch: [1840/2467]	Loss: 0.245260
 Train Epoch: [1840/2467]	Loss: 0.041419
     Train Epoch: [1860/2467]	Loss: 0.125615
     Train Epoch: [1860/2467]	Loss: 0.158873
         Train Epoch: [1860/2467]	Loss: 0.110837
 Train Epoch: [1860/2467]	Loss: 0.104724
         Train Epoch: [1880/2467]	Loss: 0.019068
 Train Epoch: [1880/2467]	Loss: 0.080279
     Train Epoch: [1880/2467]	Loss: 0.041714
     Train Epoch: [1880/2467]	Loss: 0.049527
         Train Epoch: [1900/2467]	Loss: 0.100817
 Train Epoch: [1900/2467]	Loss: 0.102862
         Train Epoch: [1900/2467]	Loss: 0.269430
 Train Epoch: [1900/2467]	Loss: 0.264646
     Train Epoch: [1920/2467]	Loss: 0.014100
         Train Epoch: [1920/2467]	Loss: 0.159487
     Train Epoch: [1920/2467]	Loss: 0.098098
 Train Epoch: [1920/2467]	Loss: 0.127491
         Train Epoch: [1940/2467]	Loss: 0.053906
 Train Epoch: [1940/2467]	Loss: 0.118177
     Train Epoch: [1940/2467]	Loss: 0.300262
     Train Epoch: [1940/2467]	Loss: 0.108887
     Train Epoch: [1960/2467]	Loss: 0.095260
         Train Epoch: [1960/2467]	Loss: 0.100478
     Train Epoch: [1960/2467]	Loss: 0.067570
 Train Epoch: [1960/2467]	Loss: 0.157945
             Train Epoch: [1980/2467]	Loss: 0.078522
 Train Epoch: [1980/2467]	Loss: 0.178542
 Train Epoch: [1980/2467]	Loss: 0.181523
     Train Epoch: [1980/2467]	Loss: 0.215749
     Train Epoch: [2000/2467]	Loss: 0.128374
         Train Epoch: [2000/2467]	Loss: 0.044409
 Train Epoch: [2000/2467]	Loss: 0.179690
     Train Epoch: [2000/2467]	Loss: 0.019644
     Train Epoch: [2020/2467]	Loss: 0.147174
         Train Epoch: [2020/2467]	Loss: 0.148960
 Train Epoch: [2020/2467]	Loss: 0.124859
     Train Epoch: [2020/2467]	Loss: 0.044886
         Train Epoch: [2040/2467]	Loss: 0.070173
 Train Epoch: [2040/2467]	Loss: 0.134536
         Train Epoch: [2040/2467]	Loss: 0.070255
 Train Epoch: [2040/2467]	Loss: 0.345642
     Train Epoch: [2060/2467]	Loss: 0.082318
          Train Epoch: [2060/2467]	Loss: 0.136514Train Epoch: [2060/2467]	Loss: 0.124136

     Train Epoch: [2060/2467]	Loss: 0.192438
     Train Epoch: [2080/2467]	Loss: 0.039552
              Train Epoch: [2080/2467]	Loss: 0.243794Train Epoch: [2080/2467]	Loss: 0.089658

 Train Epoch: [2080/2467]	Loss: 0.030034
     Train Epoch: [2100/2467]	Loss: 0.415099
     Train Epoch: [2100/2467]	Loss: 0.091437
     Train Epoch: [2100/2467]	Loss: 0.112480
     Train Epoch: [2100/2467]	Loss: 0.052230
         Train Epoch: [2120/2467]	Loss: 0.130758
 Train Epoch: [2120/2467]	Loss: 0.039208
         Train Epoch: [2120/2467]	Loss: 0.080989
 Train Epoch: [2120/2467]	Loss: 0.165311
     Train Epoch: [2140/2467]	Loss: 0.436804    
 Train Epoch: [2140/2467]	Loss: 0.289585    
 Train Epoch: [2140/2467]	Loss: 0.287453
     Train Epoch: [2140/2467]	Loss: 0.164600
          Train Epoch: [2160/2467]	Loss: 0.272749Train Epoch: [2160/2467]	Loss: 0.047210

         Train Epoch: [2160/2467]	Loss: 0.067118
 Train Epoch: [2160/2467]	Loss: 0.121515
     Train Epoch: [2180/2467]	Loss: 0.063640
     Train Epoch: [2180/2467]	Loss: 0.231650
     Train Epoch: [2180/2467]	Loss: 0.102209
     Train Epoch: [2180/2467]	Loss: 0.146072
         Train Epoch: [2200/2467]	Loss: 0.026159
     Train Epoch: [2200/2467]	Loss: 0.120921
     Train Epoch: [2200/2467]	Loss: 0.117383 
Train Epoch: [2200/2467]	Loss: 0.096455
     Train Epoch: [2220/2467]	Loss: 0.054290
          Train Epoch: [2220/2467]	Loss: 0.120002
Train Epoch: [2220/2467]	Loss: 0.053846
     Train Epoch: [2220/2467]	Loss: 0.101992
         Train Epoch: [2240/2467]	Loss: 0.068492
 Train Epoch: [2240/2467]	Loss: 0.082954
     Train Epoch: [2240/2467]	Loss: 0.020821
     Train Epoch: [2240/2467]	Loss: 0.046511
     Train Epoch: [2260/2467]	Loss: 0.143379
     Train Epoch: [2260/2467]	Loss: 0.181946
     Train Epoch: [2260/2467]	Loss: 0.254828
     Train Epoch: [2260/2467]	Loss: 0.034157
     Train Epoch: [2280/2467]	Loss: 0.120962
     Train Epoch: [2280/2467]	Loss: 0.080516
         Train Epoch: [2280/2467]	Loss: 0.058180
 Train Epoch: [2280/2467]	Loss: 0.083411
     Train Epoch: [2300/2467]	Loss: 0.051872    
      Train Epoch: [2300/2467]	Loss: 0.244804Train Epoch: [2300/2467]	Loss: 0.075213

     Train Epoch: [2300/2467]	Loss: 0.038878
             Train Epoch: [2320/2467]	Loss: 0.118257
 Train Epoch: [2320/2467]	Loss: 0.016860
     Train Epoch: [2320/2467]	Loss: 0.154664
 Train Epoch: [2320/2467]	Loss: 0.141679
         Train Epoch: [2340/2467]	Loss: 0.181502    
 Train Epoch: [2340/2467]	Loss: 0.120498
 Train Epoch: [2340/2467]	Loss: 0.429131
     Train Epoch: [2340/2467]	Loss: 0.093866
     Train Epoch: [2360/2467]	Loss: 0.126705
              Train Epoch: [2360/2467]	Loss: 0.289055Train Epoch: [2360/2467]	Loss: 0.127232
 
Train Epoch: [2360/2467]	Loss: 0.058769
     Train Epoch: [2380/2467]	Loss: 0.060768
         Train Epoch: [2380/2467]	Loss: 0.199735
 Train Epoch: [2380/2467]	Loss: 0.349099
     Train Epoch: [2380/2467]	Loss: 0.109741
         Train Epoch: [2400/2467]	Loss: 0.090115
 Train Epoch: [2400/2467]	Loss: 0.154121
     Train Epoch: [2400/2467]	Loss: 0.081225
     Train Epoch: [2400/2467]	Loss: 0.117749
              Train Epoch: [2420/2467]	Loss: 0.039233Train Epoch: [2420/2467]	Loss: 0.118011
 
Train Epoch: [2420/2467]	Loss: 0.198172    
 Train Epoch: [2420/2467]	Loss: 0.254505
     Train Epoch: [2440/2467]	Loss: 0.011245
         Train Epoch: [2440/2467]	Loss: 0.230681    
 Train Epoch: [2440/2467]	Loss: 0.128317
 Train Epoch: [2440/2467]	Loss: 0.112120
         Train Epoch: [2460/2467]	Loss: 0.114307
 Train Epoch: [2460/2467]	Loss: 0.079492    
     Train Epoch: [2460/2467]	Loss: 0.131996
 Train Epoch: [2460/2467]	Loss: 0.270067
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 36 epoch =====
     2025-05-11.13-09-21
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 36 epoch =====
     2025-05-11.13-09-21
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 36 epoch =====
     2025-05-11.13-09-21
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 36 epoch =====
     2025-05-11.13-09-21
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.173432
             Train Epoch: [0/2467]	Loss: 0.252503
 Train Epoch: [0/2467]	Loss: 0.210781 
Train Epoch: [0/2467]	Loss: 0.089073
     Train Epoch: [20/2467]	Loss: 0.290085    
         Train Epoch: [20/2467]	Loss: 0.123123
  Train Epoch: [20/2467]	Loss: 0.092650Train Epoch: [20/2467]	Loss: 0.043294

             Train Epoch: [40/2467]	Loss: 0.301655
  Train Epoch: [40/2467]	Loss: 0.113820
Train Epoch: [40/2467]	Loss: 0.095203
     Train Epoch: [40/2467]	Loss: 0.144614
             Train Epoch: [60/2467]	Loss: 0.113213
  Train Epoch: [60/2467]	Loss: 0.099345Train Epoch: [60/2467]	Loss: 0.034627

     Train Epoch: [60/2467]	Loss: 0.171577
              Train Epoch: [80/2467]	Loss: 0.055302Train Epoch: [80/2467]	Loss: 0.094840

 Train Epoch: [80/2467]	Loss: 0.104773
     Train Epoch: [80/2467]	Loss: 0.235952
     Train Epoch: [100/2467]	Loss: 0.061467
          Train Epoch: [100/2467]	Loss: 0.109700Train Epoch: [100/2467]	Loss: 0.138883

     Train Epoch: [100/2467]	Loss: 0.175994
     Train Epoch: [120/2467]	Loss: 0.052814
     Train Epoch: [120/2467]	Loss: 0.181541    
 Train Epoch: [120/2467]	Loss: 0.073196
     Train Epoch: [120/2467]	Loss: 0.328309
     Train Epoch: [140/2467]	Loss: 0.021769    
     Train Epoch: [140/2467]	Loss: 0.083750
     Train Epoch: [140/2467]	Loss: 0.168602
 Train Epoch: [140/2467]	Loss: 0.051615
          Train Epoch: [160/2467]	Loss: 0.109042    Train Epoch: [160/2467]	Loss: 0.069026

     Train Epoch: [160/2467]	Loss: 0.175484
 Train Epoch: [160/2467]	Loss: 0.114278
     Train Epoch: [180/2467]	Loss: 0.110910
     Train Epoch: [180/2467]	Loss: 0.075420
     Train Epoch: [180/2467]	Loss: 0.157777
     Train Epoch: [180/2467]	Loss: 0.088418
          Train Epoch: [200/2467]	Loss: 0.104059Train Epoch: [200/2467]	Loss: 0.042790

     Train Epoch: [200/2467]	Loss: 0.070963
     Train Epoch: [200/2467]	Loss: 0.047966
         Train Epoch: [220/2467]	Loss: 0.048608
 Train Epoch: [220/2467]	Loss: 0.135448
     Train Epoch: [220/2467]	Loss: 0.213926
     Train Epoch: [220/2467]	Loss: 0.096505
         Train Epoch: [240/2467]	Loss: 0.015553
 Train Epoch: [240/2467]	Loss: 0.238407
     Train Epoch: [240/2467]	Loss: 0.165186
     Train Epoch: [240/2467]	Loss: 0.032569
     Train Epoch: [260/2467]	Loss: 0.047390
     Train Epoch: [260/2467]	Loss: 0.047324
     Train Epoch: [260/2467]	Loss: 0.031112
     Train Epoch: [260/2467]	Loss: 0.030102
     Train Epoch: [280/2467]	Loss: 0.069402
     Train Epoch: [280/2467]	Loss: 0.177960
     Train Epoch: [280/2467]	Loss: 0.132532
     Train Epoch: [280/2467]	Loss: 0.101361
         Train Epoch: [300/2467]	Loss: 0.140647 
Train Epoch: [300/2467]	Loss: 0.030025
     Train Epoch: [300/2467]	Loss: 0.057899
     Train Epoch: [300/2467]	Loss: 0.291848
         Train Epoch: [320/2467]	Loss: 0.137904
 Train Epoch: [320/2467]	Loss: 0.201836
     Train Epoch: [320/2467]	Loss: 0.153340
     Train Epoch: [320/2467]	Loss: 0.374003
     Train Epoch: [340/2467]	Loss: 0.039619
          Train Epoch: [340/2467]	Loss: 0.016373Train Epoch: [340/2467]	Loss: 0.175278

     Train Epoch: [340/2467]	Loss: 0.061045
          Train Epoch: [360/2467]	Loss: 0.186612Train Epoch: [360/2467]	Loss: 0.062249

         Train Epoch: [360/2467]	Loss: 0.060143
 Train Epoch: [360/2467]	Loss: 0.142807
         Train Epoch: [380/2467]	Loss: 0.072375
 Train Epoch: [380/2467]	Loss: 0.163856
         Train Epoch: [380/2467]	Loss: 0.079204 
Train Epoch: [380/2467]	Loss: 0.263107
     Train Epoch: [400/2467]	Loss: 0.055120
         Train Epoch: [400/2467]	Loss: 0.097412
 Train Epoch: [400/2467]	Loss: 0.144704
     Train Epoch: [400/2467]	Loss: 0.044682
     Train Epoch: [420/2467]	Loss: 0.208771
     Train Epoch: [420/2467]	Loss: 0.116595
     Train Epoch: [420/2467]	Loss: 0.089542
     Train Epoch: [420/2467]	Loss: 0.258774
     Train Epoch: [440/2467]	Loss: 0.120631
     Train Epoch: [440/2467]	Loss: 0.064913    
 Train Epoch: [440/2467]	Loss: 0.041358
     Train Epoch: [440/2467]	Loss: 0.289989
         Train Epoch: [460/2467]	Loss: 0.211772
 Train Epoch: [460/2467]	Loss: 0.147915
     Train Epoch: [460/2467]	Loss: 0.083646
     Train Epoch: [460/2467]	Loss: 0.129393
          Train Epoch: [480/2467]	Loss: 0.131731Train Epoch: [480/2467]	Loss: 0.033218

     Train Epoch: [480/2467]	Loss: 0.240289
     Train Epoch: [480/2467]	Loss: 0.318208
     Train Epoch: [500/2467]	Loss: 0.042954
          Train Epoch: [500/2467]	Loss: 0.078922Train Epoch: [500/2467]	Loss: 0.067575

     Train Epoch: [500/2467]	Loss: 0.165479
     Train Epoch: [520/2467]	Loss: 0.131632
     Train Epoch: [520/2467]	Loss: 0.222138
         Train Epoch: [520/2467]	Loss: 0.094809 
Train Epoch: [520/2467]	Loss: 0.055897
          Train Epoch: [540/2467]	Loss: 0.137769Train Epoch: [540/2467]	Loss: 0.198225

         Train Epoch: [540/2467]	Loss: 0.050942
 Train Epoch: [540/2467]	Loss: 0.076121
     Train Epoch: [560/2467]	Loss: 0.125985
         Train Epoch: [560/2467]	Loss: 0.112418
     Train Epoch: [560/2467]	Loss: 0.161702
 Train Epoch: [560/2467]	Loss: 0.140822
         Train Epoch: [580/2467]	Loss: 0.225043
 Train Epoch: [580/2467]	Loss: 0.146326
     Train Epoch: [580/2467]	Loss: 0.045637    
 Train Epoch: [580/2467]	Loss: 0.122822
     Train Epoch: [600/2467]	Loss: 0.099598
     Train Epoch: [600/2467]	Loss: 0.130980
     Train Epoch: [600/2467]	Loss: 0.102215
     Train Epoch: [600/2467]	Loss: 0.161773
     Train Epoch: [620/2467]	Loss: 0.106654    
     Train Epoch: [620/2467]	Loss: 0.024647
     Train Epoch: [620/2467]	Loss: 0.121140
 Train Epoch: [620/2467]	Loss: 0.125564
          Train Epoch: [640/2467]	Loss: 0.132029
Train Epoch: [640/2467]	Loss: 0.106451
         Train Epoch: [640/2467]	Loss: 0.120350
 Train Epoch: [640/2467]	Loss: 0.034597
         Train Epoch: [660/2467]	Loss: 0.022621
 Train Epoch: [660/2467]	Loss: 0.044347
         Train Epoch: [660/2467]	Loss: 0.299638 
Train Epoch: [660/2467]	Loss: 0.117821
     Train Epoch: [680/2467]	Loss: 0.099382
          Train Epoch: [680/2467]	Loss: 0.048315Train Epoch: [680/2467]	Loss: 0.021408

     Train Epoch: [680/2467]	Loss: 0.067837
     Train Epoch: [700/2467]	Loss: 0.262380
          Train Epoch: [700/2467]	Loss: 0.287468Train Epoch: [700/2467]	Loss: 0.060099

     Train Epoch: [700/2467]	Loss: 0.033243
     Train Epoch: [720/2467]	Loss: 0.035762
     Train Epoch: [720/2467]	Loss: 0.091481
         Train Epoch: [720/2467]	Loss: 0.170094
 Train Epoch: [720/2467]	Loss: 0.088300
     Train Epoch: [740/2467]	Loss: 0.039010
             Train Epoch: [740/2467]	Loss: 0.088095
  Train Epoch: [740/2467]	Loss: 0.278947Train Epoch: [740/2467]	Loss: 0.132271

     Train Epoch: [760/2467]	Loss: 0.269632
         Train Epoch: [760/2467]	Loss: 0.309825
     Train Epoch: [760/2467]	Loss: 0.340475
 Train Epoch: [760/2467]	Loss: 0.041545
     Train Epoch: [780/2467]	Loss: 0.305795
     Train Epoch: [780/2467]	Loss: 0.281916
     Train Epoch: [780/2467]	Loss: 0.259865
     Train Epoch: [780/2467]	Loss: 0.125155
             Train Epoch: [800/2467]	Loss: 0.174061 
Train Epoch: [800/2467]	Loss: 0.130688
     Train Epoch: [800/2467]	Loss: 0.113364
 Train Epoch: [800/2467]	Loss: 0.098495
         Train Epoch: [820/2467]	Loss: 0.039666
 Train Epoch: [820/2467]	Loss: 0.341261
     Train Epoch: [820/2467]	Loss: 0.051077
     Train Epoch: [820/2467]	Loss: 0.147855
         Train Epoch: [840/2467]	Loss: 0.069508 
Train Epoch: [840/2467]	Loss: 0.084586
     Train Epoch: [840/2467]	Loss: 0.040751
     Train Epoch: [840/2467]	Loss: 0.349156
         Train Epoch: [860/2467]	Loss: 0.115476 
Train Epoch: [860/2467]	Loss: 0.208615
     Train Epoch: [860/2467]	Loss: 0.042848
     Train Epoch: [860/2467]	Loss: 0.039982
             Train Epoch: [880/2467]	Loss: 0.125239 
 Train Epoch: [880/2467]	Loss: 0.172675Train Epoch: [880/2467]	Loss: 0.195205

     Train Epoch: [880/2467]	Loss: 0.115670
     Train Epoch: [900/2467]	Loss: 0.148877
         Train Epoch: [900/2467]	Loss: 0.160915
     Train Epoch: [900/2467]	Loss: 0.041246
 Train Epoch: [900/2467]	Loss: 0.172332
         Train Epoch: [920/2467]	Loss: 0.188524
 Train Epoch: [920/2467]	Loss: 0.234394
     Train Epoch: [920/2467]	Loss: 0.037260
     Train Epoch: [920/2467]	Loss: 0.182093
     Train Epoch: [940/2467]	Loss: 0.182348
          Train Epoch: [940/2467]	Loss: 0.059289Train Epoch: [940/2467]	Loss: 0.022487

     Train Epoch: [940/2467]	Loss: 0.035587
     Train Epoch: [960/2467]	Loss: 0.224322
             Train Epoch: [960/2467]	Loss: 0.062027
  Train Epoch: [960/2467]	Loss: 0.063738Train Epoch: [960/2467]	Loss: 0.069734

         Train Epoch: [980/2467]	Loss: 0.045099
         Train Epoch: [980/2467]	Loss: 0.203329
 Train Epoch: [980/2467]	Loss: 0.085032 
Train Epoch: [980/2467]	Loss: 0.103387
         Train Epoch: [1000/2467]	Loss: 0.210157
 Train Epoch: [1000/2467]	Loss: 0.027333
     Train Epoch: [1000/2467]	Loss: 0.181771
     Train Epoch: [1000/2467]	Loss: 0.028968
     Train Epoch: [1020/2467]	Loss: 0.332554
         Train Epoch: [1020/2467]	Loss: 0.070598
 Train Epoch: [1020/2467]	Loss: 0.029376
     Train Epoch: [1020/2467]	Loss: 0.116982
         Train Epoch: [1040/2467]	Loss: 0.102060
 Train Epoch: [1040/2467]	Loss: 0.117134
         Train Epoch: [1040/2467]	Loss: 0.083076 
Train Epoch: [1040/2467]	Loss: 0.222795
     Train Epoch: [1060/2467]	Loss: 0.094620
          Train Epoch: [1060/2467]	Loss: 0.344371Train Epoch: [1060/2467]	Loss: 0.100763

     Train Epoch: [1060/2467]	Loss: 0.185220
     Train Epoch: [1080/2467]	Loss: 0.114196    
 Train Epoch: [1080/2467]	Loss: 0.211866
     Train Epoch: [1080/2467]	Loss: 0.190854
     Train Epoch: [1080/2467]	Loss: 0.142940
          Train Epoch: [1100/2467]	Loss: 0.246986
Train Epoch: [1100/2467]	Loss: 0.188728
         Train Epoch: [1100/2467]	Loss: 0.171702 
Train Epoch: [1100/2467]	Loss: 0.215853
         Train Epoch: [1120/2467]	Loss: 0.094355
 Train Epoch: [1120/2467]	Loss: 0.102877    
 Train Epoch: [1120/2467]	Loss: 0.135063
     Train Epoch: [1120/2467]	Loss: 0.228067
             Train Epoch: [1140/2467]	Loss: 0.040409
  Train Epoch: [1140/2467]	Loss: 0.075216
Train Epoch: [1140/2467]	Loss: 0.333981
     Train Epoch: [1140/2467]	Loss: 0.236333
         Train Epoch: [1160/2467]	Loss: 0.078238
 Train Epoch: [1160/2467]	Loss: 0.061906
         Train Epoch: [1160/2467]	Loss: 0.081030
 Train Epoch: [1160/2467]	Loss: 0.201333
     Train Epoch: [1180/2467]	Loss: 0.108794    
 Train Epoch: [1180/2467]	Loss: 0.071801
     Train Epoch: [1180/2467]	Loss: 0.126561
     Train Epoch: [1180/2467]	Loss: 0.048610
         Train Epoch: [1200/2467]	Loss: 0.063790
 Train Epoch: [1200/2467]	Loss: 0.095276
     Train Epoch: [1200/2467]	Loss: 0.177196
     Train Epoch: [1200/2467]	Loss: 0.256181
         Train Epoch: [1220/2467]	Loss: 0.209971
 Train Epoch: [1220/2467]	Loss: 0.085168
         Train Epoch: [1220/2467]	Loss: 0.204859
 Train Epoch: [1220/2467]	Loss: 0.218170
         Train Epoch: [1240/2467]	Loss: 0.257162
     Train Epoch: [1240/2467]	Loss: 0.240051
 Train Epoch: [1240/2467]	Loss: 0.113797
     Train Epoch: [1240/2467]	Loss: 0.093335
     Train Epoch: [1260/2467]	Loss: 0.077695
     Train Epoch: [1260/2467]	Loss: 0.043126
         Train Epoch: [1260/2467]	Loss: 0.377333
 Train Epoch: [1260/2467]	Loss: 0.098846
     Train Epoch: [1280/2467]	Loss: 0.125013    
 Train Epoch: [1280/2467]	Loss: 0.181576    
 Train Epoch: [1280/2467]	Loss: 0.059836
     Train Epoch: [1280/2467]	Loss: 0.404107
     Train Epoch: [1300/2467]	Loss: 0.058337
             Train Epoch: [1300/2467]	Loss: 0.043767
  Train Epoch: [1300/2467]	Loss: 0.244571Train Epoch: [1300/2467]	Loss: 0.157714

     Train Epoch: [1320/2467]	Loss: 0.039911
         Train Epoch: [1320/2467]	Loss: 0.127759
     Train Epoch: [1320/2467]	Loss: 0.117877
 Train Epoch: [1320/2467]	Loss: 0.169879
     Train Epoch: [1340/2467]	Loss: 0.179042
     Train Epoch: [1340/2467]	Loss: 0.107566    
     Train Epoch: [1340/2467]	Loss: 0.276461
 Train Epoch: [1340/2467]	Loss: 0.124263
         Train Epoch: [1360/2467]	Loss: 0.046337
 Train Epoch: [1360/2467]	Loss: 0.111467
         Train Epoch: [1360/2467]	Loss: 0.105672 
Train Epoch: [1360/2467]	Loss: 0.033947
         Train Epoch: [1380/2467]	Loss: 0.154772
     Train Epoch: [1380/2467]	Loss: 0.216542
 Train Epoch: [1380/2467]	Loss: 0.246934
     Train Epoch: [1380/2467]	Loss: 0.199865
     Train Epoch: [1400/2467]	Loss: 0.141324
         Train Epoch: [1400/2467]	Loss: 0.154363
 Train Epoch: [1400/2467]	Loss: 0.080944
     Train Epoch: [1400/2467]	Loss: 0.073025
     Train Epoch: [1420/2467]	Loss: 0.299138    
 Train Epoch: [1420/2467]	Loss: 0.079088    
 Train Epoch: [1420/2467]	Loss: 0.104989
     Train Epoch: [1420/2467]	Loss: 0.133210
         Train Epoch: [1440/2467]	Loss: 0.014110
     Train Epoch: [1440/2467]	Loss: 0.043266
 Train Epoch: [1440/2467]	Loss: 0.071128
     Train Epoch: [1440/2467]	Loss: 0.041925
     Train Epoch: [1460/2467]	Loss: 0.040428
         Train Epoch: [1460/2467]	Loss: 0.097790
     Train Epoch: [1460/2467]	Loss: 0.087453
 Train Epoch: [1460/2467]	Loss: 0.176952
     Train Epoch: [1480/2467]	Loss: 0.018889
         Train Epoch: [1480/2467]	Loss: 0.010156
 Train Epoch: [1480/2467]	Loss: 0.178557
     Train Epoch: [1480/2467]	Loss: 0.195229
     Train Epoch: [1500/2467]	Loss: 0.128140
     Train Epoch: [1500/2467]	Loss: 0.017516
     Train Epoch: [1500/2467]	Loss: 0.062283
     Train Epoch: [1500/2467]	Loss: 0.153197
          Train Epoch: [1520/2467]	Loss: 0.130127Train Epoch: [1520/2467]	Loss: 0.084814

         Train Epoch: [1520/2467]	Loss: 0.208280
 Train Epoch: [1520/2467]	Loss: 0.184293
     Train Epoch: [1540/2467]	Loss: 0.077056
     Train Epoch: [1540/2467]	Loss: 0.019262
         Train Epoch: [1540/2467]	Loss: 0.113091
 Train Epoch: [1540/2467]	Loss: 0.250212
     Train Epoch: [1560/2467]	Loss: 0.097813
              Train Epoch: [1560/2467]	Loss: 0.291602Train Epoch: [1560/2467]	Loss: 0.220604

 Train Epoch: [1560/2467]	Loss: 0.414635
         Train Epoch: [1580/2467]	Loss: 0.061053
         Train Epoch: [1580/2467]	Loss: 0.195855
  Train Epoch: [1580/2467]	Loss: 0.224570Train Epoch: [1580/2467]	Loss: 0.094165

     Train Epoch: [1600/2467]	Loss: 0.025113
     Train Epoch: [1600/2467]	Loss: 0.188573
         Train Epoch: [1600/2467]	Loss: 0.071359
 Train Epoch: [1600/2467]	Loss: 0.211977
         Train Epoch: [1620/2467]	Loss: 0.035327 
Train Epoch: [1620/2467]	Loss: 0.140441
         Train Epoch: [1620/2467]	Loss: 0.084845
 Train Epoch: [1620/2467]	Loss: 0.223098
     Train Epoch: [1640/2467]	Loss: 0.163589    
 Train Epoch: [1640/2467]	Loss: 0.123447
         Train Epoch: [1640/2467]	Loss: 0.249744
 Train Epoch: [1640/2467]	Loss: 0.053883
         Train Epoch: [1660/2467]	Loss: 0.116409
 Train Epoch: [1660/2467]	Loss: 0.153795
         Train Epoch: [1660/2467]	Loss: 0.053518 
Train Epoch: [1660/2467]	Loss: 0.169758
     Train Epoch: [1680/2467]	Loss: 0.138477
     Train Epoch: [1680/2467]	Loss: 0.040802
         Train Epoch: [1680/2467]	Loss: 0.130519
 Train Epoch: [1680/2467]	Loss: 0.131477
     Train Epoch: [1700/2467]	Loss: 0.118197
     Train Epoch: [1700/2467]	Loss: 0.062178    
     Train Epoch: [1700/2467]	Loss: 0.262717
 Train Epoch: [1700/2467]	Loss: 0.093102
     Train Epoch: [1720/2467]	Loss: 0.118604
         Train Epoch: [1720/2467]	Loss: 0.191808    
 Train Epoch: [1720/2467]	Loss: 0.039440
 Train Epoch: [1720/2467]	Loss: 0.119351
     Train Epoch: [1740/2467]	Loss: 0.023338
     Train Epoch: [1740/2467]	Loss: 0.267196    
     Train Epoch: [1740/2467]	Loss: 0.020414
 Train Epoch: [1740/2467]	Loss: 0.155265
         Train Epoch: [1760/2467]	Loss: 0.105625
 Train Epoch: [1760/2467]	Loss: 0.091728
         Train Epoch: [1760/2467]	Loss: 0.064164 
Train Epoch: [1760/2467]	Loss: 0.044261
     Train Epoch: [1780/2467]	Loss: 0.133414
         Train Epoch: [1780/2467]	Loss: 0.073107
 Train Epoch: [1780/2467]	Loss: 0.251838
     Train Epoch: [1780/2467]	Loss: 0.173496
         Train Epoch: [1800/2467]	Loss: 0.237416
 Train Epoch: [1800/2467]	Loss: 0.094628
         Train Epoch: [1800/2467]	Loss: 0.108156
 Train Epoch: [1800/2467]	Loss: 0.218468
     Train Epoch: [1820/2467]	Loss: 0.125723
              Train Epoch: [1820/2467]	Loss: 0.088758 
Train Epoch: [1820/2467]	Loss: 0.278538Train Epoch: [1820/2467]	Loss: 0.125063

              Train Epoch: [1840/2467]	Loss: 0.296329Train Epoch: [1840/2467]	Loss: 0.133954

 Train Epoch: [1840/2467]	Loss: 0.036797
     Train Epoch: [1840/2467]	Loss: 0.117335
                  Train Epoch: [1860/2467]	Loss: 0.155904Train Epoch: [1860/2467]	Loss: 0.121219

 Train Epoch: [1860/2467]	Loss: 0.100736 
Train Epoch: [1860/2467]	Loss: 0.115595
         Train Epoch: [1880/2467]	Loss: 0.073874
         Train Epoch: [1880/2467]	Loss: 0.020143 
Train Epoch: [1880/2467]	Loss: 0.051022
 Train Epoch: [1880/2467]	Loss: 0.041152
     Train Epoch: [1900/2467]	Loss: 0.089794
         Train Epoch: [1900/2467]	Loss: 0.098213
 Train Epoch: [1900/2467]	Loss: 0.217675
     Train Epoch: [1900/2467]	Loss: 0.237970
     Train Epoch: [1920/2467]	Loss: 0.016924
          Train Epoch: [1920/2467]	Loss: 0.105098Train Epoch: [1920/2467]	Loss: 0.167272

     Train Epoch: [1920/2467]	Loss: 0.138866
              Train Epoch: [1940/2467]	Loss: 0.113366Train Epoch: [1940/2467]	Loss: 0.052647

 Train Epoch: [1940/2467]	Loss: 0.101633
     Train Epoch: [1940/2467]	Loss: 0.308239
     Train Epoch: [1960/2467]	Loss: 0.105303
         Train Epoch: [1960/2467]	Loss: 0.141342 
    Train Epoch: [1960/2467]	Loss: 0.100603
 Train Epoch: [1960/2467]	Loss: 0.094726
         Train Epoch: [1980/2467]	Loss: 0.095482
     Train Epoch: [1980/2467]	Loss: 0.220749
 Train Epoch: [1980/2467]	Loss: 0.168432
     Train Epoch: [1980/2467]	Loss: 0.228742
          Train Epoch: [2000/2467]	Loss: 0.115743Train Epoch: [2000/2467]	Loss: 0.022358

     Train Epoch: [2000/2467]	Loss: 0.193301
     Train Epoch: [2000/2467]	Loss: 0.055246
     Train Epoch: [2020/2467]	Loss: 0.158078
     Train Epoch: [2020/2467]	Loss: 0.118966
     Train Epoch: [2020/2467]	Loss: 0.059131
     Train Epoch: [2020/2467]	Loss: 0.129295
         Train Epoch: [2040/2467]	Loss: 0.138756
 Train Epoch: [2040/2467]	Loss: 0.061121
     Train Epoch: [2040/2467]	Loss: 0.348256
     Train Epoch: [2040/2467]	Loss: 0.084927
     Train Epoch: [2060/2467]	Loss: 0.157643
     Train Epoch: [2060/2467]	Loss: 0.055301
     Train Epoch: [2060/2467]	Loss: 0.092207
     Train Epoch: [2060/2467]	Loss: 0.193299
         Train Epoch: [2080/2467]	Loss: 0.215197
     Train Epoch: [2080/2467]	Loss: 0.069729
 Train Epoch: [2080/2467]	Loss: 0.022980
     Train Epoch: [2080/2467]	Loss: 0.037051
     Train Epoch: [2100/2467]	Loss: 0.385432
     Train Epoch: [2100/2467]	Loss: 0.094253    
     Train Epoch: [2100/2467]	Loss: 0.082997 
Train Epoch: [2100/2467]	Loss: 0.031733
          Train Epoch: [2120/2467]	Loss: 0.033699Train Epoch: [2120/2467]	Loss: 0.091646

         Train Epoch: [2120/2467]	Loss: 0.079723 
Train Epoch: [2120/2467]	Loss: 0.175554
     Train Epoch: [2140/2467]	Loss: 0.450681
         Train Epoch: [2140/2467]	Loss: 0.234673
 Train Epoch: [2140/2467]	Loss: 0.250864
     Train Epoch: [2140/2467]	Loss: 0.156411
          Train Epoch: [2160/2467]	Loss: 0.274674
Train Epoch: [2160/2467]	Loss: 0.037458
         Train Epoch: [2160/2467]	Loss: 0.094872 
Train Epoch: [2160/2467]	Loss: 0.049951
     Train Epoch: [2180/2467]	Loss: 0.054430
          Train Epoch: [2180/2467]	Loss: 0.087546Train Epoch: [2180/2467]	Loss: 0.216805

     Train Epoch: [2180/2467]	Loss: 0.131611
         Train Epoch: [2200/2467]	Loss: 0.041156
 Train Epoch: [2200/2467]	Loss: 0.117408
         Train Epoch: [2200/2467]	Loss: 0.085024
 Train Epoch: [2200/2467]	Loss: 0.096830
     Train Epoch: [2220/2467]	Loss: 0.069047
          Train Epoch: [2220/2467]	Loss: 0.103394Train Epoch: [2220/2467]	Loss: 0.069414

     Train Epoch: [2220/2467]	Loss: 0.108503
         Train Epoch: [2240/2467]	Loss: 0.091370
     Train Epoch: [2240/2467]	Loss: 0.065047    
 Train Epoch: [2240/2467]	Loss: 0.018453
 Train Epoch: [2240/2467]	Loss: 0.038538
     Train Epoch: [2260/2467]	Loss: 0.187927
     Train Epoch: [2260/2467]	Loss: 0.181672
         Train Epoch: [2260/2467]	Loss: 0.304318
 Train Epoch: [2260/2467]	Loss: 0.023445
     Train Epoch: [2280/2467]	Loss: 0.124502
          Train Epoch: [2280/2467]	Loss: 0.086504
    Train Epoch: [2280/2467]	Loss: 0.055419
 Train Epoch: [2280/2467]	Loss: 0.086968
     Train Epoch: [2300/2467]	Loss: 0.235128
     Train Epoch: [2300/2467]	Loss: 0.042691
     Train Epoch: [2300/2467]	Loss: 0.077023
     Train Epoch: [2300/2467]	Loss: 0.055193
     Train Epoch: [2320/2467]	Loss: 0.120754
         Train Epoch: [2320/2467]	Loss: 0.020927
 Train Epoch: [2320/2467]	Loss: 0.122727    
 Train Epoch: [2320/2467]	Loss: 0.147569
     Train Epoch: [2340/2467]	Loss: 0.099759
     Train Epoch: [2340/2467]	Loss: 0.212601
         Train Epoch: [2340/2467]	Loss: 0.129968
 Train Epoch: [2340/2467]	Loss: 0.410835
     Train Epoch: [2360/2467]	Loss: 0.132171
     Train Epoch: [2360/2467]	Loss: 0.314367
     Train Epoch: [2360/2467]	Loss: 0.112131
     Train Epoch: [2360/2467]	Loss: 0.046084
     Train Epoch: [2380/2467]	Loss: 0.056733
     Train Epoch: [2380/2467]	Loss: 0.148844
     Train Epoch: [2380/2467]	Loss: 0.329576    
 Train Epoch: [2380/2467]	Loss: 0.119664
             Train Epoch: [2400/2467]	Loss: 0.116502
 Train Epoch: [2400/2467]	Loss: 0.093172 
Train Epoch: [2400/2467]	Loss: 0.147212
     Train Epoch: [2400/2467]	Loss: 0.073057
         Train Epoch: [2420/2467]	Loss: 0.019192
 Train Epoch: [2420/2467]	Loss: 0.081155
         Train Epoch: [2420/2467]	Loss: 0.248983
 Train Epoch: [2420/2467]	Loss: 0.187460
         Train Epoch: [2440/2467]	Loss: 0.012638 
Train Epoch: [2440/2467]	Loss: 0.214887
         Train Epoch: [2440/2467]	Loss: 0.121937
 Train Epoch: [2440/2467]	Loss: 0.089780
         Train Epoch: [2460/2467]	Loss: 0.114648
 Train Epoch: [2460/2467]	Loss: 0.065114
     Train Epoch: [2460/2467]	Loss: 0.088831    
 Train Epoch: [2460/2467]	Loss: 0.220471
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 37 epoch =====
     2025-05-11.13-31-11
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 37 epoch =====
     2025-05-11.13-31-11
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 37 epoch =====
     2025-05-11.13-31-11
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 37 epoch =====
     2025-05-11.13-31-11
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.076006
         Train Epoch: [0/2467]	Loss: 0.248521
     Train Epoch: [0/2467]	Loss: 0.159901
 Train Epoch: [0/2467]	Loss: 0.208214
         Train Epoch: [20/2467]	Loss: 0.312383
         Train Epoch: [20/2467]	Loss: 0.081515
  Train Epoch: [20/2467]	Loss: 0.092066Train Epoch: [20/2467]	Loss: 0.053424

     Train Epoch: [40/2467]	Loss: 0.089971
         Train Epoch: [40/2467]	Loss: 0.098055
 Train Epoch: [40/2467]	Loss: 0.370634
     Train Epoch: [40/2467]	Loss: 0.096334
         Train Epoch: [60/2467]	Loss: 0.176654 
Train Epoch: [60/2467]	Loss: 0.023741
     Train Epoch: [60/2467]	Loss: 0.078051
     Train Epoch: [60/2467]	Loss: 0.121876
         Train Epoch: [80/2467]	Loss: 0.094352    
  Train Epoch: [80/2467]	Loss: 0.047132Train Epoch: [80/2467]	Loss: 0.114153

     Train Epoch: [80/2467]	Loss: 0.225547
     Train Epoch: [100/2467]	Loss: 0.058372
         Train Epoch: [100/2467]	Loss: 0.129437 
Train Epoch: [100/2467]	Loss: 0.079081
     Train Epoch: [100/2467]	Loss: 0.169446
     Train Epoch: [120/2467]	Loss: 0.058907
     Train Epoch: [120/2467]	Loss: 0.079406
     Train Epoch: [120/2467]	Loss: 0.224858
     Train Epoch: [120/2467]	Loss: 0.325757
     Train Epoch: [140/2467]	Loss: 0.090283    
         Train Epoch: [140/2467]	Loss: 0.149885 
Train Epoch: [140/2467]	Loss: 0.052171
 Train Epoch: [140/2467]	Loss: 0.025047
         Train Epoch: [160/2467]	Loss: 0.068220 
Train Epoch: [160/2467]	Loss: 0.278595
         Train Epoch: [160/2467]	Loss: 0.192039
 Train Epoch: [160/2467]	Loss: 0.115997
     Train Epoch: [180/2467]	Loss: 0.103630
         Train Epoch: [180/2467]	Loss: 0.157900
 Train Epoch: [180/2467]	Loss: 0.071431
     Train Epoch: [180/2467]	Loss: 0.105957
         Train Epoch: [200/2467]	Loss: 0.115677
 Train Epoch: [200/2467]	Loss: 0.065729
          Train Epoch: [200/2467]	Loss: 0.089714Train Epoch: [200/2467]	Loss: 0.046493

     Train Epoch: [220/2467]	Loss: 0.094836
     Train Epoch: [220/2467]	Loss: 0.117821
     Train Epoch: [220/2467]	Loss: 0.197406
     Train Epoch: [220/2467]	Loss: 0.059879
     Train Epoch: [240/2467]	Loss: 0.017996
             Train Epoch: [240/2467]	Loss: 0.149850
 Train Epoch: [240/2467]	Loss: 0.192377 
Train Epoch: [240/2467]	Loss: 0.034910
     Train Epoch: [260/2467]	Loss: 0.049294
     Train Epoch: [260/2467]	Loss: 0.026442    
 Train Epoch: [260/2467]	Loss: 0.048112
     Train Epoch: [260/2467]	Loss: 0.045028
     Train Epoch: [280/2467]	Loss: 0.161689    
 Train Epoch: [280/2467]	Loss: 0.076846
     Train Epoch: [280/2467]	Loss: 0.097621
     Train Epoch: [280/2467]	Loss: 0.077622
         Train Epoch: [300/2467]	Loss: 0.145425
 Train Epoch: [300/2467]	Loss: 0.035344    
 Train Epoch: [300/2467]	Loss: 0.303105
     Train Epoch: [300/2467]	Loss: 0.073229
     Train Epoch: [320/2467]	Loss: 0.151350
     Train Epoch: [320/2467]	Loss: 0.327481
     Train Epoch: [320/2467]	Loss: 0.365202
     Train Epoch: [320/2467]	Loss: 0.142954
         Train Epoch: [340/2467]	Loss: 0.076537
 Train Epoch: [340/2467]	Loss: 0.043905
          Train Epoch: [340/2467]	Loss: 0.015002Train Epoch: [340/2467]	Loss: 0.167798

     Train Epoch: [360/2467]	Loss: 0.186962
         Train Epoch: [360/2467]	Loss: 0.051403
     Train Epoch: [360/2467]	Loss: 0.043041
 Train Epoch: [360/2467]	Loss: 0.151111
     Train Epoch: [380/2467]	Loss: 0.064398
          Train Epoch: [380/2467]	Loss: 0.175724Train Epoch: [380/2467]	Loss: 0.168729

     Train Epoch: [380/2467]	Loss: 0.290299
     Train Epoch: [400/2467]	Loss: 0.054046
         Train Epoch: [400/2467]	Loss: 0.031855
     Train Epoch: [400/2467]	Loss: 0.198903
 Train Epoch: [400/2467]	Loss: 0.141074
     Train Epoch: [420/2467]	Loss: 0.210082
         Train Epoch: [420/2467]	Loss: 0.269589
 Train Epoch: [420/2467]	Loss: 0.123770
     Train Epoch: [420/2467]	Loss: 0.074636
     Train Epoch: [440/2467]	Loss: 0.131826
     Train Epoch: [440/2467]	Loss: 0.297801
     Train Epoch: [440/2467]	Loss: 0.087927
     Train Epoch: [440/2467]	Loss: 0.045896
             Train Epoch: [460/2467]	Loss: 0.138519 
Train Epoch: [460/2467]	Loss: 0.081819
     Train Epoch: [460/2467]	Loss: 0.205471
 Train Epoch: [460/2467]	Loss: 0.115072
     Train Epoch: [480/2467]	Loss: 0.131871
         Train Epoch: [480/2467]	Loss: 0.029731
 Train Epoch: [480/2467]	Loss: 0.233958
     Train Epoch: [480/2467]	Loss: 0.316410
         Train Epoch: [500/2467]	Loss: 0.139018
     Train Epoch: [500/2467]	Loss: 0.089831
 Train Epoch: [500/2467]	Loss: 0.073883
     Train Epoch: [500/2467]	Loss: 0.201546
     Train Epoch: [520/2467]	Loss: 0.126900
     Train Epoch: [520/2467]	Loss: 0.233050
         Train Epoch: [520/2467]	Loss: 0.105478
 Train Epoch: [520/2467]	Loss: 0.054778
          Train Epoch: [540/2467]	Loss: 0.197836Train Epoch: [540/2467]	Loss: 0.147905

         Train Epoch: [540/2467]	Loss: 0.079343
 Train Epoch: [540/2467]	Loss: 0.058810
     Train Epoch: [560/2467]	Loss: 0.154516    
     Train Epoch: [560/2467]	Loss: 0.107781 
Train Epoch: [560/2467]	Loss: 0.130884
     Train Epoch: [560/2467]	Loss: 0.132131
     Train Epoch: [580/2467]	Loss: 0.210819
     Train Epoch: [580/2467]	Loss: 0.117803    
 Train Epoch: [580/2467]	Loss: 0.052560
     Train Epoch: [580/2467]	Loss: 0.124077
     Train Epoch: [600/2467]	Loss: 0.110183    
 Train Epoch: [600/2467]	Loss: 0.132228    
 Train Epoch: [600/2467]	Loss: 0.119822
     Train Epoch: [600/2467]	Loss: 0.155243
     Train Epoch: [620/2467]	Loss: 0.097527    
 Train Epoch: [620/2467]	Loss: 0.024686
          Train Epoch: [620/2467]	Loss: 0.141028Train Epoch: [620/2467]	Loss: 0.163378

     Train Epoch: [640/2467]	Loss: 0.101572
     Train Epoch: [640/2467]	Loss: 0.142688
     Train Epoch: [640/2467]	Loss: 0.111812
     Train Epoch: [640/2467]	Loss: 0.037107
         Train Epoch: [660/2467]	Loss: 0.024626
 Train Epoch: [660/2467]	Loss: 0.038401
          Train Epoch: [660/2467]	Loss: 0.121626Train Epoch: [660/2467]	Loss: 0.318618

         Train Epoch: [680/2467]	Loss: 0.108500
 Train Epoch: [680/2467]	Loss: 0.053698
     Train Epoch: [680/2467]	Loss: 0.061990
     Train Epoch: [680/2467]	Loss: 0.035872
     Train Epoch: [700/2467]	Loss: 0.030644
         Train Epoch: [700/2467]	Loss: 0.066266    
 Train Epoch: [700/2467]	Loss: 0.308746
 Train Epoch: [700/2467]	Loss: 0.293697
         Train Epoch: [720/2467]	Loss: 0.029324 
Train Epoch: [720/2467]	Loss: 0.140932
     Train Epoch: [720/2467]	Loss: 0.105500
     Train Epoch: [720/2467]	Loss: 0.065246
         Train Epoch: [740/2467]	Loss: 0.052116    
 Train Epoch: [740/2467]	Loss: 0.262357 
Train Epoch: [740/2467]	Loss: 0.070347
     Train Epoch: [740/2467]	Loss: 0.050855
     Train Epoch: [760/2467]	Loss: 0.065238
             Train Epoch: [760/2467]	Loss: 0.223417 
Train Epoch: [760/2467]	Loss: 0.323433
 Train Epoch: [760/2467]	Loss: 0.237380
     Train Epoch: [780/2467]	Loss: 0.344253
          Train Epoch: [780/2467]	Loss: 0.318502
Train Epoch: [780/2467]	Loss: 0.265279
     Train Epoch: [780/2467]	Loss: 0.097720
         Train Epoch: [800/2467]	Loss: 0.159927
     Train Epoch: [800/2467]	Loss: 0.162735
 Train Epoch: [800/2467]	Loss: 0.107238
     Train Epoch: [800/2467]	Loss: 0.087959
         Train Epoch: [820/2467]	Loss: 0.042780
 Train Epoch: [820/2467]	Loss: 0.377061
         Train Epoch: [820/2467]	Loss: 0.041320
 Train Epoch: [820/2467]	Loss: 0.152491
     Train Epoch: [840/2467]	Loss: 0.310737
     Train Epoch: [840/2467]	Loss: 0.086965
     Train Epoch: [840/2467]	Loss: 0.096356    
 Train Epoch: [840/2467]	Loss: 0.041435
         Train Epoch: [860/2467]	Loss: 0.224629
 Train Epoch: [860/2467]	Loss: 0.123938
     Train Epoch: [860/2467]	Loss: 0.031943
     Train Epoch: [860/2467]	Loss: 0.047014
     Train Epoch: [880/2467]	Loss: 0.125598
         Train Epoch: [880/2467]	Loss: 0.127456
     Train Epoch: [880/2467]	Loss: 0.171172
 Train Epoch: [880/2467]	Loss: 0.160540
     Train Epoch: [900/2467]	Loss: 0.168344
         Train Epoch: [900/2467]	Loss: 0.190788
 Train Epoch: [900/2467]	Loss: 0.089394
     Train Epoch: [900/2467]	Loss: 0.158019
          Train Epoch: [920/2467]	Loss: 0.241656Train Epoch: [920/2467]	Loss: 0.212695

          Train Epoch: [920/2467]	Loss: 0.094054Train Epoch: [920/2467]	Loss: 0.300844

         Train Epoch: [940/2467]	Loss: 0.034998
     Train Epoch: [940/2467]	Loss: 0.025920
 Train Epoch: [940/2467]	Loss: 0.164849
     Train Epoch: [940/2467]	Loss: 0.056325
         Train Epoch: [960/2467]	Loss: 0.256920
 Train Epoch: [960/2467]	Loss: 0.051105    
 Train Epoch: [960/2467]	Loss: 0.066420
     Train Epoch: [960/2467]	Loss: 0.059575
     Train Epoch: [980/2467]	Loss: 0.213910
             Train Epoch: [980/2467]	Loss: 0.051356
  Train Epoch: [980/2467]	Loss: 0.090581Train Epoch: [980/2467]	Loss: 0.061904

     Train Epoch: [1000/2467]	Loss: 0.034943
     Train Epoch: [1000/2467]	Loss: 0.023794
     Train Epoch: [1000/2467]	Loss: 0.166369
     Train Epoch: [1000/2467]	Loss: 0.229935
         Train Epoch: [1020/2467]	Loss: 0.130555
     Train Epoch: [1020/2467]	Loss: 0.323469
 Train Epoch: [1020/2467]	Loss: 0.078578
     Train Epoch: [1020/2467]	Loss: 0.048727
     Train Epoch: [1040/2467]	Loss: 0.103979
         Train Epoch: [1040/2467]	Loss: 0.140226
 Train Epoch: [1040/2467]	Loss: 0.228154
     Train Epoch: [1040/2467]	Loss: 0.084930
     Train Epoch: [1060/2467]	Loss: 0.117743
     Train Epoch: [1060/2467]	Loss: 0.076106
     Train Epoch: [1060/2467]	Loss: 0.097116
     Train Epoch: [1060/2467]	Loss: 0.349097
          Train Epoch: [1080/2467]	Loss: 0.098262Train Epoch: [1080/2467]	Loss: 0.187553

     Train Epoch: [1080/2467]	Loss: 0.295403
     Train Epoch: [1080/2467]	Loss: 0.153701
     Train Epoch: [1100/2467]	Loss: 0.271894
     Train Epoch: [1100/2467]	Loss: 0.186536
     Train Epoch: [1100/2467]	Loss: 0.189870
     Train Epoch: [1100/2467]	Loss: 0.231464
         Train Epoch: [1120/2467]	Loss: 0.173620
 Train Epoch: [1120/2467]	Loss: 0.111885
     Train Epoch: [1120/2467]	Loss: 0.105794
     Train Epoch: [1120/2467]	Loss: 0.089516
         Train Epoch: [1140/2467]	Loss: 0.313087
 Train Epoch: [1140/2467]	Loss: 0.042452
     Train Epoch: [1140/2467]	Loss: 0.261389    
 Train Epoch: [1140/2467]	Loss: 0.090505
         Train Epoch: [1160/2467]	Loss: 0.084558
 Train Epoch: [1160/2467]	Loss: 0.066295
     Train Epoch: [1160/2467]	Loss: 0.108811
     Train Epoch: [1160/2467]	Loss: 0.209519
     Train Epoch: [1180/2467]	Loss: 0.037587
     Train Epoch: [1180/2467]	Loss: 0.086410
     Train Epoch: [1180/2467]	Loss: 0.100177
     Train Epoch: [1180/2467]	Loss: 0.100827
     Train Epoch: [1200/2467]	Loss: 0.054823
     Train Epoch: [1200/2467]	Loss: 0.104112    
     Train Epoch: [1200/2467]	Loss: 0.185080
 Train Epoch: [1200/2467]	Loss: 0.277333
          Train Epoch: [1220/2467]	Loss: 0.213211Train Epoch: [1220/2467]	Loss: 0.078385

          Train Epoch: [1220/2467]	Loss: 0.229963Train Epoch: [1220/2467]	Loss: 0.238620

         Train Epoch: [1240/2467]	Loss: 0.264670
 Train Epoch: [1240/2467]	Loss: 0.309553
         Train Epoch: [1240/2467]	Loss: 0.115858 
Train Epoch: [1240/2467]	Loss: 0.117344
             Train Epoch: [1260/2467]	Loss: 0.051440
 Train Epoch: [1260/2467]	Loss: 0.051416 
Train Epoch: [1260/2467]	Loss: 0.359904
     Train Epoch: [1260/2467]	Loss: 0.097530
     Train Epoch: [1280/2467]	Loss: 0.146183        
  Train Epoch: [1280/2467]	Loss: 0.059503Train Epoch: [1280/2467]	Loss: 0.427309

     Train Epoch: [1280/2467]	Loss: 0.097222
     Train Epoch: [1300/2467]	Loss: 0.034461
     Train Epoch: [1300/2467]	Loss: 0.244441
     Train Epoch: [1300/2467]	Loss: 0.133660
     Train Epoch: [1300/2467]	Loss: 0.070857
     Train Epoch: [1320/2467]	Loss: 0.038899
             Train Epoch: [1320/2467]	Loss: 0.110836
  Train Epoch: [1320/2467]	Loss: 0.093313Train Epoch: [1320/2467]	Loss: 0.146696

         Train Epoch: [1340/2467]	Loss: 0.103136
 Train Epoch: [1340/2467]	Loss: 0.261995
         Train Epoch: [1340/2467]	Loss: 0.132165
 Train Epoch: [1340/2467]	Loss: 0.339582
     Train Epoch: [1360/2467]	Loss: 0.102634
         Train Epoch: [1360/2467]	Loss: 0.035354 
Train Epoch: [1360/2467]	Loss: 0.109869
     Train Epoch: [1360/2467]	Loss: 0.052667
     Train Epoch: [1380/2467]	Loss: 0.157590
     Train Epoch: [1380/2467]	Loss: 0.241796
     Train Epoch: [1380/2467]	Loss: 0.205487
     Train Epoch: [1380/2467]	Loss: 0.223798
     Train Epoch: [1400/2467]	Loss: 0.080854
          Train Epoch: [1400/2467]	Loss: 0.065663Train Epoch: [1400/2467]	Loss: 0.122160

     Train Epoch: [1400/2467]	Loss: 0.121571
         Train Epoch: [1420/2467]	Loss: 0.271773
 Train Epoch: [1420/2467]	Loss: 0.065652
          Train Epoch: [1420/2467]	Loss: 0.105271Train Epoch: [1420/2467]	Loss: 0.130459

         Train Epoch: [1440/2467]	Loss: 0.036315
     Train Epoch: [1440/2467]	Loss: 0.068727
 Train Epoch: [1440/2467]	Loss: 0.040061
     Train Epoch: [1440/2467]	Loss: 0.021067
             Train Epoch: [1460/2467]	Loss: 0.033406
  Train Epoch: [1460/2467]	Loss: 0.096670Train Epoch: [1460/2467]	Loss: 0.082793
    
 Train Epoch: [1460/2467]	Loss: 0.131100
     Train Epoch: [1480/2467]	Loss: 0.017075
     Train Epoch: [1480/2467]	Loss: 0.011455
         Train Epoch: [1480/2467]	Loss: 0.200803
 Train Epoch: [1480/2467]	Loss: 0.162997
     Train Epoch: [1500/2467]	Loss: 0.071959
         Train Epoch: [1500/2467]	Loss: 0.134683
     Train Epoch: [1500/2467]	Loss: 0.064622
 Train Epoch: [1500/2467]	Loss: 0.012547
         Train Epoch: [1520/2467]	Loss: 0.091178 
Train Epoch: [1520/2467]	Loss: 0.333412    
 Train Epoch: [1520/2467]	Loss: 0.208975
     Train Epoch: [1520/2467]	Loss: 0.131403
     Train Epoch: [1540/2467]	Loss: 0.249094
         Train Epoch: [1540/2467]	Loss: 0.086040
 Train Epoch: [1540/2467]	Loss: 0.019440
     Train Epoch: [1540/2467]	Loss: 0.081595
         Train Epoch: [1560/2467]	Loss: 0.287176
 Train Epoch: [1560/2467]	Loss: 0.236095
         Train Epoch: [1560/2467]	Loss: 0.049297
 Train Epoch: [1560/2467]	Loss: 0.392997
     Train Epoch: [1580/2467]	Loss: 0.182645
         Train Epoch: [1580/2467]	Loss: 0.096356
 Train Epoch: [1580/2467]	Loss: 0.219289
     Train Epoch: [1580/2467]	Loss: 0.067774
         Train Epoch: [1600/2467]	Loss: 0.033052
 Train Epoch: [1600/2467]	Loss: 0.180048    
     Train Epoch: [1600/2467]	Loss: 0.233494
 Train Epoch: [1600/2467]	Loss: 0.065394
         Train Epoch: [1620/2467]	Loss: 0.156368
     Train Epoch: [1620/2467]	Loss: 0.040259
 Train Epoch: [1620/2467]	Loss: 0.213807
     Train Epoch: [1620/2467]	Loss: 0.094797
          Train Epoch: [1640/2467]	Loss: 0.107784
Train Epoch: [1640/2467]	Loss: 0.178382
         Train Epoch: [1640/2467]	Loss: 0.185007
 Train Epoch: [1640/2467]	Loss: 0.065033
         Train Epoch: [1660/2467]	Loss: 0.118662
     Train Epoch: [1660/2467]	Loss: 0.140139
     Train Epoch: [1660/2467]	Loss: 0.159154
 Train Epoch: [1660/2467]	Loss: 0.044188
     Train Epoch: [1680/2467]	Loss: 0.117284
         Train Epoch: [1680/2467]	Loss: 0.041583
 Train Epoch: [1680/2467]	Loss: 0.123245
     Train Epoch: [1680/2467]	Loss: 0.204653
         Train Epoch: [1700/2467]	Loss: 0.074071
     Train Epoch: [1700/2467]	Loss: 0.062426
 Train Epoch: [1700/2467]	Loss: 0.224798
     Train Epoch: [1700/2467]	Loss: 0.099295
     Train Epoch: [1720/2467]	Loss: 0.193512
     Train Epoch: [1720/2467]	Loss: 0.035353    
 Train Epoch: [1720/2467]	Loss: 0.123744
     Train Epoch: [1720/2467]	Loss: 0.103631
              Train Epoch: [1740/2467]	Loss: 0.025686Train Epoch: [1740/2467]	Loss: 0.262786

     Train Epoch: [1740/2467]	Loss: 0.017684
 Train Epoch: [1740/2467]	Loss: 0.165498
          Train Epoch: [1760/2467]	Loss: 0.102244Train Epoch: [1760/2467]	Loss: 0.115475

     Train Epoch: [1760/2467]	Loss: 0.044008
     Train Epoch: [1760/2467]	Loss: 0.074378
     Train Epoch: [1780/2467]	Loss: 0.145577
         Train Epoch: [1780/2467]	Loss: 0.116624
 Train Epoch: [1780/2467]	Loss: 0.065922
     Train Epoch: [1780/2467]	Loss: 0.209048
          Train Epoch: [1800/2467]	Loss: 0.242448
Train Epoch: [1800/2467]	Loss: 0.092689
          Train Epoch: [1800/2467]	Loss: 0.107189Train Epoch: [1800/2467]	Loss: 0.210939

         Train Epoch: [1820/2467]	Loss: 0.125037
 Train Epoch: [1820/2467]	Loss: 0.238673
     Train Epoch: [1820/2467]	Loss: 0.087606
     Train Epoch: [1820/2467]	Loss: 0.123967
     Train Epoch: [1840/2467]	Loss: 0.122840
         Train Epoch: [1840/2467]	Loss: 0.230925
 Train Epoch: [1840/2467]	Loss: 0.037009
     Train Epoch: [1840/2467]	Loss: 0.120545
         Train Epoch: [1860/2467]	Loss: 0.105573
 Train Epoch: [1860/2467]	Loss: 0.103331
          Train Epoch: [1860/2467]	Loss: 0.140387Train Epoch: [1860/2467]	Loss: 0.083164

         Train Epoch: [1880/2467]	Loss: 0.021258
 Train Epoch: [1880/2467]	Loss: 0.070094    
      Train Epoch: [1880/2467]	Loss: 0.051185Train Epoch: [1880/2467]	Loss: 0.044060

         Train Epoch: [1900/2467]	Loss: 0.120412
 Train Epoch: [1900/2467]	Loss: 0.249441
     Train Epoch: [1900/2467]	Loss: 0.256161
     Train Epoch: [1900/2467]	Loss: 0.084273
         Train Epoch: [1920/2467]	Loss: 0.013870
     Train Epoch: [1920/2467]	Loss: 0.166414 
Train Epoch: [1920/2467]	Loss: 0.099819
     Train Epoch: [1920/2467]	Loss: 0.145485
     Train Epoch: [1940/2467]	Loss: 0.071164    
     Train Epoch: [1940/2467]	Loss: 0.210252 
Train Epoch: [1940/2467]	Loss: 0.105421
     Train Epoch: [1940/2467]	Loss: 0.298441
     Train Epoch: [1960/2467]	Loss: 0.092445
         Train Epoch: [1960/2467]	Loss: 0.162674
 Train Epoch: [1960/2467]	Loss: 0.086992
     Train Epoch: [1960/2467]	Loss: 0.081573
     Train Epoch: [1980/2467]	Loss: 0.086670
         Train Epoch: [1980/2467]	Loss: 0.178773
 Train Epoch: [1980/2467]	Loss: 0.183923
     Train Epoch: [1980/2467]	Loss: 0.218647
         Train Epoch: [2000/2467]	Loss: 0.109178
 Train Epoch: [2000/2467]	Loss: 0.016319
         Train Epoch: [2000/2467]	Loss: 0.043392
 Train Epoch: [2000/2467]	Loss: 0.160623
         Train Epoch: [2020/2467]	Loss: 0.121992
 Train Epoch: [2020/2467]	Loss: 0.114252
     Train Epoch: [2020/2467]	Loss: 0.040985
     Train Epoch: [2020/2467]	Loss: 0.135811
     Train Epoch: [2040/2467]	Loss: 0.048461
             Train Epoch: [2040/2467]	Loss: 0.143456
  Train Epoch: [2040/2467]	Loss: 0.347706Train Epoch: [2040/2467]	Loss: 0.071518

     Train Epoch: [2060/2467]	Loss: 0.152187
     Train Epoch: [2060/2467]	Loss: 0.059271    
 Train Epoch: [2060/2467]	Loss: 0.113241
     Train Epoch: [2060/2467]	Loss: 0.193549
     Train Epoch: [2080/2467]	Loss: 0.197583
     Train Epoch: [2080/2467]	Loss: 0.023358
     Train Epoch: [2080/2467]	Loss: 0.032092
     Train Epoch: [2080/2467]	Loss: 0.060697
     Train Epoch: [2100/2467]	Loss: 0.090404
              Train Epoch: [2100/2467]	Loss: 0.093468Train Epoch: [2100/2467]	Loss: 0.055447

 Train Epoch: [2100/2467]	Loss: 0.436593
     Train Epoch: [2120/2467]	Loss: 0.098421
     Train Epoch: [2120/2467]	Loss: 0.078499
          Train Epoch: [2120/2467]	Loss: 0.042986Train Epoch: [2120/2467]	Loss: 0.184331

     Train Epoch: [2140/2467]	Loss: 0.454886
         Train Epoch: [2140/2467]	Loss: 0.304267
 Train Epoch: [2140/2467]	Loss: 0.282933
     Train Epoch: [2140/2467]	Loss: 0.153133
     Train Epoch: [2160/2467]	Loss: 0.259630
     Train Epoch: [2160/2467]	Loss: 0.101310
     Train Epoch: [2160/2467]	Loss: 0.035025
     Train Epoch: [2160/2467]	Loss: 0.056578
          Train Epoch: [2180/2467]	Loss: 0.114750Train Epoch: [2180/2467]	Loss: 0.218416

     Train Epoch: [2180/2467]	Loss: 0.141278
     Train Epoch: [2180/2467]	Loss: 0.050537
         Train Epoch: [2200/2467]	Loss: 0.185249
         Train Epoch: [2200/2467]	Loss: 0.028536
 Train Epoch: [2200/2467]	Loss: 0.073758
 Train Epoch: [2200/2467]	Loss: 0.092612
         Train Epoch: [2220/2467]	Loss: 0.080247
     Train Epoch: [2220/2467]	Loss: 0.050925
 Train Epoch: [2220/2467]	Loss: 0.094955
     Train Epoch: [2220/2467]	Loss: 0.050151
         Train Epoch: [2240/2467]	Loss: 0.093344
 Train Epoch: [2240/2467]	Loss: 0.027018
     Train Epoch: [2240/2467]	Loss: 0.047322
     Train Epoch: [2240/2467]	Loss: 0.064781
     Train Epoch: [2260/2467]	Loss: 0.146878
     Train Epoch: [2260/2467]	Loss: 0.196893
          Train Epoch: [2260/2467]	Loss: 0.269765
Train Epoch: [2260/2467]	Loss: 0.034465
     Train Epoch: [2280/2467]	Loss: 0.135904
         Train Epoch: [2280/2467]	Loss: 0.086887
 Train Epoch: [2280/2467]	Loss: 0.049396
     Train Epoch: [2280/2467]	Loss: 0.081079
     Train Epoch: [2300/2467]	Loss: 0.223992
     Train Epoch: [2300/2467]	Loss: 0.054167
     Train Epoch: [2300/2467]	Loss: 0.039150    
 Train Epoch: [2300/2467]	Loss: 0.083711
     Train Epoch: [2320/2467]	Loss: 0.095187
     Train Epoch: [2320/2467]	Loss: 0.013875
         Train Epoch: [2320/2467]	Loss: 0.129325
 Train Epoch: [2320/2467]	Loss: 0.129276
     Train Epoch: [2340/2467]	Loss: 0.089150
     Train Epoch: [2340/2467]	Loss: 0.129739
         Train Epoch: [2340/2467]	Loss: 0.448150
 Train Epoch: [2340/2467]	Loss: 0.187319
     Train Epoch: [2360/2467]	Loss: 0.130977
         Train Epoch: [2360/2467]	Loss: 0.041428
      Train Epoch: [2360/2467]	Loss: 0.299383Train Epoch: [2360/2467]	Loss: 0.129933

             Train Epoch: [2380/2467]	Loss: 0.144540 
Train Epoch: [2380/2467]	Loss: 0.363867
 Train Epoch: [2380/2467]	Loss: 0.111882
     Train Epoch: [2380/2467]	Loss: 0.070515
         Train Epoch: [2400/2467]	Loss: 0.137719
 Train Epoch: [2400/2467]	Loss: 0.066886
     Train Epoch: [2400/2467]	Loss: 0.084801
     Train Epoch: [2400/2467]	Loss: 0.120143
         Train Epoch: [2420/2467]	Loss: 0.016907 
Train Epoch: [2420/2467]	Loss: 0.098782    
 Train Epoch: [2420/2467]	Loss: 0.227967
     Train Epoch: [2420/2467]	Loss: 0.204898
         Train Epoch: [2440/2467]	Loss: 0.012009
 Train Epoch: [2440/2467]	Loss: 0.201389
     Train Epoch: [2440/2467]	Loss: 0.103953
     Train Epoch: [2440/2467]	Loss: 0.126476
     Train Epoch: [2460/2467]	Loss: 0.116865
     Train Epoch: [2460/2467]	Loss: 0.080289
          Train Epoch: [2460/2467]	Loss: 0.062384Train Epoch: [2460/2467]	Loss: 0.206452

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 38 epoch =====
     2025-05-11.13-53-03
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 38 epoch =====
     2025-05-11.13-53-03
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 38 epoch =====
     2025-05-11.13-53-03
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 38 epoch =====
     2025-05-11.13-53-03
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
              Train Epoch: [0/2467]	Loss: 0.211793 Train Epoch: [0/2467]	Loss: 0.171492

Train Epoch: [0/2467]	Loss: 0.071496
     Train Epoch: [0/2467]	Loss: 0.251954
         Train Epoch: [20/2467]	Loss: 0.090912
 Train Epoch: [20/2467]	Loss: 0.295708
         Train Epoch: [20/2467]	Loss: 0.111269
 Train Epoch: [20/2467]	Loss: 0.039890
     Train Epoch: [40/2467]	Loss: 0.078823
     Train Epoch: [40/2467]	Loss: 0.103751    
 Train Epoch: [40/2467]	Loss: 0.338503
     Train Epoch: [40/2467]	Loss: 0.115297
          Train Epoch: [60/2467]	Loss: 0.025477Train Epoch: [60/2467]	Loss: 0.160071

     Train Epoch: [60/2467]	Loss: 0.072755
     Train Epoch: [60/2467]	Loss: 0.116455
         Train Epoch: [80/2467]	Loss: 0.105757    
 Train Epoch: [80/2467]	Loss: 0.216237
 Train Epoch: [80/2467]	Loss: 0.088951
     Train Epoch: [80/2467]	Loss: 0.050864
         Train Epoch: [100/2467]	Loss: 0.078860
 Train Epoch: [100/2467]	Loss: 0.053513
         Train Epoch: [100/2467]	Loss: 0.145281
 Train Epoch: [100/2467]	Loss: 0.132754
         Train Epoch: [120/2467]	Loss: 0.043047
 Train Epoch: [120/2467]	Loss: 0.076137
          Train Epoch: [120/2467]	Loss: 0.178562
Train Epoch: [120/2467]	Loss: 0.306741
             Train Epoch: [140/2467]	Loss: 0.093059 
 Train Epoch: [140/2467]	Loss: 0.050857Train Epoch: [140/2467]	Loss: 0.190335

     Train Epoch: [140/2467]	Loss: 0.024559
          Train Epoch: [160/2467]	Loss: 0.070591
Train Epoch: [160/2467]	Loss: 0.187518
     Train Epoch: [160/2467]	Loss: 0.111820
     Train Epoch: [160/2467]	Loss: 0.165080
     Train Epoch: [180/2467]	Loss: 0.150284
     Train Epoch: [180/2467]	Loss: 0.066630
     Train Epoch: [180/2467]	Loss: 0.077616
     Train Epoch: [180/2467]	Loss: 0.163764
         Train Epoch: [200/2467]	Loss: 0.106819
         Train Epoch: [200/2467]	Loss: 0.042544
  Train Epoch: [200/2467]	Loss: 0.060252Train Epoch: [200/2467]	Loss: 0.043562

     Train Epoch: [220/2467]	Loss: 0.087385
     Train Epoch: [220/2467]	Loss: 0.044019
     Train Epoch: [220/2467]	Loss: 0.150565
     Train Epoch: [220/2467]	Loss: 0.192604
         Train Epoch: [240/2467]	Loss: 0.186526
     Train Epoch: [240/2467]	Loss: 0.015612
 Train Epoch: [240/2467]	Loss: 0.040179
     Train Epoch: [240/2467]	Loss: 0.133160
         Train Epoch: [260/2467]	Loss: 0.049161
 Train Epoch: [260/2467]	Loss: 0.047919
     Train Epoch: [260/2467]	Loss: 0.026095
     Train Epoch: [260/2467]	Loss: 0.023459
         Train Epoch: [280/2467]	Loss: 0.095227
 Train Epoch: [280/2467]	Loss: 0.164611
     Train Epoch: [280/2467]	Loss: 0.070116
     Train Epoch: [280/2467]	Loss: 0.101519
         Train Epoch: [300/2467]	Loss: 0.134791
 Train Epoch: [300/2467]	Loss: 0.041493
     Train Epoch: [300/2467]	Loss: 0.286389
     Train Epoch: [300/2467]	Loss: 0.067110
         Train Epoch: [320/2467]	Loss: 0.136866
 Train Epoch: [320/2467]	Loss: 0.198940
         Train Epoch: [320/2467]	Loss: 0.157183
 Train Epoch: [320/2467]	Loss: 0.366805
         Train Epoch: [340/2467]	Loss: 0.049807
 Train Epoch: [340/2467]	Loss: 0.040188
         Train Epoch: [340/2467]	Loss: 0.154007 
Train Epoch: [340/2467]	Loss: 0.012655
          Train Epoch: [360/2467]	Loss: 0.031695Train Epoch: [360/2467]	Loss: 0.047150

     Train Epoch: [360/2467]	Loss: 0.129027
     Train Epoch: [360/2467]	Loss: 0.207912
     Train Epoch: [380/2467]	Loss: 0.078090
         Train Epoch: [380/2467]	Loss: 0.193006
     Train Epoch: [380/2467]	Loss: 0.061713
 Train Epoch: [380/2467]	Loss: 0.307212
     Train Epoch: [400/2467]	Loss: 0.064036
     Train Epoch: [400/2467]	Loss: 0.043148
          Train Epoch: [400/2467]	Loss: 0.103143
Train Epoch: [400/2467]	Loss: 0.139489
     Train Epoch: [420/2467]	Loss: 0.204715
         Train Epoch: [420/2467]	Loss: 0.275978
 Train Epoch: [420/2467]	Loss: 0.099984
     Train Epoch: [420/2467]	Loss: 0.075416
          Train Epoch: [440/2467]	Loss: 0.119311Train Epoch: [440/2467]	Loss: 0.309427

     Train Epoch: [440/2467]	Loss: 0.071522    
 Train Epoch: [440/2467]	Loss: 0.040445
     Train Epoch: [460/2467]	Loss: 0.152345
         Train Epoch: [460/2467]	Loss: 0.083631
 Train Epoch: [460/2467]	Loss: 0.100568
     Train Epoch: [460/2467]	Loss: 0.200638
          Train Epoch: [480/2467]	Loss: 0.025530Train Epoch: [480/2467]	Loss: 0.259840

          Train Epoch: [480/2467]	Loss: 0.158405
Train Epoch: [480/2467]	Loss: 0.318727
          Train Epoch: [500/2467]	Loss: 0.080753Train Epoch: [500/2467]	Loss: 0.063077

     Train Epoch: [500/2467]	Loss: 0.168658
     Train Epoch: [500/2467]	Loss: 0.034425
     Train Epoch: [520/2467]	Loss: 0.226178
         Train Epoch: [520/2467]	Loss: 0.084475
 Train Epoch: [520/2467]	Loss: 0.056208
     Train Epoch: [520/2467]	Loss: 0.060156
         Train Epoch: [540/2467]	Loss: 0.235393 
Train Epoch: [540/2467]	Loss: 0.147100
     Train Epoch: [540/2467]	Loss: 0.076946
     Train Epoch: [540/2467]	Loss: 0.048886
     Train Epoch: [560/2467]	Loss: 0.122990
          Train Epoch: [560/2467]	Loss: 0.156177    Train Epoch: [560/2467]	Loss: 0.112243

 Train Epoch: [560/2467]	Loss: 0.115580
          Train Epoch: [580/2467]	Loss: 0.241230Train Epoch: [580/2467]	Loss: 0.131600

     Train Epoch: [580/2467]	Loss: 0.051696
     Train Epoch: [580/2467]	Loss: 0.116112
             Train Epoch: [600/2467]	Loss: 0.190328 
    Train Epoch: [600/2467]	Loss: 0.104614
 Train Epoch: [600/2467]	Loss: 0.158311
 Train Epoch: [600/2467]	Loss: 0.105524
         Train Epoch: [620/2467]	Loss: 0.026573     
Train Epoch: [620/2467]	Loss: 0.128363
 Train Epoch: [620/2467]	Loss: 0.186401
     Train Epoch: [620/2467]	Loss: 0.118793
         Train Epoch: [640/2467]	Loss: 0.061439
          Train Epoch: [640/2467]	Loss: 0.138995Train Epoch: [640/2467]	Loss: 0.097358

 Train Epoch: [640/2467]	Loss: 0.030713
     Train Epoch: [660/2467]	Loss: 0.351693
     Train Epoch: [660/2467]	Loss: 0.111654
     Train Epoch: [660/2467]	Loss: 0.055408
     Train Epoch: [660/2467]	Loss: 0.020235
             Train Epoch: [680/2467]	Loss: 0.063427
  Train Epoch: [680/2467]	Loss: 0.025153
Train Epoch: [680/2467]	Loss: 0.054961
     Train Epoch: [680/2467]	Loss: 0.100700
     Train Epoch: [700/2467]	Loss: 0.034141
     Train Epoch: [700/2467]	Loss: 0.306012
          Train Epoch: [700/2467]	Loss: 0.237684Train Epoch: [700/2467]	Loss: 0.062688

         Train Epoch: [720/2467]	Loss: 0.178122
         Train Epoch: [720/2467]	Loss: 0.013351
 Train Epoch: [720/2467]	Loss: 0.058021 
Train Epoch: [720/2467]	Loss: 0.084762
     Train Epoch: [740/2467]	Loss: 0.041627
     Train Epoch: [740/2467]	Loss: 0.066016
     Train Epoch: [740/2467]	Loss: 0.063063
     Train Epoch: [740/2467]	Loss: 0.313394
         Train Epoch: [760/2467]	Loss: 0.045898
 Train Epoch: [760/2467]	Loss: 0.209572
          Train Epoch: [760/2467]	Loss: 0.263734Train Epoch: [760/2467]	Loss: 0.331963

     Train Epoch: [780/2467]	Loss: 0.270483
     Train Epoch: [780/2467]	Loss: 0.291001
     Train Epoch: [780/2467]	Loss: 0.272071
     Train Epoch: [780/2467]	Loss: 0.100105
         Train Epoch: [800/2467]	Loss: 0.151919
 Train Epoch: [800/2467]	Loss: 0.170405
         Train Epoch: [800/2467]	Loss: 0.091211 
Train Epoch: [800/2467]	Loss: 0.076198
         Train Epoch: [820/2467]	Loss: 0.183113
     Train Epoch: [820/2467]	Loss: 0.349971
 Train Epoch: [820/2467]	Loss: 0.052209
     Train Epoch: [820/2467]	Loss: 0.037083
     Train Epoch: [840/2467]	Loss: 0.295107
     Train Epoch: [840/2467]	Loss: 0.111727    
 Train Epoch: [840/2467]	Loss: 0.089848
     Train Epoch: [840/2467]	Loss: 0.048008
     Train Epoch: [860/2467]	Loss: 0.193478
     Train Epoch: [860/2467]	Loss: 0.092294
          Train Epoch: [860/2467]	Loss: 0.044192Train Epoch: [860/2467]	Loss: 0.023851

          Train Epoch: [880/2467]	Loss: 0.109489
Train Epoch: [880/2467]	Loss: 0.132776
          Train Epoch: [880/2467]	Loss: 0.178143Train Epoch: [880/2467]	Loss: 0.147190

     Train Epoch: [900/2467]	Loss: 0.152000
     Train Epoch: [900/2467]	Loss: 0.069634
     Train Epoch: [900/2467]	Loss: 0.151170
     Train Epoch: [900/2467]	Loss: 0.140871
         Train Epoch: [920/2467]	Loss: 0.198746    
 Train Epoch: [920/2467]	Loss: 0.219623
 Train Epoch: [920/2467]	Loss: 0.033730
     Train Epoch: [920/2467]	Loss: 0.271234
          Train Epoch: [940/2467]	Loss: 0.034012Train Epoch: [940/2467]	Loss: 0.085083

     Train Epoch: [940/2467]	Loss: 0.151728
     Train Epoch: [940/2467]	Loss: 0.071687
         Train Epoch: [960/2467]	Loss: 0.246533
 Train Epoch: [960/2467]	Loss: 0.052679
     Train Epoch: [960/2467]	Loss: 0.059905
     Train Epoch: [960/2467]	Loss: 0.058023
     Train Epoch: [980/2467]	Loss: 0.201249
             Train Epoch: [980/2467]	Loss: 0.047165 
Train Epoch: [980/2467]	Loss: 0.067940
 Train Epoch: [980/2467]	Loss: 0.086659
             Train Epoch: [1000/2467]	Loss: 0.025375
  Train Epoch: [1000/2467]	Loss: 0.189965Train Epoch: [1000/2467]	Loss: 0.021257

     Train Epoch: [1000/2467]	Loss: 0.173883
     Train Epoch: [1020/2467]	Loss: 0.310280
         Train Epoch: [1020/2467]	Loss: 0.076197 
Train Epoch: [1020/2467]	Loss: 0.033799
     Train Epoch: [1020/2467]	Loss: 0.108159
         Train Epoch: [1040/2467]	Loss: 0.101228
     Train Epoch: [1040/2467]	Loss: 0.136408
     Train Epoch: [1040/2467]	Loss: 0.211601
 Train Epoch: [1040/2467]	Loss: 0.082877
     Train Epoch: [1060/2467]	Loss: 0.107645
     Train Epoch: [1060/2467]	Loss: 0.332581
     Train Epoch: [1060/2467]	Loss: 0.081999
     Train Epoch: [1060/2467]	Loss: 0.074419
     Train Epoch: [1080/2467]	Loss: 0.092698
             Train Epoch: [1080/2467]	Loss: 0.140248
 Train Epoch: [1080/2467]	Loss: 0.176766
 Train Epoch: [1080/2467]	Loss: 0.196279
          Train Epoch: [1100/2467]	Loss: 0.141146
Train Epoch: [1100/2467]	Loss: 0.245202
     Train Epoch: [1100/2467]	Loss: 0.204983
     Train Epoch: [1100/2467]	Loss: 0.167247
     Train Epoch: [1120/2467]	Loss: 0.210321
     Train Epoch: [1120/2467]	Loss: 0.090702
     Train Epoch: [1120/2467]	Loss: 0.142120
     Train Epoch: [1120/2467]	Loss: 0.086510
     Train Epoch: [1140/2467]	Loss: 0.290479    
     Train Epoch: [1140/2467]	Loss: 0.073556
 Train Epoch: [1140/2467]	Loss: 0.039738
     Train Epoch: [1140/2467]	Loss: 0.251481
     Train Epoch: [1160/2467]	Loss: 0.063417
          Train Epoch: [1160/2467]	Loss: 0.083946
Train Epoch: [1160/2467]	Loss: 0.202985
     Train Epoch: [1160/2467]	Loss: 0.092996
         Train Epoch: [1180/2467]	Loss: 0.069881
     Train Epoch: [1180/2467]	Loss: 0.122341
 Train Epoch: [1180/2467]	Loss: 0.137916
     Train Epoch: [1180/2467]	Loss: 0.048074
     Train Epoch: [1200/2467]	Loss: 0.067080    
     Train Epoch: [1200/2467]	Loss: 0.249329
 Train Epoch: [1200/2467]	Loss: 0.090352
     Train Epoch: [1200/2467]	Loss: 0.187552
     Train Epoch: [1220/2467]	Loss: 0.075641    
          Train Epoch: [1220/2467]	Loss: 0.224619
Train Epoch: [1220/2467]	Loss: 0.157760
 Train Epoch: [1220/2467]	Loss: 0.293015
     Train Epoch: [1240/2467]	Loss: 0.270583
     Train Epoch: [1240/2467]	Loss: 0.087373
     Train Epoch: [1240/2467]	Loss: 0.136184
     Train Epoch: [1240/2467]	Loss: 0.242370
     Train Epoch: [1260/2467]	Loss: 0.043981
          Train Epoch: [1260/2467]	Loss: 0.095344Train Epoch: [1260/2467]	Loss: 0.337430

     Train Epoch: [1260/2467]	Loss: 0.052182
              Train Epoch: [1280/2467]	Loss: 0.104205Train Epoch: [1280/2467]	Loss: 0.164053

     Train Epoch: [1280/2467]	Loss: 0.049747
 Train Epoch: [1280/2467]	Loss: 0.387339
          Train Epoch: [1300/2467]	Loss: 0.078528Train Epoch: [1300/2467]	Loss: 0.271111

          Train Epoch: [1300/2467]	Loss: 0.034569
Train Epoch: [1300/2467]	Loss: 0.183045
     Train Epoch: [1320/2467]	Loss: 0.038171
         Train Epoch: [1320/2467]	Loss: 0.108698
 Train Epoch: [1320/2467]	Loss: 0.091192
     Train Epoch: [1320/2467]	Loss: 0.122679
         Train Epoch: [1340/2467]	Loss: 0.093334
     Train Epoch: [1340/2467]	Loss: 0.198956
 Train Epoch: [1340/2467]	Loss: 0.262046
     Train Epoch: [1340/2467]	Loss: 0.119636
         Train Epoch: [1360/2467]	Loss: 0.050794
 Train Epoch: [1360/2467]	Loss: 0.089324
         Train Epoch: [1360/2467]	Loss: 0.132851
 Train Epoch: [1360/2467]	Loss: 0.036899
         Train Epoch: [1380/2467]	Loss: 0.133278
     Train Epoch: [1380/2467]	Loss: 0.187893
 Train Epoch: [1380/2467]	Loss: 0.254468
     Train Epoch: [1380/2467]	Loss: 0.192540
     Train Epoch: [1400/2467]	Loss: 0.085490
         Train Epoch: [1400/2467]	Loss: 0.081151 
Train Epoch: [1400/2467]	Loss: 0.139610
     Train Epoch: [1400/2467]	Loss: 0.111091
         Train Epoch: [1420/2467]	Loss: 0.315829
 Train Epoch: [1420/2467]	Loss: 0.064595
     Train Epoch: [1420/2467]	Loss: 0.127002
     Train Epoch: [1420/2467]	Loss: 0.110097
         Train Epoch: [1440/2467]	Loss: 0.035949
         Train Epoch: [1440/2467]	Loss: 0.078395
  Train Epoch: [1440/2467]	Loss: 0.037306
Train Epoch: [1440/2467]	Loss: 0.016551
     Train Epoch: [1460/2467]	Loss: 0.034377
         Train Epoch: [1460/2467]	Loss: 0.084847    
 Train Epoch: [1460/2467]	Loss: 0.084488
 Train Epoch: [1460/2467]	Loss: 0.127533
     Train Epoch: [1480/2467]	Loss: 0.017524
     Train Epoch: [1480/2467]	Loss: 0.010810
     Train Epoch: [1480/2467]	Loss: 0.163590
     Train Epoch: [1480/2467]	Loss: 0.201559
     Train Epoch: [1500/2467]	Loss: 0.067747
             Train Epoch: [1500/2467]	Loss: 0.133266
 Train Epoch: [1500/2467]	Loss: 0.013754
 Train Epoch: [1500/2467]	Loss: 0.121495
         Train Epoch: [1520/2467]	Loss: 0.076444
     Train Epoch: [1520/2467]	Loss: 0.269121
 Train Epoch: [1520/2467]	Loss: 0.203800
     Train Epoch: [1520/2467]	Loss: 0.126391
     Train Epoch: [1540/2467]	Loss: 0.062147
         Train Epoch: [1540/2467]	Loss: 0.097164
     Train Epoch: [1540/2467]	Loss: 0.248490
 Train Epoch: [1540/2467]	Loss: 0.079896
         Train Epoch: [1560/2467]	Loss: 0.059561
 Train Epoch: [1560/2467]	Loss: 0.313520
     Train Epoch: [1560/2467]	Loss: 0.377186
     Train Epoch: [1560/2467]	Loss: 0.225689
     Train Epoch: [1580/2467]	Loss: 0.082514
     Train Epoch: [1580/2467]	Loss: 0.181336
         Train Epoch: [1580/2467]	Loss: 0.205960
 Train Epoch: [1580/2467]	Loss: 0.083068
     Train Epoch: [1600/2467]	Loss: 0.169546
          Train Epoch: [1600/2467]	Loss: 0.212487Train Epoch: [1600/2467]	Loss: 0.056764

     Train Epoch: [1600/2467]	Loss: 0.020000
     Train Epoch: [1620/2467]	Loss: 0.038109
             Train Epoch: [1620/2467]	Loss: 0.083306 
Train Epoch: [1620/2467]	Loss: 0.143100
 Train Epoch: [1620/2467]	Loss: 0.219781
             Train Epoch: [1640/2467]	Loss: 0.121565
  Train Epoch: [1640/2467]	Loss: 0.047241Train Epoch: [1640/2467]	Loss: 0.193019

     Train Epoch: [1640/2467]	Loss: 0.160744
         Train Epoch: [1660/2467]	Loss: 0.119773
 Train Epoch: [1660/2467]	Loss: 0.159868
         Train Epoch: [1660/2467]	Loss: 0.147242
 Train Epoch: [1660/2467]	Loss: 0.054611
     Train Epoch: [1680/2467]	Loss: 0.106562
     Train Epoch: [1680/2467]	Loss: 0.036929
         Train Epoch: [1680/2467]	Loss: 0.119258 
Train Epoch: [1680/2467]	Loss: 0.155422
     Train Epoch: [1700/2467]	Loss: 0.080913
         Train Epoch: [1700/2467]	Loss: 0.050788
 Train Epoch: [1700/2467]	Loss: 0.256496
     Train Epoch: [1700/2467]	Loss: 0.114319
     Train Epoch: [1720/2467]	Loss: 0.093971    
 Train Epoch: [1720/2467]	Loss: 0.180304
         Train Epoch: [1720/2467]	Loss: 0.033434
 Train Epoch: [1720/2467]	Loss: 0.119884
     Train Epoch: [1740/2467]	Loss: 0.028297
     Train Epoch: [1740/2467]	Loss: 0.247157
         Train Epoch: [1740/2467]	Loss: 0.024918 
Train Epoch: [1740/2467]	Loss: 0.159307
         Train Epoch: [1760/2467]	Loss: 0.093236
 Train Epoch: [1760/2467]	Loss: 0.073005
     Train Epoch: [1760/2467]	Loss: 0.046791
     Train Epoch: [1760/2467]	Loss: 0.128269
         Train Epoch: [1780/2467]	Loss: 0.145188 
Train Epoch: [1780/2467]	Loss: 0.108213
     Train Epoch: [1780/2467]	Loss: 0.253277
     Train Epoch: [1780/2467]	Loss: 0.077453
     Train Epoch: [1800/2467]	Loss: 0.122332
     Train Epoch: [1800/2467]	Loss: 0.117511
     Train Epoch: [1800/2467]	Loss: 0.211792
     Train Epoch: [1800/2467]	Loss: 0.257878
         Train Epoch: [1820/2467]	Loss: 0.131433
 Train Epoch: [1820/2467]	Loss: 0.234225
     Train Epoch: [1820/2467]	Loss: 0.081556
     Train Epoch: [1820/2467]	Loss: 0.091389
         Train Epoch: [1840/2467]	Loss: 0.090291
 Train Epoch: [1840/2467]	Loss: 0.168333
          Train Epoch: [1840/2467]	Loss: 0.039006Train Epoch: [1840/2467]	Loss: 0.243628

     Train Epoch: [1860/2467]	Loss: 0.098562
         Train Epoch: [1860/2467]	Loss: 0.144483
 Train Epoch: [1860/2467]	Loss: 0.085199
     Train Epoch: [1860/2467]	Loss: 0.118631
         Train Epoch: [1880/2467]	Loss: 0.054589
     Train Epoch: [1880/2467]	Loss: 0.018733
 Train Epoch: [1880/2467]	Loss: 0.049275
     Train Epoch: [1880/2467]	Loss: 0.040420
     Train Epoch: [1900/2467]	Loss: 0.087190
         Train Epoch: [1900/2467]	Loss: 0.250365
 Train Epoch: [1900/2467]	Loss: 0.139615    
 Train Epoch: [1900/2467]	Loss: 0.246634
     Train Epoch: [1920/2467]	Loss: 0.021574
         Train Epoch: [1920/2467]	Loss: 0.168869
 Train Epoch: [1920/2467]	Loss: 0.098790    
 Train Epoch: [1920/2467]	Loss: 0.143080
     Train Epoch: [1940/2467]	Loss: 0.051360
         Train Epoch: [1940/2467]	Loss: 0.094779 
Train Epoch: [1940/2467]	Loss: 0.119484
     Train Epoch: [1940/2467]	Loss: 0.285465
     Train Epoch: [1960/2467]	Loss: 0.136565
         Train Epoch: [1960/2467]	Loss: 0.108724
     Train Epoch: [1960/2467]	Loss: 0.147449
 Train Epoch: [1960/2467]	Loss: 0.082338
     Train Epoch: [1980/2467]	Loss: 0.089411
         Train Epoch: [1980/2467]	Loss: 0.169939
 Train Epoch: [1980/2467]	Loss: 0.167139
     Train Epoch: [1980/2467]	Loss: 0.216900
         Train Epoch: [2000/2467]	Loss: 0.101781
 Train Epoch: [2000/2467]	Loss: 0.023573
     Train Epoch: [2000/2467]	Loss: 0.130691
     Train Epoch: [2000/2467]	Loss: 0.041956
     Train Epoch: [2020/2467]	Loss: 0.134154
         Train Epoch: [2020/2467]	Loss: 0.161149
     Train Epoch: [2020/2467]	Loss: 0.117580
 Train Epoch: [2020/2467]	Loss: 0.045065
         Train Epoch: [2040/2467]	Loss: 0.131484
 Train Epoch: [2040/2467]	Loss: 0.322066    
 Train Epoch: [2040/2467]	Loss: 0.064021
     Train Epoch: [2040/2467]	Loss: 0.044721
     Train Epoch: [2060/2467]	Loss: 0.140567
         Train Epoch: [2060/2467]	Loss: 0.064479
 Train Epoch: [2060/2467]	Loss: 0.170933
     Train Epoch: [2060/2467]	Loss: 0.191090
     Train Epoch: [2080/2467]	Loss: 0.193803
         Train Epoch: [2080/2467]	Loss: 0.056549
 Train Epoch: [2080/2467]	Loss: 0.022047
     Train Epoch: [2080/2467]	Loss: 0.033502
         Train Epoch: [2100/2467]	Loss: 0.082322    
       Train Epoch: [2100/2467]	Loss: 0.392964Train Epoch: [2100/2467]	Loss: 0.083453
Train Epoch: [2100/2467]	Loss: 0.038652

     Train Epoch: [2120/2467]	Loss: 0.078656
     Train Epoch: [2120/2467]	Loss: 0.078407
     Train Epoch: [2120/2467]	Loss: 0.155717
     Train Epoch: [2120/2467]	Loss: 0.033910
     Train Epoch: [2140/2467]	Loss: 0.392829
     Train Epoch: [2140/2467]	Loss: 0.285878
     Train Epoch: [2140/2467]	Loss: 0.141308
     Train Epoch: [2140/2467]	Loss: 0.220087
         Train Epoch: [2160/2467]	Loss: 0.043095
 Train Epoch: [2160/2467]	Loss: 0.262346
     Train Epoch: [2160/2467]	Loss: 0.092238
     Train Epoch: [2160/2467]	Loss: 0.048671
     Train Epoch: [2180/2467]	Loss: 0.056552    
 Train Epoch: [2180/2467]	Loss: 0.211141
     Train Epoch: [2180/2467]	Loss: 0.099760
     Train Epoch: [2180/2467]	Loss: 0.135186
     Train Epoch: [2200/2467]	Loss: 0.024370
     Train Epoch: [2200/2467]	Loss: 0.150716
     Train Epoch: [2200/2467]	Loss: 0.107721
     Train Epoch: [2200/2467]	Loss: 0.093744
     Train Epoch: [2220/2467]	Loss: 0.054098
     Train Epoch: [2220/2467]	Loss: 0.101464
     Train Epoch: [2220/2467]	Loss: 0.098021
     Train Epoch: [2220/2467]	Loss: 0.043750
         Train Epoch: [2240/2467]	Loss: 0.063790
 Train Epoch: [2240/2467]	Loss: 0.076987
         Train Epoch: [2240/2467]	Loss: 0.020300 
Train Epoch: [2240/2467]	Loss: 0.029994
     Train Epoch: [2260/2467]	Loss: 0.123662
         Train Epoch: [2260/2467]	Loss: 0.181437
 Train Epoch: [2260/2467]	Loss: 0.020434
     Train Epoch: [2260/2467]	Loss: 0.229936
     Train Epoch: [2280/2467]	Loss: 0.111307
         Train Epoch: [2280/2467]	Loss: 0.052174 
Train Epoch: [2280/2467]	Loss: 0.079835
     Train Epoch: [2280/2467]	Loss: 0.076875
         Train Epoch: [2300/2467]	Loss: 0.264616
 Train Epoch: [2300/2467]	Loss: 0.041631    
 Train Epoch: [2300/2467]	Loss: 0.088744
     Train Epoch: [2300/2467]	Loss: 0.049109
          Train Epoch: [2320/2467]	Loss: 0.014273Train Epoch: [2320/2467]	Loss: 0.127627

     Train Epoch: [2320/2467]	Loss: 0.122899
     Train Epoch: [2320/2467]	Loss: 0.109909
         Train Epoch: [2340/2467]	Loss: 0.202357    
 Train Epoch: [2340/2467]	Loss: 0.131293
 Train Epoch: [2340/2467]	Loss: 0.438552
     Train Epoch: [2340/2467]	Loss: 0.098831
     Train Epoch: [2360/2467]	Loss: 0.140849    
      Train Epoch: [2360/2467]	Loss: 0.261894Train Epoch: [2360/2467]	Loss: 0.041688
    
 Train Epoch: [2360/2467]	Loss: 0.133627
     Train Epoch: [2380/2467]	Loss: 0.051022
         Train Epoch: [2380/2467]	Loss: 0.273763
 Train Epoch: [2380/2467]	Loss: 0.321055
     Train Epoch: [2380/2467]	Loss: 0.127292
     Train Epoch: [2400/2467]	Loss: 0.087903
     Train Epoch: [2400/2467]	Loss: 0.139743
     Train Epoch: [2400/2467]	Loss: 0.083209
     Train Epoch: [2400/2467]	Loss: 0.095997
         Train Epoch: [2420/2467]	Loss: 0.045797
     Train Epoch: [2420/2467]	Loss: 0.213538
 Train Epoch: [2420/2467]	Loss: 0.082200
     Train Epoch: [2420/2467]	Loss: 0.218985
          Train Epoch: [2440/2467]	Loss: 0.015444Train Epoch: [2440/2467]	Loss: 0.210164

         Train Epoch: [2440/2467]	Loss: 0.090512
 Train Epoch: [2440/2467]	Loss: 0.130911
         Train Epoch: [2460/2467]	Loss: 0.116904
 Train Epoch: [2460/2467]	Loss: 0.065835
          Train Epoch: [2460/2467]	Loss: 0.092164Train Epoch: [2460/2467]	Loss: 0.202450

huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 39 epoch =====
     2025-05-11.14-14-54
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 39 epoch =====
     2025-05-11.14-14-55
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 39 epoch =====
     2025-05-11.14-14-55
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 39 epoch =====
     2025-05-11.14-14-55
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
             Train Epoch: [0/2467]	Loss: 0.186766 
Train Epoch: [0/2467]	Loss: 0.160574 
Train Epoch: [0/2467]	Loss: 0.048058
     Train Epoch: [0/2467]	Loss: 0.279950
     Train Epoch: [20/2467]	Loss: 0.077721
         Train Epoch: [20/2467]	Loss: 0.049538 
Train Epoch: [20/2467]	Loss: 0.117457
     Train Epoch: [20/2467]	Loss: 0.285968
     Train Epoch: [40/2467]	Loss: 0.379998
     Train Epoch: [40/2467]	Loss: 0.110317
     Train Epoch: [40/2467]	Loss: 0.087688
     Train Epoch: [40/2467]	Loss: 0.112368
     Train Epoch: [60/2467]	Loss: 0.148194    
 Train Epoch: [60/2467]	Loss: 0.116978
         Train Epoch: [60/2467]	Loss: 0.078693
 Train Epoch: [60/2467]	Loss: 0.023926
     Train Epoch: [80/2467]	Loss: 0.082975
             Train Epoch: [80/2467]	Loss: 0.094584
 Train Epoch: [80/2467]	Loss: 0.109409
 Train Epoch: [80/2467]	Loss: 0.219401
     Train Epoch: [100/2467]	Loss: 0.048988
         Train Epoch: [100/2467]	Loss: 0.197332
 Train Epoch: [100/2467]	Loss: 0.075276
     Train Epoch: [100/2467]	Loss: 0.142180
     Train Epoch: [120/2467]	Loss: 0.038264
     Train Epoch: [120/2467]	Loss: 0.327983
         Train Epoch: [120/2467]	Loss: 0.071013
 Train Epoch: [120/2467]	Loss: 0.158929
     Train Epoch: [140/2467]	Loss: 0.021761    
 Train Epoch: [140/2467]	Loss: 0.086340
     Train Epoch: [140/2467]	Loss: 0.171578    
 Train Epoch: [140/2467]	Loss: 0.053594
         Train Epoch: [160/2467]	Loss: 0.195597
 Train Epoch: [160/2467]	Loss: 0.054776    
     Train Epoch: [160/2467]	Loss: 0.178361 
Train Epoch: [160/2467]	Loss: 0.108212
     Train Epoch: [180/2467]	Loss: 0.112404
         Train Epoch: [180/2467]	Loss: 0.160758
 Train Epoch: [180/2467]	Loss: 0.087086
     Train Epoch: [180/2467]	Loss: 0.087703
     Train Epoch: [200/2467]	Loss: 0.040968
         Train Epoch: [200/2467]	Loss: 0.039806 
Train Epoch: [200/2467]	Loss: 0.100011
     Train Epoch: [200/2467]	Loss: 0.054075
     Train Epoch: [220/2467]	Loss: 0.082200
         Train Epoch: [220/2467]	Loss: 0.135628
 Train Epoch: [220/2467]	Loss: 0.043587
     Train Epoch: [220/2467]	Loss: 0.205301
         Train Epoch: [240/2467]	Loss: 0.015990
 Train Epoch: [240/2467]	Loss: 0.208153
         Train Epoch: [240/2467]	Loss: 0.137926
 Train Epoch: [240/2467]	Loss: 0.033986
     Train Epoch: [260/2467]	Loss: 0.027392
     Train Epoch: [260/2467]	Loss: 0.050614
     Train Epoch: [260/2467]	Loss: 0.051090
     Train Epoch: [260/2467]	Loss: 0.048491
     Train Epoch: [280/2467]	Loss: 0.117794
     Train Epoch: [280/2467]	Loss: 0.117412
     Train Epoch: [280/2467]	Loss: 0.182240
     Train Epoch: [280/2467]	Loss: 0.068508
         Train Epoch: [300/2467]	Loss: 0.150588
 Train Epoch: [300/2467]	Loss: 0.033560
     Train Epoch: [300/2467]	Loss: 0.317806
     Train Epoch: [300/2467]	Loss: 0.065370
     Train Epoch: [320/2467]	Loss: 0.119292
     Train Epoch: [320/2467]	Loss: 0.185206
     Train Epoch: [320/2467]	Loss: 0.331921
     Train Epoch: [320/2467]	Loss: 0.144926
         Train Epoch: [340/2467]	Loss: 0.038792
     Train Epoch: [340/2467]	Loss: 0.054043    
 Train Epoch: [340/2467]	Loss: 0.011715
 Train Epoch: [340/2467]	Loss: 0.161387
          Train Epoch: [360/2467]	Loss: 0.047808Train Epoch: [360/2467]	Loss: 0.035356

     Train Epoch: [360/2467]	Loss: 0.120417
     Train Epoch: [360/2467]	Loss: 0.194802
              Train Epoch: [380/2467]	Loss: 0.061561    
Train Epoch: [380/2467]	Loss: 0.165174
  Train Epoch: [380/2467]	Loss: 0.255968
Train Epoch: [380/2467]	Loss: 0.112174
         Train Epoch: [400/2467]	Loss: 0.051816
     Train Epoch: [400/2467]	Loss: 0.024648
 Train Epoch: [400/2467]	Loss: 0.131819
     Train Epoch: [400/2467]	Loss: 0.148283
     Train Epoch: [420/2467]	Loss: 0.193780
     Train Epoch: [420/2467]	Loss: 0.089969
     Train Epoch: [420/2467]	Loss: 0.073739
     Train Epoch: [420/2467]	Loss: 0.289372
         Train Epoch: [440/2467]	Loss: 0.294918
 Train Epoch: [440/2467]	Loss: 0.063783
     Train Epoch: [440/2467]	Loss: 0.041256
     Train Epoch: [440/2467]	Loss: 0.118514
     Train Epoch: [460/2467]	Loss: 0.201817
     Train Epoch: [460/2467]	Loss: 0.130331
     Train Epoch: [460/2467]	Loss: 0.081393
     Train Epoch: [460/2467]	Loss: 0.093229
             Train Epoch: [480/2467]	Loss: 0.102377    
  Train Epoch: [480/2467]	Loss: 0.026697 
Train Epoch: [480/2467]	Loss: 0.233771
Train Epoch: [480/2467]	Loss: 0.303268
     Train Epoch: [500/2467]	Loss: 0.049393
     Train Epoch: [500/2467]	Loss: 0.093522
         Train Epoch: [500/2467]	Loss: 0.062773 
Train Epoch: [500/2467]	Loss: 0.157642
               Train Epoch: [520/2467]	Loss: 0.076943Train Epoch: [520/2467]	Loss: 0.097144Train Epoch: [520/2467]	Loss: 0.239057


     Train Epoch: [520/2467]	Loss: 0.071385
         Train Epoch: [540/2467]	Loss: 0.120054 
Train Epoch: [540/2467]	Loss: 0.193628
     Train Epoch: [540/2467]	Loss: 0.073796
     Train Epoch: [540/2467]	Loss: 0.049051
     Train Epoch: [560/2467]	Loss: 0.122386
         Train Epoch: [560/2467]	Loss: 0.115492 
Train Epoch: [560/2467]	Loss: 0.137140
     Train Epoch: [560/2467]	Loss: 0.111228
     Train Epoch: [580/2467]	Loss: 0.217095
         Train Epoch: [580/2467]	Loss: 0.131122
 Train Epoch: [580/2467]	Loss: 0.067619
     Train Epoch: [580/2467]	Loss: 0.109178
         Train Epoch: [600/2467]	Loss: 0.211844
 Train Epoch: [600/2467]	Loss: 0.113744
         Train Epoch: [600/2467]	Loss: 0.109835 
Train Epoch: [600/2467]	Loss: 0.079544
         Train Epoch: [620/2467]	Loss: 0.090569
 Train Epoch: [620/2467]	Loss: 0.028516
          Train Epoch: [620/2467]	Loss: 0.126086Train Epoch: [620/2467]	Loss: 0.117771

     Train Epoch: [640/2467]	Loss: 0.179466
     Train Epoch: [640/2467]	Loss: 0.091185
     Train Epoch: [640/2467]	Loss: 0.034012
     Train Epoch: [640/2467]	Loss: 0.069770
         Train Epoch: [660/2467]	Loss: 0.020813    
  Train Epoch: [660/2467]	Loss: 0.039109
Train Epoch: [660/2467]	Loss: 0.304644
     Train Epoch: [660/2467]	Loss: 0.110213
     Train Epoch: [680/2467]	Loss: 0.105089
     Train Epoch: [680/2467]	Loss: 0.069717
     Train Epoch: [680/2467]	Loss: 0.027526
     Train Epoch: [680/2467]	Loss: 0.023418
         Train Epoch: [700/2467]	Loss: 0.033249
 Train Epoch: [700/2467]	Loss: 0.257512
          Train Epoch: [700/2467]	Loss: 0.063427Train Epoch: [700/2467]	Loss: 0.231215

         Train Epoch: [720/2467]	Loss: 0.079825
 Train Epoch: [720/2467]	Loss: 0.054115
     Train Epoch: [720/2467]	Loss: 0.139087
     Train Epoch: [720/2467]	Loss: 0.009426
     Train Epoch: [740/2467]	Loss: 0.042282
         Train Epoch: [740/2467]	Loss: 0.077305
 Train Epoch: [740/2467]	Loss: 0.060204
     Train Epoch: [740/2467]	Loss: 0.265402
     Train Epoch: [760/2467]	Loss: 0.222845
     Train Epoch: [760/2467]	Loss: 0.037717    
 Train Epoch: [760/2467]	Loss: 0.319283    
 Train Epoch: [760/2467]	Loss: 0.239971
     Train Epoch: [780/2467]	Loss: 0.314230
         Train Epoch: [780/2467]	Loss: 0.263901 
Train Epoch: [780/2467]	Loss: 0.084491
     Train Epoch: [780/2467]	Loss: 0.261261
         Train Epoch: [800/2467]	Loss: 0.150449
 Train Epoch: [800/2467]	Loss: 0.160618
     Train Epoch: [800/2467]	Loss: 0.080227
     Train Epoch: [800/2467]	Loss: 0.092095
     Train Epoch: [820/2467]	Loss: 0.357251
     Train Epoch: [820/2467]	Loss: 0.038831
     Train Epoch: [820/2467]	Loss: 0.153907
     Train Epoch: [820/2467]	Loss: 0.038606
     Train Epoch: [840/2467]	Loss: 0.299242    
     Train Epoch: [840/2467]	Loss: 0.069744
     Train Epoch: [840/2467]	Loss: 0.087689
 Train Epoch: [840/2467]	Loss: 0.041221
         Train Epoch: [860/2467]	Loss: 0.188480 
Train Epoch: [860/2467]	Loss: 0.091358    
 Train Epoch: [860/2467]	Loss: 0.046690
     Train Epoch: [860/2467]	Loss: 0.040202
         Train Epoch: [880/2467]	Loss: 0.116941
     Train Epoch: [880/2467]	Loss: 0.126682
      Train Epoch: [880/2467]	Loss: 0.130273Train Epoch: [880/2467]	Loss: 0.186709

     Train Epoch: [900/2467]	Loss: 0.142086
     Train Epoch: [900/2467]	Loss: 0.154672    
     Train Epoch: [900/2467]	Loss: 0.146359 
Train Epoch: [900/2467]	Loss: 0.049332
          Train Epoch: [920/2467]	Loss: 0.202099Train Epoch: [920/2467]	Loss: 0.318926

     Train Epoch: [920/2467]	Loss: 0.035746
     Train Epoch: [920/2467]	Loss: 0.190553
     Train Epoch: [940/2467]	Loss: 0.051366
     Train Epoch: [940/2467]	Loss: 0.144283
     Train Epoch: [940/2467]	Loss: 0.060787
     Train Epoch: [940/2467]	Loss: 0.024277
         Train Epoch: [960/2467]	Loss: 0.236789
 Train Epoch: [960/2467]	Loss: 0.069457
         Train Epoch: [960/2467]	Loss: 0.057130 
Train Epoch: [960/2467]	Loss: 0.058076
         Train Epoch: [980/2467]	Loss: 0.042302
     Train Epoch: [980/2467]	Loss: 0.207129
     Train Epoch: [980/2467]	Loss: 0.104508
 Train Epoch: [980/2467]	Loss: 0.059711
          Train Epoch: [1000/2467]	Loss: 0.027276Train Epoch: [1000/2467]	Loss: 0.031566
    
 Train Epoch: [1000/2467]	Loss: 0.172237
     Train Epoch: [1000/2467]	Loss: 0.267002
         Train Epoch: [1020/2467]	Loss: 0.227112
 Train Epoch: [1020/2467]	Loss: 0.322694
     Train Epoch: [1020/2467]	Loss: 0.067254
     Train Epoch: [1020/2467]	Loss: 0.014784
     Train Epoch: [1040/2467]	Loss: 0.096961    
 Train Epoch: [1040/2467]	Loss: 0.122369
         Train Epoch: [1040/2467]	Loss: 0.185719 
Train Epoch: [1040/2467]	Loss: 0.088863
         Train Epoch: [1060/2467]	Loss: 0.358428 
Train Epoch: [1060/2467]	Loss: 0.082495
          Train Epoch: [1060/2467]	Loss: 0.071118Train Epoch: [1060/2467]	Loss: 0.173890

         Train Epoch: [1080/2467]	Loss: 0.145598
     Train Epoch: [1080/2467]	Loss: 0.100470     
Train Epoch: [1080/2467]	Loss: 0.128634
 Train Epoch: [1080/2467]	Loss: 0.204897
     Train Epoch: [1100/2467]	Loss: 0.252787
     Train Epoch: [1100/2467]	Loss: 0.146989
     Train Epoch: [1100/2467]	Loss: 0.196718
     Train Epoch: [1100/2467]	Loss: 0.190808
     Train Epoch: [1120/2467]	Loss: 0.105532
          Train Epoch: [1120/2467]	Loss: 0.132362Train Epoch: [1120/2467]	Loss: 0.079230

     Train Epoch: [1120/2467]	Loss: 0.205609
              Train Epoch: [1140/2467]	Loss: 0.060940     
Train Epoch: [1140/2467]	Loss: 0.313379Train Epoch: [1140/2467]	Loss: 0.037206

 Train Epoch: [1140/2467]	Loss: 0.233360
     Train Epoch: [1160/2467]	Loss: 0.058679
         Train Epoch: [1160/2467]	Loss: 0.086829
 Train Epoch: [1160/2467]	Loss: 0.199694
     Train Epoch: [1160/2467]	Loss: 0.075537
     Train Epoch: [1180/2467]	Loss: 0.038073
     Train Epoch: [1180/2467]	Loss: 0.098231
     Train Epoch: [1180/2467]	Loss: 0.122936
     Train Epoch: [1180/2467]	Loss: 0.104835
         Train Epoch: [1200/2467]	Loss: 0.062269
 Train Epoch: [1200/2467]	Loss: 0.084665
     Train Epoch: [1200/2467]	Loss: 0.183925
     Train Epoch: [1200/2467]	Loss: 0.276065
     Train Epoch: [1220/2467]	Loss: 0.130263
     Train Epoch: [1220/2467]	Loss: 0.089205
         Train Epoch: [1220/2467]	Loss: 0.224514
 Train Epoch: [1220/2467]	Loss: 0.169340
         Train Epoch: [1240/2467]	Loss: 0.216255
 Train Epoch: [1240/2467]	Loss: 0.250716
         Train Epoch: [1240/2467]	Loss: 0.140878
 Train Epoch: [1240/2467]	Loss: 0.141435
          Train Epoch: [1260/2467]	Loss: 0.047234Train Epoch: [1260/2467]	Loss: 0.042624

     Train Epoch: [1260/2467]	Loss: 0.343437
     Train Epoch: [1260/2467]	Loss: 0.116381
     Train Epoch: [1280/2467]	Loss: 0.095847
     Train Epoch: [1280/2467]	Loss: 0.153509
     Train Epoch: [1280/2467]	Loss: 0.389293
     Train Epoch: [1280/2467]	Loss: 0.059538
         Train Epoch: [1300/2467]	Loss: 0.232254
     Train Epoch: [1300/2467]	Loss: 0.039604
 Train Epoch: [1300/2467]	Loss: 0.212856
     Train Epoch: [1300/2467]	Loss: 0.065255
         Train Epoch: [1320/2467]	Loss: 0.103731
 Train Epoch: [1320/2467]	Loss: 0.092906
     Train Epoch: [1320/2467]	Loss: 0.117017
     Train Epoch: [1320/2467]	Loss: 0.039437
         Train Epoch: [1340/2467]	Loss: 0.178076
 Train Epoch: [1340/2467]	Loss: 0.087946
     Train Epoch: [1340/2467]	Loss: 0.128203
     Train Epoch: [1340/2467]	Loss: 0.271572
     Train Epoch: [1360/2467]	Loss: 0.086884    
 Train Epoch: [1360/2467]	Loss: 0.127791
          Train Epoch: [1360/2467]	Loss: 0.032832Train Epoch: [1360/2467]	Loss: 0.094831

     Train Epoch: [1380/2467]	Loss: 0.129754
         Train Epoch: [1380/2467]	Loss: 0.197931 
Train Epoch: [1380/2467]	Loss: 0.282978
     Train Epoch: [1380/2467]	Loss: 0.193344
         Train Epoch: [1400/2467]	Loss: 0.129349
 Train Epoch: [1400/2467]	Loss: 0.066799
     Train Epoch: [1400/2467]	Loss: 0.118407
     Train Epoch: [1400/2467]	Loss: 0.064412
         Train Epoch: [1420/2467]	Loss: 0.294359
 Train Epoch: [1420/2467]	Loss: 0.070110
          Train Epoch: [1420/2467]	Loss: 0.103884Train Epoch: [1420/2467]	Loss: 0.132911

          Train Epoch: [1440/2467]	Loss: 0.013660Train Epoch: [1440/2467]	Loss: 0.037359

          Train Epoch: [1440/2467]	Loss: 0.081836Train Epoch: [1440/2467]	Loss: 0.039458

     Train Epoch: [1460/2467]	Loss: 0.090272
          Train Epoch: [1460/2467]	Loss: 0.126200
Train Epoch: [1460/2467]	Loss: 0.111457
     Train Epoch: [1460/2467]	Loss: 0.032580
     Train Epoch: [1480/2467]	Loss: 0.018348
     Train Epoch: [1480/2467]	Loss: 0.011709
     Train Epoch: [1480/2467]	Loss: 0.149557
     Train Epoch: [1480/2467]	Loss: 0.196702
         Train Epoch: [1500/2467]	Loss: 0.062065
 Train Epoch: [1500/2467]	Loss: 0.012631
     Train Epoch: [1500/2467]	Loss: 0.062187
     Train Epoch: [1500/2467]	Loss: 0.125261
          Train Epoch: [1520/2467]	Loss: 0.090472Train Epoch: [1520/2467]	Loss: 0.162564

     Train Epoch: [1520/2467]	Loss: 0.189201
     Train Epoch: [1520/2467]	Loss: 0.131872
         Train Epoch: [1540/2467]	Loss: 0.246174
     Train Epoch: [1540/2467]	Loss: 0.017850
 Train Epoch: [1540/2467]	Loss: 0.090644
     Train Epoch: [1540/2467]	Loss: 0.068183
         Train Epoch: [1560/2467]	Loss: 0.068818
 Train Epoch: [1560/2467]	Loss: 0.291936
     Train Epoch: [1560/2467]	Loss: 0.355587
     Train Epoch: [1560/2467]	Loss: 0.213266
         Train Epoch: [1580/2467]	Loss: 0.166532        
 Train Epoch: [1580/2467]	Loss: 0.215987
  Train Epoch: [1580/2467]	Loss: 0.096716Train Epoch: [1580/2467]	Loss: 0.069891

         Train Epoch: [1600/2467]	Loss: 0.172052
     Train Epoch: [1600/2467]	Loss: 0.200946
 Train Epoch: [1600/2467]	Loss: 0.051090
     Train Epoch: [1600/2467]	Loss: 0.037232
     Train Epoch: [1620/2467]	Loss: 0.027146
     Train Epoch: [1620/2467]	Loss: 0.080851
         Train Epoch: [1620/2467]	Loss: 0.205598
 Train Epoch: [1620/2467]	Loss: 0.139752
         Train Epoch: [1640/2467]	Loss: 0.152802
 Train Epoch: [1640/2467]	Loss: 0.114310
         Train Epoch: [1640/2467]	Loss: 0.168210
 Train Epoch: [1640/2467]	Loss: 0.048853
     Train Epoch: [1660/2467]	Loss: 0.113145
         Train Epoch: [1660/2467]	Loss: 0.133640 
    Train Epoch: [1660/2467]	Loss: 0.061605
 Train Epoch: [1660/2467]	Loss: 0.156343
         Train Epoch: [1680/2467]	Loss: 0.038262
     Train Epoch: [1680/2467]	Loss: 0.140389    
 Train Epoch: [1680/2467]	Loss: 0.099155 
Train Epoch: [1680/2467]	Loss: 0.172592
     Train Epoch: [1700/2467]	Loss: 0.106509
         Train Epoch: [1700/2467]	Loss: 0.060349
     Train Epoch: [1700/2467]	Loss: 0.222233
 Train Epoch: [1700/2467]	Loss: 0.147231
         Train Epoch: [1720/2467]	Loss: 0.171155
 Train Epoch: [1720/2467]	Loss: 0.081622        
  Train Epoch: [1720/2467]	Loss: 0.196703Train Epoch: [1720/2467]	Loss: 0.038053

     Train Epoch: [1740/2467]	Loss: 0.035492
     Train Epoch: [1740/2467]	Loss: 0.253895
     Train Epoch: [1740/2467]	Loss: 0.185421
     Train Epoch: [1740/2467]	Loss: 0.020405
     Train Epoch: [1760/2467]	Loss: 0.116323
         Train Epoch: [1760/2467]	Loss: 0.044202
 Train Epoch: [1760/2467]	Loss: 0.063746
     Train Epoch: [1760/2467]	Loss: 0.092037
         Train Epoch: [1780/2467]	Loss: 0.135883     
Train Epoch: [1780/2467]	Loss: 0.151307
     Train Epoch: [1780/2467]	Loss: 0.231997
 Train Epoch: [1780/2467]	Loss: 0.071258
               Train Epoch: [1800/2467]	Loss: 0.134668Train Epoch: [1800/2467]	Loss: 0.113951Train Epoch: [1800/2467]	Loss: 0.210259


     Train Epoch: [1800/2467]	Loss: 0.234784
          Train Epoch: [1820/2467]	Loss: 0.123428Train Epoch: [1820/2467]	Loss: 0.251938

         Train Epoch: [1820/2467]	Loss: 0.095653
 Train Epoch: [1820/2467]	Loss: 0.109091
         Train Epoch: [1840/2467]	Loss: 0.098156
 Train Epoch: [1840/2467]	Loss: 0.122493
          Train Epoch: [1840/2467]	Loss: 0.039933Train Epoch: [1840/2467]	Loss: 0.232567

     Train Epoch: [1860/2467]	Loss: 0.113276
     Train Epoch: [1860/2467]	Loss: 0.089797
     Train Epoch: [1860/2467]	Loss: 0.174481
     Train Epoch: [1860/2467]	Loss: 0.123539
         Train Epoch: [1880/2467]	Loss: 0.020148
 Train Epoch: [1880/2467]	Loss: 0.077498
          Train Epoch: [1880/2467]	Loss: 0.039152Train Epoch: [1880/2467]	Loss: 0.047737

     Train Epoch: [1900/2467]	Loss: 0.078969
          Train Epoch: [1900/2467]	Loss: 0.095602Train Epoch: [1900/2467]	Loss: 0.233188

     Train Epoch: [1900/2467]	Loss: 0.251719
              Train Epoch: [1920/2467]	Loss: 0.167703Train Epoch: [1920/2467]	Loss: 0.102560     

Train Epoch: [1920/2467]	Loss: 0.027314
 Train Epoch: [1920/2467]	Loss: 0.124004
          Train Epoch: [1940/2467]	Loss: 0.109451Train Epoch: [1940/2467]	Loss: 0.103930

     Train Epoch: [1940/2467]	Loss: 0.053932    
 Train Epoch: [1940/2467]	Loss: 0.271508
     Train Epoch: [1960/2467]	Loss: 0.092389
         Train Epoch: [1960/2467]	Loss: 0.085102
 Train Epoch: [1960/2467]	Loss: 0.136055
     Train Epoch: [1960/2467]	Loss: 0.056914
     Train Epoch: [1980/2467]	Loss: 0.086465
         Train Epoch: [1980/2467]	Loss: 0.177904
 Train Epoch: [1980/2467]	Loss: 0.163630
     Train Epoch: [1980/2467]	Loss: 0.190203
         Train Epoch: [2000/2467]	Loss: 0.111530
 Train Epoch: [2000/2467]	Loss: 0.011975
          Train Epoch: [2000/2467]	Loss: 0.048863Train Epoch: [2000/2467]	Loss: 0.126904

     Train Epoch: [2020/2467]	Loss: 0.125001
     Train Epoch: [2020/2467]	Loss: 0.102437
         Train Epoch: [2020/2467]	Loss: 0.046244
 Train Epoch: [2020/2467]	Loss: 0.117924
         Train Epoch: [2040/2467]	Loss: 0.042235 
Train Epoch: [2040/2467]	Loss: 0.121603
          Train Epoch: [2040/2467]	Loss: 0.342190Train Epoch: [2040/2467]	Loss: 0.062520

         Train Epoch: [2060/2467]	Loss: 0.157911
 Train Epoch: [2060/2467]	Loss: 0.075464
          Train Epoch: [2060/2467]	Loss: 0.189094Train Epoch: [2060/2467]	Loss: 0.185775

         Train Epoch: [2080/2467]	Loss: 0.031726
 Train Epoch: [2080/2467]	Loss: 0.195915
         Train Epoch: [2080/2467]	Loss: 0.053418 
Train Epoch: [2080/2467]	Loss: 0.024305
         Train Epoch: [2100/2467]	Loss: 0.096513
 Train Epoch: [2100/2467]	Loss: 0.387362
         Train Epoch: [2100/2467]	Loss: 0.035035
 Train Epoch: [2100/2467]	Loss: 0.074260
         Train Epoch: [2120/2467]	Loss: 0.100760
 Train Epoch: [2120/2467]	Loss: 0.033163
         Train Epoch: [2120/2467]	Loss: 0.070336
 Train Epoch: [2120/2467]	Loss: 0.163845
             Train Epoch: [2140/2467]	Loss: 0.128183
  Train Epoch: [2140/2467]	Loss: 0.195578Train Epoch: [2140/2467]	Loss: 0.246315

     Train Epoch: [2140/2467]	Loss: 0.417185
     Train Epoch: [2160/2467]	Loss: 0.040083
     Train Epoch: [2160/2467]	Loss: 0.271039    
      Train Epoch: [2160/2467]	Loss: 0.061205Train Epoch: [2160/2467]	Loss: 0.173305

         Train Epoch: [2180/2467]	Loss: 0.055360
 Train Epoch: [2180/2467]	Loss: 0.210450
     Train Epoch: [2180/2467]	Loss: 0.136568
     Train Epoch: [2180/2467]	Loss: 0.100795
              Train Epoch: [2200/2467]	Loss: 0.050288Train Epoch: [2200/2467]	Loss: 0.087600

     Train Epoch: [2200/2467]	Loss: 0.095722
 Train Epoch: [2200/2467]	Loss: 0.069970
     Train Epoch: [2220/2467]	Loss: 0.058511
     Train Epoch: [2220/2467]	Loss: 0.104442
         Train Epoch: [2220/2467]	Loss: 0.051458 
Train Epoch: [2220/2467]	Loss: 0.093995
     Train Epoch: [2240/2467]	Loss: 0.067356    
 Train Epoch: [2240/2467]	Loss: 0.085962
         Train Epoch: [2240/2467]	Loss: 0.031791
 Train Epoch: [2240/2467]	Loss: 0.021460
              Train Epoch: [2260/2467]	Loss: 0.174392Train Epoch: [2260/2467]	Loss: 0.024143

 Train Epoch: [2260/2467]	Loss: 0.138710
     Train Epoch: [2260/2467]	Loss: 0.222413
         Train Epoch: [2280/2467]	Loss: 0.128632    
 Train Epoch: [2280/2467]	Loss: 0.057204
 Train Epoch: [2280/2467]	Loss: 0.079085
     Train Epoch: [2280/2467]	Loss: 0.085102
     Train Epoch: [2300/2467]	Loss: 0.218760    
     Train Epoch: [2300/2467]	Loss: 0.053999
 Train Epoch: [2300/2467]	Loss: 0.091059
     Train Epoch: [2300/2467]	Loss: 0.038636
         Train Epoch: [2320/2467]	Loss: 0.111700 
Train Epoch: [2320/2467]	Loss: 0.114072    
     Train Epoch: [2320/2467]	Loss: 0.125173
 Train Epoch: [2320/2467]	Loss: 0.016567
     Train Epoch: [2340/2467]	Loss: 0.079562
     Train Epoch: [2340/2467]	Loss: 0.208873
     Train Epoch: [2340/2467]	Loss: 0.413370
     Train Epoch: [2340/2467]	Loss: 0.130665
             Train Epoch: [2360/2467]	Loss: 0.048668
      Train Epoch: [2360/2467]	Loss: 0.289922Train Epoch: [2360/2467]	Loss: 0.116700

 Train Epoch: [2360/2467]	Loss: 0.122919
     Train Epoch: [2380/2467]	Loss: 0.055229
     Train Epoch: [2380/2467]	Loss: 0.248530
     Train Epoch: [2380/2467]	Loss: 0.292757
     Train Epoch: [2380/2467]	Loss: 0.103158
         Train Epoch: [2400/2467]	Loss: 0.111189
     Train Epoch: [2400/2467]	Loss: 0.083775 
Train Epoch: [2400/2467]	Loss: 0.131019
     Train Epoch: [2400/2467]	Loss: 0.076922
     Train Epoch: [2420/2467]	Loss: 0.039161
     Train Epoch: [2420/2467]	Loss: 0.231925
     Train Epoch: [2420/2467]	Loss: 0.074713
     Train Epoch: [2420/2467]	Loss: 0.178788
     Train Epoch: [2440/2467]	Loss: 0.014842
             Train Epoch: [2440/2467]	Loss: 0.241644
 Train Epoch: [2440/2467]	Loss: 0.116621 
Train Epoch: [2440/2467]	Loss: 0.088838
     Train Epoch: [2460/2467]	Loss: 0.116569
     Train Epoch: [2460/2467]	Loss: 0.065739    
 Train Epoch: [2460/2467]	Loss: 0.194415
     Train Epoch: [2460/2467]	Loss: 0.085666
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 40 epoch =====
     2025-05-11.14-36-45
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 40 epoch =====
     2025-05-11.14-36-46
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 40 epoch =====
     2025-05-11.14-36-46
     ===== running 40 epoch =====
     2025-05-11.14-36-46
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
          Train Epoch: [0/2467]	Loss: 0.081807Train Epoch: [0/2467]	Loss: 0.215728

     Train Epoch: [0/2467]	Loss: 0.165538    
 Train Epoch: [0/2467]	Loss: 0.217578
         Train Epoch: [20/2467]	Loss: 0.068812
 Train Epoch: [20/2467]	Loss: 0.252620
         Train Epoch: [20/2467]	Loss: 0.042734
 Train Epoch: [20/2467]	Loss: 0.082292
     Train Epoch: [40/2467]	Loss: 0.067008
         Train Epoch: [40/2467]	Loss: 0.100061
 Train Epoch: [40/2467]	Loss: 0.302251    
 Train Epoch: [40/2467]	Loss: 0.086506
     Train Epoch: [60/2467]	Loss: 0.179197
     Train Epoch: [60/2467]	Loss: 0.024515    
 Train Epoch: [60/2467]	Loss: 0.109946
     Train Epoch: [60/2467]	Loss: 0.056075
     Train Epoch: [80/2467]	Loss: 0.089188
         Train Epoch: [80/2467]	Loss: 0.093343 
Train Epoch: [80/2467]	Loss: 0.042091
     Train Epoch: [80/2467]	Loss: 0.195710
     Train Epoch: [100/2467]	Loss: 0.047605    
 Train Epoch: [100/2467]	Loss: 0.123868    
 Train Epoch: [100/2467]	Loss: 0.068742
     Train Epoch: [100/2467]	Loss: 0.129200
              Train Epoch: [120/2467]	Loss: 0.263905
Train Epoch: [120/2467]	Loss: 0.038314
 Train Epoch: [120/2467]	Loss: 0.158139    
 Train Epoch: [120/2467]	Loss: 0.068721
             Train Epoch: [140/2467]	Loss: 0.025501 
Train Epoch: [140/2467]	Loss: 0.080380
     Train Epoch: [140/2467]	Loss: 0.154171
 Train Epoch: [140/2467]	Loss: 0.050173
     Train Epoch: [160/2467]	Loss: 0.111993
          Train Epoch: [160/2467]	Loss: 0.108684
Train Epoch: [160/2467]	Loss: 0.155613
     Train Epoch: [160/2467]	Loss: 0.096446
     Train Epoch: [180/2467]	Loss: 0.109946
         Train Epoch: [180/2467]	Loss: 0.156092
 Train Epoch: [180/2467]	Loss: 0.083554    
 Train Epoch: [180/2467]	Loss: 0.078634
         Train Epoch: [200/2467]	Loss: 0.034354
     Train Epoch: [200/2467]	Loss: 0.096700
     Train Epoch: [200/2467]	Loss: 0.067485 
Train Epoch: [200/2467]	Loss: 0.040003
     Train Epoch: [220/2467]	Loss: 0.132716
         Train Epoch: [220/2467]	Loss: 0.196692
 Train Epoch: [220/2467]	Loss: 0.040265
     Train Epoch: [220/2467]	Loss: 0.081498
     Train Epoch: [240/2467]	Loss: 0.016418
         Train Epoch: [240/2467]	Loss: 0.131165    
 Train Epoch: [240/2467]	Loss: 0.180277
 Train Epoch: [240/2467]	Loss: 0.034939
         Train Epoch: [260/2467]	Loss: 0.053536
 Train Epoch: [260/2467]	Loss: 0.028086
     Train Epoch: [260/2467]	Loss: 0.050892    
 Train Epoch: [260/2467]	Loss: 0.026664
     Train Epoch: [280/2467]	Loss: 0.072717    
     Train Epoch: [280/2467]	Loss: 0.136707
     Train Epoch: [280/2467]	Loss: 0.074360
 Train Epoch: [280/2467]	Loss: 0.097075
             Train Epoch: [300/2467]	Loss: 0.031175
 Train Epoch: [300/2467]	Loss: 0.310234
 Train Epoch: [300/2467]	Loss: 0.062302
     Train Epoch: [300/2467]	Loss: 0.149929
     Train Epoch: [320/2467]	Loss: 0.124793
         Train Epoch: [320/2467]	Loss: 0.183420
 Train Epoch: [320/2467]	Loss: 0.141513
     Train Epoch: [320/2467]	Loss: 0.342514
         Train Epoch: [340/2467]	Loss: 0.049794
     Train Epoch: [340/2467]	Loss: 0.041804
 Train Epoch: [340/2467]	Loss: 0.163494
     Train Epoch: [340/2467]	Loss: 0.012616
     Train Epoch: [360/2467]	Loss: 0.209362
     Train Epoch: [360/2467]	Loss: 0.048593
     Train Epoch: [360/2467]	Loss: 0.036314
     Train Epoch: [360/2467]	Loss: 0.120166
     Train Epoch: [380/2467]	Loss: 0.058062    
          Train Epoch: [380/2467]	Loss: 0.155274
Train Epoch: [380/2467]	Loss: 0.090336
 Train Epoch: [380/2467]	Loss: 0.230949
     Train Epoch: [400/2467]	Loss: 0.042734
          Train Epoch: [400/2467]	Loss: 0.040330
Train Epoch: [400/2467]	Loss: 0.128533
     Train Epoch: [400/2467]	Loss: 0.091681
         Train Epoch: [420/2467]	Loss: 0.260116
 Train Epoch: [420/2467]	Loss: 0.090961
     Train Epoch: [420/2467]	Loss: 0.069044
     Train Epoch: [420/2467]	Loss: 0.205265
     Train Epoch: [440/2467]	Loss: 0.108833
     Train Epoch: [440/2467]	Loss: 0.305989
     Train Epoch: [440/2467]	Loss: 0.062279
     Train Epoch: [440/2467]	Loss: 0.042838
         Train Epoch: [460/2467]	Loss: 0.200924
 Train Epoch: [460/2467]	Loss: 0.158375
     Train Epoch: [460/2467]	Loss: 0.082013
     Train Epoch: [460/2467]	Loss: 0.099117
     Train Epoch: [480/2467]	Loss: 0.115154    
 Train Epoch: [480/2467]	Loss: 0.023826
     Train Epoch: [480/2467]	Loss: 0.247526
     Train Epoch: [480/2467]	Loss: 0.291144
     Train Epoch: [500/2467]	Loss: 0.033000
         Train Epoch: [500/2467]	Loss: 0.060875
 Train Epoch: [500/2467]	Loss: 0.082200
     Train Epoch: [500/2467]	Loss: 0.240609
         Train Epoch: [520/2467]	Loss: 0.233421        
   Train Epoch: [520/2467]	Loss: 0.085056Train Epoch: [520/2467]	Loss: 0.060010
Train Epoch: [520/2467]	Loss: 0.081299

     Train Epoch: [540/2467]	Loss: 0.193397
     Train Epoch: [540/2467]	Loss: 0.076497
     Train Epoch: [540/2467]	Loss: 0.137534
     Train Epoch: [540/2467]	Loss: 0.054514
     Train Epoch: [560/2467]	Loss: 0.128375
         Train Epoch: [560/2467]	Loss: 0.121310
 Train Epoch: [560/2467]	Loss: 0.104291
     Train Epoch: [560/2467]	Loss: 0.110410
         Train Epoch: [580/2467]	Loss: 0.099477 
Train Epoch: [580/2467]	Loss: 0.252996
     Train Epoch: [580/2467]	Loss: 0.103723
     Train Epoch: [580/2467]	Loss: 0.055192
         Train Epoch: [600/2467]	Loss: 0.164468
 Train Epoch: [600/2467]	Loss: 0.103937
         Train Epoch: [600/2467]	Loss: 0.141298
 Train Epoch: [600/2467]	Loss: 0.079090
     Train Epoch: [620/2467]	Loss: 0.026003
         Train Epoch: [620/2467]	Loss: 0.122771
 Train Epoch: [620/2467]	Loss: 0.125824
     Train Epoch: [620/2467]	Loss: 0.082178
         Train Epoch: [640/2467]	Loss: 0.158983
 Train Epoch: [640/2467]	Loss: 0.079397
     Train Epoch: [640/2467]	Loss: 0.099534
     Train Epoch: [640/2467]	Loss: 0.032347
     Train Epoch: [660/2467]	Loss: 0.020306
         Train Epoch: [660/2467]	Loss: 0.051963    
 Train Epoch: [660/2467]	Loss: 0.307903
 Train Epoch: [660/2467]	Loss: 0.123717
     Train Epoch: [680/2467]	Loss: 0.032945
         Train Epoch: [680/2467]	Loss: 0.056807
 Train Epoch: [680/2467]	Loss: 0.022063
     Train Epoch: [680/2467]	Loss: 0.094721
         Train Epoch: [700/2467]	Loss: 0.031021
 Train Epoch: [700/2467]	Loss: 0.263716
     Train Epoch: [700/2467]	Loss: 0.253416
     Train Epoch: [700/2467]	Loss: 0.076979
     Train Epoch: [720/2467]	Loss: 0.190418
         Train Epoch: [720/2467]	Loss: 0.066882
 Train Epoch: [720/2467]	Loss: 0.097693
     Train Epoch: [720/2467]	Loss: 0.020276
     Train Epoch: [740/2467]	Loss: 0.066669
         Train Epoch: [740/2467]	Loss: 0.070551 
Train Epoch: [740/2467]	Loss: 0.043466
     Train Epoch: [740/2467]	Loss: 0.268384
         Train Epoch: [760/2467]	Loss: 0.222751
     Train Epoch: [760/2467]	Loss: 0.238791
 Train Epoch: [760/2467]	Loss: 0.311343
     Train Epoch: [760/2467]	Loss: 0.049123
     Train Epoch: [780/2467]	Loss: 0.333891
         Train Epoch: [780/2467]	Loss: 0.261986
 Train Epoch: [780/2467]	Loss: 0.280470
     Train Epoch: [780/2467]	Loss: 0.105807
         Train Epoch: [800/2467]	Loss: 0.161117
         Train Epoch: [800/2467]	Loss: 0.093434
 Train Epoch: [800/2467]	Loss: 0.077033
 Train Epoch: [800/2467]	Loss: 0.135176
         Train Epoch: [820/2467]	Loss: 0.038497
 Train Epoch: [820/2467]	Loss: 0.351238
         Train Epoch: [820/2467]	Loss: 0.163402 
Train Epoch: [820/2467]	Loss: 0.037595
     Train Epoch: [840/2467]	Loss: 0.069292
         Train Epoch: [840/2467]	Loss: 0.044205
 Train Epoch: [840/2467]	Loss: 0.116257
     Train Epoch: [840/2467]	Loss: 0.320939
     Train Epoch: [860/2467]	Loss: 0.199033
         Train Epoch: [860/2467]	Loss: 0.089962 
Train Epoch: [860/2467]	Loss: 0.047358
     Train Epoch: [860/2467]	Loss: 0.031821
     Train Epoch: [880/2467]	Loss: 0.113240
     Train Epoch: [880/2467]	Loss: 0.163730
     Train Epoch: [880/2467]	Loss: 0.122116
     Train Epoch: [880/2467]	Loss: 0.119297
     Train Epoch: [900/2467]	Loss: 0.142735
         Train Epoch: [900/2467]	Loss: 0.149898
     Train Epoch: [900/2467]	Loss: 0.040493
 Train Epoch: [900/2467]	Loss: 0.139677
     Train Epoch: [920/2467]	Loss: 0.215503    
 Train Epoch: [920/2467]	Loss: 0.033278
     Train Epoch: [920/2467]	Loss: 0.203772
     Train Epoch: [920/2467]	Loss: 0.242088
     Train Epoch: [940/2467]	Loss: 0.034613
     Train Epoch: [940/2467]	Loss: 0.174286
     Train Epoch: [940/2467]	Loss: 0.034171
     Train Epoch: [940/2467]	Loss: 0.082407
          Train Epoch: [960/2467]	Loss: 0.058147Train Epoch: [960/2467]	Loss: 0.230306

         Train Epoch: [960/2467]	Loss: 0.066179
 Train Epoch: [960/2467]	Loss: 0.063344
         Train Epoch: [980/2467]	Loss: 0.061757 
Train Epoch: [980/2467]	Loss: 0.179400
          Train Epoch: [980/2467]	Loss: 0.086807Train Epoch: [980/2467]	Loss: 0.099335

          Train Epoch: [1000/2467]	Loss: 0.027425
Train Epoch: [1000/2467]	Loss: 0.025825
     Train Epoch: [1000/2467]	Loss: 0.200104
     Train Epoch: [1000/2467]	Loss: 0.169351
     Train Epoch: [1020/2467]	Loss: 0.118823
             Train Epoch: [1020/2467]	Loss: 0.015353
  Train Epoch: [1020/2467]	Loss: 0.309375Train Epoch: [1020/2467]	Loss: 0.069138

     Train Epoch: [1040/2467]	Loss: 0.094557
     Train Epoch: [1040/2467]	Loss: 0.127111
     Train Epoch: [1040/2467]	Loss: 0.076054
     Train Epoch: [1040/2467]	Loss: 0.205054
         Train Epoch: [1060/2467]	Loss: 0.080084
 Train Epoch: [1060/2467]	Loss: 0.335172
     Train Epoch: [1060/2467]	Loss: 0.063683
     Train Epoch: [1060/2467]	Loss: 0.152180
         Train Epoch: [1080/2467]	Loss: 0.090840
 Train Epoch: [1080/2467]	Loss: 0.157957
         Train Epoch: [1080/2467]	Loss: 0.130768
 Train Epoch: [1080/2467]	Loss: 0.198396
     Train Epoch: [1100/2467]	Loss: 0.242522
             Train Epoch: [1100/2467]	Loss: 0.126600
 Train Epoch: [1100/2467]	Loss: 0.193360 
Train Epoch: [1100/2467]	Loss: 0.174748
     Train Epoch: [1120/2467]	Loss: 0.191370
     Train Epoch: [1120/2467]	Loss: 0.101757
     Train Epoch: [1120/2467]	Loss: 0.111002
     Train Epoch: [1120/2467]	Loss: 0.083254
     Train Epoch: [1140/2467]	Loss: 0.042544
             Train Epoch: [1140/2467]	Loss: 0.324773 
Train Epoch: [1140/2467]	Loss: 0.069943
 Train Epoch: [1140/2467]	Loss: 0.224906
         Train Epoch: [1160/2467]	Loss: 0.073074
 Train Epoch: [1160/2467]	Loss: 0.058958
         Train Epoch: [1160/2467]	Loss: 0.105603
 Train Epoch: [1160/2467]	Loss: 0.239484
         Train Epoch: [1180/2467]	Loss: 0.112592
 Train Epoch: [1180/2467]	Loss: 0.073983
     Train Epoch: [1180/2467]	Loss: 0.125053
     Train Epoch: [1180/2467]	Loss: 0.035797
         Train Epoch: [1200/2467]	Loss: 0.262613
     Train Epoch: [1200/2467]	Loss: 0.081294
 Train Epoch: [1200/2467]	Loss: 0.152262
     Train Epoch: [1200/2467]	Loss: 0.050773
     Train Epoch: [1220/2467]	Loss: 0.073491
         Train Epoch: [1220/2467]	Loss: 0.216774
 Train Epoch: [1220/2467]	Loss: 0.263356
     Train Epoch: [1220/2467]	Loss: 0.144673
     Train Epoch: [1240/2467]	Loss: 0.256311
     Train Epoch: [1240/2467]	Loss: 0.108678
     Train Epoch: [1240/2467]	Loss: 0.100474
     Train Epoch: [1240/2467]	Loss: 0.257039
         Train Epoch: [1260/2467]	Loss: 0.044704
     Train Epoch: [1260/2467]	Loss: 0.044334    
 Train Epoch: [1260/2467]	Loss: 0.088183
 Train Epoch: [1260/2467]	Loss: 0.340245
     Train Epoch: [1280/2467]	Loss: 0.092828
     Train Epoch: [1280/2467]	Loss: 0.167163
     Train Epoch: [1280/2467]	Loss: 0.062822
     Train Epoch: [1280/2467]	Loss: 0.372396
     Train Epoch: [1300/2467]	Loss: 0.063646
     Train Epoch: [1300/2467]	Loss: 0.241547
         Train Epoch: [1300/2467]	Loss: 0.028460
 Train Epoch: [1300/2467]	Loss: 0.164781
     Train Epoch: [1320/2467]	Loss: 0.039213
         Train Epoch: [1320/2467]	Loss: 0.086155     
Train Epoch: [1320/2467]	Loss: 0.114337
 Train Epoch: [1320/2467]	Loss: 0.152365
         Train Epoch: [1340/2467]	Loss: 0.191338
 Train Epoch: [1340/2467]	Loss: 0.102128
         Train Epoch: [1340/2467]	Loss: 0.120357
 Train Epoch: [1340/2467]	Loss: 0.247933
         Train Epoch: [1360/2467]	Loss: 0.051838
     Train Epoch: [1360/2467]	Loss: 0.101295
      Train Epoch: [1360/2467]	Loss: 0.104206Train Epoch: [1360/2467]	Loss: 0.033673

     Train Epoch: [1380/2467]	Loss: 0.135141
     Train Epoch: [1380/2467]	Loss: 0.225831
          Train Epoch: [1380/2467]	Loss: 0.184005Train Epoch: [1380/2467]	Loss: 0.179907

     Train Epoch: [1400/2467]	Loss: 0.060583
          Train Epoch: [1400/2467]	Loss: 0.054644Train Epoch: [1400/2467]	Loss: 0.114487

     Train Epoch: [1400/2467]	Loss: 0.104284
         Train Epoch: [1420/2467]	Loss: 0.266426
 Train Epoch: [1420/2467]	Loss: 0.071445
         Train Epoch: [1420/2467]	Loss: 0.125409
 Train Epoch: [1420/2467]	Loss: 0.107613
         Train Epoch: [1440/2467]	Loss: 0.036055
     Train Epoch: [1440/2467]	Loss: 0.088174
 Train Epoch: [1440/2467]	Loss: 0.040730
     Train Epoch: [1440/2467]	Loss: 0.014096
     Train Epoch: [1460/2467]	Loss: 0.036886
     Train Epoch: [1460/2467]	Loss: 0.119479
     Train Epoch: [1460/2467]	Loss: 0.145595
     Train Epoch: [1460/2467]	Loss: 0.114142
     Train Epoch: [1480/2467]	Loss: 0.027022
     Train Epoch: [1480/2467]	Loss: 0.012173
     Train Epoch: [1480/2467]	Loss: 0.157340
     Train Epoch: [1480/2467]	Loss: 0.190630
     Train Epoch: [1500/2467]	Loss: 0.083986
             Train Epoch: [1500/2467]	Loss: 0.019734 
Train Epoch: [1500/2467]	Loss: 0.137369
 Train Epoch: [1500/2467]	Loss: 0.052456
         Train Epoch: [1520/2467]	Loss: 0.098515
     Train Epoch: [1520/2467]	Loss: 0.179445
 Train Epoch: [1520/2467]	Loss: 0.186225
     Train Epoch: [1520/2467]	Loss: 0.122194
     Train Epoch: [1540/2467]	Loss: 0.073035
     Train Epoch: [1540/2467]	Loss: 0.245399
         Train Epoch: [1540/2467]	Loss: 0.092444 
Train Epoch: [1540/2467]	Loss: 0.069175
     Train Epoch: [1560/2467]	Loss: 0.059482
         Train Epoch: [1560/2467]	Loss: 0.286965
 Train Epoch: [1560/2467]	Loss: 0.221318
     Train Epoch: [1560/2467]	Loss: 0.379066
     Train Epoch: [1580/2467]	Loss: 0.063021
             Train Epoch: [1580/2467]	Loss: 0.191991
 Train Epoch: [1580/2467]	Loss: 0.083908 
Train Epoch: [1580/2467]	Loss: 0.196111
     Train Epoch: [1600/2467]	Loss: 0.025704
     Train Epoch: [1600/2467]	Loss: 0.161057
     Train Epoch: [1600/2467]	Loss: 0.210189
     Train Epoch: [1600/2467]	Loss: 0.080874
     Train Epoch: [1620/2467]	Loss: 0.148975    
 Train Epoch: [1620/2467]	Loss: 0.091486
     Train Epoch: [1620/2467]	Loss: 0.228665
     Train Epoch: [1620/2467]	Loss: 0.031844
          Train Epoch: [1640/2467]	Loss: 0.099586Train Epoch: [1640/2467]	Loss: 0.155818

     Train Epoch: [1640/2467]	Loss: 0.046436
     Train Epoch: [1640/2467]	Loss: 0.258938
         Train Epoch: [1660/2467]	Loss: 0.136871
 Train Epoch: [1660/2467]	Loss: 0.106357
     Train Epoch: [1660/2467]	Loss: 0.149318
     Train Epoch: [1660/2467]	Loss: 0.044652
     Train Epoch: [1680/2467]	Loss: 0.040351
     Train Epoch: [1680/2467]	Loss: 0.099493
     Train Epoch: [1680/2467]	Loss: 0.193182
     Train Epoch: [1680/2467]	Loss: 0.126882
              Train Epoch: [1700/2467]	Loss: 0.068604
Train Epoch: [1700/2467]	Loss: 0.085561
 Train Epoch: [1700/2467]	Loss: 0.216722
     Train Epoch: [1700/2467]	Loss: 0.117243
     Train Epoch: [1720/2467]	Loss: 0.180088
             Train Epoch: [1720/2467]	Loss: 0.111264
  Train Epoch: [1720/2467]	Loss: 0.148940
Train Epoch: [1720/2467]	Loss: 0.034257
     Train Epoch: [1740/2467]	Loss: 0.027212
             Train Epoch: [1740/2467]	Loss: 0.033956
  Train Epoch: [1740/2467]	Loss: 0.244861Train Epoch: [1740/2467]	Loss: 0.177940

     Train Epoch: [1760/2467]	Loss: 0.092062
          Train Epoch: [1760/2467]	Loss: 0.045705
Train Epoch: [1760/2467]	Loss: 0.096139
     Train Epoch: [1760/2467]	Loss: 0.064388
         Train Epoch: [1780/2467]	Loss: 0.155360
     Train Epoch: [1780/2467]	Loss: 0.123714
 Train Epoch: [1780/2467]	Loss: 0.171322    
 Train Epoch: [1780/2467]	Loss: 0.069428
     Train Epoch: [1800/2467]	Loss: 0.066210
         Train Epoch: [1800/2467]	Loss: 0.120862
 Train Epoch: [1800/2467]	Loss: 0.244566
     Train Epoch: [1800/2467]	Loss: 0.211741
     Train Epoch: [1820/2467]	Loss: 0.250216
         Train Epoch: [1820/2467]	Loss: 0.077072    
 Train Epoch: [1820/2467]	Loss: 0.088326
 Train Epoch: [1820/2467]	Loss: 0.123981
     Train Epoch: [1840/2467]	Loss: 0.117217
         Train Epoch: [1840/2467]	Loss: 0.232109
 Train Epoch: [1840/2467]	Loss: 0.043244
     Train Epoch: [1840/2467]	Loss: 0.092326
     Train Epoch: [1860/2467]	Loss: 0.137931
         Train Epoch: [1860/2467]	Loss: 0.081150
 Train Epoch: [1860/2467]	Loss: 0.115561
     Train Epoch: [1860/2467]	Loss: 0.105093
         Train Epoch: [1880/2467]	Loss: 0.024513
 Train Epoch: [1880/2467]	Loss: 0.069979
     Train Epoch: [1880/2467]	Loss: 0.053580
     Train Epoch: [1880/2467]	Loss: 0.040762
         Train Epoch: [1900/2467]	Loss: 0.087454
 Train Epoch: [1900/2467]	Loss: 0.101294
     Train Epoch: [1900/2467]	Loss: 0.236327
     Train Epoch: [1900/2467]	Loss: 0.225413
     Train Epoch: [1920/2467]	Loss: 0.017161
         Train Epoch: [1920/2467]	Loss: 0.092626
 Train Epoch: [1920/2467]	Loss: 0.146444
     Train Epoch: [1920/2467]	Loss: 0.171293
     Train Epoch: [1940/2467]	Loss: 0.056783
         Train Epoch: [1940/2467]	Loss: 0.090382
     Train Epoch: [1940/2467]	Loss: 0.301332
 Train Epoch: [1940/2467]	Loss: 0.121227
     Train Epoch: [1960/2467]	Loss: 0.094063
     Train Epoch: [1960/2467]	Loss: 0.090629    
     Train Epoch: [1960/2467]	Loss: 0.145118
 Train Epoch: [1960/2467]	Loss: 0.062143
          Train Epoch: [1980/2467]	Loss: 0.087316
Train Epoch: [1980/2467]	Loss: 0.171129
     Train Epoch: [1980/2467]	Loss: 0.222212
     Train Epoch: [1980/2467]	Loss: 0.187197
     Train Epoch: [2000/2467]	Loss: 0.041718
         Train Epoch: [2000/2467]	Loss: 0.117375 
Train Epoch: [2000/2467]	Loss: 0.131066
     Train Epoch: [2000/2467]	Loss: 0.013673
     Train Epoch: [2020/2467]	Loss: 0.141071
     Train Epoch: [2020/2467]	Loss: 0.111651
     Train Epoch: [2020/2467]	Loss: 0.052321
     Train Epoch: [2020/2467]	Loss: 0.114273
         Train Epoch: [2040/2467]	Loss: 0.128345
 Train Epoch: [2040/2467]	Loss: 0.068780    
     Train Epoch: [2040/2467]	Loss: 0.055625
 Train Epoch: [2040/2467]	Loss: 0.347186
         Train Epoch: [2060/2467]	Loss: 0.144428
 Train Epoch: [2060/2467]	Loss: 0.058886
     Train Epoch: [2060/2467]	Loss: 0.134037
     Train Epoch: [2060/2467]	Loss: 0.185585
         Train Epoch: [2080/2467]	Loss: 0.032275 
Train Epoch: [2080/2467]	Loss: 0.205016
     Train Epoch: [2080/2467]	Loss: 0.060286
     Train Epoch: [2080/2467]	Loss: 0.022366
         Train Epoch: [2100/2467]	Loss: 0.368477
 Train Epoch: [2100/2467]	Loss: 0.098604
         Train Epoch: [2100/2467]	Loss: 0.078767
 Train Epoch: [2100/2467]	Loss: 0.038980
     Train Epoch: [2120/2467]	Loss: 0.082219
     Train Epoch: [2120/2467]	Loss: 0.081333
     Train Epoch: [2120/2467]	Loss: 0.161023
     Train Epoch: [2120/2467]	Loss: 0.030857
     Train Epoch: [2140/2467]	Loss: 0.375425
     Train Epoch: [2140/2467]	Loss: 0.195175
         Train Epoch: [2140/2467]	Loss: 0.269547
 Train Epoch: [2140/2467]	Loss: 0.179170
         Train Epoch: [2160/2467]	Loss: 0.041295
 Train Epoch: [2160/2467]	Loss: 0.256652
         Train Epoch: [2160/2467]	Loss: 0.083026
 Train Epoch: [2160/2467]	Loss: 0.045867
         Train Epoch: [2180/2467]	Loss: 0.046215
 Train Epoch: [2180/2467]	Loss: 0.221503
         Train Epoch: [2180/2467]	Loss: 0.100367
 Train Epoch: [2180/2467]	Loss: 0.124414
     Train Epoch: [2200/2467]	Loss: 0.037122    
 Train Epoch: [2200/2467]	Loss: 0.128930        
 Train Epoch: [2200/2467]	Loss: 0.088291
 Train Epoch: [2200/2467]	Loss: 0.065866
     Train Epoch: [2220/2467]	Loss: 0.046702
     Train Epoch: [2220/2467]	Loss: 0.086660
     Train Epoch: [2220/2467]	Loss: 0.092108
     Train Epoch: [2220/2467]	Loss: 0.042353
          Train Epoch: [2240/2467]	Loss: 0.075087
Train Epoch: [2240/2467]	Loss: 0.064758
     Train Epoch: [2240/2467]	Loss: 0.029602
     Train Epoch: [2240/2467]	Loss: 0.024952
     Train Epoch: [2260/2467]	Loss: 0.121931
         Train Epoch: [2260/2467]	Loss: 0.168881 
Train Epoch: [2260/2467]	Loss: 0.034671
     Train Epoch: [2260/2467]	Loss: 0.195553
     Train Epoch: [2280/2467]	Loss: 0.121322
         Train Epoch: [2280/2467]	Loss: 0.077732 
Train Epoch: [2280/2467]	Loss: 0.052387
     Train Epoch: [2280/2467]	Loss: 0.066021
              Train Epoch: [2300/2467]	Loss: 0.047249Train Epoch: [2300/2467]	Loss: 0.245156

 Train Epoch: [2300/2467]	Loss: 0.080040
     Train Epoch: [2300/2467]	Loss: 0.040475
     Train Epoch: [2320/2467]	Loss: 0.078596
          Train Epoch: [2320/2467]	Loss: 0.107844Train Epoch: [2320/2467]	Loss: 0.011228

     Train Epoch: [2320/2467]	Loss: 0.119915
     Train Epoch: [2340/2467]	Loss: 0.079623
          Train Epoch: [2340/2467]	Loss: 0.177451
Train Epoch: [2340/2467]	Loss: 0.129010
     Train Epoch: [2340/2467]	Loss: 0.411299
     Train Epoch: [2360/2467]	Loss: 0.117438
         Train Epoch: [2360/2467]	Loss: 0.053438 
Train Epoch: [2360/2467]	Loss: 0.276710
     Train Epoch: [2360/2467]	Loss: 0.108707
     Train Epoch: [2380/2467]	Loss: 0.039083
     Train Epoch: [2380/2467]	Loss: 0.291794
     Train Epoch: [2380/2467]	Loss: 0.097340
     Train Epoch: [2380/2467]	Loss: 0.239700
         Train Epoch: [2400/2467]	Loss: 0.084144 
Train Epoch: [2400/2467]	Loss: 0.088249
     Train Epoch: [2400/2467]	Loss: 0.057891
     Train Epoch: [2400/2467]	Loss: 0.132882
     Train Epoch: [2420/2467]	Loss: 0.058862
          Train Epoch: [2420/2467]	Loss: 0.204925Train Epoch: [2420/2467]	Loss: 0.180851

     Train Epoch: [2420/2467]	Loss: 0.014459
     Train Epoch: [2440/2467]	Loss: 0.009429
     Train Epoch: [2440/2467]	Loss: 0.176354
         Train Epoch: [2440/2467]	Loss: 0.110678
 Train Epoch: [2440/2467]	Loss: 0.100243
     Train Epoch: [2460/2467]	Loss: 0.118049
     Train Epoch: [2460/2467]	Loss: 0.064589
         Train Epoch: [2460/2467]	Loss: 0.092181
 Train Epoch: [2460/2467]	Loss: 0.181055
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 41 epoch =====
     2025-05-11.14-58-36
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 41 epoch =====
     2025-05-11.14-58-36
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 41 epoch =====
     2025-05-11.14-58-36
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 41 epoch =====
     2025-05-11.14-58-36
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.191850
         Train Epoch: [0/2467]	Loss: 0.050287
 Train Epoch: [0/2467]	Loss: 0.168309
     Train Epoch: [0/2467]	Loss: 0.230236
             Train Epoch: [20/2467]	Loss: 0.263987
 Train Epoch: [20/2467]	Loss: 0.071904
 Train Epoch: [20/2467]	Loss: 0.084522
     Train Epoch: [20/2467]	Loss: 0.044927
     Train Epoch: [40/2467]	Loss: 0.074228
         Train Epoch: [40/2467]	Loss: 0.080766
 Train Epoch: [40/2467]	Loss: 0.320306
     Train Epoch: [40/2467]	Loss: 0.087546
             Train Epoch: [60/2467]	Loss: 0.025877
 Train Epoch: [60/2467]	Loss: 0.058189 
Train Epoch: [60/2467]	Loss: 0.108830
     Train Epoch: [60/2467]	Loss: 0.139827
         Train Epoch: [80/2467]	Loss: 0.221082
 Train Epoch: [80/2467]	Loss: 0.095597
         Train Epoch: [80/2467]	Loss: 0.052382
 Train Epoch: [80/2467]	Loss: 0.085823
     Train Epoch: [100/2467]	Loss: 0.053290
         Train Epoch: [100/2467]	Loss: 0.119487
 Train Epoch: [100/2467]	Loss: 0.066839
     Train Epoch: [100/2467]	Loss: 0.136086
     Train Epoch: [120/2467]	Loss: 0.041049
         Train Epoch: [120/2467]	Loss: 0.291965
 Train Epoch: [120/2467]	Loss: 0.160663    
 Train Epoch: [120/2467]	Loss: 0.073187
         Train Epoch: [140/2467]	Loss: 0.018129 
    Train Epoch: [140/2467]	Loss: 0.092035
     Train Epoch: [140/2467]	Loss: 0.142455
 Train Epoch: [140/2467]	Loss: 0.046032
          Train Epoch: [160/2467]	Loss: 0.105889Train Epoch: [160/2467]	Loss: 0.082895

     Train Epoch: [160/2467]	Loss: 0.156577
     Train Epoch: [160/2467]	Loss: 0.103922
     Train Epoch: [180/2467]	Loss: 0.088782
         Train Epoch: [180/2467]	Loss: 0.148319
 Train Epoch: [180/2467]	Loss: 0.107024
     Train Epoch: [180/2467]	Loss: 0.137545
         Train Epoch: [200/2467]	Loss: 0.092783
 Train Epoch: [200/2467]	Loss: 0.042301
     Train Epoch: [200/2467]	Loss: 0.061404
     Train Epoch: [200/2467]	Loss: 0.063678
     Train Epoch: [220/2467]	Loss: 0.072060
         Train Epoch: [220/2467]	Loss: 0.043141
 Train Epoch: [220/2467]	Loss: 0.118735
     Train Epoch: [220/2467]	Loss: 0.179740
         Train Epoch: [240/2467]	Loss: 0.014821
 Train Epoch: [240/2467]	Loss: 0.165395
     Train Epoch: [240/2467]	Loss: 0.031617
     Train Epoch: [240/2467]	Loss: 0.154690
         Train Epoch: [260/2467]	Loss: 0.045694
     Train Epoch: [260/2467]	Loss: 0.027142
 Train Epoch: [260/2467]	Loss: 0.023655
     Train Epoch: [260/2467]	Loss: 0.047939
         Train Epoch: [280/2467]	Loss: 0.083068
 Train Epoch: [280/2467]	Loss: 0.133807
     Train Epoch: [280/2467]	Loss: 0.083091
     Train Epoch: [280/2467]	Loss: 0.088716
         Train Epoch: [300/2467]	Loss: 0.155733
 Train Epoch: [300/2467]	Loss: 0.025511
         Train Epoch: [300/2467]	Loss: 0.063297
 Train Epoch: [300/2467]	Loss: 0.310918
     Train Epoch: [320/2467]	Loss: 0.120706    
 Train Epoch: [320/2467]	Loss: 0.189581
         Train Epoch: [320/2467]	Loss: 0.144632
 Train Epoch: [320/2467]	Loss: 0.316933
         Train Epoch: [340/2467]	Loss: 0.042969 
Train Epoch: [340/2467]	Loss: 0.056487
         Train Epoch: [340/2467]	Loss: 0.012467
 Train Epoch: [340/2467]	Loss: 0.146659
     Train Epoch: [360/2467]	Loss: 0.177505
         Train Epoch: [360/2467]	Loss: 0.029119 
Train Epoch: [360/2467]	Loss: 0.058073
     Train Epoch: [360/2467]	Loss: 0.117214
                 Train Epoch: [380/2467]	Loss: 0.159866 
Train Epoch: [380/2467]	Loss: 0.053365
  Train Epoch: [380/2467]	Loss: 0.227643Train Epoch: [380/2467]	Loss: 0.070929

     Train Epoch: [400/2467]	Loss: 0.045037
         Train Epoch: [400/2467]	Loss: 0.020372    
 Train Epoch: [400/2467]	Loss: 0.134540
 Train Epoch: [400/2467]	Loss: 0.093570
     Train Epoch: [420/2467]	Loss: 0.190136
     Train Epoch: [420/2467]	Loss: 0.251200
          Train Epoch: [420/2467]	Loss: 0.085973Train Epoch: [420/2467]	Loss: 0.071229

     Train Epoch: [440/2467]	Loss: 0.107263
     Train Epoch: [440/2467]	Loss: 0.263726
     Train Epoch: [440/2467]	Loss: 0.065189
     Train Epoch: [440/2467]	Loss: 0.052313
         Train Epoch: [460/2467]	Loss: 0.197553
 Train Epoch: [460/2467]	Loss: 0.132128
         Train Epoch: [460/2467]	Loss: 0.084532
 Train Epoch: [460/2467]	Loss: 0.071483
     Train Epoch: [480/2467]	Loss: 0.120477
         Train Epoch: [480/2467]	Loss: 0.223267
 Train Epoch: [480/2467]	Loss: 0.024556
     Train Epoch: [480/2467]	Loss: 0.306565
             Train Epoch: [500/2467]	Loss: 0.039087
 Train Epoch: [500/2467]	Loss: 0.090769
 Train Epoch: [500/2467]	Loss: 0.055499
     Train Epoch: [500/2467]	Loss: 0.163113
     Train Epoch: [520/2467]	Loss: 0.077946
         Train Epoch: [520/2467]	Loss: 0.227223
 Train Epoch: [520/2467]	Loss: 0.075840
     Train Epoch: [520/2467]	Loss: 0.080868
     Train Epoch: [540/2467]	Loss: 0.178369
     Train Epoch: [540/2467]	Loss: 0.081916
     Train Epoch: [540/2467]	Loss: 0.047755
     Train Epoch: [540/2467]	Loss: 0.129628
     Train Epoch: [560/2467]	Loss: 0.118883
     Train Epoch: [560/2467]	Loss: 0.161589
     Train Epoch: [560/2467]	Loss: 0.127095
     Train Epoch: [560/2467]	Loss: 0.107666
     Train Epoch: [580/2467]	Loss: 0.257611
             Train Epoch: [580/2467]	Loss: 0.105772
  Train Epoch: [580/2467]	Loss: 0.108404Train Epoch: [580/2467]	Loss: 0.055409

               Train Epoch: [600/2467]	Loss: 0.100729Train Epoch: [600/2467]	Loss: 0.103825
Train Epoch: [600/2467]	Loss: 0.112283

     Train Epoch: [600/2467]	Loss: 0.153512
     Train Epoch: [620/2467]	Loss: 0.025559
          Train Epoch: [620/2467]	Loss: 0.120535
Train Epoch: [620/2467]	Loss: 0.102492
     Train Epoch: [620/2467]	Loss: 0.106185
     Train Epoch: [640/2467]	Loss: 0.063666
         Train Epoch: [640/2467]	Loss: 0.146746
 Train Epoch: [640/2467]	Loss: 0.034849
     Train Epoch: [640/2467]	Loss: 0.101882
     Train Epoch: [660/2467]	Loss: 0.035495
          Train Epoch: [660/2467]	Loss: 0.108046
Train Epoch: [660/2467]	Loss: 0.345723
     Train Epoch: [660/2467]	Loss: 0.020410
     Train Epoch: [680/2467]	Loss: 0.092767
          Train Epoch: [680/2467]	Loss: 0.032875Train Epoch: [680/2467]	Loss: 0.021297

     Train Epoch: [680/2467]	Loss: 0.060524
             Train Epoch: [700/2467]	Loss: 0.278104
 Train Epoch: [700/2467]	Loss: 0.033727
     Train Epoch: [700/2467]	Loss: 0.072469
 Train Epoch: [700/2467]	Loss: 0.239473
     Train Epoch: [720/2467]	Loss: 0.140353
          Train Epoch: [720/2467]	Loss: 0.081542Train Epoch: [720/2467]	Loss: 0.012935

     Train Epoch: [720/2467]	Loss: 0.053845
     Train Epoch: [740/2467]	Loss: 0.038035
     Train Epoch: [740/2467]	Loss: 0.049300    
 Train Epoch: [740/2467]	Loss: 0.072809
     Train Epoch: [740/2467]	Loss: 0.274777
     Train Epoch: [760/2467]	Loss: 0.199041
          Train Epoch: [760/2467]	Loss: 0.328772
Train Epoch: [760/2467]	Loss: 0.229305
     Train Epoch: [760/2467]	Loss: 0.053263
         Train Epoch: [780/2467]	Loss: 0.259924
 Train Epoch: [780/2467]	Loss: 0.255314
     Train Epoch: [780/2467]	Loss: 0.086817
     Train Epoch: [780/2467]	Loss: 0.296652
     Train Epoch: [800/2467]	Loss: 0.133337
     Train Epoch: [800/2467]	Loss: 0.080298
     Train Epoch: [800/2467]	Loss: 0.071032
     Train Epoch: [800/2467]	Loss: 0.122775
         Train Epoch: [820/2467]	Loss: 0.035705 
Train Epoch: [820/2467]	Loss: 0.346264
          Train Epoch: [820/2467]	Loss: 0.051113
Train Epoch: [820/2467]	Loss: 0.148291
     Train Epoch: [840/2467]	Loss: 0.291877
     Train Epoch: [840/2467]	Loss: 0.081086
          Train Epoch: [840/2467]	Loss: 0.046768
Train Epoch: [840/2467]	Loss: 0.094193
     Train Epoch: [860/2467]	Loss: 0.186319
             Train Epoch: [860/2467]	Loss: 0.084630
 Train Epoch: [860/2467]	Loss: 0.023708
 Train Epoch: [860/2467]	Loss: 0.038961
         Train Epoch: [880/2467]	Loss: 0.098775
 Train Epoch: [880/2467]	Loss: 0.120962
         Train Epoch: [880/2467]	Loss: 0.122954
 Train Epoch: [880/2467]	Loss: 0.141830
     Train Epoch: [900/2467]	Loss: 0.148767
          Train Epoch: [900/2467]	Loss: 0.140596Train Epoch: [900/2467]	Loss: 0.042948

     Train Epoch: [900/2467]	Loss: 0.132562
          Train Epoch: [920/2467]	Loss: 0.200324Train Epoch: [920/2467]	Loss: 0.190083

     Train Epoch: [920/2467]	Loss: 0.030750
     Train Epoch: [920/2467]	Loss: 0.196872
          Train Epoch: [940/2467]	Loss: 0.024447Train Epoch: [940/2467]	Loss: 0.036024

     Train Epoch: [940/2467]	Loss: 0.152292
     Train Epoch: [940/2467]	Loss: 0.054746
         Train Epoch: [960/2467]	Loss: 0.224087
 Train Epoch: [960/2467]	Loss: 0.050320
         Train Epoch: [960/2467]	Loss: 0.054641 
Train Epoch: [960/2467]	Loss: 0.057209
         Train Epoch: [980/2467]	Loss: 0.059581
     Train Epoch: [980/2467]	Loss: 0.187793    
 Train Epoch: [980/2467]	Loss: 0.095761 
Train Epoch: [980/2467]	Loss: 0.077155
         Train Epoch: [1000/2467]	Loss: 0.024332
     Train Epoch: [1000/2467]	Loss: 0.022219    
  Train Epoch: [1000/2467]	Loss: 0.157233
Train Epoch: [1000/2467]	Loss: 0.181340
         Train Epoch: [1020/2467]	Loss: 0.108465
 Train Epoch: [1020/2467]	Loss: 0.305704
     Train Epoch: [1020/2467]	Loss: 0.062691
     Train Epoch: [1020/2467]	Loss: 0.015924
     Train Epoch: [1040/2467]	Loss: 0.091392
         Train Epoch: [1040/2467]	Loss: 0.108297
 Train Epoch: [1040/2467]	Loss: 0.191032
     Train Epoch: [1040/2467]	Loss: 0.070489
         Train Epoch: [1060/2467]	Loss: 0.075766 
    Train Epoch: [1060/2467]	Loss: 0.172823
 Train Epoch: [1060/2467]	Loss: 0.357140
     Train Epoch: [1060/2467]	Loss: 0.063517
         Train Epoch: [1080/2467]	Loss: 0.093773
 Train Epoch: [1080/2467]	Loss: 0.150521
         Train Epoch: [1080/2467]	Loss: 0.122385
 Train Epoch: [1080/2467]	Loss: 0.168145
         Train Epoch: [1100/2467]	Loss: 0.246071
 Train Epoch: [1100/2467]	Loss: 0.154402
     Train Epoch: [1100/2467]	Loss: 0.141796
     Train Epoch: [1100/2467]	Loss: 0.232715
     Train Epoch: [1120/2467]	Loss: 0.182916
         Train Epoch: [1120/2467]	Loss: 0.100381 
Train Epoch: [1120/2467]	Loss: 0.094388
     Train Epoch: [1120/2467]	Loss: 0.101243
              Train Epoch: [1140/2467]	Loss: 0.288914Train Epoch: [1140/2467]	Loss: 0.067996

     Train Epoch: [1140/2467]	Loss: 0.246477
 Train Epoch: [1140/2467]	Loss: 0.037216
     Train Epoch: [1160/2467]	Loss: 0.063818
         Train Epoch: [1160/2467]	Loss: 0.194353 
Train Epoch: [1160/2467]	Loss: 0.096793
     Train Epoch: [1160/2467]	Loss: 0.061865
         Train Epoch: [1180/2467]	Loss: 0.085910
 Train Epoch: [1180/2467]	Loss: 0.093266
     Train Epoch: [1180/2467]	Loss: 0.103705
     Train Epoch: [1180/2467]	Loss: 0.036455
     Train Epoch: [1200/2467]	Loss: 0.075589
         Train Epoch: [1200/2467]	Loss: 0.152114 
Train Epoch: [1200/2467]	Loss: 0.270629
     Train Epoch: [1200/2467]	Loss: 0.050918
         Train Epoch: [1220/2467]	Loss: 0.067841
 Train Epoch: [1220/2467]	Loss: 0.132286
         Train Epoch: [1220/2467]	Loss: 0.239451
 Train Epoch: [1220/2467]	Loss: 0.233340
         Train Epoch: [1240/2467]	Loss: 0.212784
 Train Epoch: [1240/2467]	Loss: 0.246694
         Train Epoch: [1240/2467]	Loss: 0.120127
 Train Epoch: [1240/2467]	Loss: 0.081831
     Train Epoch: [1260/2467]	Loss: 0.045560
         Train Epoch: [1260/2467]	Loss: 0.032066
     Train Epoch: [1260/2467]	Loss: 0.089543
 Train Epoch: [1260/2467]	Loss: 0.329451
     Train Epoch: [1280/2467]	Loss: 0.088525    
     Train Epoch: [1280/2467]	Loss: 0.156051
     Train Epoch: [1280/2467]	Loss: 0.057044
 Train Epoch: [1280/2467]	Loss: 0.352567
         Train Epoch: [1300/2467]	Loss: 0.063681
 Train Epoch: [1300/2467]	Loss: 0.221423
          Train Epoch: [1300/2467]	Loss: 0.028814Train Epoch: [1300/2467]	Loss: 0.153411

     Train Epoch: [1320/2467]	Loss: 0.043237
         Train Epoch: [1320/2467]	Loss: 0.094059    
 Train Epoch: [1320/2467]	Loss: 0.115909
 Train Epoch: [1320/2467]	Loss: 0.121472
     Train Epoch: [1340/2467]	Loss: 0.096661
         Train Epoch: [1340/2467]	Loss: 0.241501 
Train Epoch: [1340/2467]	Loss: 0.117948
     Train Epoch: [1340/2467]	Loss: 0.189022
         Train Epoch: [1360/2467]	Loss: 0.033260
 Train Epoch: [1360/2467]	Loss: 0.084103
         Train Epoch: [1360/2467]	Loss: 0.042874 
Train Epoch: [1360/2467]	Loss: 0.087560
     Train Epoch: [1380/2467]	Loss: 0.206153
     Train Epoch: [1380/2467]	Loss: 0.248854    
 Train Epoch: [1380/2467]	Loss: 0.183260
     Train Epoch: [1380/2467]	Loss: 0.206526
         Train Epoch: [1400/2467]	Loss: 0.066966 
    Train Epoch: [1400/2467]	Loss: 0.100995
     Train Epoch: [1400/2467]	Loss: 0.141354
 Train Epoch: [1400/2467]	Loss: 0.155453
         Train Epoch: [1420/2467]	Loss: 0.262303
 Train Epoch: [1420/2467]	Loss: 0.064896
         Train Epoch: [1420/2467]	Loss: 0.138069
 Train Epoch: [1420/2467]	Loss: 0.099249
         Train Epoch: [1440/2467]	Loss: 0.043172    
 Train Epoch: [1440/2467]	Loss: 0.068359 
Train Epoch: [1440/2467]	Loss: 0.038824
     Train Epoch: [1440/2467]	Loss: 0.015149
     Train Epoch: [1460/2467]	Loss: 0.033675
     Train Epoch: [1460/2467]	Loss: 0.077952
          Train Epoch: [1460/2467]	Loss: 0.135409Train Epoch: [1460/2467]	Loss: 0.086224

         Train Epoch: [1480/2467]	Loss: 0.012102     
Train Epoch: [1480/2467]	Loss: 0.023798
 Train Epoch: [1480/2467]	Loss: 0.187203
     Train Epoch: [1480/2467]	Loss: 0.163763
         Train Epoch: [1500/2467]	Loss: 0.111772
 Train Epoch: [1500/2467]	Loss: 0.010287    
 Train Epoch: [1500/2467]	Loss: 0.071698
     Train Epoch: [1500/2467]	Loss: 0.131578
         Train Epoch: [1520/2467]	Loss: 0.081571
     Train Epoch: [1520/2467]	Loss: 0.198925    
 Train Epoch: [1520/2467]	Loss: 0.223063
 Train Epoch: [1520/2467]	Loss: 0.127905
     Train Epoch: [1540/2467]	Loss: 0.056428
         Train Epoch: [1540/2467]	Loss: 0.243807
     Train Epoch: [1540/2467]	Loss: 0.021221
 Train Epoch: [1540/2467]	Loss: 0.079259
         Train Epoch: [1560/2467]	Loss: 0.274960
     Train Epoch: [1560/2467]	Loss: 0.394284
 Train Epoch: [1560/2467]	Loss: 0.213628
     Train Epoch: [1560/2467]	Loss: 0.052305
     Train Epoch: [1580/2467]	Loss: 0.074974
          Train Epoch: [1580/2467]	Loss: 0.180459
Train Epoch: [1580/2467]	Loss: 0.212258
     Train Epoch: [1580/2467]	Loss: 0.080691
     Train Epoch: [1600/2467]	Loss: 0.021308
         Train Epoch: [1600/2467]	Loss: 0.202867
 Train Epoch: [1600/2467]	Loss: 0.177088    
 Train Epoch: [1600/2467]	Loss: 0.051977
     Train Epoch: [1620/2467]	Loss: 0.024402
               Train Epoch: [1620/2467]	Loss: 0.209867Train Epoch: [1620/2467]	Loss: 0.086815
Train Epoch: [1620/2467]	Loss: 0.144232

         Train Epoch: [1640/2467]	Loss: 0.144385
 Train Epoch: [1640/2467]	Loss: 0.100061
         Train Epoch: [1640/2467]	Loss: 0.197083
 Train Epoch: [1640/2467]	Loss: 0.046761
     Train Epoch: [1660/2467]	Loss: 0.124870
          Train Epoch: [1660/2467]	Loss: 0.040892Train Epoch: [1660/2467]	Loss: 0.149255

     Train Epoch: [1660/2467]	Loss: 0.144005
     Train Epoch: [1680/2467]	Loss: 0.105848
             Train Epoch: [1680/2467]	Loss: 0.034871
  Train Epoch: [1680/2467]	Loss: 0.142261Train Epoch: [1680/2467]	Loss: 0.117513

     Train Epoch: [1700/2467]	Loss: 0.076184
         Train Epoch: [1700/2467]	Loss: 0.236581    
 Train Epoch: [1700/2467]	Loss: 0.048959
 Train Epoch: [1700/2467]	Loss: 0.111309
     Train Epoch: [1720/2467]	Loss: 0.080035
         Train Epoch: [1720/2467]	Loss: 0.203420    
 Train Epoch: [1720/2467]	Loss: 0.125155
 Train Epoch: [1720/2467]	Loss: 0.037623
         Train Epoch: [1740/2467]	Loss: 0.026254
     Train Epoch: [1740/2467]	Loss: 0.235459
 Train Epoch: [1740/2467]	Loss: 0.015934
     Train Epoch: [1740/2467]	Loss: 0.171043
         Train Epoch: [1760/2467]	Loss: 0.096195
 Train Epoch: [1760/2467]	Loss: 0.050235
     Train Epoch: [1760/2467]	Loss: 0.057523
     Train Epoch: [1760/2467]	Loss: 0.091954
             Train Epoch: [1780/2467]	Loss: 0.145757
  Train Epoch: [1780/2467]	Loss: 0.204016Train Epoch: [1780/2467]	Loss: 0.100231

     Train Epoch: [1780/2467]	Loss: 0.065497
          Train Epoch: [1800/2467]	Loss: 0.226149
Train Epoch: [1800/2467]	Loss: 0.062090
         Train Epoch: [1800/2467]	Loss: 0.108172
 Train Epoch: [1800/2467]	Loss: 0.233637
          Train Epoch: [1820/2467]	Loss: 0.125145Train Epoch: [1820/2467]	Loss: 0.233534

     Train Epoch: [1820/2467]	Loss: 0.101675
     Train Epoch: [1820/2467]	Loss: 0.094901
     Train Epoch: [1840/2467]	Loss: 0.134837
     Train Epoch: [1840/2467]	Loss: 0.039832
     Train Epoch: [1840/2467]	Loss: 0.098638
     Train Epoch: [1840/2467]	Loss: 0.231618
          Train Epoch: [1860/2467]	Loss: 0.099881Train Epoch: [1860/2467]	Loss: 0.093848

     Train Epoch: [1860/2467]	Loss: 0.095530
     Train Epoch: [1860/2467]	Loss: 0.141607
     Train Epoch: [1880/2467]	Loss: 0.072423
         Train Epoch: [1880/2467]	Loss: 0.051499
 Train Epoch: [1880/2467]	Loss: 0.042359
     Train Epoch: [1880/2467]	Loss: 0.019439
     Train Epoch: [1900/2467]	Loss: 0.083891
         Train Epoch: [1900/2467]	Loss: 0.104359
 Train Epoch: [1900/2467]	Loss: 0.234094
     Train Epoch: [1900/2467]	Loss: 0.236322
     Train Epoch: [1920/2467]	Loss: 0.014892
         Train Epoch: [1920/2467]	Loss: 0.090838
 Train Epoch: [1920/2467]	Loss: 0.141009
     Train Epoch: [1920/2467]	Loss: 0.158571
         Train Epoch: [1940/2467]	Loss: 0.054494    
 Train Epoch: [1940/2467]	Loss: 0.109427
 Train Epoch: [1940/2467]	Loss: 0.102552
     Train Epoch: [1940/2467]	Loss: 0.280054
     Train Epoch: [1960/2467]	Loss: 0.095643
     Train Epoch: [1960/2467]	Loss: 0.084195
     Train Epoch: [1960/2467]	Loss: 0.063360
     Train Epoch: [1960/2467]	Loss: 0.133013
     Train Epoch: [1980/2467]	Loss: 0.083230
          Train Epoch: [1980/2467]	Loss: 0.153502Train Epoch: [1980/2467]	Loss: 0.168298

     Train Epoch: [1980/2467]	Loss: 0.193268
         Train Epoch: [2000/2467]	Loss: 0.122181
 Train Epoch: [2000/2467]	Loss: 0.015030
     Train Epoch: [2000/2467]	Loss: 0.156452
     Train Epoch: [2000/2467]	Loss: 0.044573
     Train Epoch: [2020/2467]	Loss: 0.128622
         Train Epoch: [2020/2467]	Loss: 0.117903
 Train Epoch: [2020/2467]	Loss: 0.109323
     Train Epoch: [2020/2467]	Loss: 0.044461
          Train Epoch: [2040/2467]	Loss: 0.060184Train Epoch: [2040/2467]	Loss: 0.321270

     Train Epoch: [2040/2467]	Loss: 0.036829
     Train Epoch: [2040/2467]	Loss: 0.121352
         Train Epoch: [2060/2467]	Loss: 0.187741
     Train Epoch: [2060/2467]	Loss: 0.090939
 Train Epoch: [2060/2467]	Loss: 0.051500
     Train Epoch: [2060/2467]	Loss: 0.144128
     Train Epoch: [2080/2467]	Loss: 0.031776
     Train Epoch: [2080/2467]	Loss: 0.190908
         Train Epoch: [2080/2467]	Loss: 0.020573
 Train Epoch: [2080/2467]	Loss: 0.049845
         Train Epoch: [2100/2467]	Loss: 0.365166
 Train Epoch: [2100/2467]	Loss: 0.071478
          Train Epoch: [2100/2467]	Loss: 0.069979
Train Epoch: [2100/2467]	Loss: 0.032190
              Train Epoch: [2120/2467]	Loss: 0.085493
Train Epoch: [2120/2467]	Loss: 0.033263
 Train Epoch: [2120/2467]	Loss: 0.071519
     Train Epoch: [2120/2467]	Loss: 0.161946
         Train Epoch: [2140/2467]	Loss: 0.390640
     Train Epoch: [2140/2467]	Loss: 0.253129 
Train Epoch: [2140/2467]	Loss: 0.144359
     Train Epoch: [2140/2467]	Loss: 0.226896
     Train Epoch: [2160/2467]	Loss: 0.037382
         Train Epoch: [2160/2467]	Loss: 0.290021
 Train Epoch: [2160/2467]	Loss: 0.084797
     Train Epoch: [2160/2467]	Loss: 0.043520
     Train Epoch: [2180/2467]	Loss: 0.217263
         Train Epoch: [2180/2467]	Loss: 0.131330
 Train Epoch: [2180/2467]	Loss: 0.094900
     Train Epoch: [2180/2467]	Loss: 0.052849
         Train Epoch: [2200/2467]	Loss: 0.032878 
Train Epoch: [2200/2467]	Loss: 0.147575
     Train Epoch: [2200/2467]	Loss: 0.080275
     Train Epoch: [2200/2467]	Loss: 0.085185
     Train Epoch: [2220/2467]	Loss: 0.055813
     Train Epoch: [2220/2467]	Loss: 0.091577
         Train Epoch: [2220/2467]	Loss: 0.076443 
Train Epoch: [2220/2467]	Loss: 0.093768
     Train Epoch: [2240/2467]	Loss: 0.088463
         Train Epoch: [2240/2467]	Loss: 0.036012
 Train Epoch: [2240/2467]	Loss: 0.020609
     Train Epoch: [2240/2467]	Loss: 0.060803
     Train Epoch: [2260/2467]	Loss: 0.142831
          Train Epoch: [2260/2467]	Loss: 0.184596Train Epoch: [2260/2467]	Loss: 0.026096

     Train Epoch: [2260/2467]	Loss: 0.222067
     Train Epoch: [2280/2467]	Loss: 0.121539
         Train Epoch: [2280/2467]	Loss: 0.059227 
Train Epoch: [2280/2467]	Loss: 0.080894
     Train Epoch: [2280/2467]	Loss: 0.077251
              Train Epoch: [2300/2467]	Loss: 0.051223Train Epoch: [2300/2467]	Loss: 0.073138

 Train Epoch: [2300/2467]	Loss: 0.238391
     Train Epoch: [2300/2467]	Loss: 0.042617
     Train Epoch: [2320/2467]	Loss: 0.091331
         Train Epoch: [2320/2467]	Loss: 0.014247
 Train Epoch: [2320/2467]	Loss: 0.111915
     Train Epoch: [2320/2467]	Loss: 0.113081
          Train Epoch: [2340/2467]	Loss: 0.122486Train Epoch: [2340/2467]	Loss: 0.167303

     Train Epoch: [2340/2467]	Loss: 0.428945
     Train Epoch: [2340/2467]	Loss: 0.080102
     Train Epoch: [2360/2467]	Loss: 0.124101
              Train Epoch: [2360/2467]	Loss: 0.040563Train Epoch: [2360/2467]	Loss: 0.106065

 Train Epoch: [2360/2467]	Loss: 0.301362
     Train Epoch: [2380/2467]	Loss: 0.049931
         Train Epoch: [2380/2467]	Loss: 0.186717
 Train Epoch: [2380/2467]	Loss: 0.291026
     Train Epoch: [2380/2467]	Loss: 0.102577
     Train Epoch: [2400/2467]	Loss: 0.103512
     Train Epoch: [2400/2467]	Loss: 0.090061
     Train Epoch: [2400/2467]	Loss: 0.083832
     Train Epoch: [2400/2467]	Loss: 0.129681
         Train Epoch: [2420/2467]	Loss: 0.017036
 Train Epoch: [2420/2467]	Loss: 0.059073
          Train Epoch: [2420/2467]	Loss: 0.178419Train Epoch: [2420/2467]	Loss: 0.204378

         Train Epoch: [2440/2467]	Loss: 0.011711
 Train Epoch: [2440/2467]	Loss: 0.200827
          Train Epoch: [2440/2467]	Loss: 0.089706Train Epoch: [2440/2467]	Loss: 0.112813

     Train Epoch: [2460/2467]	Loss: 0.071598
         Train Epoch: [2460/2467]	Loss: 0.080689 
Train Epoch: [2460/2467]	Loss: 0.190363
     Train Epoch: [2460/2467]	Loss: 0.121356
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 42 epoch =====
     2025-05-11.15-20-27
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 42 epoch =====
     2025-05-11.15-20-27
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 42 epoch =====
     2025-05-11.15-20-28
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 42 epoch =====
     2025-05-11.15-20-28
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.056073
             Train Epoch: [0/2467]	Loss: 0.234463
 Train Epoch: [0/2467]	Loss: 0.162919
 Train Epoch: [0/2467]	Loss: 0.191550
     Train Epoch: [20/2467]	Loss: 0.266715
         Train Epoch: [20/2467]	Loss: 0.040244
 Train Epoch: [20/2467]	Loss: 0.103739
     Train Epoch: [20/2467]	Loss: 0.113142
             Train Epoch: [40/2467]	Loss: 0.293548
 Train Epoch: [40/2467]	Loss: 0.088936     
Train Epoch: [40/2467]	Loss: 0.100478
 Train Epoch: [40/2467]	Loss: 0.071271
     Train Epoch: [60/2467]	Loss: 0.019980
     Train Epoch: [60/2467]	Loss: 0.106338
     Train Epoch: [60/2467]	Loss: 0.142766
     Train Epoch: [60/2467]	Loss: 0.070936
     Train Epoch: [80/2467]	Loss: 0.100369        
  Train Epoch: [80/2467]	Loss: 0.090257Train Epoch: [80/2467]	Loss: 0.257595

     Train Epoch: [80/2467]	Loss: 0.048502
             Train Epoch: [100/2467]	Loss: 0.080375
  Train Epoch: [100/2467]	Loss: 0.053042Train Epoch: [100/2467]	Loss: 0.146693

     Train Epoch: [100/2467]	Loss: 0.111152
     Train Epoch: [120/2467]	Loss: 0.148110
             Train Epoch: [120/2467]	Loss: 0.070308
  Train Epoch: [120/2467]	Loss: 0.037072Train Epoch: [120/2467]	Loss: 0.300705

     Train Epoch: [140/2467]	Loss: 0.085417
     Train Epoch: [140/2467]	Loss: 0.145654
         Train Epoch: [140/2467]	Loss: 0.046747
 Train Epoch: [140/2467]	Loss: 0.021431
     Train Epoch: [160/2467]	Loss: 0.068080    
         Train Epoch: [160/2467]	Loss: 0.105838 
Train Epoch: [160/2467]	Loss: 0.167385
 Train Epoch: [160/2467]	Loss: 0.092076
          Train Epoch: [180/2467]	Loss: 0.052647Train Epoch: [180/2467]	Loss: 0.144447
    
 Train Epoch: [180/2467]	Loss: 0.075226
     Train Epoch: [180/2467]	Loss: 0.092875
     Train Epoch: [200/2467]	Loss: 0.040788
          Train Epoch: [200/2467]	Loss: 0.069000Train Epoch: [200/2467]	Loss: 0.041154

     Train Epoch: [200/2467]	Loss: 0.101589
         Train Epoch: [220/2467]	Loss: 0.170025
 Train Epoch: [220/2467]	Loss: 0.040601    
     Train Epoch: [220/2467]	Loss: 0.077762
 Train Epoch: [220/2467]	Loss: 0.187127
              Train Epoch: [240/2467]	Loss: 0.016372Train Epoch: [240/2467]	Loss: 0.213975

 Train Epoch: [240/2467]	Loss: 0.139106
     Train Epoch: [240/2467]	Loss: 0.026981
          Train Epoch: [260/2467]	Loss: 0.024915Train Epoch: [260/2467]	Loss: 0.059449
    
 Train Epoch: [260/2467]	Loss: 0.017338
     Train Epoch: [260/2467]	Loss: 0.046609
     Train Epoch: [280/2467]	Loss: 0.113187
         Train Epoch: [280/2467]	Loss: 0.041277 
Train Epoch: [280/2467]	Loss: 0.088916    
 Train Epoch: [280/2467]	Loss: 0.089254
     Train Epoch: [300/2467]	Loss: 0.032928
             Train Epoch: [300/2467]	Loss: 0.142978
 Train Epoch: [300/2467]	Loss: 0.287707
 Train Epoch: [300/2467]	Loss: 0.068267
     Train Epoch: [320/2467]	Loss: 0.171269
              Train Epoch: [320/2467]	Loss: 0.144067Train Epoch: [320/2467]	Loss: 0.297701

 Train Epoch: [320/2467]	Loss: 0.137886
     Train Epoch: [340/2467]	Loss: 0.044148    
         Train Epoch: [340/2467]	Loss: 0.010939 
 Train Epoch: [340/2467]	Loss: 0.131014Train Epoch: [340/2467]	Loss: 0.056265

             Train Epoch: [360/2467]	Loss: 0.042005    
  Train Epoch: [360/2467]	Loss: 0.037661 
Train Epoch: [360/2467]	Loss: 0.127704Train Epoch: [360/2467]	Loss: 0.185519

         Train Epoch: [380/2467]	Loss: 0.047786
 Train Epoch: [380/2467]	Loss: 0.159605
     Train Epoch: [380/2467]	Loss: 0.083425
     Train Epoch: [380/2467]	Loss: 0.252772
         Train Epoch: [400/2467]	Loss: 0.046700
 Train Epoch: [400/2467]	Loss: 0.138115
         Train Epoch: [400/2467]	Loss: 0.027768 
Train Epoch: [400/2467]	Loss: 0.084091
         Train Epoch: [420/2467]	Loss: 0.081858
     Train Epoch: [420/2467]	Loss: 0.060259
 Train Epoch: [420/2467]	Loss: 0.254018
     Train Epoch: [420/2467]	Loss: 0.196302
     Train Epoch: [440/2467]	Loss: 0.261716
     Train Epoch: [440/2467]	Loss: 0.062775
     Train Epoch: [440/2467]	Loss: 0.038881
     Train Epoch: [440/2467]	Loss: 0.109992
         Train Epoch: [460/2467]	Loss: 0.185032
 Train Epoch: [460/2467]	Loss: 0.137694
         Train Epoch: [460/2467]	Loss: 0.092450 
Train Epoch: [460/2467]	Loss: 0.069297
         Train Epoch: [480/2467]	Loss: 0.028646
     Train Epoch: [480/2467]	Loss: 0.111358
     Train Epoch: [480/2467]	Loss: 0.297481
 Train Epoch: [480/2467]	Loss: 0.231900
     Train Epoch: [500/2467]	Loss: 0.086809    
 Train Epoch: [500/2467]	Loss: 0.055476
         Train Epoch: [500/2467]	Loss: 0.132649
 Train Epoch: [500/2467]	Loss: 0.038870
     Train Epoch: [520/2467]	Loss: 0.074406
         Train Epoch: [520/2467]	Loss: 0.228654    
 Train Epoch: [520/2467]	Loss: 0.063568
 Train Epoch: [520/2467]	Loss: 0.078403
               Train Epoch: [540/2467]	Loss: 0.048856Train Epoch: [540/2467]	Loss: 0.064814
Train Epoch: [540/2467]	Loss: 0.179351

     Train Epoch: [540/2467]	Loss: 0.123799
         Train Epoch: [560/2467]	Loss: 0.124091 
Train Epoch: [560/2467]	Loss: 0.102407    
 Train Epoch: [560/2467]	Loss: 0.101263
     Train Epoch: [560/2467]	Loss: 0.115586
          Train Epoch: [580/2467]	Loss: 0.100293Train Epoch: [580/2467]	Loss: 0.046188

     Train Epoch: [580/2467]	Loss: 0.102711
     Train Epoch: [580/2467]	Loss: 0.220341
         Train Epoch: [600/2467]	Loss: 0.177227
 Train Epoch: [600/2467]	Loss: 0.103057
         Train Epoch: [600/2467]	Loss: 0.114118
 Train Epoch: [600/2467]	Loss: 0.082910
     Train Epoch: [620/2467]	Loss: 0.023045
         Train Epoch: [620/2467]	Loss: 0.125449 
Train Epoch: [620/2467]	Loss: 0.127123
     Train Epoch: [620/2467]	Loss: 0.082592
     Train Epoch: [640/2467]	Loss: 0.072005
              Train Epoch: [640/2467]	Loss: 0.151343Train Epoch: [640/2467]	Loss: 0.036506

 Train Epoch: [640/2467]	Loss: 0.085255
         Train Epoch: [660/2467]	Loss: 0.023391
 Train Epoch: [660/2467]	Loss: 0.046653
     Train Epoch: [660/2467]	Loss: 0.101044
     Train Epoch: [660/2467]	Loss: 0.365421
     Train Epoch: [680/2467]	Loss: 0.093000    
 Train Epoch: [680/2467]	Loss: 0.032164
     Train Epoch: [680/2467]	Loss: 0.024821
     Train Epoch: [680/2467]	Loss: 0.086207
     Train Epoch: [700/2467]	Loss: 0.029124    
 Train Epoch: [700/2467]	Loss: 0.257959
     Train Epoch: [700/2467]	Loss: 0.255542
     Train Epoch: [700/2467]	Loss: 0.056895
     Train Epoch: [720/2467]	Loss: 0.145755
         Train Epoch: [720/2467]	Loss: 0.059005
 Train Epoch: [720/2467]	Loss: 0.077140
     Train Epoch: [720/2467]	Loss: 0.008502
     Train Epoch: [740/2467]	Loss: 0.041959
         Train Epoch: [740/2467]	Loss: 0.258685
 Train Epoch: [740/2467]	Loss: 0.064209
     Train Epoch: [740/2467]	Loss: 0.060684
     Train Epoch: [760/2467]	Loss: 0.039102
         Train Epoch: [760/2467]	Loss: 0.215050
     Train Epoch: [760/2467]	Loss: 0.313921
 Train Epoch: [760/2467]	Loss: 0.204008
     Train Epoch: [780/2467]	Loss: 0.271012
     Train Epoch: [780/2467]	Loss: 0.092156    
 Train Epoch: [780/2467]	Loss: 0.241007
     Train Epoch: [780/2467]	Loss: 0.260760
     Train Epoch: [800/2467]	Loss: 0.137365
         Train Epoch: [800/2467]	Loss: 0.148785
     Train Epoch: [800/2467]	Loss: 0.080895
 Train Epoch: [800/2467]	Loss: 0.059093
     Train Epoch: [820/2467]	Loss: 0.342530
         Train Epoch: [820/2467]	Loss: 0.151318
 Train Epoch: [820/2467]	Loss: 0.044198
     Train Epoch: [820/2467]	Loss: 0.046190
     Train Epoch: [840/2467]	Loss: 0.300862
     Train Epoch: [840/2467]	Loss: 0.085598
         Train Epoch: [840/2467]	Loss: 0.034782 
Train Epoch: [840/2467]	Loss: 0.063587
         Train Epoch: [860/2467]	Loss: 0.078007
     Train Epoch: [860/2467]	Loss: 0.040160    
 Train Epoch: [860/2467]	Loss: 0.027633 
Train Epoch: [860/2467]	Loss: 0.205441
         Train Epoch: [880/2467]	Loss: 0.121440
 Train Epoch: [880/2467]	Loss: 0.110332
     Train Epoch: [880/2467]	Loss: 0.119032
     Train Epoch: [880/2467]	Loss: 0.141867
     Train Epoch: [900/2467]	Loss: 0.124197
          Train Epoch: [900/2467]	Loss: 0.032668Train Epoch: [900/2467]	Loss: 0.129499

     Train Epoch: [900/2467]	Loss: 0.149334
     Train Epoch: [920/2467]	Loss: 0.199639
             Train Epoch: [920/2467]	Loss: 0.030670
  Train Epoch: [920/2467]	Loss: 0.201557Train Epoch: [920/2467]	Loss: 0.209948

          Train Epoch: [940/2467]	Loss: 0.037284Train Epoch: [940/2467]	Loss: 0.028414

     Train Epoch: [940/2467]	Loss: 0.137089
     Train Epoch: [940/2467]	Loss: 0.076367
         Train Epoch: [960/2467]	Loss: 0.217491
 Train Epoch: [960/2467]	Loss: 0.056417
     Train Epoch: [960/2467]	Loss: 0.055678
     Train Epoch: [960/2467]	Loss: 0.054046
     Train Epoch: [980/2467]	Loss: 0.168386
     Train Epoch: [980/2467]	Loss: 0.049066
     Train Epoch: [980/2467]	Loss: 0.089968
     Train Epoch: [980/2467]	Loss: 0.062712
          Train Epoch: [1000/2467]	Loss: 0.026481
Train Epoch: [1000/2467]	Loss: 0.019971
          Train Epoch: [1000/2467]	Loss: 0.166979Train Epoch: [1000/2467]	Loss: 0.164534

     Train Epoch: [1020/2467]	Loss: 0.282681
     Train Epoch: [1020/2467]	Loss: 0.014120
     Train Epoch: [1020/2467]	Loss: 0.073317
     Train Epoch: [1020/2467]	Loss: 0.110333
     Train Epoch: [1040/2467]	Loss: 0.094194
     Train Epoch: [1040/2467]	Loss: 0.121464
     Train Epoch: [1040/2467]	Loss: 0.073080
     Train Epoch: [1040/2467]	Loss: 0.228366
         Train Epoch: [1060/2467]	Loss: 0.117561
 Train Epoch: [1060/2467]	Loss: 0.079052
     Train Epoch: [1060/2467]	Loss: 0.057754
     Train Epoch: [1060/2467]	Loss: 0.341986
         Train Epoch: [1080/2467]	Loss: 0.092732
 Train Epoch: [1080/2467]	Loss: 0.161163
     Train Epoch: [1080/2467]	Loss: 0.188331
     Train Epoch: [1080/2467]	Loss: 0.125198
         Train Epoch: [1100/2467]	Loss: 0.245908    
 Train Epoch: [1100/2467]	Loss: 0.143858 
Train Epoch: [1100/2467]	Loss: 0.148370
     Train Epoch: [1100/2467]	Loss: 0.187134
     Train Epoch: [1120/2467]	Loss: 0.187811
         Train Epoch: [1120/2467]	Loss: 0.088995
     Train Epoch: [1120/2467]	Loss: 0.075829
 Train Epoch: [1120/2467]	Loss: 0.114053
             Train Epoch: [1140/2467]	Loss: 0.300463
 Train Epoch: [1140/2467]	Loss: 0.062039
 Train Epoch: [1140/2467]	Loss: 0.226634
     Train Epoch: [1140/2467]	Loss: 0.034193
          Train Epoch: [1160/2467]	Loss: 0.089353
Train Epoch: [1160/2467]	Loss: 0.201857
     Train Epoch: [1160/2467]	Loss: 0.055448
     Train Epoch: [1160/2467]	Loss: 0.055303
     Train Epoch: [1180/2467]	Loss: 0.035117
     Train Epoch: [1180/2467]	Loss: 0.096309
          Train Epoch: [1180/2467]	Loss: 0.076732
Train Epoch: [1180/2467]	Loss: 0.056670
             Train Epoch: [1200/2467]	Loss: 0.245404
 Train Epoch: [1200/2467]	Loss: 0.212096 
Train Epoch: [1200/2467]	Loss: 0.080961
     Train Epoch: [1200/2467]	Loss: 0.045482
         Train Epoch: [1220/2467]	Loss: 0.075172
     Train Epoch: [1220/2467]	Loss: 0.177367
 Train Epoch: [1220/2467]	Loss: 0.218749
     Train Epoch: [1220/2467]	Loss: 0.121766
     Train Epoch: [1240/2467]	Loss: 0.234364
         Train Epoch: [1240/2467]	Loss: 0.215552
 Train Epoch: [1240/2467]	Loss: 0.109866
     Train Epoch: [1240/2467]	Loss: 0.097641
     Train Epoch: [1260/2467]	Loss: 0.040818
         Train Epoch: [1260/2467]	Loss: 0.038335
     Train Epoch: [1260/2467]	Loss: 0.084798
 Train Epoch: [1260/2467]	Loss: 0.342055
     Train Epoch: [1280/2467]	Loss: 0.088205
     Train Epoch: [1280/2467]	Loss: 0.144380
         Train Epoch: [1280/2467]	Loss: 0.056666
 Train Epoch: [1280/2467]	Loss: 0.377017
         Train Epoch: [1300/2467]	Loss: 0.060479
 Train Epoch: [1300/2467]	Loss: 0.276958
         Train Epoch: [1300/2467]	Loss: 0.126898 
Train Epoch: [1300/2467]	Loss: 0.030553
              Train Epoch: [1320/2467]	Loss: 0.119962Train Epoch: [1320/2467]	Loss: 0.115897

 Train Epoch: [1320/2467]	Loss: 0.085284
     Train Epoch: [1320/2467]	Loss: 0.035762
          Train Epoch: [1340/2467]	Loss: 0.113939
Train Epoch: [1340/2467]	Loss: 0.165634
     Train Epoch: [1340/2467]	Loss: 0.111299
     Train Epoch: [1340/2467]	Loss: 0.269459
         Train Epoch: [1360/2467]	Loss: 0.046919
 Train Epoch: [1360/2467]	Loss: 0.079271
          Train Epoch: [1360/2467]	Loss: 0.028453Train Epoch: [1360/2467]	Loss: 0.123753

              Train Epoch: [1380/2467]	Loss: 0.244046Train Epoch: [1380/2467]	Loss: 0.222928

 Train Epoch: [1380/2467]	Loss: 0.179535
     Train Epoch: [1380/2467]	Loss: 0.187635
     Train Epoch: [1400/2467]	Loss: 0.064072
          Train Epoch: [1400/2467]	Loss: 0.118179Train Epoch: [1400/2467]	Loss: 0.062540

     Train Epoch: [1400/2467]	Loss: 0.118948
     Train Epoch: [1420/2467]	Loss: 0.265193
     Train Epoch: [1420/2467]	Loss: 0.068965    
 Train Epoch: [1420/2467]	Loss: 0.132104
     Train Epoch: [1420/2467]	Loss: 0.097102
     Train Epoch: [1440/2467]	Loss: 0.012169
             Train Epoch: [1440/2467]	Loss: 0.034548
 Train Epoch: [1440/2467]	Loss: 0.038404
 Train Epoch: [1440/2467]	Loss: 0.064533
     Train Epoch: [1460/2467]	Loss: 0.035694
     Train Epoch: [1460/2467]	Loss: 0.100783    
 Train Epoch: [1460/2467]	Loss: 0.114350
     Train Epoch: [1460/2467]	Loss: 0.078922
     Train Epoch: [1480/2467]	Loss: 0.016201
         Train Epoch: [1480/2467]	Loss: 0.013351
 Train Epoch: [1480/2467]	Loss: 0.156538
     Train Epoch: [1480/2467]	Loss: 0.201833
     Train Epoch: [1500/2467]	Loss: 0.063753
          Train Epoch: [1500/2467]	Loss: 0.058624
Train Epoch: [1500/2467]	Loss: 0.012855
     Train Epoch: [1500/2467]	Loss: 0.128373
     Train Epoch: [1520/2467]	Loss: 0.143084
     Train Epoch: [1520/2467]	Loss: 0.069351
          Train Epoch: [1520/2467]	Loss: 0.129455Train Epoch: [1520/2467]	Loss: 0.194925

     Train Epoch: [1540/2467]	Loss: 0.230175
         Train Epoch: [1540/2467]	Loss: 0.017563 
Train Epoch: [1540/2467]	Loss: 0.084898
     Train Epoch: [1540/2467]	Loss: 0.066166
     Train Epoch: [1560/2467]	Loss: 0.057449
     Train Epoch: [1560/2467]	Loss: 0.249847
         Train Epoch: [1560/2467]	Loss: 0.352979
 Train Epoch: [1560/2467]	Loss: 0.205438
     Train Epoch: [1580/2467]	Loss: 0.060766
               Train Epoch: [1580/2467]	Loss: 0.182276Train Epoch: [1580/2467]	Loss: 0.076924Train Epoch: [1580/2467]	Loss: 0.202542


     Train Epoch: [1600/2467]	Loss: 0.016147
     Train Epoch: [1600/2467]	Loss: 0.166429
         Train Epoch: [1600/2467]	Loss: 0.048223
 Train Epoch: [1600/2467]	Loss: 0.202964
     Train Epoch: [1620/2467]	Loss: 0.028231
         Train Epoch: [1620/2467]	Loss: 0.147758
     Train Epoch: [1620/2467]	Loss: 0.079708
 Train Epoch: [1620/2467]	Loss: 0.213585
     Train Epoch: [1640/2467]	Loss: 0.100722
         Train Epoch: [1640/2467]	Loss: 0.179353
 Train Epoch: [1640/2467]	Loss: 0.049376
     Train Epoch: [1640/2467]	Loss: 0.142969
     Train Epoch: [1660/2467]	Loss: 0.111666
     Train Epoch: [1660/2467]	Loss: 0.150718
          Train Epoch: [1660/2467]	Loss: 0.037292Train Epoch: [1660/2467]	Loss: 0.126560

         Train Epoch: [1680/2467]	Loss: 0.078176 
Train Epoch: [1680/2467]	Loss: 0.037685
     Train Epoch: [1680/2467]	Loss: 0.132048
     Train Epoch: [1680/2467]	Loss: 0.089396
     Train Epoch: [1700/2467]	Loss: 0.058846
         Train Epoch: [1700/2467]	Loss: 0.048098
 Train Epoch: [1700/2467]	Loss: 0.212839
     Train Epoch: [1700/2467]	Loss: 0.087054
          Train Epoch: [1720/2467]	Loss: 0.094232Train Epoch: [1720/2467]	Loss: 0.165310

          Train Epoch: [1720/2467]	Loss: 0.114651Train Epoch: [1720/2467]	Loss: 0.033881

     Train Epoch: [1740/2467]	Loss: 0.268861
         Train Epoch: [1740/2467]	Loss: 0.017149
 Train Epoch: [1740/2467]	Loss: 0.184651
     Train Epoch: [1740/2467]	Loss: 0.030039
         Train Epoch: [1760/2467]	Loss: 0.084982    
  Train Epoch: [1760/2467]	Loss: 0.096640
Train Epoch: [1760/2467]	Loss: 0.045755
     Train Epoch: [1760/2467]	Loss: 0.049581
             Train Epoch: [1780/2467]	Loss: 0.109426
  Train Epoch: [1780/2467]	Loss: 0.153364
Train Epoch: [1780/2467]	Loss: 0.168665
     Train Epoch: [1780/2467]	Loss: 0.059884
         Train Epoch: [1800/2467]	Loss: 0.216663
     Train Epoch: [1800/2467]	Loss: 0.105611
 Train Epoch: [1800/2467]	Loss: 0.098929
     Train Epoch: [1800/2467]	Loss: 0.201529
         Train Epoch: [1820/2467]	Loss: 0.134045
 Train Epoch: [1820/2467]	Loss: 0.214926
          Train Epoch: [1820/2467]	Loss: 0.085949
Train Epoch: [1820/2467]	Loss: 0.088190
     Train Epoch: [1840/2467]	Loss: 0.143691
          Train Epoch: [1840/2467]	Loss: 0.231787Train Epoch: [1840/2467]	Loss: 0.038004

     Train Epoch: [1840/2467]	Loss: 0.094640
     Train Epoch: [1860/2467]	Loss: 0.098089
         Train Epoch: [1860/2467]	Loss: 0.106440 
Train Epoch: [1860/2467]	Loss: 0.131745
     Train Epoch: [1860/2467]	Loss: 0.108329
         Train Epoch: [1880/2467]	Loss: 0.019268
 Train Epoch: [1880/2467]	Loss: 0.051361
         Train Epoch: [1880/2467]	Loss: 0.056371
 Train Epoch: [1880/2467]	Loss: 0.042942
         Train Epoch: [1900/2467]	Loss: 0.105310
     Train Epoch: [1900/2467]	Loss: 0.225688
 Train Epoch: [1900/2467]	Loss: 0.211193
     Train Epoch: [1900/2467]	Loss: 0.079839
     Train Epoch: [1920/2467]	Loss: 0.015836
     Train Epoch: [1920/2467]	Loss: 0.148893
     Train Epoch: [1920/2467]	Loss: 0.116289
     Train Epoch: [1920/2467]	Loss: 0.089720
     Train Epoch: [1940/2467]	Loss: 0.057626
         Train Epoch: [1940/2467]	Loss: 0.098278
 Train Epoch: [1940/2467]	Loss: 0.108327
     Train Epoch: [1940/2467]	Loss: 0.276755
     Train Epoch: [1960/2467]	Loss: 0.089838
          Train Epoch: [1960/2467]	Loss: 0.079388Train Epoch: [1960/2467]	Loss: 0.161364

     Train Epoch: [1960/2467]	Loss: 0.113557
     Train Epoch: [1980/2467]	Loss: 0.076250
         Train Epoch: [1980/2467]	Loss: 0.205968
 Train Epoch: [1980/2467]	Loss: 0.152785
     Train Epoch: [1980/2467]	Loss: 0.170444
         Train Epoch: [2000/2467]	Loss: 0.090258
 Train Epoch: [2000/2467]	Loss: 0.021551
          Train Epoch: [2000/2467]	Loss: 0.134178Train Epoch: [2000/2467]	Loss: 0.041997

         Train Epoch: [2020/2467]	Loss: 0.090696
 Train Epoch: [2020/2467]	Loss: 0.114212
     Train Epoch: [2020/2467]	Loss: 0.043472
     Train Epoch: [2020/2467]	Loss: 0.125104
         Train Epoch: [2040/2467]	Loss: 0.035396
 Train Epoch: [2040/2467]	Loss: 0.115854
     Train Epoch: [2040/2467]	Loss: 0.060594    
 Train Epoch: [2040/2467]	Loss: 0.338900
     Train Epoch: [2060/2467]	Loss: 0.119515
              Train Epoch: [2060/2467]	Loss: 0.052609 Train Epoch: [2060/2467]	Loss: 0.176575

Train Epoch: [2060/2467]	Loss: 0.076962
     Train Epoch: [2080/2467]	Loss: 0.028167
             Train Epoch: [2080/2467]	Loss: 0.054132
  Train Epoch: [2080/2467]	Loss: 0.202620Train Epoch: [2080/2467]	Loss: 0.024964

     Train Epoch: [2100/2467]	Loss: 0.352596    
     Train Epoch: [2100/2467]	Loss: 0.075316
     Train Epoch: [2100/2467]	Loss: 0.077974
 Train Epoch: [2100/2467]	Loss: 0.037207
     Train Epoch: [2120/2467]	Loss: 0.056513
     Train Epoch: [2120/2467]	Loss: 0.077793
     Train Epoch: [2120/2467]	Loss: 0.159730
     Train Epoch: [2120/2467]	Loss: 0.033262
         Train Epoch: [2140/2467]	Loss: 0.372409
 Train Epoch: [2140/2467]	Loss: 0.248891
     Train Epoch: [2140/2467]	Loss: 0.252884
     Train Epoch: [2140/2467]	Loss: 0.146987
             Train Epoch: [2160/2467]	Loss: 0.033638 
Train Epoch: [2160/2467]	Loss: 0.265383
     Train Epoch: [2160/2467]	Loss: 0.078218
 Train Epoch: [2160/2467]	Loss: 0.054246
     Train Epoch: [2180/2467]	Loss: 0.049869
         Train Epoch: [2180/2467]	Loss: 0.093584
     Train Epoch: [2180/2467]	Loss: 0.230553
 Train Epoch: [2180/2467]	Loss: 0.132166
         Train Epoch: [2200/2467]	Loss: 0.027943
 Train Epoch: [2200/2467]	Loss: 0.084674
         Train Epoch: [2200/2467]	Loss: 0.071507
 Train Epoch: [2200/2467]	Loss: 0.084574
     Train Epoch: [2220/2467]	Loss: 0.046984
         Train Epoch: [2220/2467]	Loss: 0.041213
 Train Epoch: [2220/2467]	Loss: 0.092375
     Train Epoch: [2220/2467]	Loss: 0.089087
         Train Epoch: [2240/2467]	Loss: 0.060219
     Train Epoch: [2240/2467]	Loss: 0.074604
     Train Epoch: [2240/2467]	Loss: 0.031308
 Train Epoch: [2240/2467]	Loss: 0.019572
     Train Epoch: [2260/2467]	Loss: 0.092429
         Train Epoch: [2260/2467]	Loss: 0.192354
 Train Epoch: [2260/2467]	Loss: 0.022587
     Train Epoch: [2260/2467]	Loss: 0.211787
     Train Epoch: [2280/2467]	Loss: 0.107118
         Train Epoch: [2280/2467]	Loss: 0.053804 
Train Epoch: [2280/2467]	Loss: 0.081840
     Train Epoch: [2280/2467]	Loss: 0.087169
             Train Epoch: [2300/2467]	Loss: 0.050853
      Train Epoch: [2300/2467]	Loss: 0.076773Train Epoch: [2300/2467]	Loss: 0.235463

 Train Epoch: [2300/2467]	Loss: 0.042450
     Train Epoch: [2320/2467]	Loss: 0.076317
     Train Epoch: [2320/2467]	Loss: 0.105839
     Train Epoch: [2320/2467]	Loss: 0.121096
     Train Epoch: [2320/2467]	Loss: 0.019052
     Train Epoch: [2340/2467]	Loss: 0.076709
         Train Epoch: [2340/2467]	Loss: 0.178202
     Train Epoch: [2340/2467]	Loss: 0.126850
 Train Epoch: [2340/2467]	Loss: 0.418786
     Train Epoch: [2360/2467]	Loss: 0.117507
         Train Epoch: [2360/2467]	Loss: 0.107491
 Train Epoch: [2360/2467]	Loss: 0.253553
     Train Epoch: [2360/2467]	Loss: 0.042309
     Train Epoch: [2380/2467]	Loss: 0.203829    
 Train Epoch: [2380/2467]	Loss: 0.295932
     Train Epoch: [2380/2467]	Loss: 0.095113
     Train Epoch: [2380/2467]	Loss: 0.088657
     Train Epoch: [2400/2467]	Loss: 0.087393
     Train Epoch: [2400/2467]	Loss: 0.089973
     Train Epoch: [2400/2467]	Loss: 0.126492
     Train Epoch: [2400/2467]	Loss: 0.112816
     Train Epoch: [2420/2467]	Loss: 0.015893
     Train Epoch: [2420/2467]	Loss: 0.093939
     Train Epoch: [2420/2467]	Loss: 0.160605
     Train Epoch: [2420/2467]	Loss: 0.247476
     Train Epoch: [2440/2467]	Loss: 0.013808
     Train Epoch: [2440/2467]	Loss: 0.170084
         Train Epoch: [2440/2467]	Loss: 0.113095
 Train Epoch: [2440/2467]	Loss: 0.112292
     Train Epoch: [2460/2467]	Loss: 0.112098
     Train Epoch: [2460/2467]	Loss: 0.053550    
     Train Epoch: [2460/2467]	Loss: 0.079781
 Train Epoch: [2460/2467]	Loss: 0.206483
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 43 epoch =====
     2025-05-11.15-42-18
after set grad
after prog
start loop
     ===== running 43 epoch =====
     2025-05-11.15-42-18
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 43 epoch =====
     2025-05-11.15-42-18
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 43 epoch =====
     2025-05-11.15-42-19
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     Train Epoch: [0/2467]	Loss: 0.048950
         Train Epoch: [0/2467]	Loss: 0.224136
 Train Epoch: [0/2467]	Loss: 0.198802
     Train Epoch: [0/2467]	Loss: 0.161312
         Train Epoch: [20/2467]	Loss: 0.248180
 Train Epoch: [20/2467]	Loss: 0.039563
     Train Epoch: [20/2467]	Loss: 0.097578
     Train Epoch: [20/2467]	Loss: 0.105195
     Train Epoch: [40/2467]	Loss: 0.068633
         Train Epoch: [40/2467]	Loss: 0.277875
     Train Epoch: [40/2467]	Loss: 0.086641
 Train Epoch: [40/2467]	Loss: 0.087257
     Train Epoch: [60/2467]	Loss: 0.129594     Train Epoch: [60/2467]	Loss: 0.022882         Train Epoch: [60/2467]	Loss: 0.103487 Train Epoch: [60/2467]	Loss: 0.056163



          Train Epoch: [80/2467]	Loss: 0.082670Train Epoch: [80/2467]	Loss: 0.198990

     Train Epoch: [80/2467]	Loss: 0.082099
     Train Epoch: [80/2467]	Loss: 0.043756
     Train Epoch: [100/2467]	Loss: 0.042093
          Train Epoch: [100/2467]	Loss: 0.062408Train Epoch: [100/2467]	Loss: 0.135432

     Train Epoch: [100/2467]	Loss: 0.146443
         Train Epoch: [120/2467]	Loss: 0.288126
 Train Epoch: [120/2467]	Loss: 0.187923
     Train Epoch: [120/2467]	Loss: 0.075094
     Train Epoch: [120/2467]	Loss: 0.033253
         Train Epoch: [140/2467]	Loss: 0.022972
 Train Epoch: [140/2467]	Loss: 0.070349
     Train Epoch: [140/2467]	Loss: 0.140151
     Train Epoch: [140/2467]	Loss: 0.045639
         Train Epoch: [160/2467]	Loss: 0.094408 
Train Epoch: [160/2467]	Loss: 0.056218    
     Train Epoch: [160/2467]	Loss: 0.103763 
Train Epoch: [160/2467]	Loss: 0.172791
     Train Epoch: [180/2467]	Loss: 0.083489
         Train Epoch: [180/2467]	Loss: 0.147791
 Train Epoch: [180/2467]	Loss: 0.053129
     Train Epoch: [180/2467]	Loss: 0.084926
     Train Epoch: [200/2467]	Loss: 0.057695
     Train Epoch: [200/2467]	Loss: 0.097086
     Train Epoch: [200/2467]	Loss: 0.050234
     Train Epoch: [200/2467]	Loss: 0.035411
     Train Epoch: [220/2467]	Loss: 0.068932
     Train Epoch: [220/2467]	Loss: 0.040859
         Train Epoch: [220/2467]	Loss: 0.184404
 Train Epoch: [220/2467]	Loss: 0.128216
     Train Epoch: [240/2467]	Loss: 0.214520
     Train Epoch: [240/2467]	Loss: 0.029905
     Train Epoch: [240/2467]	Loss: 0.014961
     Train Epoch: [240/2467]	Loss: 0.135631
     Train Epoch: [260/2467]	Loss: 0.047992
         Train Epoch: [260/2467]	Loss: 0.024785
 Train Epoch: [260/2467]	Loss: 0.045219
     Train Epoch: [260/2467]	Loss: 0.018225
     Train Epoch: [280/2467]	Loss: 0.121752
     Train Epoch: [280/2467]	Loss: 0.068221
     Train Epoch: [280/2467]	Loss: 0.058378
     Train Epoch: [280/2467]	Loss: 0.077503
     Train Epoch: [300/2467]	Loss: 0.034154
         Train Epoch: [300/2467]	Loss: 0.278327
 Train Epoch: [300/2467]	Loss: 0.062661
     Train Epoch: [300/2467]	Loss: 0.127430
     Train Epoch: [320/2467]	Loss: 0.122851
         Train Epoch: [320/2467]	Loss: 0.168456
     Train Epoch: [320/2467]	Loss: 0.143618
 Train Epoch: [320/2467]	Loss: 0.341998
             Train Epoch: [340/2467]	Loss: 0.052820
  Train Epoch: [340/2467]	Loss: 0.142065Train Epoch: [340/2467]	Loss: 0.012638

     Train Epoch: [340/2467]	Loss: 0.052336
         Train Epoch: [360/2467]	Loss: 0.038831
     Train Epoch: [360/2467]	Loss: 0.028871
 Train Epoch: [360/2467]	Loss: 0.126164
     Train Epoch: [360/2467]	Loss: 0.189090
     Train Epoch: [380/2467]	Loss: 0.052055
          Train Epoch: [380/2467]	Loss: 0.080811
Train Epoch: [380/2467]	Loss: 0.226704
     Train Epoch: [380/2467]	Loss: 0.175067
     Train Epoch: [400/2467]	Loss: 0.045515
     Train Epoch: [400/2467]	Loss: 0.030951
         Train Epoch: [400/2467]	Loss: 0.155259
 Train Epoch: [400/2467]	Loss: 0.078368
         Train Epoch: [420/2467]	Loss: 0.191161
 Train Epoch: [420/2467]	Loss: 0.102166
     Train Epoch: [420/2467]	Loss: 0.060689
     Train Epoch: [420/2467]	Loss: 0.246900
     Train Epoch: [440/2467]	Loss: 0.112955
     Train Epoch: [440/2467]	Loss: 0.275844
     Train Epoch: [440/2467]	Loss: 0.061788
     Train Epoch: [440/2467]	Loss: 0.036945
         Train Epoch: [460/2467]	Loss: 0.184918
 Train Epoch: [460/2467]	Loss: 0.117698
          Train Epoch: [460/2467]	Loss: 0.091492Train Epoch: [460/2467]	Loss: 0.089494

         Train Epoch: [480/2467]	Loss: 0.020871 
Train Epoch: [480/2467]	Loss: 0.105071
          Train Epoch: [480/2467]	Loss: 0.216722
Train Epoch: [480/2467]	Loss: 0.309450
     Train Epoch: [500/2467]	Loss: 0.042225
          Train Epoch: [500/2467]	Loss: 0.155091Train Epoch: [500/2467]	Loss: 0.090005

     Train Epoch: [500/2467]	Loss: 0.060020
     Train Epoch: [520/2467]	Loss: 0.046372
         Train Epoch: [520/2467]	Loss: 0.212813
     Train Epoch: [520/2467]	Loss: 0.075072
 Train Epoch: [520/2467]	Loss: 0.066982
     Train Epoch: [540/2467]	Loss: 0.133398
     Train Epoch: [540/2467]	Loss: 0.177965
     Train Epoch: [540/2467]	Loss: 0.078028
     Train Epoch: [540/2467]	Loss: 0.050369
     Train Epoch: [560/2467]	Loss: 0.118394    
 Train Epoch: [560/2467]	Loss: 0.149177
         Train Epoch: [560/2467]	Loss: 0.108880
 Train Epoch: [560/2467]	Loss: 0.120775
     Train Epoch: [580/2467]	Loss: 0.199233
         Train Epoch: [580/2467]	Loss: 0.106282
 Train Epoch: [580/2467]	Loss: 0.102164
     Train Epoch: [580/2467]	Loss: 0.048677
     Train Epoch: [600/2467]	Loss: 0.154166
         Train Epoch: [600/2467]	Loss: 0.128775
     Train Epoch: [600/2467]	Loss: 0.099498
 Train Epoch: [600/2467]	Loss: 0.065659
         Train Epoch: [620/2467]	Loss: 0.087811
 Train Epoch: [620/2467]	Loss: 0.020815
         Train Epoch: [620/2467]	Loss: 0.102025 
Train Epoch: [620/2467]	Loss: 0.143327
     Train Epoch: [640/2467]	Loss: 0.055426
     Train Epoch: [640/2467]	Loss: 0.145124
         Train Epoch: [640/2467]	Loss: 0.035560
 Train Epoch: [640/2467]	Loss: 0.070303
     Train Epoch: [660/2467]	Loss: 0.022175    
 Train Epoch: [660/2467]	Loss: 0.033403
     Train Epoch: [660/2467]	Loss: 0.097674    
 Train Epoch: [660/2467]	Loss: 0.285059
     Train Epoch: [680/2467]	Loss: 0.033650    
      Train Epoch: [680/2467]	Loss: 0.060467
Train Epoch: [680/2467]	Loss: 0.022929
     Train Epoch: [680/2467]	Loss: 0.113548
     Train Epoch: [700/2467]	Loss: 0.033968
     Train Epoch: [700/2467]	Loss: 0.249088    
     Train Epoch: [700/2467]	Loss: 0.059163
 Train Epoch: [700/2467]	Loss: 0.230884
         Train Epoch: [720/2467]	Loss: 0.010158
 Train Epoch: [720/2467]	Loss: 0.115342
          Train Epoch: [720/2467]	Loss: 0.051119Train Epoch: [720/2467]	Loss: 0.062237

         Train Epoch: [740/2467]	Loss: 0.037467
 Train Epoch: [740/2467]	Loss: 0.067881
     Train Epoch: [740/2467]	Loss: 0.239764
     Train Epoch: [740/2467]	Loss: 0.059967
     Train Epoch: [760/2467]	Loss: 0.036824
         Train Epoch: [760/2467]	Loss: 0.188493
     Train Epoch: [760/2467]	Loss: 0.240790
 Train Epoch: [760/2467]	Loss: 0.323469
     Train Epoch: [780/2467]	Loss: 0.280446
     Train Epoch: [780/2467]	Loss: 0.306508
     Train Epoch: [780/2467]	Loss: 0.252118
     Train Epoch: [780/2467]	Loss: 0.083809
         Train Epoch: [800/2467]	Loss: 0.134958 
Train Epoch: [800/2467]	Loss: 0.127541    
 Train Epoch: [800/2467]	Loss: 0.091622
     Train Epoch: [800/2467]	Loss: 0.059013
     Train Epoch: [820/2467]	Loss: 0.037798
     Train Epoch: [820/2467]	Loss: 0.316760
         Train Epoch: [820/2467]	Loss: 0.040936
 Train Epoch: [820/2467]	Loss: 0.161834
     Train Epoch: [840/2467]	Loss: 0.296499
     Train Epoch: [840/2467]	Loss: 0.047290
     Train Epoch: [840/2467]	Loss: 0.059147
     Train Epoch: [840/2467]	Loss: 0.075359
         Train Epoch: [860/2467]	Loss: 0.180195
 Train Epoch: [860/2467]	Loss: 0.145090
     Train Epoch: [860/2467]	Loss: 0.030035
     Train Epoch: [860/2467]	Loss: 0.041247
         Train Epoch: [880/2467]	Loss: 0.113568
 Train Epoch: [880/2467]	Loss: 0.108220
         Train Epoch: [880/2467]	Loss: 0.133386
 Train Epoch: [880/2467]	Loss: 0.100123
     Train Epoch: [900/2467]	Loss: 0.148684
         Train Epoch: [900/2467]	Loss: 0.135357
 Train Epoch: [900/2467]	Loss: 0.036358
     Train Epoch: [900/2467]	Loss: 0.126859
         Train Epoch: [920/2467]	Loss: 0.267163
 Train Epoch: [920/2467]	Loss: 0.193792
     Train Epoch: [920/2467]	Loss: 0.030865
     Train Epoch: [920/2467]	Loss: 0.179272
          Train Epoch: [940/2467]	Loss: 0.022214Train Epoch: [940/2467]	Loss: 0.032291

         Train Epoch: [940/2467]	Loss: 0.051083
 Train Epoch: [940/2467]	Loss: 0.133328
     Train Epoch: [960/2467]	Loss: 0.224653
         Train Epoch: [960/2467]	Loss: 0.054130
 Train Epoch: [960/2467]	Loss: 0.048681
     Train Epoch: [960/2467]	Loss: 0.050469
     Train Epoch: [980/2467]	Loss: 0.179707
     Train Epoch: [980/2467]	Loss: 0.038338
         Train Epoch: [980/2467]	Loss: 0.078891
 Train Epoch: [980/2467]	Loss: 0.071818
         Train Epoch: [1000/2467]	Loss: 0.024676
 Train Epoch: [1000/2467]	Loss: 0.171362
         Train Epoch: [1000/2467]	Loss: 0.157711
 Train Epoch: [1000/2467]	Loss: 0.021617
     Train Epoch: [1020/2467]	Loss: 0.303498
          Train Epoch: [1020/2467]	Loss: 0.015482Train Epoch: [1020/2467]	Loss: 0.062884

     Train Epoch: [1020/2467]	Loss: 0.099112
         Train Epoch: [1040/2467]	Loss: 0.114780
     Train Epoch: [1040/2467]	Loss: 0.198160 
Train Epoch: [1040/2467]	Loss: 0.126758    
 Train Epoch: [1040/2467]	Loss: 0.066912
             Train Epoch: [1060/2467]	Loss: 0.309841
  Train Epoch: [1060/2467]	Loss: 0.054081Train Epoch: [1060/2467]	Loss: 0.089417

     Train Epoch: [1060/2467]	Loss: 0.118991
         Train Epoch: [1080/2467]	Loss: 0.101567
 Train Epoch: [1080/2467]	Loss: 0.202657
     Train Epoch: [1080/2467]	Loss: 0.147613
     Train Epoch: [1080/2467]	Loss: 0.192866
         Train Epoch: [1100/2467]	Loss: 0.241385
 Train Epoch: [1100/2467]	Loss: 0.160143
         Train Epoch: [1100/2467]	Loss: 0.147438
 Train Epoch: [1100/2467]	Loss: 0.182156
     Train Epoch: [1120/2467]	Loss: 0.239425
         Train Epoch: [1120/2467]	Loss: 0.099978
 Train Epoch: [1120/2467]	Loss: 0.071309
     Train Epoch: [1120/2467]	Loss: 0.123636
         Train Epoch: [1140/2467]	Loss: 0.031609
 Train Epoch: [1140/2467]	Loss: 0.322393
         Train Epoch: [1140/2467]	Loss: 0.062505 
Train Epoch: [1140/2467]	Loss: 0.236921
         Train Epoch: [1160/2467]	Loss: 0.053870 
Train Epoch: [1160/2467]	Loss: 0.071221
     Train Epoch: [1160/2467]	Loss: 0.201046    
 Train Epoch: [1160/2467]	Loss: 0.077814
         Train Epoch: [1180/2467]	Loss: 0.089680
 Train Epoch: [1180/2467]	Loss: 0.058392
     Train Epoch: [1180/2467]	Loss: 0.076955
     Train Epoch: [1180/2467]	Loss: 0.036271
     Train Epoch: [1200/2467]	Loss: 0.044584    
 Train Epoch: [1200/2467]	Loss: 0.069935
         Train Epoch: [1200/2467]	Loss: 0.248382
 Train Epoch: [1200/2467]	Loss: 0.161494
     Train Epoch: [1220/2467]	Loss: 0.077704
          Train Epoch: [1220/2467]	Loss: 0.150416Train Epoch: [1220/2467]	Loss: 0.209711

     Train Epoch: [1220/2467]	Loss: 0.118876
         Train Epoch: [1240/2467]	Loss: 0.203150
     Train Epoch: [1240/2467]	Loss: 0.238991
 Train Epoch: [1240/2467]	Loss: 0.186694
     Train Epoch: [1240/2467]	Loss: 0.113798
     Train Epoch: [1260/2467]	Loss: 0.037437
         Train Epoch: [1260/2467]	Loss: 0.095950
     Train Epoch: [1260/2467]	Loss: 0.036434
 Train Epoch: [1260/2467]	Loss: 0.327325
         Train Epoch: [1280/2467]	Loss: 0.132221
 Train Epoch: [1280/2467]	Loss: 0.062299
     Train Epoch: [1280/2467]	Loss: 0.365673
     Train Epoch: [1280/2467]	Loss: 0.074176
     Train Epoch: [1300/2467]	Loss: 0.055270    
     Train Epoch: [1300/2467]	Loss: 0.207981    
 Train Epoch: [1300/2467]	Loss: 0.028196
 Train Epoch: [1300/2467]	Loss: 0.123809
     Train Epoch: [1320/2467]	Loss: 0.036545
         Train Epoch: [1320/2467]	Loss: 0.087990
     Train Epoch: [1320/2467]	Loss: 0.111041
 Train Epoch: [1320/2467]	Loss: 0.108305
          Train Epoch: [1340/2467]	Loss: 0.084969    
Train Epoch: [1340/2467]	Loss: 0.169097
     Train Epoch: [1340/2467]	Loss: 0.236291
 Train Epoch: [1340/2467]	Loss: 0.133790
         Train Epoch: [1360/2467]	Loss: 0.030437
 Train Epoch: [1360/2467]	Loss: 0.070732
          Train Epoch: [1360/2467]	Loss: 0.091287
Train Epoch: [1360/2467]	Loss: 0.027815
          Train Epoch: [1380/2467]	Loss: 0.204589Train Epoch: [1380/2467]	Loss: 0.132991

     Train Epoch: [1380/2467]	Loss: 0.223531    
 Train Epoch: [1380/2467]	Loss: 0.179476
     Train Epoch: [1400/2467]	Loss: 0.063146
     Train Epoch: [1400/2467]	Loss: 0.115561
     Train Epoch: [1400/2467]	Loss: 0.063659
     Train Epoch: [1400/2467]	Loss: 0.097952
          Train Epoch: [1420/2467]	Loss: 0.096043Train Epoch: [1420/2467]	Loss: 0.142471

     Train Epoch: [1420/2467]	Loss: 0.058900
     Train Epoch: [1420/2467]	Loss: 0.247500
     Train Epoch: [1440/2467]	Loss: 0.012884
             Train Epoch: [1440/2467]	Loss: 0.029467
  Train Epoch: [1440/2467]	Loss: 0.041761Train Epoch: [1440/2467]	Loss: 0.071776

     Train Epoch: [1460/2467]	Loss: 0.041472
     Train Epoch: [1460/2467]	Loss: 0.123449    
 Train Epoch: [1460/2467]	Loss: 0.081005
     Train Epoch: [1460/2467]	Loss: 0.132676
     Train Epoch: [1480/2467]	Loss: 0.018882
     Train Epoch: [1480/2467]	Loss: 0.011897
     Train Epoch: [1480/2467]	Loss: 0.186539
     Train Epoch: [1480/2467]	Loss: 0.149853
     Train Epoch: [1500/2467]	Loss: 0.011190        
  Train Epoch: [1500/2467]	Loss: 0.052240Train Epoch: [1500/2467]	Loss: 0.123230

     Train Epoch: [1500/2467]	Loss: 0.067059
     Train Epoch: [1520/2467]	Loss: 0.072352
     Train Epoch: [1520/2467]	Loss: 0.117982
     Train Epoch: [1520/2467]	Loss: 0.187062
     Train Epoch: [1520/2467]	Loss: 0.148715
     Train Epoch: [1540/2467]	Loss: 0.053665
          Train Epoch: [1540/2467]	Loss: 0.254760Train Epoch: [1540/2467]	Loss: 0.018923

     Train Epoch: [1540/2467]	Loss: 0.062977
     Train Epoch: [1560/2467]	Loss: 0.056649
             Train Epoch: [1560/2467]	Loss: 0.231622
  Train Epoch: [1560/2467]	Loss: 0.278919Train Epoch: [1560/2467]	Loss: 0.339783

         Train Epoch: [1580/2467]	Loss: 0.062030
 Train Epoch: [1580/2467]	Loss: 0.176393
          Train Epoch: [1580/2467]	Loss: 0.085779Train Epoch: [1580/2467]	Loss: 0.193865

     Train Epoch: [1600/2467]	Loss: 0.020255
         Train Epoch: [1600/2467]	Loss: 0.061478
     Train Epoch: [1600/2467]	Loss: 0.158676
 Train Epoch: [1600/2467]	Loss: 0.193027
         Train Epoch: [1620/2467]	Loss: 0.153062    
     Train Epoch: [1620/2467]	Loss: 0.031603
 Train Epoch: [1620/2467]	Loss: 0.215391
 Train Epoch: [1620/2467]	Loss: 0.081181
     Train Epoch: [1640/2467]	Loss: 0.093868
         Train Epoch: [1640/2467]	Loss: 0.156576 
Train Epoch: [1640/2467]	Loss: 0.052938
     Train Epoch: [1640/2467]	Loss: 0.134053
         Train Epoch: [1660/2467]	Loss: 0.134867 
Train Epoch: [1660/2467]	Loss: 0.101403    
     Train Epoch: [1660/2467]	Loss: 0.113848 
Train Epoch: [1660/2467]	Loss: 0.041103
     Train Epoch: [1680/2467]	Loss: 0.107840
         Train Epoch: [1680/2467]	Loss: 0.033132
 Train Epoch: [1680/2467]	Loss: 0.074161
     Train Epoch: [1680/2467]	Loss: 0.125916
     Train Epoch: [1700/2467]	Loss: 0.068308
         Train Epoch: [1700/2467]	Loss: 0.065967
 Train Epoch: [1700/2467]	Loss: 0.190229
     Train Epoch: [1700/2467]	Loss: 0.082988
         Train Epoch: [1720/2467]	Loss: 0.197392 
Train Epoch: [1720/2467]	Loss: 0.068937
     Train Epoch: [1720/2467]	Loss: 0.033869
     Train Epoch: [1720/2467]	Loss: 0.143597
     Train Epoch: [1740/2467]	Loss: 0.017998
         Train Epoch: [1740/2467]	Loss: 0.219865
 Train Epoch: [1740/2467]	Loss: 0.016382
     Train Epoch: [1740/2467]	Loss: 0.163115
     Train Epoch: [1760/2467]	Loss: 0.086992
     Train Epoch: [1760/2467]	Loss: 0.042536
     Train Epoch: [1760/2467]	Loss: 0.058590
     Train Epoch: [1760/2467]	Loss: 0.084028
         Train Epoch: [1780/2467]	Loss: 0.122241
 Train Epoch: [1780/2467]	Loss: 0.104864
          Train Epoch: [1780/2467]	Loss: 0.168891Train Epoch: [1780/2467]	Loss: 0.065766

         Train Epoch: [1800/2467]	Loss: 0.053811
 Train Epoch: [1800/2467]	Loss: 0.215106
     Train Epoch: [1800/2467]	Loss: 0.091013
     Train Epoch: [1800/2467]	Loss: 0.229477
     Train Epoch: [1820/2467]	Loss: 0.217799
         Train Epoch: [1820/2467]	Loss: 0.089890
 Train Epoch: [1820/2467]	Loss: 0.088761
     Train Epoch: [1820/2467]	Loss: 0.125946
     Train Epoch: [1840/2467]	Loss: 0.084953
         Train Epoch: [1840/2467]	Loss: 0.215850
 Train Epoch: [1840/2467]	Loss: 0.035140
     Train Epoch: [1840/2467]	Loss: 0.113008
     Train Epoch: [1860/2467]	Loss: 0.139614
     Train Epoch: [1860/2467]	Loss: 0.089444
     Train Epoch: [1860/2467]	Loss: 0.098472
     Train Epoch: [1860/2467]	Loss: 0.105186
         Train Epoch: [1880/2467]	Loss: 0.021213
 Train Epoch: [1880/2467]	Loss: 0.059132
         Train Epoch: [1880/2467]	Loss: 0.048799
 Train Epoch: [1880/2467]	Loss: 0.046568
     Train Epoch: [1900/2467]	Loss: 0.082182
              Train Epoch: [1900/2467]	Loss: 0.198429Train Epoch: [1900/2467]	Loss: 0.084924

 Train Epoch: [1900/2467]	Loss: 0.233648
     Train Epoch: [1920/2467]	Loss: 0.015121
     Train Epoch: [1920/2467]	Loss: 0.155348
     Train Epoch: [1920/2467]	Loss: 0.119084
     Train Epoch: [1920/2467]	Loss: 0.093534
         Train Epoch: [1940/2467]	Loss: 0.048398
 Train Epoch: [1940/2467]	Loss: 0.104317
         Train Epoch: [1940/2467]	Loss: 0.100878
 Train Epoch: [1940/2467]	Loss: 0.306622
     Train Epoch: [1960/2467]	Loss: 0.086665
         Train Epoch: [1960/2467]	Loss: 0.076931
 Train Epoch: [1960/2467]	Loss: 0.137167
     Train Epoch: [1960/2467]	Loss: 0.067299
     Train Epoch: [1980/2467]	Loss: 0.077995
     Train Epoch: [1980/2467]	Loss: 0.159177
     Train Epoch: [1980/2467]	Loss: 0.213760
     Train Epoch: [1980/2467]	Loss: 0.140498
     Train Epoch: [2000/2467]	Loss: 0.111928
     Train Epoch: [2000/2467]	Loss: 0.022213
         Train Epoch: [2000/2467]	Loss: 0.133866
 Train Epoch: [2000/2467]	Loss: 0.043313
         Train Epoch: [2020/2467]	Loss: 0.133173
 Train Epoch: [2020/2467]	Loss: 0.137267
          Train Epoch: [2020/2467]	Loss: 0.045805Train Epoch: [2020/2467]	Loss: 0.119838

          Train Epoch: [2040/2467]	Loss: 0.106838
Train Epoch: [2040/2467]	Loss: 0.051154
         Train Epoch: [2040/2467]	Loss: 0.067403
 Train Epoch: [2040/2467]	Loss: 0.358988
     Train Epoch: [2060/2467]	Loss: 0.139175
     Train Epoch: [2060/2467]	Loss: 0.052435
         Train Epoch: [2060/2467]	Loss: 0.172202
 Train Epoch: [2060/2467]	Loss: 0.093060
     Train Epoch: [2080/2467]	Loss: 0.028727
         Train Epoch: [2080/2467]	Loss: 0.192537 
Train Epoch: [2080/2467]	Loss: 0.024155
     Train Epoch: [2080/2467]	Loss: 0.054098
         Train Epoch: [2100/2467]	Loss: 0.393891
 Train Epoch: [2100/2467]	Loss: 0.082794
     Train Epoch: [2100/2467]	Loss: 0.081331
     Train Epoch: [2100/2467]	Loss: 0.034959
     Train Epoch: [2120/2467]	Loss: 0.062755
         Train Epoch: [2120/2467]	Loss: 0.030943
 Train Epoch: [2120/2467]	Loss: 0.073996
     Train Epoch: [2120/2467]	Loss: 0.151927
         Train Epoch: [2140/2467]	Loss: 0.342207    
 Train Epoch: [2140/2467]	Loss: 0.189676
 Train Epoch: [2140/2467]	Loss: 0.239593
     Train Epoch: [2140/2467]	Loss: 0.128293
              Train Epoch: [2160/2467]	Loss: 0.263293Train Epoch: [2160/2467]	Loss: 0.039919

 Train Epoch: [2160/2467]	Loss: 0.052243
     Train Epoch: [2160/2467]	Loss: 0.073157
     Train Epoch: [2180/2467]	Loss: 0.047880
     Train Epoch: [2180/2467]	Loss: 0.211053
     Train Epoch: [2180/2467]	Loss: 0.116864
     Train Epoch: [2180/2467]	Loss: 0.097836
     Train Epoch: [2200/2467]	Loss: 0.086499
         Train Epoch: [2200/2467]	Loss: 0.081402
 Train Epoch: [2200/2467]	Loss: 0.076503
     Train Epoch: [2200/2467]	Loss: 0.037745
     Train Epoch: [2220/2467]	Loss: 0.046292
     Train Epoch: [2220/2467]	Loss: 0.095543
     Train Epoch: [2220/2467]	Loss: 0.084895
     Train Epoch: [2220/2467]	Loss: 0.040751
         Train Epoch: [2240/2467]	Loss: 0.059762
 Train Epoch: [2240/2467]	Loss: 0.076470    
     Train Epoch: [2240/2467]	Loss: 0.026506
 Train Epoch: [2240/2467]	Loss: 0.019185
         Train Epoch: [2260/2467]	Loss: 0.096007
 Train Epoch: [2260/2467]	Loss: 0.028262
         Train Epoch: [2260/2467]	Loss: 0.185636 
Train Epoch: [2260/2467]	Loss: 0.237539
              Train Epoch: [2280/2467]	Loss: 0.069140
Train Epoch: [2280/2467]	Loss: 0.120876
 Train Epoch: [2280/2467]	Loss: 0.076211
     Train Epoch: [2280/2467]	Loss: 0.046634
         Train Epoch: [2300/2467]	Loss: 0.211254 
Train Epoch: [2300/2467]	Loss: 0.043465    
 Train Epoch: [2300/2467]	Loss: 0.075303
     Train Epoch: [2300/2467]	Loss: 0.040940
             Train Epoch: [2320/2467]	Loss: 0.091931
      Train Epoch: [2320/2467]	Loss: 0.103010Train Epoch: [2320/2467]	Loss: 0.012640

 Train Epoch: [2320/2467]	Loss: 0.108962
     Train Epoch: [2340/2467]	Loss: 0.080659
             Train Epoch: [2340/2467]	Loss: 0.177058 
Train Epoch: [2340/2467]	Loss: 0.424701 
Train Epoch: [2340/2467]	Loss: 0.128404
     Train Epoch: [2360/2467]	Loss: 0.116473
         Train Epoch: [2360/2467]	Loss: 0.251543    
 Train Epoch: [2360/2467]	Loss: 0.103680
 Train Epoch: [2360/2467]	Loss: 0.047607
     Train Epoch: [2380/2467]	Loss: 0.047596
     Train Epoch: [2380/2467]	Loss: 0.156540
         Train Epoch: [2380/2467]	Loss: 0.115519 
Train Epoch: [2380/2467]	Loss: 0.290348
     Train Epoch: [2400/2467]	Loss: 0.097730
     Train Epoch: [2400/2467]	Loss: 0.082406
     Train Epoch: [2400/2467]	Loss: 0.059621
     Train Epoch: [2400/2467]	Loss: 0.137044
     Train Epoch: [2420/2467]	Loss: 0.015902
     Train Epoch: [2420/2467]	Loss: 0.208779
     Train Epoch: [2420/2467]	Loss: 0.072199
     Train Epoch: [2420/2467]	Loss: 0.158791
     Train Epoch: [2440/2467]	Loss: 0.009131
     Train Epoch: [2440/2467]	Loss: 0.181133    
     Train Epoch: [2440/2467]	Loss: 0.108170
 Train Epoch: [2440/2467]	Loss: 0.094441
     Train Epoch: [2460/2467]	Loss: 0.116151
     Train Epoch: [2460/2467]	Loss: 0.065269
     Train Epoch: [2460/2467]	Loss: 0.093788
     Train Epoch: [2460/2467]	Loss: 0.162538
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 44 epoch =====
     2025-05-11.16-04-08
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 44 epoch =====
     2025-05-11.16-04-08
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 44 epoch =====
     2025-05-11.16-04-08
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 44 epoch =====
     2025-05-11.16-04-09
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
         Train Epoch: [0/2467]	Loss: 0.157148
         Train Epoch: [0/2467]	Loss: 0.050391
 Train Epoch: [0/2467]	Loss: 0.215652 
Train Epoch: [0/2467]	Loss: 0.171054
     Train Epoch: [20/2467]	Loss: 0.249808
     Train Epoch: [20/2467]	Loss: 0.077328
     Train Epoch: [20/2467]	Loss: 0.068113
     Train Epoch: [20/2467]	Loss: 0.037489
     Train Epoch: [40/2467]	Loss: 0.070969
          Train Epoch: [40/2467]	Loss: 0.316098
Train Epoch: [40/2467]	Loss: 0.066310
     Train Epoch: [40/2467]	Loss: 0.094462
         Train Epoch: [60/2467]	Loss: 0.111589
 Train Epoch: [60/2467]	Loss: 0.018995    
     Train Epoch: [60/2467]	Loss: 0.058705 
Train Epoch: [60/2467]	Loss: 0.098145
         Train Epoch: [80/2467]	Loss: 0.199302 
Train Epoch: [80/2467]	Loss: 0.082800
         Train Epoch: [80/2467]	Loss: 0.089478
 Train Epoch: [80/2467]	Loss: 0.036518
              Train Epoch: [100/2467]	Loss: 0.326446Train Epoch: [100/2467]	Loss: 0.050777

 Train Epoch: [100/2467]	Loss: 0.040733
     Train Epoch: [100/2467]	Loss: 0.184299
     Train Epoch: [120/2467]	Loss: 0.036539
     Train Epoch: [120/2467]	Loss: 0.264179
     Train Epoch: [120/2467]	Loss: 0.159244
     Train Epoch: [120/2467]	Loss: 0.079849
         Train Epoch: [140/2467]	Loss: 0.019043
     Train Epoch: [140/2467]	Loss: 0.080382
 Train Epoch: [140/2467]	Loss: 0.150370
     Train Epoch: [140/2467]	Loss: 0.045870
         Train Epoch: [160/2467]	Loss: 0.055206
 Train Epoch: [160/2467]	Loss: 0.093353
     Train Epoch: [160/2467]	Loss: 0.109670
     Train Epoch: [160/2467]	Loss: 0.150610
     Train Epoch: [180/2467]	Loss: 0.076437
         Train Epoch: [180/2467]	Loss: 0.156566
     Train Epoch: [180/2467]	Loss: 0.036470
 Train Epoch: [180/2467]	Loss: 0.101181
     Train Epoch: [200/2467]	Loss: 0.096462
          Train Epoch: [200/2467]	Loss: 0.031718
Train Epoch: [200/2467]	Loss: 0.036000
     Train Epoch: [200/2467]	Loss: 0.059987
     Train Epoch: [220/2467]	Loss: 0.072129
          Train Epoch: [220/2467]	Loss: 0.119961Train Epoch: [220/2467]	Loss: 0.040828

     Train Epoch: [220/2467]	Loss: 0.184559
         Train Epoch: [240/2467]	Loss: 0.014786
 Train Epoch: [240/2467]	Loss: 0.181651
         Train Epoch: [240/2467]	Loss: 0.132000
 Train Epoch: [240/2467]	Loss: 0.029108
     Train Epoch: [260/2467]	Loss: 0.042313
         Train Epoch: [260/2467]	Loss: 0.031472 
Train Epoch: [260/2467]	Loss: 0.041305
     Train Epoch: [260/2467]	Loss: 0.023958
     Train Epoch: [280/2467]	Loss: 0.065455
     Train Epoch: [280/2467]	Loss: 0.119353
     Train Epoch: [280/2467]	Loss: 0.085331    
 Train Epoch: [280/2467]	Loss: 0.053718
          Train Epoch: [300/2467]	Loss: 0.146897Train Epoch: [300/2467]	Loss: 0.023117

     Train Epoch: [300/2467]	Loss: 0.060096
     Train Epoch: [300/2467]	Loss: 0.290264
         Train Epoch: [320/2467]	Loss: 0.106091
 Train Epoch: [320/2467]	Loss: 0.177509
         Train Epoch: [320/2467]	Loss: 0.301256 
Train Epoch: [320/2467]	Loss: 0.133677
         Train Epoch: [340/2467]	Loss: 0.041884
     Train Epoch: [340/2467]	Loss: 0.054366    
 Train Epoch: [340/2467]	Loss: 0.138777
 Train Epoch: [340/2467]	Loss: 0.013302
         Train Epoch: [360/2467]	Loss: 0.045345 
Train Epoch: [360/2467]	Loss: 0.027644
     Train Epoch: [360/2467]	Loss: 0.122998
     Train Epoch: [360/2467]	Loss: 0.176710
         Train Epoch: [380/2467]	Loss: 0.066886
 Train Epoch: [380/2467]	Loss: 0.042856
     Train Epoch: [380/2467]	Loss: 0.237625
     Train Epoch: [380/2467]	Loss: 0.152657
     Train Epoch: [400/2467]	Loss: 0.043757
         Train Epoch: [400/2467]	Loss: 0.029710
     Train Epoch: [400/2467]	Loss: 0.141706
 Train Epoch: [400/2467]	Loss: 0.073794
     Train Epoch: [420/2467]	Loss: 0.248292
     Train Epoch: [420/2467]	Loss: 0.085768
     Train Epoch: [420/2467]	Loss: 0.062370
     Train Epoch: [420/2467]	Loss: 0.201196
     Train Epoch: [440/2467]	Loss: 0.107046
         Train Epoch: [440/2467]	Loss: 0.058884
 Train Epoch: [440/2467]	Loss: 0.274422
     Train Epoch: [440/2467]	Loss: 0.034570
               Train Epoch: [460/2467]	Loss: 0.132316
Train Epoch: [460/2467]	Loss: 0.064719
Train Epoch: [460/2467]	Loss: 0.100638
     Train Epoch: [460/2467]	Loss: 0.197862
         Train Epoch: [480/2467]	Loss: 0.106716
 Train Epoch: [480/2467]	Loss: 0.042481    
 Train Epoch: [480/2467]	Loss: 0.309423
     Train Epoch: [480/2467]	Loss: 0.213295
     Train Epoch: [500/2467]	Loss: 0.024180
         Train Epoch: [500/2467]	Loss: 0.076212
 Train Epoch: [500/2467]	Loss: 0.057143
     Train Epoch: [500/2467]	Loss: 0.138620
     Train Epoch: [520/2467]	Loss: 0.071280
         Train Epoch: [520/2467]	Loss: 0.077824 
Train Epoch: [520/2467]	Loss: 0.219255
     Train Epoch: [520/2467]	Loss: 0.060234
         Train Epoch: [540/2467]	Loss: 0.119373
 Train Epoch: [540/2467]	Loss: 0.169157
          Train Epoch: [540/2467]	Loss: 0.050138Train Epoch: [540/2467]	Loss: 0.072248

     Train Epoch: [560/2467]	Loss: 0.110092
         Train Epoch: [560/2467]	Loss: 0.115289
 Train Epoch: [560/2467]	Loss: 0.113102
     Train Epoch: [560/2467]	Loss: 0.095962
     Train Epoch: [580/2467]	Loss: 0.194259
              Train Epoch: [580/2467]	Loss: 0.102073Train Epoch: [580/2467]	Loss: 0.104451

 Train Epoch: [580/2467]	Loss: 0.045690
         Train Epoch: [600/2467]	Loss: 0.184847
 Train Epoch: [600/2467]	Loss: 0.108670
          Train Epoch: [600/2467]	Loss: 0.115796
Train Epoch: [600/2467]	Loss: 0.078791
     Train Epoch: [620/2467]	Loss: 0.024121
         Train Epoch: [620/2467]	Loss: 0.173986 
Train Epoch: [620/2467]	Loss: 0.105236
     Train Epoch: [620/2467]	Loss: 0.077610
     Train Epoch: [640/2467]	Loss: 0.067381
          Train Epoch: [640/2467]	Loss: 0.082786Train Epoch: [640/2467]	Loss: 0.123540

     Train Epoch: [640/2467]	Loss: 0.030707
     Train Epoch: [660/2467]	Loss: 0.034657    
 Train Epoch: [660/2467]	Loss: 0.274496
          Train Epoch: [660/2467]	Loss: 0.022335Train Epoch: [660/2467]	Loss: 0.140393

         Train Epoch: [680/2467]	Loss: 0.077532
 Train Epoch: [680/2467]	Loss: 0.030546
     Train Epoch: [680/2467]	Loss: 0.022310
     Train Epoch: [680/2467]	Loss: 0.060064
         Train Epoch: [700/2467]	Loss: 0.030168
     Train Epoch: [700/2467]	Loss: 0.239757
 Train Epoch: [700/2467]	Loss: 0.061373
     Train Epoch: [700/2467]	Loss: 0.221353
         Train Epoch: [720/2467]	Loss: 0.012005 
Train Epoch: [720/2467]	Loss: 0.113206
     Train Epoch: [720/2467]	Loss: 0.049644
     Train Epoch: [720/2467]	Loss: 0.066983
             Train Epoch: [740/2467]	Loss: 0.035344
 Train Epoch: [740/2467]	Loss: 0.064445
     Train Epoch: [740/2467]	Loss: 0.205402
 Train Epoch: [740/2467]	Loss: 0.066929
     Train Epoch: [760/2467]	Loss: 0.198876
     Train Epoch: [760/2467]	Loss: 0.038195
     Train Epoch: [760/2467]	Loss: 0.232303
     Train Epoch: [760/2467]	Loss: 0.304673
     Train Epoch: [780/2467]	Loss: 0.263012
         Train Epoch: [780/2467]	Loss: 0.251849
     Train Epoch: [780/2467]	Loss: 0.251349
 Train Epoch: [780/2467]	Loss: 0.076441
     Train Epoch: [800/2467]	Loss: 0.148717
     Train Epoch: [800/2467]	Loss: 0.074613
     Train Epoch: [800/2467]	Loss: 0.058148
     Train Epoch: [800/2467]	Loss: 0.137242
     Train Epoch: [820/2467]	Loss: 0.042346
     Train Epoch: [820/2467]	Loss: 0.328728
     Train Epoch: [820/2467]	Loss: 0.036740
     Train Epoch: [820/2467]	Loss: 0.138812
         Train Epoch: [840/2467]	Loss: 0.298687
 Train Epoch: [840/2467]	Loss: 0.066428
         Train Epoch: [840/2467]	Loss: 0.083403
 Train Epoch: [840/2467]	Loss: 0.045099
     Train Epoch: [860/2467]	Loss: 0.067777
          Train Epoch: [860/2467]	Loss: 0.041766Train Epoch: [860/2467]	Loss: 0.031004

     Train Epoch: [860/2467]	Loss: 0.176927
         Train Epoch: [880/2467]	Loss: 0.110876
 Train Epoch: [880/2467]	Loss: 0.101627
     Train Epoch: [880/2467]	Loss: 0.111627
     Train Epoch: [880/2467]	Loss: 0.152745
     Train Epoch: [900/2467]	Loss: 0.159771
         Train Epoch: [900/2467]	Loss: 0.156945
 Train Epoch: [900/2467]	Loss: 0.049680
     Train Epoch: [900/2467]	Loss: 0.132811
     Train Epoch: [920/2467]	Loss: 0.198208
         Train Epoch: [920/2467]	Loss: 0.028942
 Train Epoch: [920/2467]	Loss: 0.186846
     Train Epoch: [920/2467]	Loss: 0.169931
         Train Epoch: [940/2467]	Loss: 0.025836
 Train Epoch: [940/2467]	Loss: 0.027109
     Train Epoch: [940/2467]	Loss: 0.156154
     Train Epoch: [940/2467]	Loss: 0.052768
     Train Epoch: [960/2467]	Loss: 0.050386
     Train Epoch: [960/2467]	Loss: 0.040975
     Train Epoch: [960/2467]	Loss: 0.053999
     Train Epoch: [960/2467]	Loss: 0.197280
     Train Epoch: [980/2467]	Loss: 0.039439
         Train Epoch: [980/2467]	Loss: 0.191138
 Train Epoch: [980/2467]	Loss: 0.084640
     Train Epoch: [980/2467]	Loss: 0.073029
         Train Epoch: [1000/2467]	Loss: 0.028173    
 Train Epoch: [1000/2467]	Loss: 0.201369 
Train Epoch: [1000/2467]	Loss: 0.022248
     Train Epoch: [1000/2467]	Loss: 0.161869
         Train Epoch: [1020/2467]	Loss: 0.094970 
Train Epoch: [1020/2467]	Loss: 0.291929
     Train Epoch: [1020/2467]	Loss: 0.065745
     Train Epoch: [1020/2467]	Loss: 0.022399
     Train Epoch: [1040/2467]	Loss: 0.097799
         Train Epoch: [1040/2467]	Loss: 0.176869
 Train Epoch: [1040/2467]	Loss: 0.121336
     Train Epoch: [1040/2467]	Loss: 0.079195
     Train Epoch: [1060/2467]	Loss: 0.103520
          Train Epoch: [1060/2467]	Loss: 0.085653Train Epoch: [1060/2467]	Loss: 0.066606

     Train Epoch: [1060/2467]	Loss: 0.334647
     Train Epoch: [1080/2467]	Loss: 0.148793
          Train Epoch: [1080/2467]	Loss: 0.176071
Train Epoch: [1080/2467]	Loss: 0.122586
     Train Epoch: [1080/2467]	Loss: 0.092562
             Train Epoch: [1100/2467]	Loss: 0.228056
 Train Epoch: [1100/2467]	Loss: 0.131384    
  Train Epoch: [1100/2467]	Loss: 0.198393Train Epoch: [1100/2467]	Loss: 0.146097

     Train Epoch: [1120/2467]	Loss: 0.168595
         Train Epoch: [1120/2467]	Loss: 0.086088
 Train Epoch: [1120/2467]	Loss: 0.074313    
 Train Epoch: [1120/2467]	Loss: 0.102532
         Train Epoch: [1140/2467]	Loss: 0.031066
     Train Epoch: [1140/2467]	Loss: 0.058336 
Train Epoch: [1140/2467]	Loss: 0.268722
     Train Epoch: [1140/2467]	Loss: 0.219952
         Train Epoch: [1160/2467]	Loss: 0.059874
     Train Epoch: [1160/2467]	Loss: 0.056603
     Train Epoch: [1160/2467]	Loss: 0.124547
 Train Epoch: [1160/2467]	Loss: 0.177440
     Train Epoch: [1180/2467]	Loss: 0.038126
         Train Epoch: [1180/2467]	Loss: 0.084140
 Train Epoch: [1180/2467]	Loss: 0.053009
     Train Epoch: [1180/2467]	Loss: 0.095430
         Train Epoch: [1200/2467]	Loss: 0.047631
 Train Epoch: [1200/2467]	Loss: 0.074603
     Train Epoch: [1200/2467]	Loss: 0.225526
     Train Epoch: [1200/2467]	Loss: 0.231904
     Train Epoch: [1220/2467]	Loss: 0.131265
     Train Epoch: [1220/2467]	Loss: 0.073279
         Train Epoch: [1220/2467]	Loss: 0.159373
 Train Epoch: [1220/2467]	Loss: 0.205151
     Train Epoch: [1240/2467]	Loss: 0.213665
     Train Epoch: [1240/2467]	Loss: 0.233563    
 Train Epoch: [1240/2467]	Loss: 0.098782
     Train Epoch: [1240/2467]	Loss: 0.091870
     Train Epoch: [1260/2467]	Loss: 0.040325
     Train Epoch: [1260/2467]	Loss: 0.041364
     Train Epoch: [1260/2467]	Loss: 0.334684
     Train Epoch: [1260/2467]	Loss: 0.080787
         Train Epoch: [1280/2467]	Loss: 0.078890 
Train Epoch: [1280/2467]	Loss: 0.140866
         Train Epoch: [1280/2467]	Loss: 0.053216
 Train Epoch: [1280/2467]	Loss: 0.372775
         Train Epoch: [1300/2467]	Loss: 0.217655
     Train Epoch: [1300/2467]	Loss: 0.029179
 Train Epoch: [1300/2467]	Loss: 0.133198
     Train Epoch: [1300/2467]	Loss: 0.061903
     Train Epoch: [1320/2467]	Loss: 0.039645
     Train Epoch: [1320/2467]	Loss: 0.111499
         Train Epoch: [1320/2467]	Loss: 0.078903 
Train Epoch: [1320/2467]	Loss: 0.117031
         Train Epoch: [1340/2467]	Loss: 0.090900
 Train Epoch: [1340/2467]	Loss: 0.175720    
      Train Epoch: [1340/2467]	Loss: 0.111756
Train Epoch: [1340/2467]	Loss: 0.239658
         Train Epoch: [1360/2467]	Loss: 0.035249
 Train Epoch: [1360/2467]	Loss: 0.087113
     Train Epoch: [1360/2467]	Loss: 0.034873
     Train Epoch: [1360/2467]	Loss: 0.093285
         Train Epoch: [1380/2467]	Loss: 0.173848
     Train Epoch: [1380/2467]	Loss: 0.116936
 Train Epoch: [1380/2467]	Loss: 0.223649
     Train Epoch: [1380/2467]	Loss: 0.179522
          Train Epoch: [1400/2467]	Loss: 0.149577Train Epoch: [1400/2467]	Loss: 0.055271

     Train Epoch: [1400/2467]	Loss: 0.092275
     Train Epoch: [1400/2467]	Loss: 0.066833
         Train Epoch: [1420/2467]	Loss: 0.251944
 Train Epoch: [1420/2467]	Loss: 0.055411
         Train Epoch: [1420/2467]	Loss: 0.092060
 Train Epoch: [1420/2467]	Loss: 0.144468
     Train Epoch: [1440/2467]	Loss: 0.014437
     Train Epoch: [1440/2467]	Loss: 0.074198
     Train Epoch: [1440/2467]	Loss: 0.034920
     Train Epoch: [1440/2467]	Loss: 0.044729
     Train Epoch: [1460/2467]	Loss: 0.034945
         Train Epoch: [1460/2467]	Loss: 0.067877
 Train Epoch: [1460/2467]	Loss: 0.083710
     Train Epoch: [1460/2467]	Loss: 0.143867
              Train Epoch: [1480/2467]	Loss: 0.146360 
Train Epoch: [1480/2467]	Loss: 0.016605Train Epoch: [1480/2467]	Loss: 0.192937

     Train Epoch: [1480/2467]	Loss: 0.011166
     Train Epoch: [1500/2467]	Loss: 0.071758    
     Train Epoch: [1500/2467]	Loss: 0.010915
      Train Epoch: [1500/2467]	Loss: 0.048967
Train Epoch: [1500/2467]	Loss: 0.131935
     Train Epoch: [1520/2467]	Loss: 0.140192
         Train Epoch: [1520/2467]	Loss: 0.071053
     Train Epoch: [1520/2467]	Loss: 0.182804
 Train Epoch: [1520/2467]	Loss: 0.154305
     Train Epoch: [1540/2467]	Loss: 0.054908
     Train Epoch: [1540/2467]	Loss: 0.226131
     Train Epoch: [1540/2467]	Loss: 0.066456
     Train Epoch: [1540/2467]	Loss: 0.021463
     Train Epoch: [1560/2467]	Loss: 0.050951
             Train Epoch: [1560/2467]	Loss: 0.259949
 Train Epoch: [1560/2467]	Loss: 0.318225
 Train Epoch: [1560/2467]	Loss: 0.191534
     Train Epoch: [1580/2467]	Loss: 0.168193
          Train Epoch: [1580/2467]	Loss: 0.187662Train Epoch: [1580/2467]	Loss: 0.086992

     Train Epoch: [1580/2467]	Loss: 0.093075
     Train Epoch: [1600/2467]	Loss: 0.018439    
 Train Epoch: [1600/2467]	Loss: 0.156852
         Train Epoch: [1600/2467]	Loss: 0.200912
 Train Epoch: [1600/2467]	Loss: 0.050656
     Train Epoch: [1620/2467]	Loss: 0.030104
             Train Epoch: [1620/2467]	Loss: 0.138324 
Train Epoch: [1620/2467]	Loss: 0.085772
 Train Epoch: [1620/2467]	Loss: 0.212591
     Train Epoch: [1640/2467]	Loss: 0.149987    
 Train Epoch: [1640/2467]	Loss: 0.106843
     Train Epoch: [1640/2467]	Loss: 0.043246
     Train Epoch: [1640/2467]	Loss: 0.153152
     Train Epoch: [1660/2467]	Loss: 0.103949
     Train Epoch: [1660/2467]	Loss: 0.147072
         Train Epoch: [1660/2467]	Loss: 0.037629 
Train Epoch: [1660/2467]	Loss: 0.122861
     Train Epoch: [1680/2467]	Loss: 0.032585
     Train Epoch: [1680/2467]	Loss: 0.097298
     Train Epoch: [1680/2467]	Loss: 0.111903
     Train Epoch: [1680/2467]	Loss: 0.099417
     Train Epoch: [1700/2467]	Loss: 0.070160
     Train Epoch: [1700/2467]	Loss: 0.055704
         Train Epoch: [1700/2467]	Loss: 0.184044
 Train Epoch: [1700/2467]	Loss: 0.088812
          Train Epoch: [1720/2467]	Loss: 0.091109Train Epoch: [1720/2467]	Loss: 0.165356

     Train Epoch: [1720/2467]	Loss: 0.102385
     Train Epoch: [1720/2467]	Loss: 0.035525
     Train Epoch: [1740/2467]	Loss: 0.015880
         Train Epoch: [1740/2467]	Loss: 0.256490 
Train Epoch: [1740/2467]	Loss: 0.017909
     Train Epoch: [1740/2467]	Loss: 0.159257
     Train Epoch: [1760/2467]	Loss: 0.080079
     Train Epoch: [1760/2467]	Loss: 0.043798
     Train Epoch: [1760/2467]	Loss: 0.050270
     Train Epoch: [1760/2467]	Loss: 0.093653
          Train Epoch: [1780/2467]	Loss: 0.139892
Train Epoch: [1780/2467]	Loss: 0.100411
         Train Epoch: [1780/2467]	Loss: 0.160700 
Train Epoch: [1780/2467]	Loss: 0.070386
         Train Epoch: [1800/2467]	Loss: 0.050306
 Train Epoch: [1800/2467]	Loss: 0.201351
     Train Epoch: [1800/2467]	Loss: 0.087173
     Train Epoch: [1800/2467]	Loss: 0.212621
     Train Epoch: [1820/2467]	Loss: 0.274135
         Train Epoch: [1820/2467]	Loss: 0.084156 
Train Epoch: [1820/2467]	Loss: 0.092924
     Train Epoch: [1820/2467]	Loss: 0.117829
         Train Epoch: [1840/2467]	Loss: 0.085102
 Train Epoch: [1840/2467]	Loss: 0.109610
     Train Epoch: [1840/2467]	Loss: 0.037629
     Train Epoch: [1840/2467]	Loss: 0.236569
         Train Epoch: [1860/2467]	Loss: 0.096930
 Train Epoch: [1860/2467]	Loss: 0.112804
          Train Epoch: [1860/2467]	Loss: 0.080513Train Epoch: [1860/2467]	Loss: 0.137354

     Train Epoch: [1880/2467]	Loss: 0.022360
         Train Epoch: [1880/2467]	Loss: 0.040135 
Train Epoch: [1880/2467]	Loss: 0.046447
     Train Epoch: [1880/2467]	Loss: 0.060424
     Train Epoch: [1900/2467]	Loss: 0.079606    
 Train Epoch: [1900/2467]	Loss: 0.097254
     Train Epoch: [1900/2467]	Loss: 0.217398
     Train Epoch: [1900/2467]	Loss: 0.206505
     Train Epoch: [1920/2467]	Loss: 0.012372
          Train Epoch: [1920/2467]	Loss: 0.152500Train Epoch: [1920/2467]	Loss: 0.097122

     Train Epoch: [1920/2467]	Loss: 0.115295
         Train Epoch: [1940/2467]	Loss: 0.052739 
Train Epoch: [1940/2467]	Loss: 0.118993    
 Train Epoch: [1940/2467]	Loss: 0.108114
     Train Epoch: [1940/2467]	Loss: 0.294231
     Train Epoch: [1960/2467]	Loss: 0.082183
         Train Epoch: [1960/2467]	Loss: 0.122688
 Train Epoch: [1960/2467]	Loss: 0.079765
     Train Epoch: [1960/2467]	Loss: 0.060667
     Train Epoch: [1980/2467]	Loss: 0.075792
          Train Epoch: [1980/2467]	Loss: 0.145106
Train Epoch: [1980/2467]	Loss: 0.207242
     Train Epoch: [1980/2467]	Loss: 0.134924
         Train Epoch: [2000/2467]	Loss: 0.011871
 Train Epoch: [2000/2467]	Loss: 0.095901    
     Train Epoch: [2000/2467]	Loss: 0.041427
 Train Epoch: [2000/2467]	Loss: 0.117013
          Train Epoch: [2020/2467]	Loss: 0.089030Train Epoch: [2020/2467]	Loss: 0.135984    

     Train Epoch: [2020/2467]	Loss: 0.111356
 Train Epoch: [2020/2467]	Loss: 0.043072
         Train Epoch: [2040/2467]	Loss: 0.050263
 Train Epoch: [2040/2467]	Loss: 0.124512
     Train Epoch: [2040/2467]	Loss: 0.313027
     Train Epoch: [2040/2467]	Loss: 0.062372
     Train Epoch: [2060/2467]	Loss: 0.130816
         Train Epoch: [2060/2467]	Loss: 0.065059
 Train Epoch: [2060/2467]	Loss: 0.085805
     Train Epoch: [2060/2467]	Loss: 0.157007
         Train Epoch: [2080/2467]	Loss: 0.028936
     Train Epoch: [2080/2467]	Loss: 0.183276
     Train Epoch: [2080/2467]	Loss: 0.051237 
Train Epoch: [2080/2467]	Loss: 0.023413
     Train Epoch: [2100/2467]	Loss: 0.394319    
 Train Epoch: [2100/2467]	Loss: 0.092050
         Train Epoch: [2100/2467]	Loss: 0.028196
 Train Epoch: [2100/2467]	Loss: 0.067569
         Train Epoch: [2120/2467]	Loss: 0.092809    
 Train Epoch: [2120/2467]	Loss: 0.033516
 Train Epoch: [2120/2467]	Loss: 0.077349
     Train Epoch: [2120/2467]	Loss: 0.159201
             Train Epoch: [2140/2467]	Loss: 0.206163 
Train Epoch: [2140/2467]	Loss: 0.365863
 Train Epoch: [2140/2467]	Loss: 0.125643
     Train Epoch: [2140/2467]	Loss: 0.245456
         Train Epoch: [2160/2467]	Loss: 0.035697
 Train Epoch: [2160/2467]	Loss: 0.255392
         Train Epoch: [2160/2467]	Loss: 0.052375
 Train Epoch: [2160/2467]	Loss: 0.108020
     Train Epoch: [2180/2467]	Loss: 0.048398
     Train Epoch: [2180/2467]	Loss: 0.204127
     Train Epoch: [2180/2467]	Loss: 0.135677
     Train Epoch: [2180/2467]	Loss: 0.081634
     Train Epoch: [2200/2467]	Loss: 0.021904
         Train Epoch: [2200/2467]	Loss: 0.090635    
 Train Epoch: [2200/2467]	Loss: 0.088568
 Train Epoch: [2200/2467]	Loss: 0.088485
     Train Epoch: [2220/2467]	Loss: 0.053816
          Train Epoch: [2220/2467]	Loss: 0.078589Train Epoch: [2220/2467]	Loss: 0.039825

     Train Epoch: [2220/2467]	Loss: 0.089990
     Train Epoch: [2240/2467]	Loss: 0.074145
         Train Epoch: [2240/2467]	Loss: 0.023868 
Train Epoch: [2240/2467]	Loss: 0.026596
     Train Epoch: [2240/2467]	Loss: 0.068572
     Train Epoch: [2260/2467]	Loss: 0.105199
         Train Epoch: [2260/2467]	Loss: 0.184686
 Train Epoch: [2260/2467]	Loss: 0.022062
     Train Epoch: [2260/2467]	Loss: 0.177023
     Train Epoch: [2280/2467]	Loss: 0.124834
          Train Epoch: [2280/2467]	Loss: 0.042146Train Epoch: [2280/2467]	Loss: 0.105131

     Train Epoch: [2280/2467]	Loss: 0.069886
     Train Epoch: [2300/2467]	Loss: 0.080711
     Train Epoch: [2300/2467]	Loss: 0.036936
     Train Epoch: [2300/2467]	Loss: 0.226148
     Train Epoch: [2300/2467]	Loss: 0.048966
     Train Epoch: [2320/2467]	Loss: 0.117888
         Train Epoch: [2320/2467]	Loss: 0.015365
 Train Epoch: [2320/2467]	Loss: 0.127817
     Train Epoch: [2320/2467]	Loss: 0.119758
     Train Epoch: [2340/2467]	Loss: 0.081571
         Train Epoch: [2340/2467]	Loss: 0.170795
 Train Epoch: [2340/2467]	Loss: 0.208827
     Train Epoch: [2340/2467]	Loss: 0.393883
     Train Epoch: [2360/2467]	Loss: 0.117539
         Train Epoch: [2360/2467]	Loss: 0.262548
 Train Epoch: [2360/2467]	Loss: 0.045404
     Train Epoch: [2360/2467]	Loss: 0.130687
     Train Epoch: [2380/2467]	Loss: 0.048441
     Train Epoch: [2380/2467]	Loss: 0.254351
     Train Epoch: [2380/2467]	Loss: 0.089323
     Train Epoch: [2380/2467]	Loss: 0.166707
     Train Epoch: [2400/2467]	Loss: 0.110094    
     Train Epoch: [2400/2467]	Loss: 0.074529
 Train Epoch: [2400/2467]	Loss: 0.133493    
 Train Epoch: [2400/2467]	Loss: 0.061501
     Train Epoch: [2420/2467]	Loss: 0.014196
              Train Epoch: [2420/2467]	Loss: 0.084281Train Epoch: [2420/2467]	Loss: 0.154964

 Train Epoch: [2420/2467]	Loss: 0.239692
     Train Epoch: [2440/2467]	Loss: 0.205374
         Train Epoch: [2440/2467]	Loss: 0.108465 
Train Epoch: [2440/2467]	Loss: 0.087239
     Train Epoch: [2440/2467]	Loss: 0.023027
             Train Epoch: [2460/2467]	Loss: 0.138324 
Train Epoch: [2460/2467]	Loss: 0.089854
     Train Epoch: [2460/2467]	Loss: 0.058298
 Train Epoch: [2460/2467]	Loss: 0.197945
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 45 epoch =====
     2025-05-11.16-26-00
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 45 epoch =====
     2025-05-11.16-26-00
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 45 epoch =====
     2025-05-11.16-26-00
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 45 epoch =====
     2025-05-11.16-26-01
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
         Train Epoch: [0/2467]	Loss: 0.044822
 Train Epoch: [0/2467]	Loss: 0.221027
     Train Epoch: [0/2467]	Loss: 0.193697
     Train Epoch: [0/2467]	Loss: 0.152130
     Train Epoch: [20/2467]	Loss: 0.255793
          Train Epoch: [20/2467]	Loss: 0.091862Train Epoch: [20/2467]	Loss: 0.069999

     Train Epoch: [20/2467]	Loss: 0.060573
     Train Epoch: [40/2467]	Loss: 0.070577
         Train Epoch: [40/2467]	Loss: 0.272465
 Train Epoch: [40/2467]	Loss: 0.353379
     Train Epoch: [40/2467]	Loss: 0.103230
         Train Epoch: [60/2467]	Loss: 0.132319
 Train Epoch: [60/2467]	Loss: 0.023017    
     Train Epoch: [60/2467]	Loss: 0.097827
 Train Epoch: [60/2467]	Loss: 0.061833
     Train Epoch: [80/2467]	Loss: 0.206078
             Train Epoch: [80/2467]	Loss: 0.079453
  Train Epoch: [80/2467]	Loss: 0.093587
Train Epoch: [80/2467]	Loss: 0.040537
     Train Epoch: [100/2467]	Loss: 0.038962
         Train Epoch: [100/2467]	Loss: 0.270453
 Train Epoch: [100/2467]	Loss: 0.058375
     Train Epoch: [100/2467]	Loss: 0.116330
     Train Epoch: [120/2467]	Loss: 0.034880
         Train Epoch: [120/2467]	Loss: 0.274877
 Train Epoch: [120/2467]	Loss: 0.143011
     Train Epoch: [120/2467]	Loss: 0.085934
     Train Epoch: [140/2467]	Loss: 0.021557    
 Train Epoch: [140/2467]	Loss: 0.071004
     Train Epoch: [140/2467]	Loss: 0.050513
     Train Epoch: [140/2467]	Loss: 0.144321
         Train Epoch: [160/2467]	Loss: 0.090009
 Train Epoch: [160/2467]	Loss: 0.057481
         Train Epoch: [160/2467]	Loss: 0.097757
 Train Epoch: [160/2467]	Loss: 0.151015
     Train Epoch: [180/2467]	Loss: 0.076739
     Train Epoch: [180/2467]	Loss: 0.037440
     Train Epoch: [180/2467]	Loss: 0.077615
     Train Epoch: [180/2467]	Loss: 0.171718
         Train Epoch: [200/2467]	Loss: 0.091938
 Train Epoch: [200/2467]	Loss: 0.031819
     Train Epoch: [200/2467]	Loss: 0.059628
     Train Epoch: [200/2467]	Loss: 0.035276
     Train Epoch: [220/2467]	Loss: 0.074834
         Train Epoch: [220/2467]	Loss: 0.140573    
 Train Epoch: [220/2467]	Loss: 0.038333
 Train Epoch: [220/2467]	Loss: 0.192278
         Train Epoch: [240/2467]	Loss: 0.014915
 Train Epoch: [240/2467]	Loss: 0.279400
         Train Epoch: [240/2467]	Loss: 0.027843 
Train Epoch: [240/2467]	Loss: 0.131415
     Train Epoch: [260/2467]	Loss: 0.022972    
 Train Epoch: [260/2467]	Loss: 0.041653
     Train Epoch: [260/2467]	Loss: 0.028570
     Train Epoch: [260/2467]	Loss: 0.041390
     Train Epoch: [280/2467]	Loss: 0.089085
          Train Epoch: [280/2467]	Loss: 0.061082Train Epoch: [280/2467]	Loss: 0.067617

     Train Epoch: [280/2467]	Loss: 0.054002
         Train Epoch: [300/2467]	Loss: 0.147102 
Train Epoch: [300/2467]	Loss: 0.023739
          Train Epoch: [300/2467]	Loss: 0.061743Train Epoch: [300/2467]	Loss: 0.275247

     Train Epoch: [320/2467]	Loss: 0.108009
         Train Epoch: [320/2467]	Loss: 0.176534
     Train Epoch: [320/2467]	Loss: 0.147533
 Train Epoch: [320/2467]	Loss: 0.334777
     Train Epoch: [340/2467]	Loss: 0.053910
         Train Epoch: [340/2467]	Loss: 0.039173
 Train Epoch: [340/2467]	Loss: 0.143759
     Train Epoch: [340/2467]	Loss: 0.011695
     Train Epoch: [360/2467]	Loss: 0.173790
         Train Epoch: [360/2467]	Loss: 0.022662
 Train Epoch: [360/2467]	Loss: 0.029041
     Train Epoch: [360/2467]	Loss: 0.115992
     Train Epoch: [380/2467]	Loss: 0.044646    
     Train Epoch: [380/2467]	Loss: 0.215407 
Train Epoch: [380/2467]	Loss: 0.163483
     Train Epoch: [380/2467]	Loss: 0.068473
     Train Epoch: [400/2467]	Loss: 0.039373
     Train Epoch: [400/2467]	Loss: 0.138486
     Train Epoch: [400/2467]	Loss: 0.074377
     Train Epoch: [400/2467]	Loss: 0.025250
     Train Epoch: [420/2467]	Loss: 0.203560
     Train Epoch: [420/2467]	Loss: 0.083804
     Train Epoch: [420/2467]	Loss: 0.058062
     Train Epoch: [420/2467]	Loss: 0.253649
     Train Epoch: [440/2467]	Loss: 0.103201
     Train Epoch: [440/2467]	Loss: 0.255323
     Train Epoch: [440/2467]	Loss: 0.061974
     Train Epoch: [440/2467]	Loss: 0.041901
     Train Epoch: [460/2467]	Loss: 0.200476
     Train Epoch: [460/2467]	Loss: 0.133668
         Train Epoch: [460/2467]	Loss: 0.075027
 Train Epoch: [460/2467]	Loss: 0.092508
     Train Epoch: [480/2467]	Loss: 0.099771
     Train Epoch: [480/2467]	Loss: 0.016874
         Train Epoch: [480/2467]	Loss: 0.296890
 Train Epoch: [480/2467]	Loss: 0.233042
     Train Epoch: [500/2467]	Loss: 0.069229    
 Train Epoch: [500/2467]	Loss: 0.084213
     Train Epoch: [500/2467]	Loss: 0.133180
     Train Epoch: [500/2467]	Loss: 0.027249
         Train Epoch: [520/2467]	Loss: 0.072586
 Train Epoch: [520/2467]	Loss: 0.054961
     Train Epoch: [520/2467]	Loss: 0.211584
     Train Epoch: [520/2467]	Loss: 0.122336
     Train Epoch: [540/2467]	Loss: 0.123133
         Train Epoch: [540/2467]	Loss: 0.060868 
Train Epoch: [540/2467]	Loss: 0.050768
     Train Epoch: [540/2467]	Loss: 0.165794
     Train Epoch: [560/2467]	Loss: 0.102461
         Train Epoch: [560/2467]	Loss: 0.120205
 Train Epoch: [560/2467]	Loss: 0.111019
     Train Epoch: [560/2467]	Loss: 0.102485
         Train Epoch: [580/2467]	Loss: 0.207965
 Train Epoch: [580/2467]	Loss: 0.088120
     Train Epoch: [580/2467]	Loss: 0.045939
     Train Epoch: [580/2467]	Loss: 0.093192
         Train Epoch: [600/2467]	Loss: 0.100260
 Train Epoch: [600/2467]	Loss: 0.142013
     Train Epoch: [600/2467]	Loss: 0.093963
     Train Epoch: [600/2467]	Loss: 0.081762
         Train Epoch: [620/2467]	Loss: 0.068980
 Train Epoch: [620/2467]	Loss: 0.021432
     Train Epoch: [620/2467]	Loss: 0.135429
     Train Epoch: [620/2467]	Loss: 0.093121
         Train Epoch: [640/2467]	Loss: 0.051593
 Train Epoch: [640/2467]	Loss: 0.130831
         Train Epoch: [640/2467]	Loss: 0.040272
 Train Epoch: [640/2467]	Loss: 0.104530
     Train Epoch: [660/2467]	Loss: 0.019428
             Train Epoch: [660/2467]	Loss: 0.103775
  Train Epoch: [660/2467]	Loss: 0.033927Train Epoch: [660/2467]	Loss: 0.280115

         Train Epoch: [680/2467]	Loss: 0.030037
     Train Epoch: [680/2467]	Loss: 0.022599
 Train Epoch: [680/2467]	Loss: 0.056810
     Train Epoch: [680/2467]	Loss: 0.098251
     Train Epoch: [700/2467]	Loss: 0.242477    
 Train Epoch: [700/2467]	Loss: 0.263469    
 Train Epoch: [700/2467]	Loss: 0.053763
     Train Epoch: [700/2467]	Loss: 0.028676
         Train Epoch: [720/2467]	Loss: 0.188272 
Train Epoch: [720/2467]	Loss: 0.029065
     Train Epoch: [720/2467]	Loss: 0.070668
     Train Epoch: [720/2467]	Loss: 0.051214
         Train Epoch: [740/2467]	Loss: 0.035307
 Train Epoch: [740/2467]	Loss: 0.087623
     Train Epoch: [740/2467]	Loss: 0.239953    
 Train Epoch: [740/2467]	Loss: 0.057193
     Train Epoch: [760/2467]	Loss: 0.186462
         Train Epoch: [760/2467]	Loss: 0.039285
 Train Epoch: [760/2467]	Loss: 0.299750
     Train Epoch: [760/2467]	Loss: 0.243961
     Train Epoch: [780/2467]	Loss: 0.283454
     Train Epoch: [780/2467]	Loss: 0.234793
          Train Epoch: [780/2467]	Loss: 0.261772Train Epoch: [780/2467]	Loss: 0.112258

     Train Epoch: [800/2467]	Loss: 0.128089
             Train Epoch: [800/2467]	Loss: 0.069669
  Train Epoch: [800/2467]	Loss: 0.139101
Train Epoch: [800/2467]	Loss: 0.048657
     Train Epoch: [820/2467]	Loss: 0.042884
     Train Epoch: [820/2467]	Loss: 0.322305    
     Train Epoch: [820/2467]	Loss: 0.158671
 Train Epoch: [820/2467]	Loss: 0.029762
     Train Epoch: [840/2467]	Loss: 0.287062
     Train Epoch: [840/2467]	Loss: 0.077151
     Train Epoch: [840/2467]	Loss: 0.065517
     Train Epoch: [840/2467]	Loss: 0.037013
                 Train Epoch: [860/2467]	Loss: 0.036761
 Train Epoch: [860/2467]	Loss: 0.178381
 Train Epoch: [860/2467]	Loss: 0.058872
 Train Epoch: [860/2467]	Loss: 0.025096
          Train Epoch: [880/2467]	Loss: 0.089682Train Epoch: [880/2467]	Loss: 0.102357

         Train Epoch: [880/2467]	Loss: 0.120006
 Train Epoch: [880/2467]	Loss: 0.137397
     Train Epoch: [900/2467]	Loss: 0.134506    
     Train Epoch: [900/2467]	Loss: 0.118392
 Train Epoch: [900/2467]	Loss: 0.033898
     Train Epoch: [900/2467]	Loss: 0.127509
         Train Epoch: [920/2467]	Loss: 0.198189
 Train Epoch: [920/2467]	Loss: 0.155956    
      Train Epoch: [920/2467]	Loss: 0.029323Train Epoch: [920/2467]	Loss: 0.187277

     Train Epoch: [940/2467]	Loss: 0.027009
          Train Epoch: [940/2467]	Loss: 0.033621Train Epoch: [940/2467]	Loss: 0.131994

     Train Epoch: [940/2467]	Loss: 0.071418
         Train Epoch: [960/2467]	Loss: 0.183989
 Train Epoch: [960/2467]	Loss: 0.047815    
     Train Epoch: [960/2467]	Loss: 0.046286 
Train Epoch: [960/2467]	Loss: 0.053097
     Train Epoch: [980/2467]	Loss: 0.175939
     Train Epoch: [980/2467]	Loss: 0.037397
         Train Epoch: [980/2467]	Loss: 0.059191
 Train Epoch: [980/2467]	Loss: 0.086756
     Train Epoch: [1000/2467]	Loss: 0.025753
     Train Epoch: [1000/2467]	Loss: 0.178314
     Train Epoch: [1000/2467]	Loss: 0.022282
     Train Epoch: [1000/2467]	Loss: 0.166057
         Train Epoch: [1020/2467]	Loss: 0.105498
 Train Epoch: [1020/2467]	Loss: 0.290457    
     Train Epoch: [1020/2467]	Loss: 0.017108
 Train Epoch: [1020/2467]	Loss: 0.064502
     Train Epoch: [1040/2467]	Loss: 0.090228
     Train Epoch: [1040/2467]	Loss: 0.119788
         Train Epoch: [1040/2467]	Loss: 0.199936
 Train Epoch: [1040/2467]	Loss: 0.074160
     Train Epoch: [1060/2467]	Loss: 0.101441
          Train Epoch: [1060/2467]	Loss: 0.073177Train Epoch: [1060/2467]	Loss: 0.059916

     Train Epoch: [1060/2467]	Loss: 0.308425
     Train Epoch: [1080/2467]	Loss: 0.094314
     Train Epoch: [1080/2467]	Loss: 0.239180
     Train Epoch: [1080/2467]	Loss: 0.169109
     Train Epoch: [1080/2467]	Loss: 0.139447
     Train Epoch: [1100/2467]	Loss: 0.232503
         Train Epoch: [1100/2467]	Loss: 0.128794
 Train Epoch: [1100/2467]	Loss: 0.168295
     Train Epoch: [1100/2467]	Loss: 0.150571
     Train Epoch: [1120/2467]	Loss: 0.158123
         Train Epoch: [1120/2467]	Loss: 0.088489
 Train Epoch: [1120/2467]	Loss: 0.093832
     Train Epoch: [1120/2467]	Loss: 0.106870
     Train Epoch: [1140/2467]	Loss: 0.036214
              Train Epoch: [1140/2467]	Loss: 0.278569Train Epoch: [1140/2467]	Loss: 0.057213

 Train Epoch: [1140/2467]	Loss: 0.217732
     Train Epoch: [1160/2467]	Loss: 0.054126
         Train Epoch: [1160/2467]	Loss: 0.093389
 Train Epoch: [1160/2467]	Loss: 0.187318
     Train Epoch: [1160/2467]	Loss: 0.053444
     Train Epoch: [1180/2467]	Loss: 0.035973
         Train Epoch: [1180/2467]	Loss: 0.050799 
Train Epoch: [1180/2467]	Loss: 0.246763
     Train Epoch: [1180/2467]	Loss: 0.080951
     Train Epoch: [1200/2467]	Loss: 0.049007
     Train Epoch: [1200/2467]	Loss: 0.078706
     Train Epoch: [1200/2467]	Loss: 0.196358
     Train Epoch: [1200/2467]	Loss: 0.257090
     Train Epoch: [1220/2467]	Loss: 0.155207
     Train Epoch: [1220/2467]	Loss: 0.060372
     Train Epoch: [1220/2467]	Loss: 0.275690
     Train Epoch: [1220/2467]	Loss: 0.221701
     Train Epoch: [1240/2467]	Loss: 0.190664    
     Train Epoch: [1240/2467]	Loss: 0.211025
 Train Epoch: [1240/2467]	Loss: 0.109888
     Train Epoch: [1240/2467]	Loss: 0.119964
     Train Epoch: [1260/2467]	Loss: 0.039807
         Train Epoch: [1260/2467]	Loss: 0.032767
 Train Epoch: [1260/2467]	Loss: 0.091333
     Train Epoch: [1260/2467]	Loss: 0.329234
     Train Epoch: [1280/2467]	Loss: 0.083812
     Train Epoch: [1280/2467]	Loss: 0.154236
         Train Epoch: [1280/2467]	Loss: 0.051657
 Train Epoch: [1280/2467]	Loss: 0.358546
     Train Epoch: [1300/2467]	Loss: 0.054808
     Train Epoch: [1300/2467]	Loss: 0.203942
          Train Epoch: [1300/2467]	Loss: 0.123998
Train Epoch: [1300/2467]	Loss: 0.034307
     Train Epoch: [1320/2467]	Loss: 0.035756
     Train Epoch: [1320/2467]	Loss: 0.100785
          Train Epoch: [1320/2467]	Loss: 0.099263
Train Epoch: [1320/2467]	Loss: 0.094577
         Train Epoch: [1340/2467]	Loss: 0.082126
 Train Epoch: [1340/2467]	Loss: 0.162923
     Train Epoch: [1340/2467]	Loss: 0.240608
     Train Epoch: [1340/2467]	Loss: 0.130647
     Train Epoch: [1360/2467]	Loss: 0.027907
     Train Epoch: [1360/2467]	Loss: 0.069943
     Train Epoch: [1360/2467]	Loss: 0.091435
     Train Epoch: [1360/2467]	Loss: 0.036763
     Train Epoch: [1380/2467]	Loss: 0.127616
     Train Epoch: [1380/2467]	Loss: 0.223703
     Train Epoch: [1380/2467]	Loss: 0.179099
     Train Epoch: [1380/2467]	Loss: 0.144327
         Train Epoch: [1400/2467]	Loss: 0.129319
     Train Epoch: [1400/2467]	Loss: 0.049781
 Train Epoch: [1400/2467]	Loss: 0.099016
     Train Epoch: [1400/2467]	Loss: 0.077097
     Train Epoch: [1420/2467]	Loss: 0.234388
     Train Epoch: [1420/2467]	Loss: 0.063014
     Train Epoch: [1420/2467]	Loss: 0.089647
     Train Epoch: [1420/2467]	Loss: 0.127444
     Train Epoch: [1440/2467]	Loss: 0.013530
     Train Epoch: [1440/2467]	Loss: 0.029974
          Train Epoch: [1440/2467]	Loss: 0.060218Train Epoch: [1440/2467]	Loss: 0.040510

     Train Epoch: [1460/2467]	Loss: 0.032264
         Train Epoch: [1460/2467]	Loss: 0.087538
 Train Epoch: [1460/2467]	Loss: 0.076719
     Train Epoch: [1460/2467]	Loss: 0.118704
     Train Epoch: [1480/2467]	Loss: 0.025185
     Train Epoch: [1480/2467]	Loss: 0.012158    
 Train Epoch: [1480/2467]	Loss: 0.136106
     Train Epoch: [1480/2467]	Loss: 0.180725
     Train Epoch: [1500/2467]	Loss: 0.068330
         Train Epoch: [1500/2467]	Loss: 0.012154    
 Train Epoch: [1500/2467]	Loss: 0.124328
 Train Epoch: [1500/2467]	Loss: 0.053534
     Train Epoch: [1520/2467]	Loss: 0.159792
         Train Epoch: [1520/2467]	Loss: 0.118199
     Train Epoch: [1520/2467]	Loss: 0.070767
 Train Epoch: [1520/2467]	Loss: 0.185699
     Train Epoch: [1540/2467]	Loss: 0.082205
         Train Epoch: [1540/2467]	Loss: 0.233508 
Train Epoch: [1540/2467]	Loss: 0.016830    
 Train Epoch: [1540/2467]	Loss: 0.070477
     Train Epoch: [1560/2467]	Loss: 0.056138
         Train Epoch: [1560/2467]	Loss: 0.256115
 Train Epoch: [1560/2467]	Loss: 0.179667
     Train Epoch: [1560/2467]	Loss: 0.333941
         Train Epoch: [1580/2467]	Loss: 0.053753
 Train Epoch: [1580/2467]	Loss: 0.229110    
     Train Epoch: [1580/2467]	Loss: 0.189278 
Train Epoch: [1580/2467]	Loss: 0.073443
     Train Epoch: [1600/2467]	Loss: 0.037330
     Train Epoch: [1600/2467]	Loss: 0.161970
     Train Epoch: [1600/2467]	Loss: 0.187116
     Train Epoch: [1600/2467]	Loss: 0.041765
     Train Epoch: [1620/2467]	Loss: 0.136556
         Train Epoch: [1620/2467]	Loss: 0.035250
 Train Epoch: [1620/2467]	Loss: 0.210673
     Train Epoch: [1620/2467]	Loss: 0.074591
         Train Epoch: [1640/2467]	Loss: 0.142382
 Train Epoch: [1640/2467]	Loss: 0.105287
     Train Epoch: [1640/2467]	Loss: 0.145954
     Train Epoch: [1640/2467]	Loss: 0.042720
          Train Epoch: [1660/2467]	Loss: 0.106611Train Epoch: [1660/2467]	Loss: 0.108568

     Train Epoch: [1660/2467]	Loss: 0.040726
     Train Epoch: [1660/2467]	Loss: 0.135489
         Train Epoch: [1680/2467]	Loss: 0.037784
     Train Epoch: [1680/2467]	Loss: 0.109751
 Train Epoch: [1680/2467]	Loss: 0.101363
     Train Epoch: [1680/2467]	Loss: 0.119084
     Train Epoch: [1700/2467]	Loss: 0.058810
         Train Epoch: [1700/2467]	Loss: 0.147878
 Train Epoch: [1700/2467]	Loss: 0.105334
     Train Epoch: [1700/2467]	Loss: 0.103304
         Train Epoch: [1720/2467]	Loss: 0.076600
 Train Epoch: [1720/2467]	Loss: 0.167227
         Train Epoch: [1720/2467]	Loss: 0.101004 
Train Epoch: [1720/2467]	Loss: 0.042731
     Train Epoch: [1740/2467]	Loss: 0.016569
         Train Epoch: [1740/2467]	Loss: 0.189492
 Train Epoch: [1740/2467]	Loss: 0.022016
     Train Epoch: [1740/2467]	Loss: 0.135854
         Train Epoch: [1760/2467]	Loss: 0.084249
 Train Epoch: [1760/2467]	Loss: 0.124005
     Train Epoch: [1760/2467]	Loss: 0.049393
     Train Epoch: [1760/2467]	Loss: 0.058180
     Train Epoch: [1780/2467]	Loss: 0.098760
     Train Epoch: [1780/2467]	Loss: 0.154137
     Train Epoch: [1780/2467]	Loss: 0.055635
     Train Epoch: [1780/2467]	Loss: 0.121025
     Train Epoch: [1800/2467]	Loss: 0.049904
     Train Epoch: [1800/2467]	Loss: 0.099822
     Train Epoch: [1800/2467]	Loss: 0.212981
     Train Epoch: [1800/2467]	Loss: 0.199814
     Train Epoch: [1820/2467]	Loss: 0.124038    
 Train Epoch: [1820/2467]	Loss: 0.275798
     Train Epoch: [1820/2467]	Loss: 0.102767
     Train Epoch: [1820/2467]	Loss: 0.088094
     Train Epoch: [1840/2467]	Loss: 0.092861
         Train Epoch: [1840/2467]	Loss: 0.039040    
  Train Epoch: [1840/2467]	Loss: 0.116851Train Epoch: [1840/2467]	Loss: 0.218637

         Train Epoch: [1860/2467]	Loss: 0.096647 
Train Epoch: [1860/2467]	Loss: 0.100715
         Train Epoch: [1860/2467]	Loss: 0.109095
 Train Epoch: [1860/2467]	Loss: 0.124212
          Train Epoch: [1880/2467]	Loss: 0.019793Train Epoch: [1880/2467]	Loss: 0.067045

         Train Epoch: [1880/2467]	Loss: 0.045888
 Train Epoch: [1880/2467]	Loss: 0.044587
     Train Epoch: [1900/2467]	Loss: 0.077674
         Train Epoch: [1900/2467]	Loss: 0.191120 
Train Epoch: [1900/2467]	Loss: 0.091839
     Train Epoch: [1900/2467]	Loss: 0.221958
          Train Epoch: [1920/2467]	Loss: 0.096583Train Epoch: [1920/2467]	Loss: 0.141781

     Train Epoch: [1920/2467]	Loss: 0.102595
     Train Epoch: [1920/2467]	Loss: 0.014722
     Train Epoch: [1940/2467]	Loss: 0.047212
         Train Epoch: [1940/2467]	Loss: 0.121516
 Train Epoch: [1940/2467]	Loss: 0.112740
     Train Epoch: [1940/2467]	Loss: 0.267862
     Train Epoch: [1960/2467]	Loss: 0.083806
         Train Epoch: [1960/2467]	Loss: 0.055354
 Train Epoch: [1960/2467]	Loss: 0.078994
     Train Epoch: [1960/2467]	Loss: 0.118577
         Train Epoch: [1980/2467]	Loss: 0.080618
     Train Epoch: [1980/2467]	Loss: 0.119106
 Train Epoch: [1980/2467]	Loss: 0.154303
     Train Epoch: [1980/2467]	Loss: 0.170755
     Train Epoch: [2000/2467]	Loss: 0.087382
             Train Epoch: [2000/2467]	Loss: 0.014799
 Train Epoch: [2000/2467]	Loss: 0.039999 
Train Epoch: [2000/2467]	Loss: 0.107386
     Train Epoch: [2020/2467]	Loss: 0.121943
         Train Epoch: [2020/2467]	Loss: 0.104187
 Train Epoch: [2020/2467]	Loss: 0.137319
     Train Epoch: [2020/2467]	Loss: 0.041361
         Train Epoch: [2040/2467]	Loss: 0.048181
 Train Epoch: [2040/2467]	Loss: 0.107757    
     Train Epoch: [2040/2467]	Loss: 0.294543 
Train Epoch: [2040/2467]	Loss: 0.055701
         Train Epoch: [2060/2467]	Loss: 0.096356
 Train Epoch: [2060/2467]	Loss: 0.051398
         Train Epoch: [2060/2467]	Loss: 0.186287
 Train Epoch: [2060/2467]	Loss: 0.054110
     Train Epoch: [2080/2467]	Loss: 0.028931
          Train Epoch: [2080/2467]	Loss: 0.206626Train Epoch: [2080/2467]	Loss: 0.021042

     Train Epoch: [2080/2467]	Loss: 0.050937
     Train Epoch: [2100/2467]	Loss: 0.076596
          Train Epoch: [2100/2467]	Loss: 0.072702
Train Epoch: [2100/2467]	Loss: 0.028063
     Train Epoch: [2100/2467]	Loss: 0.347550
         Train Epoch: [2120/2467]	Loss: 0.064489    
  Train Epoch: [2120/2467]	Loss: 0.030690Train Epoch: [2120/2467]	Loss: 0.073155

     Train Epoch: [2120/2467]	Loss: 0.154408
     Train Epoch: [2140/2467]	Loss: 0.346834
     Train Epoch: [2140/2467]	Loss: 0.233451    
 Train Epoch: [2140/2467]	Loss: 0.115861
     Train Epoch: [2140/2467]	Loss: 0.175948
     Train Epoch: [2160/2467]	Loss: 0.228706
     Train Epoch: [2160/2467]	Loss: 0.075373
     Train Epoch: [2160/2467]	Loss: 0.043880
     Train Epoch: [2160/2467]	Loss: 0.036151
     Train Epoch: [2180/2467]	Loss: 0.047612
     Train Epoch: [2180/2467]	Loss: 0.206920
          Train Epoch: [2180/2467]	Loss: 0.128633Train Epoch: [2180/2467]	Loss: 0.129317

     Train Epoch: [2200/2467]	Loss: 0.095293
         Train Epoch: [2200/2467]	Loss: 0.070043
 Train Epoch: [2200/2467]	Loss: 0.083369
     Train Epoch: [2200/2467]	Loss: 0.030010
     Train Epoch: [2220/2467]	Loss: 0.048670
         Train Epoch: [2220/2467]	Loss: 0.038910
 Train Epoch: [2220/2467]	Loss: 0.082839
     Train Epoch: [2220/2467]	Loss: 0.077898
         Train Epoch: [2240/2467]	Loss: 0.084556
     Train Epoch: [2240/2467]	Loss: 0.062171    
 Train Epoch: [2240/2467]	Loss: 0.022547
 Train Epoch: [2240/2467]	Loss: 0.026582
     Train Epoch: [2260/2467]	Loss: 0.096587
     Train Epoch: [2260/2467]	Loss: 0.173791
     Train Epoch: [2260/2467]	Loss: 0.170643
     Train Epoch: [2260/2467]	Loss: 0.020842
     Train Epoch: [2280/2467]	Loss: 0.091773
     Train Epoch: [2280/2467]	Loss: 0.085979
         Train Epoch: [2280/2467]	Loss: 0.050032
 Train Epoch: [2280/2467]	Loss: 0.065385
     Train Epoch: [2300/2467]	Loss: 0.209351    
 Train Epoch: [2300/2467]	Loss: 0.047497
          Train Epoch: [2300/2467]	Loss: 0.039242Train Epoch: [2300/2467]	Loss: 0.068612

     Train Epoch: [2320/2467]	Loss: 0.081326
     Train Epoch: [2320/2467]	Loss: 0.106376
     Train Epoch: [2320/2467]	Loss: 0.114111
     Train Epoch: [2320/2467]	Loss: 0.014691
     Train Epoch: [2340/2467]	Loss: 0.067839
          Train Epoch: [2340/2467]	Loss: 0.146460Train Epoch: [2340/2467]	Loss: 0.171062

     Train Epoch: [2340/2467]	Loss: 0.468575
     Train Epoch: [2360/2467]	Loss: 0.121168
          Train Epoch: [2360/2467]	Loss: 0.047923
Train Epoch: [2360/2467]	Loss: 0.246472    
 Train Epoch: [2360/2467]	Loss: 0.108120
         Train Epoch: [2380/2467]	Loss: 0.135298
     Train Epoch: [2380/2467]	Loss: 0.265581
 Train Epoch: [2380/2467]	Loss: 0.096314
     Train Epoch: [2380/2467]	Loss: 0.051352
             Train Epoch: [2400/2467]	Loss: 0.083100
  Train Epoch: [2400/2467]	Loss: 0.051190
Train Epoch: [2400/2467]	Loss: 0.116326
     Train Epoch: [2400/2467]	Loss: 0.092249
         Train Epoch: [2420/2467]	Loss: 0.016127
 Train Epoch: [2420/2467]	Loss: 0.065103
     Train Epoch: [2420/2467]	Loss: 0.189258
     Train Epoch: [2420/2467]	Loss: 0.150182
     Train Epoch: [2440/2467]	Loss: 0.169367
         Train Epoch: [2440/2467]	Loss: 0.086175
     Train Epoch: [2440/2467]	Loss: 0.115718
 Train Epoch: [2440/2467]	Loss: 0.010858
         Train Epoch: [2460/2467]	Loss: 0.108475
 Train Epoch: [2460/2467]	Loss: 0.045776
          Train Epoch: [2460/2467]	Loss: 0.079040
Train Epoch: [2460/2467]	Loss: 0.182122
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 46 epoch =====
     2025-05-11.16-47-51
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 46 epoch =====
     2025-05-11.16-47-51
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 46 epoch =====
     2025-05-11.16-47-52
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
     ===== running 46 epoch =====
     2025-05-11.16-47-52
after set grad
after prog
start loop
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
         Train Epoch: [0/2467]	Loss: 0.212195 
Train Epoch: [0/2467]	Loss: 0.035590
     Train Epoch: [0/2467]	Loss: 0.170227
     Train Epoch: [0/2467]	Loss: 0.156439
     Train Epoch: [20/2467]	Loss: 0.063509
         Train Epoch: [20/2467]	Loss: 0.036229    
  Train Epoch: [20/2467]	Loss: 0.068398Train Epoch: [20/2467]	Loss: 0.243027

         Train Epoch: [40/2467]	Loss: 0.322191
     Train Epoch: [40/2467]	Loss: 0.064656
 Train Epoch: [40/2467]	Loss: 0.100244
     Train Epoch: [40/2467]	Loss: 0.114891
     Train Epoch: [60/2467]	Loss: 0.120537
         Train Epoch: [60/2467]	Loss: 0.025242
 Train Epoch: [60/2467]	Loss: 0.109544
     Train Epoch: [60/2467]	Loss: 0.054093
     Train Epoch: [80/2467]	Loss: 0.171713
         Train Epoch: [80/2467]	Loss: 0.084024
     Train Epoch: [80/2467]	Loss: 0.033080
 Train Epoch: [80/2467]	Loss: 0.098758
     Train Epoch: [100/2467]	Loss: 0.036516
     Train Epoch: [100/2467]	Loss: 0.051524
          Train Epoch: [100/2467]	Loss: 0.117132
Train Epoch: [100/2467]	Loss: 0.102588
              Train Epoch: [120/2467]	Loss: 0.260656Train Epoch: [120/2467]	Loss: 0.036346
 
Train Epoch: [120/2467]	Loss: 0.151094
     Train Epoch: [120/2467]	Loss: 0.068927
     Train Epoch: [140/2467]	Loss: 0.022739    
 Train Epoch: [140/2467]	Loss: 0.066842
     Train Epoch: [140/2467]	Loss: 0.044126
     Train Epoch: [140/2467]	Loss: 0.132818
          Train Epoch: [160/2467]	Loss: 0.049635Train Epoch: [160/2467]	Loss: 0.106808

         Train Epoch: [160/2467]	Loss: 0.141640
 Train Epoch: [160/2467]	Loss: 0.101061
         Train Epoch: [180/2467]	Loss: 0.029920
      Train Epoch: [180/2467]	Loss: 0.136994Train Epoch: [180/2467]	Loss: 0.076171

     Train Epoch: [180/2467]	Loss: 0.076947
     Train Epoch: [200/2467]	Loss: 0.093809
     Train Epoch: [200/2467]	Loss: 0.033051
     Train Epoch: [200/2467]	Loss: 0.057581
     Train Epoch: [200/2467]	Loss: 0.038167
     Train Epoch: [220/2467]	Loss: 0.067074
         Train Epoch: [220/2467]	Loss: 0.170215    
 Train Epoch: [220/2467]	Loss: 0.030170 
Train Epoch: [220/2467]	Loss: 0.117306
         Train Epoch: [240/2467]	Loss: 0.015636
 Train Epoch: [240/2467]	Loss: 0.181017
         Train Epoch: [240/2467]	Loss: 0.026209
 Train Epoch: [240/2467]	Loss: 0.124166
     Train Epoch: [260/2467]	Loss: 0.039595
     Train Epoch: [260/2467]	Loss: 0.031900    
 Train Epoch: [260/2467]	Loss: 0.041639
     Train Epoch: [260/2467]	Loss: 0.018981
         Train Epoch: [280/2467]	Loss: 0.067054
 Train Epoch: [280/2467]	Loss: 0.101609
     Train Epoch: [280/2467]	Loss: 0.055668
     Train Epoch: [280/2467]	Loss: 0.073991
         Train Epoch: [300/2467]	Loss: 0.129514
 Train Epoch: [300/2467]	Loss: 0.024116
     Train Epoch: [300/2467]	Loss: 0.059524
     Train Epoch: [300/2467]	Loss: 0.286165
     Train Epoch: [320/2467]	Loss: 0.100460    
 Train Epoch: [320/2467]	Loss: 0.214380
         Train Epoch: [320/2467]	Loss: 0.142457
 Train Epoch: [320/2467]	Loss: 0.333143
         Train Epoch: [340/2467]	Loss: 0.036196
         Train Epoch: [340/2467]	Loss: 0.035232
 Train Epoch: [340/2467]	Loss: 0.138096 
Train Epoch: [340/2467]	Loss: 0.012736
     Train Epoch: [360/2467]	Loss: 0.171521
         Train Epoch: [360/2467]	Loss: 0.030113
 Train Epoch: [360/2467]	Loss: 0.022045
     Train Epoch: [360/2467]	Loss: 0.090991
          Train Epoch: [380/2467]	Loss: 0.153066Train Epoch: [380/2467]	Loss: 0.039463

          Train Epoch: [380/2467]	Loss: 0.187990Train Epoch: [380/2467]	Loss: 0.070801

     Train Epoch: [400/2467]	Loss: 0.044345
         Train Epoch: [400/2467]	Loss: 0.020808
 Train Epoch: [400/2467]	Loss: 0.072921
     Train Epoch: [400/2467]	Loss: 0.136883
     Train Epoch: [420/2467]	Loss: 0.187143
         Train Epoch: [420/2467]	Loss: 0.245254
     Train Epoch: [420/2467]	Loss: 0.057957
 Train Epoch: [420/2467]	Loss: 0.088313
     Train Epoch: [440/2467]	Loss: 0.097507
     Train Epoch: [440/2467]	Loss: 0.274616
     Train Epoch: [440/2467]	Loss: 0.057820    
 Train Epoch: [440/2467]	Loss: 0.035059
     Train Epoch: [460/2467]	Loss: 0.182752
         Train Epoch: [460/2467]	Loss: 0.127996
 Train Epoch: [460/2467]	Loss: 0.062915
     Train Epoch: [460/2467]	Loss: 0.080524
     Train Epoch: [480/2467]	Loss: 0.098415
         Train Epoch: [480/2467]	Loss: 0.021347
 Train Epoch: [480/2467]	Loss: 0.275013
     Train Epoch: [480/2467]	Loss: 0.216896
     Train Epoch: [500/2467]	Loss: 0.031942
     Train Epoch: [500/2467]	Loss: 0.077550
         Train Epoch: [500/2467]	Loss: 0.058198
 Train Epoch: [500/2467]	Loss: 0.128677
     Train Epoch: [520/2467]	Loss: 0.048894
          Train Epoch: [520/2467]	Loss: 0.072685
Train Epoch: [520/2467]	Loss: 0.196602
     Train Epoch: [520/2467]	Loss: 0.045953
     Train Epoch: [540/2467]	Loss: 0.126672    
 Train Epoch: [540/2467]	Loss: 0.160580
     Train Epoch: [540/2467]	Loss: 0.072095
     Train Epoch: [540/2467]	Loss: 0.044140
         Train Epoch: [560/2467]	Loss: 0.102656    
 Train Epoch: [560/2467]	Loss: 0.106011
 Train Epoch: [560/2467]	Loss: 0.099998
     Train Epoch: [560/2467]	Loss: 0.112405
     Train Epoch: [580/2467]	Loss: 0.193465
             Train Epoch: [580/2467]	Loss: 0.081606 
Train Epoch: [580/2467]	Loss: 0.050275 
Train Epoch: [580/2467]	Loss: 0.091494
         Train Epoch: [600/2467]	Loss: 0.146569 
Train Epoch: [600/2467]	Loss: 0.103327
     Train Epoch: [600/2467]	Loss: 0.100281
     Train Epoch: [600/2467]	Loss: 0.069718
             Train Epoch: [620/2467]	Loss: 0.097669
 Train Epoch: [620/2467]	Loss: 0.119632 
Train Epoch: [620/2467]	Loss: 0.024882
     Train Epoch: [620/2467]	Loss: 0.091165
             Train Epoch: [640/2467]	Loss: 0.128420
 Train Epoch: [640/2467]	Loss: 0.081816
 Train Epoch: [640/2467]	Loss: 0.031911
     Train Epoch: [640/2467]	Loss: 0.057284
     Train Epoch: [660/2467]	Loss: 0.018952
          Train Epoch: [660/2467]	Loss: 0.271442Train Epoch: [660/2467]	Loss: 0.049078

     Train Epoch: [660/2467]	Loss: 0.090649
     Train Epoch: [680/2467]	Loss: 0.077207
     Train Epoch: [680/2467]	Loss: 0.027188
     Train Epoch: [680/2467]	Loss: 0.019812
     Train Epoch: [680/2467]	Loss: 0.054866
     Train Epoch: [700/2467]	Loss: 0.027428
     Train Epoch: [700/2467]	Loss: 0.236764
         Train Epoch: [700/2467]	Loss: 0.053244
 Train Epoch: [700/2467]	Loss: 0.244259
         Train Epoch: [720/2467]	Loss: 0.138220    
 Train Epoch: [720/2467]	Loss: 0.050560
 Train Epoch: [720/2467]	Loss: 0.072796
     Train Epoch: [720/2467]	Loss: 0.011858
         Train Epoch: [740/2467]	Loss: 0.034228
 Train Epoch: [740/2467]	Loss: 0.056469
         Train Epoch: [740/2467]	Loss: 0.221118
 Train Epoch: [740/2467]	Loss: 0.059815
     Train Epoch: [760/2467]	Loss: 0.034600
         Train Epoch: [760/2467]	Loss: 0.284643 
Train Epoch: [760/2467]	Loss: 0.183430
     Train Epoch: [760/2467]	Loss: 0.229145
     Train Epoch: [780/2467]	Loss: 0.245809
          Train Epoch: [780/2467]	Loss: 0.240692Train Epoch: [780/2467]	Loss: 0.232898

     Train Epoch: [780/2467]	Loss: 0.083065
         Train Epoch: [800/2467]	Loss: 0.132350
         Train Epoch: [800/2467]	Loss: 0.119331
  Train Epoch: [800/2467]	Loss: 0.059951Train Epoch: [800/2467]	Loss: 0.075661

     Train Epoch: [820/2467]	Loss: 0.310971
     Train Epoch: [820/2467]	Loss: 0.139551
     Train Epoch: [820/2467]	Loss: 0.041433
     Train Epoch: [820/2467]	Loss: 0.038757
     Train Epoch: [840/2467]	Loss: 0.279061
     Train Epoch: [840/2467]	Loss: 0.040638
     Train Epoch: [840/2467]	Loss: 0.063335
     Train Epoch: [840/2467]	Loss: 0.073564
     Train Epoch: [860/2467]	Loss: 0.179758    
         Train Epoch: [860/2467]	Loss: 0.061162
  Train Epoch: [860/2467]	Loss: 0.039853
Train Epoch: [860/2467]	Loss: 0.023674
          Train Epoch: [880/2467]	Loss: 0.098719Train Epoch: [880/2467]	Loss: 0.091889

     Train Epoch: [880/2467]	Loss: 0.098597
     Train Epoch: [880/2467]	Loss: 0.132423
     Train Epoch: [900/2467]	Loss: 0.126904
         Train Epoch: [900/2467]	Loss: 0.110042
 Train Epoch: [900/2467]	Loss: 0.025871
     Train Epoch: [900/2467]	Loss: 0.127481
         Train Epoch: [920/2467]	Loss: 0.192722 
Train Epoch: [920/2467]	Loss: 0.165546
         Train Epoch: [920/2467]	Loss: 0.029010
 Train Epoch: [920/2467]	Loss: 0.246300
     Train Epoch: [940/2467]	Loss: 0.024059
     Train Epoch: [940/2467]	Loss: 0.032888
         Train Epoch: [940/2467]	Loss: 0.054373
 Train Epoch: [940/2467]	Loss: 0.147201
         Train Epoch: [960/2467]	Loss: 0.183060
 Train Epoch: [960/2467]	Loss: 0.048224
     Train Epoch: [960/2467]	Loss: 0.040127
     Train Epoch: [960/2467]	Loss: 0.051991
     Train Epoch: [980/2467]	Loss: 0.048648
         Train Epoch: [980/2467]	Loss: 0.163985
 Train Epoch: [980/2467]	Loss: 0.082291
     Train Epoch: [980/2467]	Loss: 0.059451
         Train Epoch: [1000/2467]	Loss: 0.181315
 Train Epoch: [1000/2467]	Loss: 0.020743
     Train Epoch: [1000/2467]	Loss: 0.180846
     Train Epoch: [1000/2467]	Loss: 0.025181
         Train Epoch: [1020/2467]	Loss: 0.103925 
Train Epoch: [1020/2467]	Loss: 0.292792
     Train Epoch: [1020/2467]	Loss: 0.075972
     Train Epoch: [1020/2467]	Loss: 0.012666
     Train Epoch: [1040/2467]	Loss: 0.093161
     Train Epoch: [1040/2467]	Loss: 0.109725
     Train Epoch: [1040/2467]	Loss: 0.181032
     Train Epoch: [1040/2467]	Loss: 0.066429
     Train Epoch: [1060/2467]	Loss: 0.097743
     Train Epoch: [1060/2467]	Loss: 0.075680
     Train Epoch: [1060/2467]	Loss: 0.068087
     Train Epoch: [1060/2467]	Loss: 0.297237
     Train Epoch: [1080/2467]	Loss: 0.088200
     Train Epoch: [1080/2467]	Loss: 0.163967    
     Train Epoch: [1080/2467]	Loss: 0.123033
 Train Epoch: [1080/2467]	Loss: 0.164035
         Train Epoch: [1100/2467]	Loss: 0.234729
 Train Epoch: [1100/2467]	Loss: 0.140340
         Train Epoch: [1100/2467]	Loss: 0.122604 
Train Epoch: [1100/2467]	Loss: 0.159341
     Train Epoch: [1120/2467]	Loss: 0.184991    
     Train Epoch: [1120/2467]	Loss: 0.056307 
Train Epoch: [1120/2467]	Loss: 0.090630
     Train Epoch: [1120/2467]	Loss: 0.111514
         Train Epoch: [1140/2467]	Loss: 0.270115
 Train Epoch: [1140/2467]	Loss: 0.039895
         Train Epoch: [1140/2467]	Loss: 0.228575
 Train Epoch: [1140/2467]	Loss: 0.055217
         Train Epoch: [1160/2467]	Loss: 0.064725
 Train Epoch: [1160/2467]	Loss: 0.055156    
     Train Epoch: [1160/2467]	Loss: 0.082066
 Train Epoch: [1160/2467]	Loss: 0.187406
          Train Epoch: [1180/2467]	Loss: 0.089165Train Epoch: [1180/2467]	Loss: 0.091117

     Train Epoch: [1180/2467]	Loss: 0.135060
     Train Epoch: [1180/2467]	Loss: 0.039242
     Train Epoch: [1200/2467]	Loss: 0.043812
         Train Epoch: [1200/2467]	Loss: 0.064728
     Train Epoch: [1200/2467]	Loss: 0.257505
 Train Epoch: [1200/2467]	Loss: 0.189481
     Train Epoch: [1220/2467]	Loss: 0.142058
     Train Epoch: [1220/2467]	Loss: 0.066857
     Train Epoch: [1220/2467]	Loss: 0.212557
     Train Epoch: [1220/2467]	Loss: 0.304939
         Train Epoch: [1240/2467]	Loss: 0.210281
     Train Epoch: [1240/2467]	Loss: 0.212422
 Train Epoch: [1240/2467]	Loss: 0.098707    
 Train Epoch: [1240/2467]	Loss: 0.107090
         Train Epoch: [1260/2467]	Loss: 0.035151    
     Train Epoch: [1260/2467]	Loss: 0.038830
  Train Epoch: [1260/2467]	Loss: 0.090142Train Epoch: [1260/2467]	Loss: 0.314775

     Train Epoch: [1280/2467]	Loss: 0.065616
         Train Epoch: [1280/2467]	Loss: 0.128055
     Train Epoch: [1280/2467]	Loss: 0.047431
 Train Epoch: [1280/2467]	Loss: 0.360511
         Train Epoch: [1300/2467]	Loss: 0.057174
 Train Epoch: [1300/2467]	Loss: 0.214271
          Train Epoch: [1300/2467]	Loss: 0.029895
Train Epoch: [1300/2467]	Loss: 0.112160
     Train Epoch: [1320/2467]	Loss: 0.038529
     Train Epoch: [1320/2467]	Loss: 0.092877
     Train Epoch: [1320/2467]	Loss: 0.086656
     Train Epoch: [1320/2467]	Loss: 0.090330
Traceback (most recent call last):
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 542, in <module>
    main(args)
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 340, in main
    loss, data_inds, batch_losses, train_time = train_one_epoch(
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/scripts/train_para.py", line 103, in train_one_epoch
    for idx, (inds, (input_1, input_2, targets, caption)) in enumerate(data_loader):
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 628, in __next__
    data = self._next_data()
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1333, in _next_data
    return self._process_data(data)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1359, in _process_data
    data.reraise()
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/_utils.py", line 543, in reraise
    raise exception
FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py", line 302, in _worker_loop
    data = fetcher.fetch(index)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 58, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 58, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/./torch_utils.py", line 110, in __getitem__
    t0, t1, gt, caption = self.dataset[idx]
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/./datasets/combined.py", line 24, in __getitem__
    sample = dataset[local_idx]
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/./datasets/pscd.py", line 200, in __getitem__
    t0_image, t1_image, t0_mask, t1_mask = super().__getitem__(id_image)
  File "/scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/./datasets/pscd.py", line 114, in __getitem__
    t0_image = Image.open(t0).convert("RGB")
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/PIL/Image.py", line 3465, in open
    fp = builtins.open(filename, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '/vast/ds5725/cd_datasets/PSCD/t0/00000358.png'

[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /scratch/ds5725/alvpr/Robust-Scene-Change-Detection/src/output/2025-05-11.00-02-49/wandb/offline-run-20250510_200249-693ddn82[0m
[1;34mwandb[0m: Find logs at: [1;35moutput/2025-05-11.00-02-49/wandb/offline-run-20250510_200249-693ddn82/logs[0m
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1327619 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1327620 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1327621 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 3 (pid: 1327622) of binary: /ext3/miniconda3/envs/rscd/bin/python
Traceback (most recent call last):
  File "/ext3/miniconda3/envs/rscd/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/ext3/miniconda3/envs/rscd/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
src/scripts/train_para.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-11_12:58:54
  host      : gr035.hpc.nyu.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 1327622)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
