# Annotation
![Annotation Pipeline](annot_pipeline.png)

## Installation

Install PyTorch environment first. We use `python=3.10`, as well as `torch >= 2.3.1`, `torchvision>=0.18.1` and `cuda-12.1` in our environment to run this demo. Please follow the instructions [here](https://pytorch.org/get-started/locally/) to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended. You can easily install the latest version of PyTorch as follows:

```bash
pip3 install torch torchvision torchaudio
```

Since we need the CUDA compilation environment to compile the `Deformable Attention` operator used in Grounding DINO, we need to check whether the CUDA environment variables have been set correctly (which you can refer to [Grounding DINO Installation](https://github.com/IDEA-Research/GroundingDINO?tab=readme-ov-file#hammer_and_wrench-install) for more details). You can set the environment variable manually as follows if you want to build a local GPU environment for Grounding DINO to run Grounded SAM 2:

```bash
export CUDA_HOME=/path/to/cuda-12.1/
```

To install the set of library package, run the following command:

```shell
pip install -r requirements.txt
```

Install `Segment Anything 2`:

```bash
cd Grounded-SAM-2
pip install --no-build-isolation -e grounding_dino
```
Install `Grounding DINO`:

```bash
pip install --no-build-isolation -e grounding_dino
```

## Visual Foundation Model

Run gpt.py to get test.txt with the description of objects appearing in only db_1.jpg or q_1.jpg.
```bash
pip install --no-build-isolation -e grounding_dino
```


Modify the variable image_pairs to store more paths of image pairs as tuples.

## Zero-shot object detection
Install Grounded-SAM following the README.md in Grounded-SAM-2 repo.

Modify the source_txt_file on line 34 of grounded_sam2_hf_model_demo.py for the path of the txt file generated by gpt.py.
dcfsdvdsnklfvd
Modify the path on line 64 of grounded_sam2_hf_model_demo.py for the paths of image pairs.

Run grounded_sam2_hf_model_demo.py to get the segmented mask as the annotation.