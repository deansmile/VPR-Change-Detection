import os

os.environ['OPENBLAS_NUM_THREADS'] = '1'
os.environ['OMP_NUM_THREADS'] = '1' 
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['NUMEXPR_NUM_THREADS'] = '1'

import torch
import torch.nn as nn
import torch.nn.functional as F
import re
from typing import List, Tuple, Optional

class DINOv2ToGroundingAdapter:
    """
    DINOv2 -> GroundingDINO ç‰¹å¾å¢å¼ºé€‚é…å™¨
    ä½¿ç”¨GroundingDINOçš„å®é™…APIæ¥å¢å¼ºDINOv2ç‰¹å¾
    """
    def __init__(self, grounding_dino_model, device='cuda'):
        self.grounding_dino = grounding_dino_model
        self.device = device
        
        # ç‰¹å¾ç»´åº¦é€‚é…å±‚
        self.feature_adapter = nn.Sequential(
            nn.Conv2d(768, 256, kernel_size=1, bias=False),  # DINOv2 -> GroundingDINO backbone dim
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 768, kernel_size=1, bias=False),  # æ¢å¤åˆ°DINOv2ç»´åº¦
            nn.BatchNorm2d(768)
        ).to(device)
        
        # æ³¨æ„åŠ›èåˆæ¨¡å— - ç®€åŒ–ç‰ˆæœ¬
        self.attention_fusion = nn.MultiheadAttention(
            embed_dim=768, 
            num_heads=8, 
            batch_first=True,
            dropout=0.1
        ).to(device)
        
        # ç®€å•çš„ç‰¹å¾èåˆå±‚ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
        self.simple_fusion = nn.Sequential(
            nn.Conv2d(768 * 2, 768, kernel_size=1),  # è¿æ¥åçš„ç‰¹å¾èåˆ
            nn.BatchNorm2d(768),
            nn.ReLU(inplace=True),
            nn.Conv2d(768, 768, kernel_size=3, padding=1),
            nn.BatchNorm2d(768)
        ).to(device)
        
        # ç‰¹å¾å¢å¼ºç½‘ç»œ
        self.feature_enhancer = nn.Sequential(
            nn.Conv2d(768, 768, kernel_size=3, padding=1, groups=768),  # Depthwise conv
            nn.BatchNorm2d(768),
            nn.ReLU(inplace=True),
            nn.Conv2d(768, 768, kernel_size=1),  # Pointwise conv
            nn.BatchNorm2d(768)
        ).to(device)
        
        print(f"âœ“ DINOv2-GroundingDINO adapter ready on {device}")
    
    def improve_dino_feature(self, dinov2_features: torch.Tensor, captions: List[str]) -> torch.Tensor:
        """
        ä½¿ç”¨GroundingDINOå¢å¼ºDINOv2ç‰¹å¾
        
        Args:
            dinov2_features: DINOv2ç‰¹å¾ [B, 768, H, W]
            captions: æ–‡æœ¬æè¿°åˆ—è¡¨
            
        Returns:
            enhanced_features: å¢å¼ºåçš„ç‰¹å¾ [B, 768, H, W]
        """
        try:
            B, C, H, W = dinov2_features.shape
            print(f"Input DINOv2 features: {dinov2_features.shape}")
            
            if C != 768:
                print(f"âš  Expected 768 channels, got {C}")
                return dinov2_features
            
            # 1. æ¸…ç†captions
            processed_captions = self._clean_captions(captions)
            print(f"Processed captions: {processed_captions}")
            
            # 2. ä½¿ç”¨GroundingDINOçš„æ–‡æœ¬ç¼–ç å™¨è·å–æ–‡æœ¬ç‰¹å¾
            text_features = self._encode_text(processed_captions)
            print(f"Text features shape: {text_features.shape}")
            
            # 3. ç‰¹å¾é€‚é…å’Œå¢å¼º
            adapted_features = self._adapt_features(dinov2_features)
            print(f"Adapted features shape: {adapted_features.shape}")
            
            # 4. æ–‡æœ¬-è§†è§‰ç‰¹å¾èåˆ
            enhanced_features = self._fuse_text_visual_features(
                adapted_features, text_features
            )
            print(f"Enhanced features shape: {enhanced_features.shape}")
            
            # 5. æ®‹å·®è¿æ¥
            final_features = dinov2_features + 0.1 * enhanced_features  # ä½¿ç”¨è¾ƒå°çš„æƒé‡
            
            print(f"âœ“ Final enhanced features: {final_features.shape}")
            return final_features
            
        except Exception as e:
            print(f"âš  GroundingDINO adapter failed: {e}")
            print(f"  Error type: {type(e).__name__}")
            import traceback
            traceback.print_exc()  # æ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆ
            print("  Falling back to original features")
            return dinov2_features
    
    # def _encode_text(self, captions: List[str]) -> torch.Tensor:
    #     """ä½¿ç”¨GroundingDINOçš„æ–‡æœ¬ç¼–ç å™¨ç¼–ç æ–‡æœ¬"""
    #     try:
    #         # å¤„ç†æ–‡æœ¬è¾“å…¥
    #         text_input = " . ".join(captions)  # åˆå¹¶æ‰€æœ‰captions
            
    #         # ç›´æ¥ä½¿ç”¨GroundingDINOçš„tokenizer
    #         tokenized = self.grounding_dino.tokenizer(
    #             text_input,
    #             padding="max_length",
    #             max_length=256,
    #             truncation=True,
    #             return_tensors="pt"
    #         )

    #         tokenized = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in tokenized.items()}
            
    #         # ä½¿ç”¨GroundingDINOçš„æ–‡æœ¬ç¼–ç å™¨
    #         with torch.no_grad():
    #             # æ£€æŸ¥bertå±æ€§æ˜¯å¦å­˜åœ¨
    #             if hasattr(self.grounding_dino, 'bert'):
    #                 text_features = self.grounding_dino.bert(
    #                     tokenized['input_ids'],
    #                     attention_mask=tokenized['attention_mask']
    #                 )[0]  # [1, seq_len, 768]
    #             elif hasattr(self.grounding_dino, 'text_encoder'):
    #                 text_features = self.grounding_dino.text_encoder(
    #                     tokenized['input_ids'],
    #                     attention_mask=tokenized['attention_mask']
    #                 )[0]  # [1, seq_len, 768]
    #             else:
    #                 # å¦‚æœæ‰¾ä¸åˆ°æ–‡æœ¬ç¼–ç å™¨ï¼Œåˆ›å»ºdummyç‰¹å¾
    #                 print("No text encoder found, using dummy features")
    #                 return torch.zeros(1, 768, device=self.device) 
    #         # ä½¿ç”¨æ³¨æ„åŠ›æ©ç è¿›è¡ŒåŠ æƒå¹³å‡æ± åŒ–
    #         mask = tokenized.attention_mask.unsqueeze(-1).float()  # [1, seq_len, 1]
    #         masked_features = text_features * mask  # [1, seq_len, 768]
    #         text_embedding = masked_features.sum(dim=1) / mask.sum(dim=1)  # [1, 768]
            
    #         return text_embedding
    def _encode_text(self, captions: List[str]) -> torch.Tensor:
        try:
            from transformers import AutoTokenizer, AutoModel
            
            text_input = " . ".join(captions)
            
            # åˆ›å»ºç‹¬ç«‹çš„tokenizerå’Œæ¨¡å‹ï¼Œå…¨éƒ¨åœ¨GPUä¸Š
            tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
            text_encoder = AutoModel.from_pretrained("bert-base-uncased").to(self.device)
            
            # tokenizeå¹¶å¼ºåˆ¶åˆ°GPU
            inputs = tokenizer(text_input, return_tensors="pt", padding=True, truncation=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                text_features = text_encoder(**inputs).last_hidden_state
                text_embedding = text_features.mean(dim=1)  # [1, 768]
            
            # ç¡®ä¿è¾“å‡ºåœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            return text_embedding.to(self.device)
            
        except Exception as e:
            print(f"Text encoding failed: {e}, using dummy features")
            return torch.zeros(1, 768, device=self.device)
            
                
        
    def _adapt_features(self, features: torch.Tensor) -> torch.Tensor:
        """ç‰¹å¾é€‚é…å’Œé¢„å¤„ç†"""
        # é€šè¿‡é€‚é…å±‚
        adapted = self.feature_adapter(features)
        return adapted
    
    def _fuse_text_visual_features(self, visual_features, text_features):
        B, C, H, W = visual_features.shape
        
        if text_features.size(0) == 1 and B > 1:
            text_features = text_features.expand(B, -1)
        
        text_broadcast = text_features.view(B, C, 1, 1).expand(-1, -1, H, W)
        concatenated = torch.cat([visual_features, text_broadcast], dim=1)
        fused_features = self.simple_fusion(concatenated)
        
        enhanced = self.feature_enhancer(fused_features)
        return enhanced
    def _clean_captions(self, captions: List[str]) -> List[str]:
        """æ¸…ç†å’Œæ ‡å‡†åŒ–captions"""
        processed = []
        for caption in captions:
            clean = str(caption).strip()
            # ç§»é™¤æ•°å­—ç¼–å·
            clean = re.sub(r'^\d+\.\s*', '', clean)
            # ç§»é™¤å¤šä½™ç©ºæ ¼
            clean = re.sub(r'\s+', ' ', clean)
            # ç¡®ä¿ä¸ä¸ºç©º
            if len(clean) < 2:
                clean = "object"
            # é™åˆ¶é•¿åº¦
            if len(clean) > 100:
                clean = clean[:100]
            # ç¡®ä¿ä»¥å¥å·ç»“å°¾
            if not clean.endswith('.'):
                clean += '.'
            processed.append(clean)
        return processed

    def get_grounding_predictions(self, image: torch.Tensor, 
                                  caption: str, 
                                  box_threshold: float = 0.35,
                                  text_threshold: float = 0.25) -> Tuple[torch.Tensor, torch.Tensor, List[str]]:
        """
        ä½¿ç”¨GroundingDINOè¿›è¡Œç›®æ ‡æ£€æµ‹
        
        Args:
            image: è¾“å…¥å›¾åƒ [3, H, W] æˆ– [B, 3, H, W]
            caption: æ–‡æœ¬æè¿°
            box_threshold: æ¡†ç½®ä¿¡åº¦é˜ˆå€¼
            text_threshold: æ–‡æœ¬ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            boxes: æ£€æµ‹æ¡† [N, 4]
            logits: ç½®ä¿¡åº¦åˆ†æ•° [N]
            phrases: å¯¹åº”çš„æ–‡æœ¬çŸ­è¯­
        """
        try:
            from groundingdino.util.inference import predict
            
            # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®
            if image.dim() == 4:
                image = image.squeeze(0)  # ç§»é™¤batchç»´åº¦
            
            # è°ƒç”¨GroundingDINOé¢„æµ‹
            boxes, logits, phrases = predict(
                model=self.grounding_dino,
                image=image,
                caption=caption,
                box_threshold=box_threshold,
                text_threshold=text_threshold,
                device=self.device
            )
            
            return boxes, logits, phrases
            
        except Exception as e:
            print(f"GroundingDINO prediction failed: {e}")
            # è¿”å›ç©ºç»“æœ
            return torch.empty(0, 4), torch.empty(0), []

    # # åŠ è½½å‡½æ•°ä¿æŒä¸å˜ï¼Œä½†ä½¿ç”¨æ–°çš„é€‚é…å™¨
    # def load_grounding_dino_with_adapter(device='cuda'):
    #     """
    #     åŠ è½½å¸¦é€‚é…å™¨çš„GroundingDINO
    #     """
    #     try:
    #         from groundingdino.util.inference import load_model
            
    #         # åŠ è½½GroundingDINO
    #         grounding_model = load_model(
    #             "/scratch/zl4701/rscd/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py", 
    #             "/scratch/zl4701/rscd/GroundingDINO/weights/groundingdino_swint_ogc.pth"
    #         )
            
    #         # åˆ›å»ºé€‚é…å™¨
    #         adapter = DINOv2ToGroundingAdapter(grounding_model, device)
            
    #         print("âœ“ GroundingDINO with DINOv2 adapter loaded successfully")
    #         return adapter
            
    #     except Exception as e:
    #         print(f"âœ— Failed to load GroundingDINO adapter: {e}")
    #         return None

def load_grounding_dino_with_adapter(device='cuda'):
    """ä½¿ç”¨HuggingFace GroundingDINO"""
    try:
        from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
        
        print("ğŸ“¥ Loading HuggingFace GroundingDINO...")
        processor = AutoProcessor.from_pretrained("IDEA-Research/grounding-dino-base")
        grounding_model = AutoModelForZeroShotObjectDetection.from_pretrained(
            "IDEA-Research/grounding-dino-base"
        ).to(device)
        
        adapter = DINOv2ToGroundingAdapter(grounding_model, device)
        print("âœ“ HuggingFace GroundingDINO adapter loaded successfully")
        return adapter
        
    except Exception as e:
        print(f"âœ— Failed to load HuggingFace GroundingDINO: {e}")
        print("  Run: pip install transformers")
        return None

# å¢å¼ºçš„æµ‹è¯•å‡½æ•°
def test_adapter():
    """æµ‹è¯•é€‚é…å™¨åŠŸèƒ½"""
    print("=== Testing DINOv2-GroundingDINO Adapter ===")
    
    adapter = load_grounding_dino_with_adapter('cuda')
    
    if adapter is None:
        return False
    
    # æµ‹è¯•1: ç‰¹å¾å¢å¼º
    print("\n--- Test 1: Feature Enhancement ---")
    test_features = torch.randn(2, 768, 36, 36)  # æ‰¹é‡DINOv2ç‰¹å¾
    test_captions = ["a red car", "a blue truck"]
    
    result = adapter.improve_dino_feature(test_features, test_captions)
    
    print(f"Input shape: {test_features.shape}")
    print(f"Output shape: {result.shape}")
    print(f"âœ“ Feature enhancement test {'passed' if result.shape == test_features.shape else 'failed'}")
    
    # æµ‹è¯•2: æ–‡æœ¬ç¼–ç 
    print("\n--- Test 2: Text Encoding ---")
    text_features = adapter._encode_text(test_captions)
    print(f"Text features shape: {text_features.shape}")
    print(f"âœ“ Text encoding test {'passed' if text_features.shape[1] == 768 else 'failed'}")
    
    # æµ‹è¯•3: ç›®æ ‡æ£€æµ‹æ¥å£ (éœ€è¦å®é™…å›¾åƒ)
    print("\n--- Test 3: Detection Interface ---")
    dummy_image = torch.randn(3, 224, 224)
    boxes, logits, phrases = adapter.get_grounding_predictions(
        dummy_image, 
        "a car . a truck"
    )
    print(f"Detection boxes: {boxes.shape}")
    print(f"Detection logits: {logits.shape}")
    print(f"Detection phrases: {len(phrases)}")
    
    print("\nâœ“ All adapter tests completed!")
    return True

if __name__ == "__main__":
    test_adapter()
